<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<title>
  
    大数据理论知识合集
  
</title>

<meta name="description" content="对于大数据理论方面知识进行搜集整理">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据理论知识合集">
<meta property="og:url" content="http://yoursite.com/2019/11/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%90%88%E9%9B%86/index.html">
<meta property="og:site_name" content="BlackC">
<meta property="og:description" content="对于大数据理论方面知识进行搜集整理">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-11-29T02:05:37.000Z">
<meta property="article:modified_time" content="2021-03-11T09:15:17.085Z">
<meta property="article:author" content="X&amp;Z">
<meta property="article:tag" content="interview">
<meta name="twitter:card" content="summary">


  <link rel="alternative" href="/atom.xml" title="BlackC" type="application/atom+xml">



  <link rel="icon" href="/images/favicon.ico">



<link rel="stylesheet" href="/perfect-scrollbar/css/perfect-scrollbar.min.css">


<link rel="stylesheet" href="/styles/main.css">







<meta name="generator" content="Hexo 5.4.0"></head>
<body
  
    class="monochrome"
  
  >
  <div class="mobile-header">
  <button class="sidebar-toggle" type="button">
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
  </button>
  <a class="title" href="/">BlackC</a>
</div>

  <div class="main-container">
    <div class="sidebar">
  <div class="header">
    <h1 class="title"><a href="/">BlackC</a></h1>
    
    <div class="info">
      <div class="content">
        
        
          <div class="author">X&amp;Z</div>
        
      </div>
      
        <div class="avatar">
          
            <a href="/about"><img src="/images/avatar.jpg"></a>
          
        </div>
      
    </div>
  </div>
  <div class="body">
    
      
        <ul class="nav">
          
            
              <li class="category-list-container">
                <a href="javascript:;">分类</a>
                <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><span class="category-list-count">169</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E5%85%B7/">工具</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%90%AD%E5%BB%BA/">搭建</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%99%E7%A8%8B/">教程</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E8%AE%B0/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%B3%BB%E7%BB%9F/">系统</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91/">编译</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%BE%91%E5%99%A8/">编辑器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BF%90%E7%BB%B4/">运维</a><span class="category-list-count">10</span></li></ul>
              </li>
            
          
            
              <li class="tag-list-container">
                <a href="javascript:;">标签</a>
                <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/" rel="tag">algorithm</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/alibaba/" rel="tag">alibaba</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cas/" rel="tag">cas</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cdh/" rel="tag">cdh</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/" rel="tag">docker</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/doris/" rel="tag">doris</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/edit/" rel="tag">edit</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/elk/" rel="tag">elk</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flink/" rel="tag">flink</a><span class="tag-list-count">96</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/grafana/" rel="tag">grafana</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/greenplum/" rel="tag">greenplum</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/" rel="tag">hbase</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/" rel="tag">hive</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/interview/" rel="tag">interview</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/" rel="tag">kafka</a><span class="tag-list-count">27</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kylin/" rel="tag">kylin</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/learn/" rel="tag">learn</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/maven/" rel="tag">maven</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/miscellany/" rel="tag">miscellany</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mr/" rel="tag">mr</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">mysql</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/neo4j/" rel="tag">neo4j</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/os/" rel="tag">os</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/oss/" rel="tag">oss</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/profile/" rel="tag">profile</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/prometheus/" rel="tag">prometheus</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/" rel="tag">redis</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sbt/" rel="tag">sbt</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/" rel="tag">shell</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/" rel="tag">spark</a><span class="tag-list-count">18</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tomcat/" rel="tag">tomcat</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tools/" rel="tag">tools</a><span class="tag-list-count">8</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/xwiki/" rel="tag">xwiki</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zk/" rel="tag">zk</a><span class="tag-list-count">1</span></li></ul>
              </li>
            
          
            
              <li class="archive-list-container">
                <a href="javascript:;">归档</a>
                <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">90</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">80</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/">2017</a><span class="archive-list-count">18</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/">2016</a><span class="archive-list-count">10</span></li></ul>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="/" title="主页" external="false">主页</a>
              </li>
            
          
            
              <li>
                <a href="/archives" title="文章" external="false">文章</a>
              </li>
            
          
            
              <li>
                <a href="/about" title="关于" external="false">关于</a>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="https://github.com/denjones/hexo-theme-chan" title="样式" target="_blank" rel="noopener">样式</a>
              </li>
            
          
            
              <li>
                <a href="https://github.com/jxeditor" title="Github" target="_blank" rel="noopener">Github</a>
              </li>
            
          
            
              <li>
                <a href="/atom.xml" title="RSS" external="false">RSS</a>
              </li>
            
          
        </ul>
      
    
  </div>
</div>

    <div class="main-content">
      
        <div style="max-width: 1000px">
      
          <article id="post-大数据理论知识合集" class="article article-type-post">
  
    <h1 class="article-header">
      大数据理论知识合集
    </h1>
  
  

  <div class="article-info">
    <span class="article-date">
  2019-11-29
</span>

    
	<span class="article-category tagcloud">
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></li></ul>
	</span>


    
	<span class="article-tag tagcloud">
		<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/interview/" rel="tag">interview</a></li></ul>
	</span>


  </div>
  <div class="article-entry">
    <blockquote>
<p>对于大数据理论方面知识进行搜集整理</p>
</blockquote>
<span id="more"></span>

<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="1-HDFS的组成"><a href="#1-HDFS的组成" class="headerlink" title="1.HDFS的组成"></a>1.HDFS的组成</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">1.NameNode：</span><br><span class="line">    管理HDFS的名称空间；</span><br><span class="line">    管理数据块的映射信息；</span><br><span class="line">    配置副本策略；</span><br><span class="line">    处理客户端的读写请求。</span><br><span class="line">    Fsimage：</span><br><span class="line">        存储了文件的基本信息，如文件路径，文件副本集个数，文件块的信息，文件所在的主机信息。</span><br><span class="line">    Editslog：</span><br><span class="line">        存了客户端对hdfs中各种写操作的日志（指令）。</span><br><span class="line">2.DataNode：</span><br><span class="line">    存储实际的数据块(Block，默认128M)；</span><br><span class="line">    执行数据块的读写操作。</span><br><span class="line">3.Secondary NameNode：</span><br><span class="line">    是NameNode的冷备；</span><br><span class="line">    辅助NameNode，分担其工作量；</span><br><span class="line">    定期（每一小时，editslog大小超过64M）合并fsimage和fsedits，并推送给NameNode；</span><br><span class="line">    在紧急情况下，可辅助恢复NameNode。</span><br><span class="line">4.Client：</span><br><span class="line">    文件切分；</span><br><span class="line">    与NameNode交互，获取文件的位置信息；</span><br><span class="line">    与DataNode交互，读取或者写入数据；</span><br><span class="line">    Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；</span><br><span class="line">    Client可以通过一些命令来访问HDFS。</span><br></pre></td></tr></table></figure>

<h2 id="2-HDFS读写数据流程"><a href="#2-HDFS读写数据流程" class="headerlink" title="2.HDFS读写数据流程"></a>2.HDFS读写数据流程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">读</span><br><span class="line">    1.HDFS客户端开启分布式文件系统</span><br><span class="line">    2.分布式文件系统向NameNode获取数据块信息</span><br><span class="line">    3.HDFS客户端通过数据块信息创建文件系统数据输入流</span><br><span class="line">    4.流通过数据块信息去各个DataNode读取信息</span><br><span class="line">    5.HDFS客户端关闭流</span><br><span class="line">写</span><br><span class="line">    1.HDFS客户端开启分布式文件系统</span><br><span class="line">    2.分布式文件系统创建NameNode</span><br><span class="line">    3.HDFS客户端创建文件系统数据输出流</span><br><span class="line">    4.通过流写入DataNode---逐步备份</span><br><span class="line">    5.逐步返回ack---流检测ack是否成功</span><br><span class="line">    6.HDFS客户端关闭流</span><br><span class="line">    7.分布式文件系统向NameNode提交完成</span><br></pre></td></tr></table></figure>

<h2 id="3-MR的shuffle过程"><a href="#3-MR的shuffle过程" class="headerlink" title="3.MR的shuffle过程"></a>3.MR的shuffle过程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Map端Shuffle</span><br><span class="line">Split切片,一般根据HDFS上的Block数量决定</span><br><span class="line">频繁的磁盘I&#x2F;O操作会严重降低效率,&quot;中间结果&quot;不会立马写入磁盘,而是写入一个环形缓冲区</span><br><span class="line">在写入过程中进行分区,对于每个键值增加一个partition属性值</span><br><span class="line">连同键值对一起序列化成字节数组写入缓冲区(默认100M)</span><br><span class="line">当写入的数据量达到预先设置的阙值后(默认0.8)便会启动溢写</span><br><span class="line">将缓冲区数据写到磁盘的临时文件中(可以压缩,默认不压缩),写入前根据Key进行排序(Sort)和合并(Combine,可选操作)</span><br><span class="line">当整个Map操作完成溢写后,会对磁盘这个Map产生的所有临时文件进行归并(Merge),生成正式输出文件</span><br><span class="line">此事的归并会将所有临时文件中相同partition合并到一起,并对每一个partition的数据再一次排序(Sort)</span><br><span class="line"></span><br><span class="line">Reduce端Shuffle</span><br><span class="line">不断拉取当前Job中每个MapTask的最终结果</span><br><span class="line">从不同地方拉取过来的数据不断做Merge,合并成分区相同的大文件</span><br><span class="line">对这个文件的键值对按Key进行Sort</span><br><span class="line">排好序之后紧接着分组,分组完成后才将整个文件交给ReduceTask处理</span><br><span class="line"></span><br><span class="line">注意</span><br><span class="line">1.Combine和Merge的区别</span><br><span class="line">    &lt;a,1&gt;,&lt;a,1&gt;变成&lt;a,2&gt;是Combine</span><br><span class="line">    &lt;a,1&gt;,&lt;a,1&gt;变成&lt;a,&lt;1,1&gt;&gt;是Merge</span><br><span class="line">2.Shuffle中的排序</span><br><span class="line">    内存缓冲区使用的是快速排序</span><br><span class="line">    文件合并阶段使用的归并排序</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><h2 id="1-Yarn的组成"><a href="#1-Yarn的组成" class="headerlink" title="1.Yarn的组成"></a>1.Yarn的组成</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1.ResourceManager：</span><br><span class="line">    处理客户端请求；</span><br><span class="line">    启动或监控ApplicationMaster；</span><br><span class="line">    监控NodeManager；</span><br><span class="line">    负责整个集群的资源管理和调度。</span><br><span class="line">2.NodeManager：</span><br><span class="line">    处理来自ResourceManager上的命令；</span><br><span class="line">    处理来自ApplicationMaster上的命令；</span><br><span class="line">    负责单个节点上的资源管理。</span><br><span class="line">3.ApplicationMaster：</span><br><span class="line">    负责数据切分；</span><br><span class="line">    为应用程序申请资源并分配给内部的任务；</span><br><span class="line">    负责应用程序相关的事务，比如任务调度，任务监控和容错等。</span><br></pre></td></tr></table></figure>

<h2 id="2-Yarn执行流程"><a href="#2-Yarn执行流程" class="headerlink" title="2.Yarn执行流程"></a>2.Yarn执行流程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">1.作业提交</span><br><span class="line">    A.Client调用job.waitForCompletion方法，向集群提交MR作业；</span><br><span class="line">    B.ResourceManager分配JobID；</span><br><span class="line">    C.Client核实作业的输出，计算输入的Split，将作业的资源(Jar包，配置文件，Split信息等)拷贝给HDFS；</span><br><span class="line">    D.调用ResourceManager的submitApplication()提交作业。</span><br><span class="line">2.作业初始化</span><br><span class="line">    A.ResourceManager收到请求，将请求发送给调度器（scheduler），调度器分配Container，然后ResourceManager在Container中启动ApplicationMaster，由NodeManager进行监控；</span><br><span class="line">    B.ApplicationMaster通过bookkeeping对象来监控作业的进度，得到任务的进度和完成报告；</span><br><span class="line">    C.通过HDFS得到Client计算好的Split，为每个输入Split创建Map任务，根据mapreduce.job.reduces创建Reduce任务对象。</span><br><span class="line">3.任务分配</span><br><span class="line">    A.如果作业小，ApplicationMaster会选择在自己的JVM中运行任务；</span><br><span class="line">    B.如果作业大，ApplicationMaster会向ResourceManager请求Container来运行所有的map和reduce任务，请求通过心跳机制传输；</span><br><span class="line">    C.调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点，或者退而分配给和存放Split的节点相同机架的节点。</span><br><span class="line">4.任务运行</span><br><span class="line">    A.当任务由ResourceManager的调度分配给Container后，ApplicationMaster通过联系NodeManager来启动Container；</span><br><span class="line">    B.任务由一个主类为YarnChild的Java应用执行（YarnChild运行在一个专用的JVM中，Yarn不支持JVM重用）；</span><br><span class="line">    C.在运行任务之前首先需要本地化任务需要的资源，比如作业配置，Jar包，以及分布式缓存的所有文件；</span><br><span class="line">    D.运行Map或Reduce任务。</span><br><span class="line">5.进度和状态更新</span><br><span class="line">    A.Yarn中的任务将其进度和状态返回给ApplicationMaster；</span><br><span class="line">    B.Client每秒向ApplicationMaster请求进度更新，展示给用户。</span><br><span class="line">6.任务完成</span><br><span class="line">    A.除了向ApplicationMaster请求作业进度外，Client每5分钟检查作业是否完成通过调用waitForCompletion()方法；</span><br><span class="line">    B.作业完成之后，ApplicationMaster和Container会清理工作状态，OutputCommiter的作业清理方法也会被调用；</span><br><span class="line">    C.作业的信息会被作业历史服务器存储以备用户核查。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h2 id="1-Hive怎么解决数据倾斜的问题"><a href="#1-Hive怎么解决数据倾斜的问题" class="headerlink" title="1.Hive怎么解决数据倾斜的问题?"></a>1.Hive怎么解决数据倾斜的问题?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">使map的输出数据更均匀的分布到reduce中去，是我们的最终目标</span><br></pre></td></tr></table></figure>

<h2 id="2-数据倾斜有哪些原因"><a href="#2-数据倾斜有哪些原因" class="headerlink" title="2.数据倾斜有哪些原因?"></a>2.数据倾斜有哪些原因?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">key分布不均匀</span><br><span class="line">业务数据本身的欠缺性</span><br><span class="line">建表设计方法不对</span><br><span class="line">有些SQL难免会有一些数据倾斜不可避免</span><br><span class="line"></span><br><span class="line">表现的形式:</span><br><span class="line">    任务完成进度卡死在99%，或者进度完成度在100%</span><br><span class="line">    但是查看任务监控，发现还是有少量（1个或几个）reduce子任务未完成</span><br><span class="line">    因为其处理的数据量和其他reduce差异过大</span><br><span class="line">    单一reduce的记录数与平均记录数差异过大</span><br><span class="line">    通常可能达到3倍甚至更多</span><br><span class="line">    最长时长远大于平均时长</span><br></pre></td></tr></table></figure>

<h2 id="3-Hive的工作过程"><a href="#3-Hive的工作过程" class="headerlink" title="3.Hive的工作过程"></a>3.Hive的工作过程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">A.用户提交查询等任务给Driver；</span><br><span class="line">B.编译器获得该用户的任务Plan；</span><br><span class="line">C.编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息；</span><br><span class="line">D.编译器Compiler得到元数据信息，对任务进行编译，先将HQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询快转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理计划（MapReduce），最后选择最优策略；</span><br><span class="line">E.将最终的计划提交给Driver；</span><br><span class="line">F.Driver将计划Plan转交给ExecutorEngine去执行，获取元数据信息，提交给JobTracker或者ResourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作；</span><br><span class="line">G.获取执行的结果；</span><br><span class="line">H.取得并返回执行结果。</span><br></pre></td></tr></table></figure>

<h2 id="4-Hive优化器的主要功能"><a href="#4-Hive优化器的主要功能" class="headerlink" title="4.Hive优化器的主要功能"></a>4.Hive优化器的主要功能</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A.将多Multiple Join合并为一个Muti-Way Join；</span><br><span class="line">B.对Join，group by和自定义的MapReduce操作重新进行划分；</span><br><span class="line">C.消减不必要的列；</span><br><span class="line">D.在表的扫描操作中推行使用断言；</span><br><span class="line">E.对于已分区的表，消减不必要的分区；</span><br><span class="line">F.在抽样查询中，消减不必要的桶；</span><br><span class="line">G.优化器还增加了局部聚合操作用于处理大分组聚合和增加再分区操作用于处理不对称的分组聚合。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h2 id="1-HBase的系统架构"><a href="#1-HBase的系统架构" class="headerlink" title="1.HBase的系统架构"></a>1.HBase的系统架构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1.Client</span><br><span class="line">    包含了访问HBase的接口，Client维护着一些cache来加快对HBase的访问，比如Region的位置信息。</span><br><span class="line">2.Zookeeper</span><br><span class="line">    A.保证任何时候，集群中只有一个Master；</span><br><span class="line">    B.存储所有Region的寻址入口，Root表在哪台服务器上；</span><br><span class="line">    C.实时监控Region Server的状态，将Region Server的上线和下线信息实时通知给Master；</span><br><span class="line">    D.存储HBase的Schema，包括有哪些table，每个table有哪些column family。</span><br><span class="line">3.Master</span><br><span class="line">    A.为Region Server分配Region；</span><br><span class="line">    B.负责Region Server的负载均衡；</span><br><span class="line">    C.发现失效的Region Server并重新分配其上的Region；</span><br><span class="line">    D.HDFS上的垃圾文件回收；</span><br><span class="line">    E.处理schema更新请求。</span><br><span class="line">4.Region Server</span><br><span class="line">    A.Region Server维护Master分配给它的Region，处理对这些Region的IO请求；</span><br><span class="line">    B.Region Server负责切分在运行过程中变得过大的Region；</span><br><span class="line">    C.Client访问HBase上数据的过程并不需要Master参与(寻址访问Zookeeper和Region Server，数据读写访问Region Server)</span><br><span class="line">        Master仅仅维护table和Region的元数据信息，负载很低。</span><br></pre></td></tr></table></figure>

<h2 id="2-HBase的存储结构"><a href="#2-HBase的存储结构" class="headerlink" title="2.HBase的存储结构"></a>2.HBase的存储结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">1.基本结构</span><br><span class="line">    A.Table中的所有行都按照rowKey的字典排列；</span><br><span class="line">    B.Table在行的方向上分割为多个Region；</span><br><span class="line">    C.Region按大小分割，每个表一开始只有一个Region，随着数据的不断插入，Region不断增大</span><br><span class="line">        当增大到一个阈值的时候，Region就会等分成两个新的Region，当table中的行数不断增多，就会有越来越多的Region；</span><br><span class="line">    D.Region是HBase中分布式存储和负载均衡的最小单元。</span><br><span class="line">        最小单元表示不同的Region可以分布在不同的Region Server上，但是一个Region是不会拆分到不同的Region Server上的；</span><br><span class="line">    E.Region虽然是分布式存储的最小单元，但并不是物理存储的最小单元。</span><br><span class="line">2.MemStore与StoreFile</span><br><span class="line">    A.Region由一个或者多个Store组成，每个Store保存一个column family；</span><br><span class="line">    B.每个Store由一个MemStore和0至多个StoreFile组成；</span><br><span class="line">    C.写操作先写入MemStore，当数据量达到一个阈值，Region Server启动flashcache进程写入StoreFile；</span><br><span class="line">    D.当Store大小超过一定阈值，会把当前的Region分割成两个，并由Master分配给相应的Region Server（先下线Region，切割好之后加入Meta元信息，加入到原本的RegionServer中，最后汇报Master）；</span><br><span class="line">    E.客户端检索数据时，先在MemStore找，找不到再找StoreFile；</span><br><span class="line">    F.StoreFile以HFile的格式保存在HDFS上。</span><br><span class="line">3.HFile</span><br><span class="line">    A.Data Block段：保存表中的数据，可被压缩；</span><br><span class="line">    B.Meta Block段：保存用户自定义的kv对，可被压缩；</span><br><span class="line">    C.File Info段：HFile的元信息，不可压缩，用户也可以在这一部分添加自己的元信息；</span><br><span class="line">    D.Data Block Index段：Data Block的索引；</span><br><span class="line">    E.Meta Block Index段：Meta Block的索引；</span><br><span class="line">    F.Trailer段：保存每一段的偏移量，读取一个HFile时，会首先读取Trailer。</span><br><span class="line">4.HLog(（WAL Log）</span><br><span class="line">    A.类似MySQL的binlog，做灾难恢复，记录数据的所有变更；</span><br><span class="line">    B.每个Region Server维护一个HLog；</span><br><span class="line">    C.如果Region Server下线，需要对其上的HLog进行拆分，分发到其他Region Server进行恢复。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="1-SparkSQL和SparkStreaming哪个比较熟"><a href="#1-SparkSQL和SparkStreaming哪个比较熟" class="headerlink" title="1.SparkSQL和SparkStreaming哪个比较熟?"></a>1.SparkSQL和SparkStreaming哪个比较熟?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">都还行，SparkSQL的DataFrame或者DataSet和SparkStreaming的DStream都是基于SparkCore的;</span><br><span class="line">最终都会转化为SparkTask执行;</span><br><span class="line">我们可以交流一下本质的东西SparkCore，而SparkCore的核心又是RDD。</span><br></pre></td></tr></table></figure>

<h2 id="2-说一下SparkShuffle"><a href="#2-说一下SparkShuffle" class="headerlink" title="2.说一下SparkShuffle"></a>2.说一下SparkShuffle</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Spark的shuffle也是一处理问题的思想：分而治之。</span><br><span class="line">Shuffle一般称为洗牌，一般会有Shuffle Write阶段和Shuffle Read阶段。</span><br><span class="line">在Spark中实现Shuffle的方式有两种，一种是HashShuffle(2.0弃用)，一种是SortShuffle。</span><br><span class="line">Shuffle的性能是影响Spark应用程序性能的关键。</span><br><span class="line">Shuffle发生在stage之间，stage中用的pipline的计算模式。</span><br><span class="line"></span><br><span class="line">决定Shuffle后数据属于哪个分区,主要由分区器决定(HashPartitioner,RangePartitioner)</span><br><span class="line">Hash: 由于Hash碰撞的存在,极端情况下可能出现数据倾斜</span><br><span class="line">Range: 划分范围,分区与分区之间是有序的,分区内不保证顺序</span><br></pre></td></tr></table></figure>

<h2 id="3-SparkShuffle的调优点"><a href="#3-SparkShuffle的调优点" class="headerlink" title="3.SparkShuffle的调优点?"></a>3.SparkShuffle的调优点?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Shuffle的选择</span><br><span class="line">缓冲区的大小</span><br><span class="line">拉取的数据量的大小</span><br><span class="line">间隔时间重试次数</span><br></pre></td></tr></table></figure>

<h2 id="4-缓存这块熟悉吗-介绍缓存级别"><a href="#4-缓存这块熟悉吗-介绍缓存级别" class="headerlink" title="4.缓存这块熟悉吗,介绍缓存级别"></a>4.缓存这块熟悉吗,介绍缓存级别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Spark的缓存机制是Spark优化的一个重要点；</span><br><span class="line">它将需要重复使用或者共用的RDD缓存在内存中，可以提高Spark的性能。</span><br><span class="line">Spark的底层源码中使用StorageLevel来表示缓存机制，</span><br><span class="line">其中包括：</span><br><span class="line">    使用内存，使用磁盘，使用序列化，使用堆外内存</span><br><span class="line">在他的半生对象中基于这几种方式提供了一些实现：</span><br><span class="line">    不使用缓存，Memory_Only，Disk_only，OffHeap</span><br><span class="line">分别都有相应的序列化，副本，组合的实现提供选择。</span><br><span class="line">持久化的级别StorageLevel可以自定义，但是一般不自定义。</span><br><span class="line">如何选择RDD的缓存级别的本质是在内存的利用率和CPU的利用率之间的权衡。</span><br><span class="line">一般默认选择的是Memory_only, 其次是Memery_only_Ser, 再次是Memory_only_and_Dis</span><br><span class="line">至于怎么选择你得自己权衡。</span><br></pre></td></tr></table></figure>

<h2 id="5-说一下cache和checkpoint的区别"><a href="#5-说一下cache和checkpoint的区别" class="headerlink" title="5.说一下cache和checkpoint的区别"></a>5.说一下cache和checkpoint的区别</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">catche是将共用的或者重复使用的RDD按照持久化的级别进行缓存</span><br><span class="line">checkpoint是将业务场景非常长的逻辑计算的中间结果缓存到HDFS上</span><br><span class="line">ck的实现原理是:</span><br><span class="line">    首先找到stage最后的finalRDD，然后按照RDD的依赖关系进行回溯</span><br><span class="line">    找到使用了checkPoint的RDD</span><br><span class="line">    然后标记这个使用了checkPoint的RDD</span><br><span class="line">    重新的启动一个线程来将checkPoint之前的RDD缓存到HDFS上面</span><br><span class="line">    最后将RDD的依赖关系从checkPoint的位置切断</span><br></pre></td></tr></table></figure>

<h2 id="6-Spark运行模式local、local-和local-分别是什么"><a href="#6-Spark运行模式local、local-和local-分别是什么" class="headerlink" title="6.Spark运行模式local、local[]和local[*]分别是什么?"></a>6.Spark运行模式<code>local</code>、<code>local[]</code>和<code>local[*]</code>分别是什么?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算</span><br><span class="line">通常用来验证开发出来的应用程序逻辑上有没有问题</span><br><span class="line">其中N代表可以使用N个线程，每个线程拥有一个core。</span><br><span class="line">如果不指定N，则默认是1个线程（该线程有1个core）。</span><br><span class="line">如果是local[*]，则代表Run Spark locally with as many worker threads as logical cores on your machine(在本地运行Spark，与您的机器上的逻辑内核一样多的工作线程)。</span><br></pre></td></tr></table></figure>

<h2 id="7-Spark怎么设置垃圾回收机制"><a href="#7-Spark怎么设置垃圾回收机制" class="headerlink" title="7.Spark怎么设置垃圾回收机制?"></a>7.Spark怎么设置垃圾回收机制?</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Spark中各个角色的JVM参数设置:</span><br><span class="line">Driver的JVM参数</span><br><span class="line">    GC方式，如果是yarn-client模式，默认读取的是spark-class文件中的JAVAOPTS；</span><br><span class="line">    如果是yarn-cluster模式，则读取的是spark-default.conf文件中的spark.driver.extraJavaOptions对应的参数值。</span><br><span class="line">Executor的JVM参数</span><br><span class="line">    GC方式，两种模式都是读取的是spark-default.conf文件中的spark.executor.extraJavaOptions对应的JVM参数值。</span><br></pre></td></tr></table></figure>

<h2 id="8-一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？"><a href="#8-一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？" class="headerlink" title="8.一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？"></a>8.一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">单独的用户只能看自己的进程</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h2 id="1-Flink中TM内存管理"><a href="#1-Flink中TM内存管理" class="headerlink" title="1.Flink中TM内存管理"></a>1.Flink中TM内存管理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Total Process Memory 总内存(设置多少就是多少)</span><br><span class="line">    Total Flink Memory 总Flink使用内存</span><br><span class="line">        JVM Heap 堆内内存</span><br><span class="line">            Framework Heap 框架堆内内存,默认128M</span><br><span class="line">            Task Heap 任务堆内内存,总Flink使用内存-框架堆内内存-管理内存-网络内存</span><br><span class="line">        Off-Heap Memory 堆外内存</span><br><span class="line">            Managed Memory 管理内存,总Flink使用内存的0.4</span><br><span class="line">            Direct Memory</span><br><span class="line">                Framework Off-Heap 框架堆外内存,默认128M</span><br><span class="line">                Task Off-Heap 任务的堆外内存,默认0</span><br><span class="line">                Network 网络缓存大小,总Flink使用内存的0.1</span><br><span class="line">    JVM Metaspace 默认占用256M</span><br><span class="line">    JVM Overhead  默认值1GB</span><br><span class="line"></span><br><span class="line">注意:JVM Metaspace和Overhead也属于堆外内存</span><br><span class="line"></span><br><span class="line">FrameWork Heap是Task Executor本身占用的堆内内存大小,不用于执行Task.</span><br><span class="line">FrameWork Off-Heap是Task Executor保留的堆外内存大小.</span><br><span class="line">Task Heap是专门用来执行Task的堆内内存.</span><br><span class="line">Task Off-Heap是Task的堆外内存,在Flink程序中有调用Native方法,可以配置.</span><br><span class="line">Managed Memory主要用于排序,哈希表和中间结果缓存,RocksDB的Backend.</span><br><span class="line">Network Memory用于Task之间的数据交换.</span><br><span class="line">JVM Metaspace是JVM的元数据空间大小</span><br><span class="line">JVM Overhead是保留给JVM其他内存开销,GC,ThreadStack,code cache等.</span><br></pre></td></tr></table></figure>

  </div>
  <footer class="article-footer">
    
  <div class="cc">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/deed.z" target="_blank" title="署名-相同方式共享">
      <img src="/images/cc/cc.png">
      
          <img src="/images/cc/by.png">
        
          <img src="/images/cc/sa.png">
      
      <span>
        本作品采用知识共享 署名-相同方式共享 4.0 国际许可协议进行许可。
      </span>
    </a>
  </div>


    

  </footer>
</article>







          <div class="main-footer">
  
    © 2021 BlackC - Powered by <a href="http://hexo.io" target="_blank">Hexo</a> - Theme <a href="https://github.com/denjones/hexo-theme-chan" target="_blank">Chan</a>
  
</div>

      
        </div>
      
    </div>
  </div>
  
<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


  
<link rel="stylesheet" href="/PhotoSwipe/photoswipe.css">

  
<link rel="stylesheet" href="/PhotoSwipe/default-skin/default-skin.css">


  <!-- Root element of PhotoSwipe. Must have class pswp. -->
  <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
             It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

      <!-- Container that holds slides.
                PhotoSwipe keeps only 3 of them in the DOM to save memory.
                Don't modify these 3 pswp__item elements, data is added later on. -->
      <div class="pswp__container">
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
      </div>

      <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
      <div class="pswp__ui pswp__ui--hidden">

        <div class="pswp__top-bar">

          <!--  Controls are self-explanatory. Order can be changed. -->

          <div class="pswp__counter"></div>

          <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

          <button class="pswp__button pswp__button--share" title="Share"></button>

          <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

          <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

          <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
          <!-- element will get class pswp__preloader--active when preloader is running -->
          <div class="pswp__preloader">
            <div class="pswp__preloader__icn">
              <div class="pswp__preloader__cut">
                <div class="pswp__preloader__donut"></div>
              </div>
            </div>
          </div>
        </div>

        <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
          <div class="pswp__share-tooltip"></div>
        </div>

        <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>

        <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

        <div class="pswp__caption">
          <div class="pswp__caption__center"></div>
        </div>
      </div>
    </div>
  </div>

  
<script src="/PhotoSwipe/photoswipe.js"></script>

  
<script src="/PhotoSwipe/photoswipe-ui-default.js"></script>




<script src="/perfect-scrollbar/js/min/perfect-scrollbar.min.js"></script>


<script src="/scripts/main.js"></script>


</body>
</html>
