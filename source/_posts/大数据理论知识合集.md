---
title: 大数据理论知识合集
date: 2019-11-29 10:05:37
categories: 大数据
tags: interview
---

> 对于大数据理论方面知识进行搜集整理

<!-- more -->

## 1.ConcurrentHashMap是怎么实现的？
```
concurrent包中线程安全的哈希表，采用分段锁;
可以理解为把一个大的Map拆分成N个小的HashTable;
根据key.hashCode()来决定把key放到哪个HashTable中。
在ConcurrentHashMap中，就是把Map分成了N个Segment;
put和get的时候，都是现根据key.hashCode()算出放到哪个Segment中。
```

---

## 2.SparkSQL和SparkStreaming哪个比较熟?
```
都还行，SparkSQL的DataFrame或者DataSet和SparkStreaming的DStream都是基于SparkCore的;
最终都会转化为SparkTask执行;
我们可以交流一下本质的东西SparkCore，而SparkCore的核心又是RDD。
```

---

## 3.说一下SparkShuffle
```
Spark的shuffle也是一处理问题的思想：分而治之。
Shuffle一般称为洗牌，一般会有Shuffle Write阶段和Shuffle Read阶段。
在Spark中实现Shuffle的方式有两种，一种是HashShuffle，一种是SortShuffle。
Shuffle的性能是影响Spark应用程序性能的关键。
Shuffle发生在stage之间，stage中用的pipline的计算模式。
```

---

## 4.SparkShuffle的调优点?
```
Shuffle的选择
缓冲区的大小
拉取的数据量的大小
间隔时间重试次数
```

---

## 5.缓存这块熟悉吗,介绍缓存级别
```
Spark的缓存机制是Spark优化的一个重要点；
它将需要重复使用或者共用的RDD缓存在内存中，可以提高Spark的性能。
Spark的底层源码中使用StorageLevel来表示缓存机制，
其中包括：
    使用内存，使用磁盘，使用序列化，使用堆外内存
在他的半生对象中基于这几种方式提供了一些实现：
    不使用缓存，Memory_Only，Disk_only，OffHeap
分别都有相应的序列化，副本，组合的实现提供选择。
持久化的级别StorageLevel可以自定义，但是一般不自定义。
如何选择RDD的缓存级别的本质是在内存的利用率和CPU的利用率之间的权衡。
一般默认选择的是Memory_only, 其次是Memery_only_Ser, 再次是Memory_only_and_Dis
至于怎么选择你得自己权衡。
```

---

## 6.说一下cache和checkpoint的区别
```
catche是将共用的或者重复使用的RDD按照持久化的级别进行缓存
checkpoint是将业务场景非常长的逻辑计算的中间结果缓存到HDFS上
ck的实现原理是:
    首先找到stage最后的finalRDD，然后按照RDD的依赖关系进行回溯
    找到使用了checkPoint的RDD
    然后标记这个使用了checkPoint的RDD
    重新的启动一个线程来将checkPoint之前的RDD缓存到HDFS上面
    最后将RDD的依赖关系从checkPoint的位置切断
```

---

## 7.Spark运行模式`local`、`local[]`和`local[*]`分别是什么?
```
该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算
通常用来验证开发出来的应用程序逻辑上有没有问题
其中N代表可以使用N个线程，每个线程拥有一个core。
如果不指定N，则默认是1个线程（该线程有1个core）。
如果是local[*]，则代表Run Spark locally with as many worker threads as logical cores on your machine(在本地运行Spark，与您的机器上的逻辑内核一样多的工作线程)。
```

---

## 8.Spark怎么设置垃圾回收机制?
```
Spark中各个角色的JVM参数设置:
Driver的JVM参数
    GC方式，如果是yarn-client模式，默认读取的是spark-class文件中的JAVAOPTS；
    如果是yarn-cluster模式，则读取的是spark-default.conf文件中的spark.driver.extraJavaOptions对应的参数值。
Executor的JVM参数
    GC方式，两种模式都是读取的是spark-default.conf文件中的spark.executor.extraJavaOptions对应的JVM参数值。
```

---

## 9.一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？
```
单独的用户只能看自己的进程
```

---

## 10.hive怎么解决数据倾斜的问题?
```
使map的输出数据更均匀的分布到reduce中去，是我们的最终目标
```

---

## 11.数据倾斜有哪些原因?
```
key分布不均匀
业务数据本身的欠缺性
建表设计方法不对
有些SQL难免会有一些数据倾斜不可避免

表现的形式:
    任务完成进度卡死在99%，或者进度完成度在100%
    但是查看任务监控，发现还是有少量（1个或几个）reduce子任务未完成
    因为其处理的数据量和其他reduce差异过大
    单一reduce的记录数与平均记录数差异过大
    通常可能达到3倍甚至更多
    最长时长远大于平均时长
```

---

## 12.HDFS读写数据流程
```
读
    1.HDFS客户端开启分布式文件系统
    2.分布式文件系统向NameNode获取数据块信息
    3.HDFS客户端通过数据块信息创建文件系统数据输入流
    4.流通过数据块信息去各个DataNode读取信息
    5.HDFS客户端关闭流
写
    1.HDFS客户端开启分布式文件系统
    2.分布式文件系统创建NameNode
    3.HDFS客户端创建文件系统数据输出流
    4.通过流写入DataNode---逐步备份
    5.逐步返回ack---流检测ack是否成功
    6.HDFS客户端关闭流
    7.分布式文件系统向NameNode提交完成
```