<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>压力测试工具</title>
    <url>/2018/10/16/AB%E6%8E%A5%E5%8F%A3%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<blockquote>
<p>通过一些简单的命令进行压力测试</p>
</blockquote>
<span id="more"></span>

<h2 id="MySQL自带的压测工具MySQLSlap"><a href="#MySQL自带的压测工具MySQLSlap" class="headerlink" title="MySQL自带的压测工具MySQLSlap"></a>MySQL自带的压测工具MySQLSlap</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line">mysqlslap –user=root –password=123456 –auto-generate-sql</span><br><span class="line"></span><br><span class="line">-auto-generate-sql: 自动生成测试SQL</span><br><span class="line">结果:运行所有语句的平均秒数,运行所有语句的最小秒数,运行所有语句的最大秒数,客户端数量,每个客户端运行查询的平均数</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加并发</span></span><br><span class="line">mysqlslap –user=root –password=123456 –concurrency=100 –number-of-queries=1000 –auto-generate-sql</span><br><span class="line"></span><br><span class="line">–concurrency=100 指定同时有100个客户端连接</span><br><span class="line">–number-of-queries=1000 指定总的测试查询次数(并发客户端数 * 每个客户端的查询次数)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动生成复杂表</span></span><br><span class="line">mysqlslap –user=root –password=123456 –concurrency=50 –number-int-cols=5 –number-char-cols=20 –auto-generate-sql</span><br><span class="line"></span><br><span class="line">–number-int-cols=5 指定生成5个int类型的列</span><br><span class="line">–number-char-cols=20 指定生成20个char类型的列</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自定义测试库与测试语句</span></span><br><span class="line">mysqlslap –user=root –password=123456 –concurrency=50 –create-schema=employees –query=<span class="string">&quot;SELECT * FROM dept_emp;&quot;</span></span><br><span class="line"></span><br><span class="line">–create-schema 用来指定测试库名称</span><br><span class="line">–query 是自定义的测试语句</span><br><span class="line"></span><br><span class="line">mysqlslap –user=root –password=123456 –concurrency=20 –number-of-queries=1000 –create-schema=employees –query=<span class="string">&quot;select_query.sql&quot;</span> –delimiter=<span class="string">&quot;;&quot;</span></span><br><span class="line"></span><br><span class="line">–query 中指定了sql文件</span><br><span class="line">–delimiter 说明sql文件中语句间的分隔符是什么</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常用参数</span></span><br><span class="line">--auto-generate-sql, -a 自动生成测试表和数据，表示用mysqlslap工具自己生成的SQL脚本来测试并发压力。</span><br><span class="line">--auto-generate-sql-load-type=<span class="built_in">type</span> 测试语句的类型。代表要测试的环境是读操作还是写操作还是两者混合的。取值包括：<span class="built_in">read</span>，key，write，update和mixed(默认)。</span><br><span class="line">--auto-generate-sql-add-auto-increment 代表对生成的表自动添加auto_increment列，从5.1.18版本开始支持。</span><br><span class="line">--number-char-cols=N, -x N 自动生成的测试表中包含多少个字符类型的列，默认1</span><br><span class="line">--number-int-cols=N, -y N 自动生成的测试表中包含多少个数字类型的列，默认1</span><br><span class="line">--number-of-queries=N 总的测试查询次数(并发客户数×每客户查询次数)</span><br><span class="line">--query=name,-q 使用自定义脚本执行测试，例如可以调用自定义的一个存储过程或者sql语句来执行测试。</span><br><span class="line">--create-schema 代表自定义的测试库名称，测试的schema，MySQL中schema也就是database。</span><br><span class="line">--commint=N 多少条DML后提交一次。</span><br><span class="line">--compress, -C 如果服务器和客户端支持都压缩，则压缩信息传递。</span><br><span class="line">--concurrency=N, -c N 表示并发量，也就是模拟多少个客户端同时执行select。可指定多个值，以逗号或者--delimiter参数指定的值做为分隔符。例如：--concurrency=100,200,500。</span><br><span class="line">--engine=engine_name, -e engine_name 代表要测试的引擎，可以有多个，用分隔符隔开。例如：--engines=myisam,innodb。</span><br><span class="line">--iterations=N, -i N 测试执行的迭代次数，代表要在不同并发环境下，各自运行测试多少次。</span><br><span class="line">--only-print 只打印测试语句而不实际执行。</span><br><span class="line">--detach=N 执行N条语句后断开重连。</span><br><span class="line">--debug-info, -T 打印内存和CPU的相关信息。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="AB接口压力测试"><a href="#AB接口压力测试" class="headerlink" title="AB接口压力测试"></a>AB接口压力测试</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ab的使用</span></span><br><span class="line">Usage: ab [options] [http[s]://]hostname[:port]/path</span><br><span class="line">Options are:</span><br><span class="line">    -n requests     请求的次数</span><br><span class="line">    -c concurrency  并发数</span><br><span class="line">    -t timelimit    持续时间</span><br><span class="line">                    This implies -n 50000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟500个客户端进行20000次请求</span></span><br><span class="line">ab -c 500 -c 20000 <span class="string">&#x27;http://127.0.0.1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟POST请求</span></span><br><span class="line">args.txt内容id=1&amp;name=zs</span><br><span class="line">ab -n 500 -c 500 -p args.txt -T application/x-www-form-urlencoded <span class="string">&#x27;http://IP:PORT/api/demo&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试结果</span></span><br><span class="line">Requests per second: 吞吐率</span><br><span class="line">Time per request: 上面的是用户平均请求等待时间</span><br><span class="line">Time per request: 下面的是服务器平均请求处理时间</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>ApacheHudi简介</title>
    <url>/2021/05/07/ApacheHudi%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<blockquote>
<p>参考Hudi<a href="https://hudi.apache.org/docs/spark_quick-start-guide.html">官网</a>,<a href="https://copyfuture.com/blogs-details/20200104193221803maaaqfq9dqvkkhf">文章</a>以及<a href="https://zhuanlan.zhihu.com/p/339932084">知乎</a>回答,只做简单介绍</p>
</blockquote>
<span id="more"></span>

<h2 id="Hudi是什么"><a href="#Hudi是什么" class="headerlink" title="Hudi是什么?"></a>Hudi是什么?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Hadoop Upserts Deletes and Incrementals.</span><br><span class="line">对于我自己的理解就是利用Hadoop存储更新&#x2F;删除&#x2F;变更的记录.</span><br><span class="line">基于时间线对数据进行存储,并保证原子性.</span><br><span class="line">Hudi的关键动作:</span><br><span class="line">    COMMITS - 将一批记录原子写入表中</span><br><span class="line">    CLEANS - 清除表中不再使用的旧版本文件</span><br><span class="line">    DELTA_COMMIT - 将一批记录原子写入MergeOnRead类型表中</span><br><span class="line">    COMPACTION - 压缩,可以看做基于时间线上的特殊提交(从基于行的日志文件移动到列格式)</span><br><span class="line">    ROLLBACK - 表示COMMITS&#x2F;DELTA_COMMIT不成功且已经回滚(删除了写入过程产生的任何文件)</span><br><span class="line">    SAVEPOINT - 将某些文件标记为SAVEPOINT,清理程序不会删除它们,便于将表还原到时间线上的某个快照上</span><br><span class="line"></span><br><span class="line">Hudi的状态:</span><br><span class="line">    REQUESTED - 已安排动作,但未开始</span><br><span class="line">    INFLIGHT - 正在执行该操作</span><br><span class="line">    COMPLETED - 动作完成</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="为什么使用Hudi"><a href="#为什么使用Hudi" class="headerlink" title="为什么使用Hudi?"></a>为什么使用Hudi?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对于大数据处理,我们一般分为实时处理与离线处理,数据同理</span><br><span class="line">Hudi可以将MySQL数据以近实时的方式映射到大数据平台中(Hive)</span><br><span class="line">MySQL处理耗时长&#x2F;没法处理&#x2F;跨库的分析可以交由大数据平台来处理</span><br><span class="line">这时候有同学会问,如何只是这样的话,也有其他的组件可以做到,为啥要选择Hudi?</span><br><span class="line">Apache Kudu,需要单独部署集群,Hudi不需要,只需要利用现在的HDFS做数据文件存储,Hive做数据分析即可,更加适合资源受限的环境</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Hudi表数据结构"><a href="#Hudi表数据结构" class="headerlink" title="Hudi表数据结构"></a>Hudi表数据结构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一般使用HDFS进行数据存储</span><br><span class="line">包含_partition_key的为实际数据文件,按分区存储,_partition_key可以指定</span><br><span class="line"></span><br><span class="line">.hoodie </span><br><span class="line">对应表的元数据信息</span><br><span class="line">由于CRUD的零散性,每一次操作都会生成一个文件,小文件越来越多后会严重影响HDFS的性能,Hudi设计了文件合并机制,.hoodie中存放了对应的文件合并操作相关日志</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Hudi常见配置"><a href="#Hudi常见配置" class="headerlink" title="Hudi常见配置"></a>Hudi常见配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">需要注意必要的配置</span><br><span class="line">TABLE_NAME(表名)</span><br><span class="line">RECORDKEY_FIELD_OPT_KEY(主键字段,用来唯一标识每个分区内的记录,默认值:uuid)</span><br><span class="line">PARTITIONPATH_FIELD_OPT_KEY(用于对表进行分区的列,设置&quot;&quot;可以避免分区,默认值:partitionpath)</span><br><span class="line">PRECOMBINE_FIELD_OPT_KEY(当一批数据中有两个记录具有相同的键时,选择指定字段最大值的记录,默认值:ts)</span><br><span class="line"></span><br><span class="line">非必要配置</span><br><span class="line">OPERATION_OPT_KEY(写操作类型,默认值:UPSERT_OPERATION_OPT_VAL,可选:BULK_INSERT_OPERATION_OPT_VAL,INSERT_OPERATION_OPT_VAL,DELETE_OPERATION_OPT_VAL)</span><br><span class="line">TABLE_TYPE_OPT_KEY(表类型,默认值:COW_TABLE_TYPE_OPT_VAL,可选:MOR_TABLE_TYPE_OPT_VAL)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="COW和MOR表"><a href="#COW和MOR表" class="headerlink" title="COW和MOR表"></a>COW和MOR表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Copy On Write</span><br><span class="line">写时复制表中的文件片,仅包含基本列文件,每次提交都会生成新版本的基本文件</span><br><span class="line">此存储类型使客户端能够以列式文件格式(当前为parquet)摄取数据</span><br><span class="line">使用COW存储类型时,任何写入Hudi数据集的新数据都将写入新的parquet文件</span><br><span class="line">更新现有的行将导致重写整个parquet文件(这些parquet文件包含要更新的受影响的行)</span><br><span class="line">因此,所有对此类数据集的写入都受parquet写性能的限制,parquet文件越大,摄取数据所花费的时间就越长</span><br><span class="line"></span><br><span class="line">Merge On Read</span><br><span class="line">读时合并表,仅通过最新版本的数据文件来支持读取查询</span><br><span class="line">此存储类型使客户端可以快速将数据摄取为基于行(如avro)的数据格式</span><br><span class="line">使用MOR存储类型时,任何写入Hudi数据集的新数据都将写入新的日志&#x2F;增量文件,这些文件在内部将数据以avro进行编码</span><br><span class="line">压缩(Compaction)过程(配置为嵌入式或异步)将日志文件格式转换为列式文件格式(parquet)</span><br><span class="line"></span><br><span class="line">如果满足以下条件,则选择写时复制(COW)存储:</span><br><span class="line">    寻找一种简单的替换现有的parquet表的方法,而无需实时数据</span><br><span class="line">    当前的工作流是重写整个表&#x2F;分区以处理更新,而每个分区中实际上只有几个文件发生更改</span><br><span class="line">    想使操作更为简单(无需压缩等),并且摄取&#x2F;写入性能仅受parquet文件大小以及受更新影响文件数量限制</span><br><span class="line">    工作流很简单,并且不会突然爆发大量更新或插入到较旧的分区</span><br><span class="line">    COW写入时付出了合并成本,因此,这些突然的更改可能会阻塞摄取,并干扰正常摄取延迟目标</span><br><span class="line"></span><br><span class="line">如果满足以下条件，则选择读时合并(MOR)存储:</span><br><span class="line">    希望数据尽快被摄取并尽可能快地可被查询</span><br><span class="line">    工作负载可能会突然出现模式的峰值&#x2F;变化(例如,对上游数据库中较旧事务的批量更新导致对DFS上旧分区的大量更新)</span><br><span class="line">    异步压缩(Compaction)有助于缓解由这种情况引起的写放大,而正常的提取则需跟上上游流的变化</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hudi</tag>
      </tags>
  </entry>
  <entry>
    <title>2020年不太顺的一年</title>
    <url>/2020/12/31/2020%E5%B9%B4%E4%B8%8D%E5%A4%AA%E9%A1%BA%E7%9A%84%E4%B8%80%E5%B9%B4/</url>
    <content><![CDATA[<blockquote>
<p>一年一下就过去了,2020年都有些什么想法</p>
</blockquote>
<span id="more"></span>

<p>鼠年最后一天了,明年牛年本命年,希望能顺一点,加油.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">工作上</span><br><span class="line">    从北京跳来杭州,新公司产出不太被领导重视,虽然依然待着,但公司的做法着实让人心寒.</span><br><span class="line"></span><br><span class="line">生活上</span><br><span class="line">    家里大事小事不断,今年庚子年,基本都是一些坏消息.</span><br><span class="line">    感觉人一下子身上的担子重了很多,很多时候不太敢像以往那样做出果断的选择,</span><br><span class="line">    需要考虑的东西,太多了.</span><br><span class="line">    终究还是生活要紧.</span><br><span class="line"></span><br><span class="line">旅游上</span><br><span class="line">    开心的年初,糟心的年末,领略了杭州的美景,希望疫情快点过去吧.</span><br><span class="line"></span><br><span class="line">社区上</span><br><span class="line">    年初定好的commiter目标,今年并没有实现,劲头没有一开始那么足了</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>miscellany</tag>
      </tags>
  </entry>
  <entry>
    <title>CAS嵌入各个子系统(Confluence)</title>
    <url>/2019/07/25/CAS%E5%B5%8C%E5%85%A5%E5%90%84%E4%B8%AA%E5%AD%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<blockquote>
<p>基本上的操作并不会改变多少,只是看认证方式复杂程度,因为这一块需要自己实现.这里使用Confluence做一个简单的Demo.</p>
</blockquote>
<span id="more"></span>

<h2 id="安装破解Confluence"><a href="#安装破解Confluence" class="headerlink" title="安装破解Confluence"></a>安装破解Confluence</h2><p>破解工具<a href="https://github.com/jxeditor/Software/blob/master/cas/confluence_keygen.jar">confluence_keygen.jar</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我使用的是Confluence6.6.15</span><br><span class="line"></span><br><span class="line">将Confluence安装好之后不启动Confluence</span><br><span class="line"></span><br><span class="line">将Confluence安装目录D:\Atlassian\Confluence\confluence\WEB-INF\lib下的atlassian-extras-decoder-v2-3.2.jar复制到其他位置并改名为atlassian-extras-2.4.jar</span><br><span class="line"></span><br><span class="line">使用破解工具的.patch!将刚复制的atlassian-extras-2.4.jar进行破解</span><br><span class="line"></span><br><span class="line">成功后将atlassian-extras-2.4.jar改名为atlassian-extras-decoder-v2-3.2.jar</span><br><span class="line"></span><br><span class="line">替换安装目录的jar文件</span><br><span class="line"></span><br><span class="line">启动Confluence</span><br><span class="line"></span><br><span class="line">选择产品验证</span><br><span class="line"></span><br><span class="line">使用破解工具,随便输入数据,将Confluence自身的Server ID复制到指定栏</span><br><span class="line"></span><br><span class="line">点击.gen!,复制生成Key,破解完成</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Confluence的Cas依赖包"><a href="#Confluence的Cas依赖包" class="headerlink" title="Confluence的Cas依赖包"></a>Confluence的Cas依赖包</h2><ul>
<li><a href="https://github.com/jxeditor/Software/blob/master/cas/cas-client-core-3.3.3.jar">cas-client-core-3.3.3.jar</a></li>
<li><a href="https://github.com/jxeditor/Software/blob/master/cas/cas-client-integration-atlassian-3.3.3.jar">cas-client-integration-atlassian-3.3.3.jar</a><br>将上述依赖包复制到Confluence安装目录D:\Atlassian\Confluence\confluence\WEB-INF\lib下</li>
</ul>
<hr>
<h2 id="Cas服务架构"><a href="#Cas服务架构" class="headerlink" title="Cas服务架构"></a>Cas服务架构</h2><p>我们使用Cas自带的Overlay来架构自己的Cas服务,有Gradle和Maven<br>我选Maven模式5.3版本的<strong>GitHub分支</strong><br>Cas服务下载<a href="https://github.com/apereo/cas-overlay-template/tree/5.3">GitHub</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">下载好之后执行.&#x2F;build package</span><br><span class="line">其实README上都有详细的介绍</span><br><span class="line"></span><br><span class="line">执行之后会出现target目录,目录中有war包</span><br><span class="line"></span><br><span class="line">新建Maven项目,将cas-overlay-template的pom文件内容复制到项目pom文件中</span><br><span class="line"></span><br><span class="line">项目中新建webapp文件</span><br><span class="line"></span><br><span class="line">将war包中的内容解压到webapp中</span><br><span class="line">webapp</span><br><span class="line">--META-INF</span><br><span class="line">--org</span><br><span class="line">--WEB-INF</span><br><span class="line"></span><br><span class="line">配置下Tomcat,就可以直接启动Cas服务了</span><br><span class="line"></span><br><span class="line">Cas的配置都在webapp&#x2F;WEB-INF&#x2F;classes&#x2F;application.properties中</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="需要修改的3个Confluence文件"><a href="#需要修改的3个Confluence文件" class="headerlink" title="需要修改的3个Confluence文件"></a>需要修改的3个Confluence文件</h2><table>
<thead>
<tr>
<th align="center">文件名</th>
<th align="center">路径</th>
</tr>
</thead>
<tbody><tr>
<td align="center">web.xml</td>
<td align="center">D:\Atlassian\Confluence\confluence\WEB-INF\web.xml</td>
</tr>
<tr>
<td align="center">seraph-config.xml</td>
<td align="center">D:\Atlassian\Confluence\confluence\WEB-INF\classes\seraph-config.xml</td>
</tr>
<tr>
<td align="center">xwork.xml</td>
<td align="center">D:\Atlassian\Confluence\confluence\WEB-INF\classes\xwork.xml</td>
</tr>
<tr>
<td align="center"><strong>其中xwork.xml来自D:\Atlassian\Confluence\confluence\WEB-INF\lib\confluence-6.6.15.jar,将其复制到classes文件夹内</strong></td>
<td align="center"></td>
</tr>
</tbody></table>
<hr>
<h2 id="配置web-xml"><a href="#配置web-xml" class="headerlink" title="配置web.xml"></a>配置web.xml</h2><h3 id="所有filter之后"><a href="#所有filter之后" class="headerlink" title="所有filter之后"></a>所有filter之后</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置过滤器和cas 以及本地服务的路径信息--&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- CAS:START - Java Client Filters --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>CasSingleSignOutFilter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-class</span>&gt;</span>org.jasig.cas.client.session.SingleSignOutFilter<span class="tag">&lt;/<span class="name">filter-class</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>casServerLoginUrl<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8080/login<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>serverName<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8090/<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>CasAuthenticationFilter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-class</span>&gt;</span>org.jasig.cas.client.authentication.AuthenticationFilter<span class="tag">&lt;/<span class="name">filter-class</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>casServerLoginUrl<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8080/login<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>serverName<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8090/<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>CasValidationFilter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-class</span>&gt;</span>org.jasig.cas.client.validation.Cas20ProxyReceivingTicketValidationFilter<span class="tag">&lt;/<span class="name">filter-class</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>casServerUrlPrefix<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8080<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>serverName<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8090/<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>redirectAfterValidation<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--- CAS:END --&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="login之前"><a href="#login之前" class="headerlink" title="login之前"></a>login之前</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置过滤器--&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- CAS:START - Java Client Filter Mappings --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter-mapping</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>CasSingleSignOutFilter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url-pattern</span>&gt;</span>/*<span class="tag">&lt;/<span class="name">url-pattern</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter-mapping</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter-mapping</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>CasAuthenticationFilter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url-pattern</span>&gt;</span>/login.action<span class="tag">&lt;/<span class="name">url-pattern</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter-mapping</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter-mapping</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">filter-name</span>&gt;</span>CasValidationFilter<span class="tag">&lt;/<span class="name">filter-name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url-pattern</span>&gt;</span>/*<span class="tag">&lt;/<span class="name">url-pattern</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter-mapping</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- CAS:END --&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="Servlet-Context-Listeners之后"><a href="#Servlet-Context-Listeners之后" class="headerlink" title="Servlet Context Listeners之后"></a>Servlet Context Listeners之后</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置单点登出的监听器--&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- CAS:START - Java Client Single Sign Out Listener --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">listener</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">listener-class</span>&gt;</span>org.jasig.cas.client.session.SingleSignOutHttpSessionListener<span class="tag">&lt;/<span class="name">listener-class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">listener</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- CAS:END --&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="配置seraph-config-xml"><a href="#配置seraph-config-xml" class="headerlink" title="配置seraph-config.xml"></a>配置seraph-config.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>login.url<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;param-value&gt;/login.action?os_destination=$&#123;originalurl&#125;&lt;/param-value&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8080/login?service=$&#123;originalurl&#125;<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">init-param</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">param-name</span>&gt;</span>link.login.url<span class="tag">&lt;/<span class="name">param-name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--&lt;param-value&gt;/login.action&lt;/param-value&gt;--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">param-value</span>&gt;</span>http://192.168.40.124:8080/login?service=$&#123;originalurl&#125;<span class="tag">&lt;/<span class="name">param-value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">init-param</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置confluence通过cas的方式来验证服务--&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- CAS:START - Java Client Confluence Authenticator --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">authenticator</span> <span class="attr">class</span>=<span class="string">&quot;org.jasig.cas.client.integration.atlassian.ConfluenceCasAuthenticator&quot;</span>/&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- CAS:END --&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="配置xwork-xml"><a href="#配置xwork-xml" class="headerlink" title="配置xwork.xml"></a>配置xwork.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;logout&quot;</span> <span class="attr">class</span>=<span class="string">&quot;com.atlassian.confluence.user.actions.LogoutAction&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">interceptor-ref</span> <span class="attr">name</span>=<span class="string">&quot;defaultStack&quot;</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- &lt;result name=&quot;error&quot; type=&quot;velocity&quot;&gt;/logout.vm&lt;/result&gt; --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- CAS:START - CAS Logout Redirect --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">result</span> <span class="attr">name</span>=<span class="string">&quot;success&quot;</span> <span class="attr">type</span>=<span class="string">&quot;redirect&quot;</span>&gt;</span>http://192.168.40.124:8080/logout<span class="tag">&lt;/<span class="name">result</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- CAS:END --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h2><ul>
<li>依赖包一定要3.3.3版本的,我下3.5.1版本的会报错</li>
<li>Cas服务需要修改支持http,修改WEB-INF\classes\services\HTTPSandIMAPS-10000001.json</li>
<li>出现403错误ticket验证不了需要使用IP的方式,不能使用localhost</li>
</ul>
<hr>
<h2 id="application-properties配置"><a href="#application-properties配置" class="headerlink" title="application.properties配置"></a>application.properties配置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 出现服务未定义时,需要加上这个配置</span><br><span class="line">cas.serviceRegistry.initFromJson&#x3D;true</span><br><span class="line">cas.serviceRegistry.json.location&#x3D;classpath:&#x2F;services</span><br><span class="line"></span><br><span class="line">##</span><br><span class="line"># CAS Authentication Credentials</span><br><span class="line">#</span><br><span class="line">#cas.authn.accept.users&#x3D;superadmin::system</span><br><span class="line">#数据库配置</span><br><span class="line">#配置密码加密</span><br><span class="line">cas.authn.jdbc.query[0].passwordEncoder.type&#x3D;DEFAULT</span><br><span class="line">cas.authn.jdbc.query[0].passwordEncoder.characterEncoding&#x3D;UTF-8</span><br><span class="line">cas.authn.jdbc.query[0].passwordEncoder.encodingAlgorithm&#x3D;MD5</span><br><span class="line"></span><br><span class="line">cas.authn.jdbc.query[0].sql&#x3D;SELECT * FROM global_users WHERE user_name &#x3D;?</span><br><span class="line">#select * from cms_auth_user where user_name&#x3D;?</span><br><span class="line">cas.authn.jdbc.query[0].healthQuery&#x3D;</span><br><span class="line">cas.authn.jdbc.query[0].isolateInternalQueries&#x3D;false</span><br><span class="line">cas.authn.jdbc.query[0].url&#x3D;jdbc:mysql:&#x2F;&#x2F;dmysql01:3306&#x2F;fdfs?useUnicode&#x3D;true&amp;characterEncoding&#x3D;UTF-8&amp;autoReconnect&#x3D;true&amp;useSSL&#x3D;false</span><br><span class="line">#cas.authn.jdbc.query[0].failFast&#x3D;true</span><br><span class="line">#cas.authn.jdbc.query[0].isolationLevelName&#x3D;ISOLATION_READ_COMMITTED</span><br><span class="line">cas.authn.jdbc.query[0].dialect&#x3D;org.hibernate.dialect.MySQLDialect</span><br><span class="line">cas.authn.jdbc.query[0].leakThreshold&#x3D;10</span><br><span class="line">#cas.authn.jdbc.query[0].propagationBehaviorName&#x3D;PROPAGATION_REQUIRED</span><br><span class="line">cas.authn.jdbc.query[0].batchSize&#x3D;1</span><br><span class="line">cas.authn.jdbc.query[0].user&#x3D;root</span><br><span class="line">#cas.authn.jdbc.query[0].ddlAuto&#x3D;create-drop</span><br><span class="line">#cas.authn.jdbc.query[0].maxAgeDays&#x3D;180</span><br><span class="line">cas.authn.jdbc.query[0].password&#x3D;123456</span><br><span class="line">cas.authn.jdbc.query[0].autocommit&#x3D;false</span><br><span class="line">cas.authn.jdbc.query[0].driverClass&#x3D;com.mysql.jdbc.Driver</span><br><span class="line">cas.authn.jdbc.query[0].idleTimeout&#x3D;5000</span><br><span class="line"># cas.authn.jdbc.query[0].credentialCriteria&#x3D;</span><br><span class="line"># cas.authn.jdbc.query[0].name&#x3D;</span><br><span class="line"># cas.authn.jdbc.query[0].order&#x3D;0</span><br><span class="line"># cas.authn.jdbc.query[0].dataSourceName&#x3D;</span><br><span class="line"># cas.authn.jdbc.query[0].dataSourceProxy&#x3D;false</span><br><span class="line">cas.authn.jdbc.query[0].fieldPassword&#x3D;PASSWORD</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>cas</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH5.15.1搭建与重装</title>
    <url>/2018/09/18/CDH5.15.1%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%87%8D%E8%A3%85/</url>
    <content><![CDATA[<blockquote>
<p>如何快速的搭建一套CDH,注意CentOS版本对应</p>
</blockquote>
<span id="more"></span>

<h2 id="系统环境-64位"><a href="#系统环境-64位" class="headerlink" title="系统环境[64位]"></a>系统环境[64位]</h2><ul>
<li>操作系统: Centos6</li>
<li>Cloudera Manager: 5.15.1.4</li>
<li>CDH: 5.15.1</li>
</ul>
<hr>
<h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><h4 id="Clouder-Manager下载地址"><a href="#Clouder-Manager下载地址" class="headerlink" title="Clouder Manager下载地址"></a>Clouder Manager下载地址</h4><ul>
<li><a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-el6-cm5.15.1_x86_64.tar.gz">Clouder Manager el6 5.15.1</a></li>
</ul>
<h4 id="CDH安装包下载地址"><a href="#CDH安装包下载地址" class="headerlink" title="CDH安装包下载地址"></a>CDH安装包下载地址</h4><ul>
<li><a href="http://archive.cloudera.com/cdh5/parcels/5.15.1.4/CDH-5.15.1-1.cdh5.15.1.p0.4-el6.parcel">CDH el6 5.15.1 </a></li>
<li><a href="http://archive.cloudera.com/cdh5/parcels/5.15.1.4/CDH-5.15.1-1.cdh5.15.1.p0.4-el6.parcel.sha1">CDH el6 5.15.1 sha1</a></li>
<li><a href="http://archive.cloudera.com/cdh5/parcels/5.15.1.4/manifest.json">ManiFest.json</a></li>
</ul>
<h4 id="JDK下载地址"><a href="#JDK下载地址" class="headerlink" title="JDK下载地址"></a>JDK下载地址</h4><ul>
<li>使用1.6,1.7以外版本会警告</li>
<li><a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html">新版本下载</a></li>
<li><a href="https://www.oracle.com/technetwork/java/javase/archive-139210.html">老版本下载</a></li>
</ul>
<h4 id="MySQL下载地址"><a href="#MySQL下载地址" class="headerlink" title="MySQL下载地址"></a>MySQL下载地址</h4><ul>
<li><a href="https://repo.mysql.com/yum/">MySQL yum仓库地址</a></li>
<li><a href="https://dev.mysql.com/downloads/connector/j/">MySQL连接器</a></li>
</ul>
<hr>
<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h4 id="1-网络配置"><a href="#1-网络配置" class="headerlink" title="1.网络配置"></a>1.网络配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;hosts</span><br><span class="line">192.168.6.129	hadoop01</span><br><span class="line">192.168.6.130	hadoop02</span><br><span class="line">192.168.6.131	hadoop03</span><br></pre></td></tr></table></figure>

<h4 id="2-免密配置"><a href="#2-免密配置" class="headerlink" title="2.免密配置"></a>2.免密配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br><span class="line">scp ~/.ssh/authorized_keys root@hadoop02:~/.ssh/</span><br><span class="line">scp ~/.ssh/authorized_keys root@hadoop03:~/.ssh/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@hadoop01</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@hadoop02</span><br><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@hadoop03</span><br></pre></td></tr></table></figure>

<h4 id="3-JDK配置"><a href="#3-JDK配置" class="headerlink" title="3.JDK配置"></a>3.JDK配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -qa | grep java     <span class="comment"># 查询</span></span><br><span class="line">rpm -e --nodeps 包名    <span class="comment"># 卸载</span></span><br><span class="line">rpm -ivh 包名           <span class="comment"># 安装</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;JAVA_HOME=/usr/java/latest/&quot;</span> &gt;&gt; /etc/environment</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;export PATH=<span class="variable">$PATH</span>:/usr/java/latest/bin&quot;</span> &gt;&gt; /etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意,无论哪种安装方法,一定保证/usr/java/default存在</span></span><br><span class="line">mkdir /usr/java</span><br><span class="line">ln -s /usr/<span class="built_in">local</span>/jdk8 /usr/java/default</span><br></pre></td></tr></table></figure>

<h4 id="4-MySQL配置"><a href="#4-MySQL配置" class="headerlink" title="4.MySQL配置"></a>4.MySQL配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># CentOS7</span><br><span class="line">wget http:&#x2F;&#x2F;repo.mysql.com&#x2F;mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line"># 继续</span><br><span class="line">yum install mysql-server</span><br><span class="line">chkconfig mysqld on</span><br><span class="line">service mysqld start</span><br><span class="line">mysqladmin -u root password &#39;123456&#39;</span><br><span class="line">mysql -u root -p</span><br><span class="line">create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line">create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line">create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line">grant all privileges on *.* to &#39;root&#39;@&#39;hadoop01&#39; identified by &#39;123456&#39; with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>

<h4 id="5-防火墙-SELinux以及Swap配置"><a href="#5-防火墙-SELinux以及Swap配置" class="headerlink" title="5.防火墙,SELinux以及Swap配置"></a>5.防火墙,SELinux以及Swap配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service iptables stop   # 临时关闭</span><br><span class="line">chkconfig iptables off  # 重启后永久生效</span><br><span class="line"></span><br><span class="line">setenforce 0            # 临时关闭</span><br><span class="line">vi &#x2F;etc&#x2F;selinux&#x2F;config  # 重启后永久生效</span><br><span class="line">SELINUX&#x3D;disabled</span><br><span class="line"></span><br><span class="line">echo 10 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness</span><br><span class="line">vi &#x2F;etc&#x2F;sysctl.conf     # 重启后永久生效</span><br><span class="line">vm.swappiness &#x3D; 10</span><br><span class="line"></span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag</span><br><span class="line">echo never &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled</span><br><span class="line">vi &#x2F;etc&#x2F;rc.local        # 重启后永久生效</span><br><span class="line"># 将上述两条语句写入rc.local文件</span><br></pre></td></tr></table></figure>

<h4 id="6-NTP时间同步"><a href="#6-NTP时间同步" class="headerlink" title="6.NTP时间同步"></a>6.NTP时间同步</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ntp</span><br><span class="line">chkconfig ntpd on</span><br><span class="line">chkconfig --list        <span class="comment"># ntpd其中2-5为on状态就代表成功</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 能联网情况下</span></span><br><span class="line">ntpdate cn.pool.ntp.org</span><br><span class="line">hwclock --systohc</span><br><span class="line">service ntpd start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不能联网情况下</span></span><br><span class="line"><span class="comment"># 设置hadoop01为NTP服务器</span></span><br><span class="line">vi /etc/ntp.conf        <span class="comment"># 默认的server都关闭</span></span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict -6 ::1</span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">server 192.168.1.128 perfer</span><br><span class="line">server 192.168.1.128</span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># NTP客户端设置</span></span><br><span class="line">vi /etc/ntp.conf        <span class="comment"># 默认的server都关闭</span></span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict -6 ::1</span><br><span class="line">server 192.168.1.128</span><br><span class="line">restrict 192.168.1.128 nomodify notrap noquery</span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h2><h4 id="1-安装Cloudera-Manager-Server-和Agent"><a href="#1-安装Cloudera-Manager-Server-和Agent" class="headerlink" title="1.安装Cloudera Manager Server 和Agent"></a>1.安装Cloudera Manager Server 和Agent</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -xzvf cloudera-manager*.tar.gz</span><br><span class="line">mv cloudera &#x2F;opt&#x2F;</span><br><span class="line">mv cm-5.15.1 &#x2F;opt&#x2F;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 添加数据库连接</span><br><span class="line">cp mysql-connector-java-5.1.47-bin.jar &#x2F;opt&#x2F;cm-5.15.1&#x2F;share&#x2F;cmf&#x2F;lib&#x2F;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 主节点初始化CM数据库</span><br><span class="line">&#x2F;opt&#x2F;cm-5.15.1&#x2F;share&#x2F;cmf&#x2F;schema&#x2F;scm_prepare_database.sh mysql cm -hlocalhost -uroot -p123456 --scm-host localhost scm scm scm</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 修改Agent配置,为主节点名</span><br><span class="line">vi &#x2F;opt&#x2F;cm-5.15.1&#x2F;etc&#x2F;cloudera-scm-agent&#x2F;config.ini</span><br><span class="line">server_host&#x3D;hadoop01</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 分发到其他节点</span><br><span class="line">scp -r &#x2F;opt&#x2F;cm-5.15.1 root@hadoop02:&#x2F;opt&#x2F;</span><br><span class="line">scp -r &#x2F;opt&#x2F;cm-5.15.1 root@hadoop03:&#x2F;opt&#x2F;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 所有节点创建cloudera-scm用户</span><br><span class="line">useradd --system --home&#x3D;&#x2F;opt&#x2F;cm-5.15.1&#x2F;run&#x2F;cloudera-scm-server&#x2F; --no-create-home --shell&#x3D;&#x2F;bin&#x2F;false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br></pre></td></tr></table></figure>

<h4 id="2-安装CDH"><a href="#2-安装CDH" class="headerlink" title="2.安装CDH"></a>2.安装CDH</h4><h5 id="sha1要mv成sha-否则系统会重新下载"><a href="#sha1要mv成sha-否则系统会重新下载" class="headerlink" title="sha1要mv成sha,否则系统会重新下载"></a>sha1要mv成sha,否则系统会重新下载</h5><ul>
<li>CDH-5.15.1-1.cdh5.15.1.p0.4-el6.parcel</li>
<li>CDH-5.15.1-1.cdh5.15.1.p0.4-el6.parcel.sha1</li>
<li>manifest.json<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mv CDH-5.15.1-1.cdh5.15.1.p0.4-el6.parcel* manifest.json &#x2F;opt&#x2F;cloudera&#x2F;parcel-repo&#x2F;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 主节点</span><br><span class="line">&#x2F;opt&#x2F;cm-5.15.1&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-server start</span><br><span class="line">&#x2F;&#x2F; 所有节点</span><br><span class="line">&#x2F;opt&#x2F;cm-5.15.1&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-agent start</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="3-配置CDH"><a href="#3-配置CDH" class="headerlink" title="3.配置CDH"></a>3.配置CDH</h4><h5 id="访问http-hadoop01-7180进行配置"><a href="#访问http-hadoop01-7180进行配置" class="headerlink" title="访问http://hadoop01:7180进行配置"></a>访问<a href="http://hadoop01:7180/">http://hadoop01:7180</a>进行配置</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用户名密码均为admin</span></span><br><span class="line"><span class="comment"># 选择CM版本</span></span><br><span class="line"><span class="comment"># 选择Agent节点</span></span><br><span class="line"><span class="comment"># 选择Parcel包</span></span><br><span class="line"><span class="comment"># 耐心等待分配</span></span><br><span class="line"><span class="comment"># 检查主机正确性</span></span><br><span class="line"><span class="comment"># 选择所有服务</span></span><br><span class="line"><span class="comment"># 服务配置一般默认&lt;zk默认只有1个节点可以调整&gt;</span></span><br><span class="line"><span class="comment"># 进行数据库设置</span></span><br><span class="line">cp mysql-connector-java-5.1.47-bin.jar /opt/cloudera/parcels/CDH-5.15.1-1.cdh5.15.1.p0.4/lib/hive/lib/</span><br><span class="line">cp mysql-connector-java-5.1.47-bin.jar /var/lib/oozie/</span><br><span class="line"><span class="comment"># 测试连接</span></span><br><span class="line"><span class="comment"># hue测试不通过,缺少依赖</span></span><br><span class="line">yum install libxml2-python krb5-devel cyrus-sasl-plain cyrus-sasl-gssapi cyrus-sasl-devel libxml2-devel libxslt-devel mysql mysql-devel openldap-devel python-devel python-simplejson sqlite-devel mod_ssl</span><br><span class="line"><span class="comment"># 耐心等待服务启动</span></span><br><span class="line"><span class="comment"># 安装完毕</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="测试与各端口"><a href="#测试与各端口" class="headerlink" title="测试与各端口"></a>测试与各端口</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 10 100</span><br><span class="line"></span><br><span class="line"><span class="comment"># CDH</span></span><br><span class="line">http://hadoop01:7180</span><br><span class="line"></span><br><span class="line"><span class="comment"># Yarn</span></span><br><span class="line">http://hadoop01:8088</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hue</span></span><br><span class="line">http://hadoop01:8888</span><br><span class="line"></span><br><span class="line"><span class="comment"># HDFS</span></span><br><span class="line">http://hadoop01:50070</span><br><span class="line"></span><br><span class="line"><span class="comment"># JobHistory</span></span><br><span class="line">http://hadoop01:19888</span><br><span class="line"></span><br><span class="line"><span class="comment"># HBase</span></span><br><span class="line">http://hadoop01:60010</span><br><span class="line">http://hadoop01:60030</span><br><span class="line"></span><br><span class="line"><span class="comment"># Spark</span></span><br><span class="line">http://hadoop01:7077</span><br><span class="line">http://hadoop01:8080</span><br><span class="line">http://hadoop01:8081</span><br><span class="line">http://hadoop01:4040</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="重装CDH"><a href="#重装CDH" class="headerlink" title="重装CDH"></a>重装CDH</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 删除Agent的UUID</span><br><span class="line">rm -rf &#x2F;opt&#x2F;cm-5.15.1&#x2F;lib&#x2F;cloudera-scm-agent&#x2F;*</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 删除主节点CM数据库</span><br><span class="line">drop database cm;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 删除Agent节点namenode和datanode节点信息</span><br><span class="line">rm -rf &#x2F;dfs&#x2F;nn&#x2F;*</span><br><span class="line">rm -rf &#x2F;dfs&#x2F;dn&#x2F;*</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 重新初始化CM数据库</span><br><span class="line">&#x2F;opt&#x2F;cm-5.15.1&#x2F;share&#x2F;cmf&#x2F;schema&#x2F;scm_prepare_database.sh mysql cm -hlocalhost -uroot -p123456 --scm-host localhost scm scm scm</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 执行Server和Agent脚本</span><br><span class="line">&#x2F;opt&#x2F;cm-5.15.1&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-server start</span><br><span class="line">&#x2F;opt&#x2F;cm-5.15.1&#x2F;etc&#x2F;init.d&#x2F;cloudera-scm-agent start</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 重新安装</span><br><span class="line">http:&#x2F;&#x2F;hadoop01:7180</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 数据库连接</span><br><span class="line">cp mysql-connector-java-5.1.47-bin.jar &#x2F;var&#x2F;lib&#x2F;oozie&#x2F;</span><br><span class="line">cp mysql-connector-java-5.1.47-bin.jar &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.15.1-1.cdh5.15.1.p0.4&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a>可能出现的问题</h2><h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">问题:</span><br><span class="line">    首个失败：主机 hadoop03 (id&#x3D;2) 上的客户端配置 (id&#x3D;4) 已使用 127 退出，而预期值为 0。</span><br><span class="line">解决:</span><br><span class="line">    检查是否是java找不到,需要&#x2F;usr&#x2F;java&#x2F;default</span><br><span class="line">    如果不是,则检查是否是内存不足导致无法部署客户端配置</span><br></pre></td></tr></table></figure>

<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.Permission denied: user&#x3D;root, access&#x3D;WRITE, inode&#x3D;&quot;&#x2F;user&quot;:hdfs:supergroup:drwxr-xr-x</span><br><span class="line">解决:</span><br><span class="line">echo &quot;export HADOOP_USER_NAME&#x3D;hdfs&quot; &gt;&gt; .bash_profile</span><br><span class="line">source .bash_profile</span><br><span class="line"></span><br><span class="line">2.WARN hdfs.DFSClient: Caught exception</span><br><span class="line">解决:</span><br><span class="line">不影响结果,暂时未找到办法</span><br></pre></td></tr></table></figure>
<h4 id="Hue"><a href="#Hue" class="headerlink" title="Hue"></a>Hue</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.Can&#39;t Open &#x2F;opt&#x2F;cm-5.15.1&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process&#x2F;65-hue-HUE_LOAD_BALANCER&#x2F;supervisor.conf权限不足</span><br><span class="line">解决:</span><br><span class="line">chown hue:hue supervisor.conf</span><br><span class="line">chmod 666 supervisor.conf</span><br><span class="line"></span><br><span class="line">2.&#x2F;usr&#x2F;sbin&#x2F;httpd没有这个命令</span><br><span class="line">解决:</span><br><span class="line">yum install httpd.x86_64</span><br><span class="line"></span><br><span class="line">3.&#x2F;usr&#x2F;lib64&#x2F;httpd&#x2F;modules&#x2F;mod_ssl.so没有这个文件</span><br><span class="line">解决:</span><br><span class="line">yum -y install mod_ssl</span><br><span class="line"></span><br><span class="line">4.Could not start SASL: Error in sasl_client_start (-4) SASL(-4): no mechanism available: No worthy mechs found</span><br><span class="line">解决:</span><br><span class="line">yum install cyrus-sasl-plain cyrus-sasl-devel cyrus-sasl-gssapi</span><br><span class="line">重启Hue</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH6.3版本安装搭建(联网)</title>
    <url>/2020/01/10/CDH6.3%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%E6%90%AD%E5%BB%BA(%E8%81%94%E7%BD%91)/</url>
    <content><![CDATA[<blockquote>
<p>联网搭建最新版本的CDH,离线参考之前5.15.1版本安装,更为繁琐</p>
</blockquote>
<span id="more"></span>

<h1 id="离线版简介"><a href="#离线版简介" class="headerlink" title="离线版简介"></a>离线版简介</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cm下载</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cm6&#x2F;</span><br><span class="line"># cdh下载</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="联网版"><a href="#联网版" class="headerlink" title="联网版"></a>联网版</h1><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">系统</span><br><span class="line">centos7</span><br><span class="line">jdk1.8</span><br><span class="line">mysql</span><br></pre></td></tr></table></figure>

<h2 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 关闭防火墙</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl disable firewalld.service</span><br><span class="line"></span><br><span class="line"># 修改hosts</span><br><span class="line">vi &#x2F;etc&#x2F;hosts</span><br><span class="line"></span><br><span class="line"># 修改主机名</span><br><span class="line">vi &#x2F;etc&#x2F;hostname</span><br><span class="line"></span><br><span class="line"># 更改host方式</span><br><span class="line">mv &#x2F;usr&#x2F;bin&#x2F;host &#x2F;usr&#x2F;bin&#x2F;host.bak</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="操作命令"><a href="#操作命令" class="headerlink" title="操作命令"></a>操作命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置CM的仓库</span><br><span class="line">yum install wget -y</span><br><span class="line">wget https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cm6&#x2F;6.3.1&#x2F;redhat7&#x2F;yum&#x2F;cloudera-manager.repo -P &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br><span class="line">rpm --import https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cm6&#x2F;6.3.0&#x2F;redhat7&#x2F;yum&#x2F;RPM-GPG-KEY-cloudera</span><br><span class="line"></span><br><span class="line"># 安装jdk</span><br><span class="line">yum install oracle-j2sdk1.8</span><br><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_181-cloudera &#x2F;usr&#x2F;java&#x2F;default</span><br><span class="line"></span><br><span class="line"># 安装mysql</span><br><span class="line">wget http:&#x2F;&#x2F;repo.mysql.com&#x2F;mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line">yum install mysql-server</span><br><span class="line">systemctl start mysqld</span><br><span class="line">systemctl enable mysqld</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;mysql_secure_installation</span><br><span class="line">wget https:&#x2F;&#x2F;dev.mysql.com&#x2F;get&#x2F;Downloads&#x2F;Connector-J&#x2F;mysql-connector-java-5.1.46.tar.gz</span><br><span class="line">tar zxvf mysql-connector-java-5.1.46.tar.gz</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;share&#x2F;java&#x2F;</span><br><span class="line">cd mysql-connector-java-5.1.46</span><br><span class="line">cp mysql-connector-java-5.1.46-bin.jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java.jar</span><br><span class="line"></span><br><span class="line"># scm库</span><br><span class="line">CREATE DATABASE scm DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON scm.* TO &#39;scm&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;</span><br><span class="line"></span><br><span class="line"># hue库</span><br><span class="line">CREATE DATABASE hue DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON hue.* TO &#39;hue&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;</span><br><span class="line"></span><br><span class="line"># hive库</span><br><span class="line">CREATE DATABASE metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON metastore.* TO &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;</span><br><span class="line"></span><br><span class="line"># oozie库</span><br><span class="line">CREATE DATABASE oozie DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;</span><br><span class="line">GRANT ALL ON oozie.* TO &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;;</span><br><span class="line"></span><br><span class="line">grant all privileges on *.* to &#39;root&#39;@&#39;cdh04&#39; identified by &#39;123456&#39; with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>

<h2 id="配置CM数据库"><a href="#配置CM数据库" class="headerlink" title="配置CM数据库"></a>配置CM数据库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;cloudera&#x2F;cm&#x2F;schema&#x2F;scm_prepare_database.sh mysql scm scm</span><br></pre></td></tr></table></figure>

<h2 id="启动安装"><a href="#启动安装" class="headerlink" title="启动安装"></a>启动安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 注意要删除</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH安装Kafka组件</title>
    <url>/2019/06/01/CDH%E5%AE%89%E8%A3%85Kafka%E7%BB%84%E4%BB%B6/</url>
    <content><![CDATA[<blockquote>
<p>在CDH下搭建kafka组件</p>
</blockquote>
<span id="more"></span>

<h2 id="查看CDH与Kafka对应版本关系"><a href="#查看CDH与Kafka对应版本关系" class="headerlink" title="查看CDH与Kafka对应版本关系"></a>查看CDH与Kafka对应版本关系</h2><p><a href="https://docs.cloudera.com/documentation/enterprise/release-notes/topics/rn_consolidated_pcm.html#pcm_kafka">传送门</a></p>
<hr>
<h2 id="下载CDH版本Kafka的parcel包"><a href="#下载CDH版本Kafka的parcel包" class="headerlink" title="下载CDH版本Kafka的parcel包"></a>下载CDH版本Kafka的parcel包</h2><p><a href="http://archive.cloudera.com/kafka/parcels/latest/">传送门</a><br>需要<code>.parcel</code>文件,对应<code>.sha1</code>文件以及<code>manifest.json</code>文件</p>
<p><strong>注意:</strong> <code>el*</code>代表Red Hat Enterprise Linux版本</p>
<hr>
<h2 id="修改文件"><a href="#修改文件" class="headerlink" title="修改文件"></a>修改文件</h2><p><code>.sha1</code>后缀改为<code>.sha</code><br>打开<code>manifest.json</code>文件,找到对应Kafka版本的hash值<br>赋值hash值,替换sha文件的hash值<br>将修改后的三个文件拷贝到/opt/cloudera/parcel-repo/目录下,manifest.json相同,则重命名之前的</p>
<hr>
<h2 id="下载CSD包"><a href="#下载CSD包" class="headerlink" title="下载CSD包"></a>下载CSD包</h2><p><a href="http://archive.cloudera.com/csds/kafka/">传送门</a><br>复制到/opt/cloudera/csd/目录下</p>
<hr>
<h2 id="Web页面进行新增服务"><a href="#Web页面进行新增服务" class="headerlink" title="Web页面进行新增服务"></a>Web页面进行新增服务</h2><p>检查更新parcel包<br>添加Kafka服务</p>
]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>cdh</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>CAS单点登录</title>
    <url>/2019/07/22/CAS%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>使用Cas进行单点登录操作实践</p>
</blockquote>
<span id="more"></span>

<h2 id="设置Https"><a href="#设置Https" class="headerlink" title="设置Https"></a>设置Https</h2><h3 id="生成证书"><a href="#生成证书" class="headerlink" title="生成证书"></a>生成证书</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">keytool -genkey -alias tomcat -keyalg RSA -validity 3650 -keystore D:\tomcat.keystore</span><br><span class="line"># 密钥库口令 changeit</span><br><span class="line"># 名字与姓氏输入域名 service.cas.com</span><br><span class="line">keytool -list -keystore D:\tomcat.keystore</span><br><span class="line">keytool -export -alias tomcat -file D:\tomcat.cer -keystore D:\tomcat.keystore -validity 3650</span><br><span class="line">keytool -import -keystore C:\Java\jdk1.8.0_191\jre\lib\security\cacerts -file D:\tomcat.cer -alias tomcat -storepass changeit</span><br><span class="line"># 密钥库口令 changeit</span><br><span class="line"># 删除操作</span><br><span class="line">keytool -delete -alias tomcat -keystore C:\Java\jdk1.8.0_191\jre\lib\security\cacerts</span><br><span class="line">keytool -list -v -keystore C:\Java\jdk1.8.0_191\jre\lib\security\cacerts</span><br></pre></td></tr></table></figure>

<h3 id="PKIX问题"><a href="#PKIX问题" class="headerlink" title="PKIX问题"></a>PKIX问题</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果CasClient与CasServer不在同一服务器上</span><br><span class="line"># 那么需要对将上述生成的tomcat.cer证书导入到JDK中</span><br><span class="line"># 尤其注意JDK的位置,用IDEA很可能JDK在C盘目录下</span><br></pre></td></tr></table></figure>

<h3 id="配置tomcat"><a href="#配置tomcat" class="headerlink" title="配置tomcat"></a>配置tomcat</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改tomcat_path&#x2F;conf&#x2F;server.xml,添加内容:</span><br><span class="line">&lt;Connector port&#x3D;&quot;8443&quot; protocol&#x3D;&quot;org.apache.coyote.http11.Http11NioProtocol&quot;</span><br><span class="line">    maxThreads&#x3D;&quot;200&quot; SSLEnabled&#x3D;&quot;true&quot; scheme&#x3D;&quot;https&quot;</span><br><span class="line">    secure&#x3D;&quot;true&quot; clientAuth&#x3D;&quot;false&quot; sslProtocol&#x3D;&quot;TLS&quot;</span><br><span class="line">    keystoreFile&#x3D;&quot;D:\tomcat.keystore&quot;</span><br><span class="line">    keystorePass&#x3D;&quot;changeit&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Cas服务端"><a href="#Cas服务端" class="headerlink" title="Cas服务端"></a>Cas服务端</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 我使用的是5.3.9</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;apereo&#x2F;cas-overlay-template.git</span><br><span class="line"># 按照README.md进行编译,将target&#x2F;cas下的文件复制</span><br><span class="line"># 新建Maven项目,将文件复制在src&#x2F;main&#x2F;webapp文件下</span><br><span class="line"># 复制pom.xml文件中的build,properties,repositories,profiles</span><br><span class="line"># 如若加入JDBC,需要在pom文件profiles.profile.dependencies处添加依赖</span><br><span class="line"></span><br><span class="line"># 修改application.properties</span><br><span class="line">server.ssl.enabled&#x3D;true</span><br><span class="line">server.ssl.key-store&#x3D;file:D:\tomcat.keystore</span><br><span class="line">server.ssl.key-store-password&#x3D;changeit</span><br><span class="line">server.ssl.key-password&#x3D;changeit</span><br><span class="line">server.ssl.keyAlias&#x3D;tomcat</span><br><span class="line"># 支持JSON</span><br><span class="line">cas.serviceRegistry.initFromJson&#x3D;true</span><br><span class="line">cas.logout.followServiceRedirects&#x3D;true</span><br><span class="line">cas.logout.redirectParameter&#x3D;service</span><br><span class="line">cas.logout.confirmLogout&#x3D;false</span><br><span class="line">cas.logout.removeDescendantTickets&#x3D;true</span><br><span class="line"></span><br><span class="line"># 支持Http,修改webapp&#x2F;WEB-INF&#x2F;classes&#x2F;services&#x2F;HTTPSandIMAPS-10000001.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;@class&quot; : &quot;org.apereo.cas.services.RegexRegisteredService&quot;,</span><br><span class="line">  &quot;serviceId&quot; : &quot;^(https|http|imaps):&#x2F;&#x2F;.*&quot;,</span><br><span class="line">  &quot;name&quot; : &quot;HTTPS and IMAPS&quot;,</span><br><span class="line">  &quot;id&quot; : 10000001,</span><br><span class="line">  &quot;description&quot; : &quot;This service definition authorizes all application urls that support HTTPS and IMAPS protocols.&quot;,</span><br><span class="line">  &quot;evaluationOrder&quot; : 10000,</span><br><span class="line">  &quot;logoutUrl&quot; : &quot;https:&#x2F;&#x2F;service.cas.com:8443&#x2F;logout&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Cas客户端"><a href="#Cas客户端" class="headerlink" title="Cas客户端"></a>Cas客户端</h2><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 我的2个Cas客户端与Cas服务端分布在3台服务器上</span><br><span class="line"># 服务端使用https,客户端使用http</span><br><span class="line">192.168.3.108 service.cas.com</span><br><span class="line">192.168.3.109 app1.cas.com</span><br><span class="line">192.168.3.110 app2.cas.com</span><br><span class="line"></span><br><span class="line"># 配置application.yml时尤其注意cas.client-host-url</span><br><span class="line">cas:</span><br><span class="line">    # cas服务端前缀,不是登录地址</span><br><span class="line">    server-url-prefix: https:&#x2F;&#x2F;service.cas.com:8443</span><br><span class="line">    # cas的登录地址</span><br><span class="line">    server-login-url: https:&#x2F;&#x2F;service.cas.com:8443&#x2F;login</span><br><span class="line">    # 当前客户端的地址&lt;我们这里使用域名&gt;</span><br><span class="line">    # a.配置为IP时,调用退出时只会退出域名访问</span><br><span class="line">    # b.配置为域名时,调用退出时只会退出IP访问</span><br><span class="line">    client-host-url: http:&#x2F;&#x2F;app1.cas.com:8081</span><br><span class="line">    # client-host-url: http:&#x2F;&#x2F;app2.cas.com:8082</span><br><span class="line">    # Ticket校验器</span><br><span class="line">    validation-type: CAS</span><br></pre></td></tr></table></figure>
<h3 id="代码-lt-以Client1为例-gt"><a href="#代码-lt-以Client1为例-gt" class="headerlink" title="代码&lt;以Client1为例&gt;"></a>代码&lt;以Client1为例&gt;</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Application类</span><br><span class="line">@SpringBootApplication</span><br><span class="line">@EnableCasClient</span><br><span class="line">public class Application &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        SpringApplication.run(Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 登出接口,退出时需要注意,将子系统的也进行退出,如果想继续重定向</span><br><span class="line">@RequestMapping(&quot;&#x2F;logout&quot;)</span><br><span class="line">public String logout(HttpSession session) &#123;</span><br><span class="line">    session.invalidate();</span><br><span class="line">    &#x2F;&#x2F; 调用当前系统的退出,会去调用其他子系统的logoutRedirect接口去注销(因为使用域名所以域名不会注销)</span><br><span class="line">    return &quot;redirect:https:&#x2F;&#x2F;cas.com:8443&#x2F;logout?service&#x3D;http:&#x2F;&#x2F;app2.cas.com:8082&#x2F;logoutRedirect&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@RequestMapping(&quot;&#x2F;logoutRedirect&quot;)</span><br><span class="line">public String logoutRedirect(HttpSession session)&#123;</span><br><span class="line">    session.invalidate();</span><br><span class="line">    &#x2F;&#x2F; 重定向去其他页面,如果客户端多的话,只能重定向到下一个客户端的注销页面了,形成一个重定向流</span><br><span class="line">    return &quot;redirect:https:&#x2F;&#x2F;service.cas.com:8443&quot;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 如果有N个系统怎么办,让用户关闭网页么?</span><br></pre></td></tr></table></figure>

<h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 新建Maven项目,添加依赖</span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot;</span><br><span class="line">         xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;com.dev&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;cas&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;packaging&gt;war&lt;&#x2F;packaging&gt;</span><br><span class="line">    &lt;parent&gt;</span><br><span class="line">        &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.0.0.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;parent&gt;</span><br><span class="line"></span><br><span class="line">    &lt;properties&gt;</span><br><span class="line">        &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class="line">        &lt;java.cas.client.version&gt;3.5.0&lt;&#x2F;java.cas.client.version&gt;</span><br><span class="line">    &lt;&#x2F;properties&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;javax.servlet&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;jstl&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.tomcat.embed&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;tomcat-embed-jasper&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;&#x2F;scope&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.3.0&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;druid-spring-boot-starter&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.1.9&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;net.unicon.cas&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;cas-client-autoconfig-support&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.4.0-GA&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;exclusions&gt;</span><br><span class="line">                &lt;exclusion&gt;</span><br><span class="line">                    &lt;groupId&gt;org.jasig.cas.client&lt;&#x2F;groupId&gt;</span><br><span class="line">                    &lt;artifactId&gt;cas-client-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;&#x2F;exclusion&gt;</span><br><span class="line">            &lt;&#x2F;exclusions&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.jasig.cas.client&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;cas-client-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;java.cas.client.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;spring-boot-starter-aop&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br><span class="line"></span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;!--打包后的项目名称  --&gt;</span><br><span class="line">        &lt;finalName&gt;cas-client&lt;&#x2F;finalName&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;&#x2F;source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;&#x2F;target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;&#x2F;encoding&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;!-- 打jar包的插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-jar-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;archive&gt;</span><br><span class="line">                        &lt;manifest&gt;</span><br><span class="line">                            &lt;addClasspath&gt;true&lt;&#x2F;addClasspath&gt;</span><br><span class="line">                            &lt;classpathPrefix&gt;lib&lt;&#x2F;classpathPrefix&gt;</span><br><span class="line">                            &lt;!-- 程序启动入口 --&gt;</span><br><span class="line">                            &lt;mainClass&gt;com.dev.cas.Application&lt;&#x2F;mainClass&gt;</span><br><span class="line">                        &lt;&#x2F;manifest&gt;</span><br><span class="line">                        &lt;manifestEntries&gt;</span><br><span class="line">                            &lt;!-- 将lib包抽到上一层文件夹中, classpathPrefix属性是包名--&gt;</span><br><span class="line">                            &lt;Class-Path&gt;.&#x2F;&lt;&#x2F;Class-Path&gt;</span><br><span class="line">                        &lt;&#x2F;manifestEntries&gt;</span><br><span class="line">                    &lt;&#x2F;archive&gt;</span><br><span class="line">                    &lt;excludes&gt;</span><br><span class="line">                        &lt;!-- 将config&#x2F;**抽离出来 --&gt;</span><br><span class="line">                        &lt;exclude&gt;config&#x2F;**&lt;&#x2F;exclude&gt;</span><br><span class="line">                    &lt;&#x2F;excludes&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;!-- not append assembly id in release file name --&gt;</span><br><span class="line">                    &lt;appendAssemblyId&gt;false&lt;&#x2F;appendAssemblyId&gt;</span><br><span class="line">                    &lt;descriptors&gt;</span><br><span class="line">                        &lt;!-- 注意这里的路径 --&gt;</span><br><span class="line">                        &lt;descriptor&gt;src&#x2F;main&#x2F;build&#x2F;package.xml&lt;&#x2F;descriptor&gt;</span><br><span class="line">                    &lt;&#x2F;descriptors&gt;</span><br><span class="line">                &lt;&#x2F;configuration&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;id&gt;make-assembly&lt;&#x2F;id&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;&#x2F;phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;single&lt;&#x2F;goal&gt;</span><br><span class="line">                        &lt;&#x2F;goals&gt;</span><br><span class="line">                    &lt;&#x2F;execution&gt;</span><br><span class="line">                &lt;&#x2F;executions&gt;</span><br><span class="line">            &lt;&#x2F;plugin&gt;</span><br><span class="line">        &lt;&#x2F;plugins&gt;</span><br><span class="line">    &lt;&#x2F;build&gt;</span><br><span class="line">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>cas</tag>
      </tags>
  </entry>
  <entry>
    <title>CURL和WGET命令详解</title>
    <url>/2016/08/30/CURL%E5%92%8CWGET%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>简单了解一下CURL和WGET命令的使用,毕竟也是常用到的命令</p>
</blockquote>
<span id="more"></span>

<h2 id="CURL命令"><a href="#CURL命令" class="headerlink" title="CURL命令"></a>CURL命令</h2><p>curl是一种命令行工具，作用是发出网络请求，然后得到和提取数据<br>显示在”标准输出”（stdout）上面。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看网页源码</span><br><span class="line">curl www.baidu.com</span><br><span class="line"></span><br><span class="line"># 保存网页或资源文件</span><br><span class="line">curl -o test.html www.baidu.com</span><br><span class="line">curl -o test.jpg https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 自动跳转</span><br><span class="line">curl -L www.baidu.com</span><br><span class="line"></span><br><span class="line"># 获取头信息(连带网页源码)-I则只显示头信息</span><br><span class="line">curl -i www.baidu.com</span><br><span class="line"></span><br><span class="line"># 显示通信过程</span><br><span class="line">curl -v www.baidu.com</span><br><span class="line">curl --trace output.txt www.baidu.com</span><br><span class="line">curl --trace-ascii output.txt www.baidu.com</span><br><span class="line"></span><br><span class="line"># 发送GET</span><br><span class="line">curl https:&#x2F;&#x2F;www.baidu.com&#x2F;s?wd&#x3D;wget</span><br><span class="line"></span><br><span class="line"># 发送POST</span><br><span class="line">curl -X POST --data &quot;wd&#x3D;wget&quot; https:&#x2F;&#x2F;www.baidu.com&#x2F;s</span><br><span class="line">curl -X POST --data-urlencode &quot;wd&#x3D;wget&quot; https:&#x2F;&#x2F;www.baidu.com&#x2F;s</span><br><span class="line"></span><br><span class="line"># 支持HEAD GET POST PUT DELETE</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="WGET命令"><a href="#WGET命令" class="headerlink" title="WGET命令"></a>WGET命令</h2><p>wget是一个下载文件的工具，它用在命令行下。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 抓取整站</span><br><span class="line">wget -r -p -np -k -E http:&#x2F;&#x2F;www.xxx.com </span><br><span class="line"></span><br><span class="line"># 抓取第一级</span><br><span class="line">wget -l 1 -p -np -k http:&#x2F;&#x2F;www.xxx.com </span><br><span class="line"></span><br><span class="line">-r 递归抓取</span><br><span class="line">-k 抓取之后修正链接，适合本地浏览</span><br><span class="line"></span><br><span class="line"># 下载文件</span><br><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 下载文件并自定义名称</span><br><span class="line">wget -O test.jpg https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 限速下载</span><br><span class="line">wget -limit-rate&#x3D;300k https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 断点续传(服务器支持断点续传)</span><br><span class="line">wget -c https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 后台下载</span><br><span class="line">wget -b https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line">tail -f wget-log</span><br><span class="line"></span><br><span class="line"># 伪装代理下载</span><br><span class="line">wget -user-agent&#x3D;&quot;Mozilla&#x2F;5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit&#x2F;534.16 (KHTML, like Gecko) Chrome&#x2F;10.0.648.204 Safari&#x2F;534.16&quot; https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 测试下载链接</span><br><span class="line">wget -spider https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 重试次数(默认20次)</span><br><span class="line">wget -tries&#x3D;40 https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 下载多个文件(通过一份链接文件)</span><br><span class="line">wget -i urlList.txt</span><br><span class="line"></span><br><span class="line"># 镜像网站</span><br><span class="line">wget -mirror -p -convert-links -P .&#x2F;LOCAL URL </span><br><span class="line"></span><br><span class="line"># 过滤指定格式下载</span><br><span class="line">wget --reject&#x3D;gif https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 下载指定格式文件</span><br><span class="line">wget -r -A.pdf https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 下载信息存入日志文件</span><br><span class="line">wget -o download.log https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line"></span><br><span class="line"># 限制总下载文件大小(对单个文件下载不起作用)</span><br><span class="line">wget -Q5m -i urlList.txt </span><br><span class="line"></span><br><span class="line"># FTP链接下载</span><br><span class="line">wget -ftp https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br><span class="line">wget -ftp-user&#x3D;USERNAME -ftp-password&#x3D;PASSWORD https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;jxeditor&#x2F;jxeditor.github.io&#x2F;hexo&#x2F;logo.jpg</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS安装Oracle数据库</title>
    <url>/2020/04/15/CentOS%E5%AE%89%E8%A3%85Oracle%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<blockquote>
<p>快速搭建Oracle数据库</p>
</blockquote>
<span id="more"></span>

<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum localinstall -y oracle-database-preinstall-19c-1.0-1.el7.x86_64.rpm</span><br><span class="line">yum localinstall -y oracle-database-ee-19c-1.0-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line"># 修改字符集</span><br><span class="line">vi &#x2F;etc&#x2F;init.d&#x2F;oracledb_ORCLCDB-19c</span><br><span class="line">export CHARSET&#x3D;ZHS16GBK</span><br><span class="line"></span><br><span class="line"># 等待初始化完成</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;oracledb_ORCLCDB-19c configure</span><br><span class="line"></span><br><span class="line">passwd oracle</span><br><span class="line">su oracle</span><br><span class="line"></span><br><span class="line">vi &#x2F;home&#x2F;oracle&#x2F;.bashrc</span><br><span class="line">export ORACLE_HOME&#x3D;&#x2F;opt&#x2F;oracle&#x2F;product&#x2F;19c&#x2F;dbhome_1</span><br><span class="line">export PATH&#x3D;$PATH:$ORACLE_HOME&#x2F;bin</span><br><span class="line">export ORACLE_SID&#x3D;ORCLCDB</span><br><span class="line"></span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line"></span><br><span class="line"># 创建自动启动pdb的触发器</span><br><span class="line">CREATE TRIGGER open_all_pdbs</span><br><span class="line">AFTER STARTUP ON DATABASE</span><br><span class="line">BEGIN</span><br><span class="line">EXECUTE IMMEDIATE &#39;alter pluggable database all open&#39;</span><br><span class="line">END open_all_pdbs;</span><br><span class="line">&#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用操作"><a href="#使用操作" class="headerlink" title="使用操作"></a>使用操作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su oracle</span><br><span class="line"># 启动监听</span><br><span class="line">lsnrctl start</span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">startup</span><br><span class="line"></span><br><span class="line"># 修改用户密码</span><br><span class="line">select username from dba_users where account_status&#x3D;&#39;OPEN&#39;;</span><br><span class="line">alter user sys identified by 123456;</span><br><span class="line">alter user system identified by 123456;</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">有可能出现乱码的情况</span><br><span class="line">select userenv(&#39;language&#39;) from dual;</span><br><span class="line">查看字符集是否有问题</span><br><span class="line">在.bashrc文件中添加</span><br><span class="line">export NLS_LANG&#x3D;AMERICAN_AMERICA.ZHS16GBK </span><br><span class="line"></span><br><span class="line">使用Navicat登陆时报错,需要下载对应版本的instantclient</span><br><span class="line">Navicat-&gt;工具-&gt;选项-&gt;环境-&gt;OCI环境-&gt;指定对应版本的oci.dll</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="修改监听端口"><a href="#修改监听端口" class="headerlink" title="修改监听端口"></a>修改监听端口</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 默认1521,修改$ORACLE_HOME&#x2F;network&#x2F;admin&#x2F;目录下的listener.ora和tnsnames.ora</span><br><span class="line">cd &#x2F;opt&#x2F;oracle&#x2F;product&#x2F;19c&#x2F;dbhome_1&#x2F;network&#x2F;admin</span><br><span class="line">vi listener.ora</span><br><span class="line">(ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; master)(PORT &#x3D; 2345))</span><br><span class="line">(ADDRESS &#x3D; (PROTOCOL &#x3D; IPC)(KEY &#x3D; EXTPROC2345))</span><br><span class="line"></span><br><span class="line">vi tnsnames.ora</span><br><span class="line">ORCLCDB &#x3D;</span><br><span class="line">  (DESCRIPTION &#x3D;</span><br><span class="line">    (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; master)(PORT &#x3D; 2345))</span><br><span class="line">    (CONNECT_DATA &#x3D;</span><br><span class="line">      (SERVER &#x3D; DEDICATED)</span><br><span class="line">      (SERVICE_NAME &#x3D; ORCLCDB)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">LISTENER_ORCLCDB &#x3D;</span><br><span class="line">  (ADDRESS &#x3D; (PROTOCOL &#x3D; TCP)(HOST &#x3D; master)(PORT &#x3D; 2345))</span><br><span class="line">  </span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">show parameter local_listener;</span><br><span class="line">alter system set local_listener&#x3D;&quot;(address&#x3D;(protocol&#x3D;tcp)(host&#x3D;master)(port&#x3D;2345))&quot;</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line">lsnrctl start</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title>CentOS的一些小问题</title>
    <url>/2016/07/24/CentOS%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>记录CentOS上发生的一些稀奇古怪的事情</p>
</blockquote>
<span id="more"></span>

<h2 id="自制yum仓库"><a href="#自制yum仓库" class="headerlink" title="自制yum仓库"></a>自制yum仓库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 挂载镜像的仓库</span><br><span class="line">mkidr &#x2F;mnt&#x2F;iso</span><br><span class="line">mount -o loop *.iso &#x2F;mnt&#x2F;iso</span><br><span class="line">vi &#x2F;etc&#x2F;yum.repos.d&#x2F;file.repo</span><br><span class="line">[base]</span><br><span class="line">name&#x3D;rhel6repo</span><br><span class="line">baseurl&#x3D;file:&#x2F;&#x2F;&#x2F;mnt&#x2F;iso</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgckeck&#x3D;0</span><br><span class="line">gpgkey&#x3D;file:&#x2F;&#x2F;&#x2F;mnt&#x2F;iso&#x2F;RPM-GPG-KEY-redhat-release</span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line"></span><br><span class="line"># 自制仓库</span><br><span class="line">yum install --downloadonly --downloaddir&#x3D;&#x2F;temp&#x2F; mysql-community-server</span><br><span class="line">yum install createrepo -y</span><br><span class="line"></span><br><span class="line">createrepo &#x2F;temp&#x2F;</span><br><span class="line"></span><br><span class="line">此时已经可以使用file:&#x2F;&#x2F;&#x2F;temp&#x2F;的形式添加本地仓库</span><br><span class="line"></span><br><span class="line"># 内网仓库</span><br><span class="line">yum install httpd</span><br><span class="line">cd &#x2F;var&#x2F;www&#x2F;html&#x2F;</span><br><span class="line">mkdir centos&#x2F;6&#x2F;os&#x2F;x86_64&#x2F;</span><br><span class="line"># 用mv移动会出现403问题</span><br><span class="line">cp -r &#x2F;temp&#x2F;* &#x2F;var&#x2F;www&#x2F;html&#x2F;centos&#x2F;6&#x2F;os&#x2F;x86_64&#x2F;</span><br><span class="line">vi  client.repo</span><br><span class="line">[client]</span><br><span class="line">name&#x3D;httpServer</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;192.168.3.201&#x2F;centos&#x2F;$releasever&#x2F;os&#x2F;$basearch</span><br><span class="line">gpgcheck&#x3D;0</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="not-in-the-sudoers-file"><a href="#not-in-the-sudoers-file" class="headerlink" title="not in the sudoers file"></a>not in the sudoers file</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解决not in the sudoers file</span></span><br><span class="line">chmod u+w /etc/sudoers</span><br><span class="line">gedit /etc/sudoers</span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow root to ruan any commands anywhere</span></span><br><span class="line">xx ALL=(ALL)  ALL</span><br><span class="line"></span><br><span class="line">chmod u-w /etc/sudoers</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="主机能ping通虚拟机-虚拟机不能ping通主机"><a href="#主机能ping通虚拟机-虚拟机不能ping通主机" class="headerlink" title="主机能ping通虚拟机,虚拟机不能ping通主机"></a>主机能ping通虚拟机,虚拟机不能ping通主机</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主机防火墙开启,禁ping</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="NAT固定IP连接外网"><a href="#NAT固定IP连接外网" class="headerlink" title="NAT固定IP连接外网"></a>NAT固定IP连接外网</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 虚拟网络编辑器NAT取消勾选使用本地DHCP,NAT设置192.168.6.1网关</span><br><span class="line"></span><br><span class="line"># 将网络适配器IP调为192.168.6.2,DNS为114.114.114.114</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens**</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;192.168.6.134</span><br><span class="line">DNS1&#x3D;114.114.114.114</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">GATEWAY&#x3D;192.168.6.1</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="动态获取IP"><a href="#动态获取IP" class="headerlink" title="动态获取IP"></a>动态获取IP</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0</span><br><span class="line"></span><br><span class="line">DEVICE&#x3D;eth0</span><br><span class="line">BOOTPROTO&#x3D;dhcp</span><br><span class="line"># HWADDR&#x3D;00:0C:29:77:C3:65</span><br><span class="line">IPV6INIT&#x3D;yes</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line"># UUID&#x3D;2e3a08e6-714a-4bbc-a78b-71e435d281e6</span><br><span class="line"></span><br><span class="line"># 重启网络服务</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;network restart</span><br><span class="line">或者service network restart</span><br><span class="line"># 自动获取IP地址命令</span><br><span class="line">dhclient</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="XShell上传下载"><a href="#XShell上传下载" class="headerlink" title="XShell上传下载"></a>XShell上传下载</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install lrzsz</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker的简单入门</title>
    <url>/2017/05/31/Docker%E7%9A%84%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<blockquote>
<p>Docker入门级教程</p>
</blockquote>
<span id="more"></span>

<h2 id="1-组成"><a href="#1-组成" class="headerlink" title="1.组成"></a>1.组成</h2><ul>
<li>镜像</li>
<li>容器</li>
<li>仓库</li>
</ul>
<hr>
<h2 id="2-命令"><a href="#2-命令" class="headerlink" title="2.命令"></a>2.命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- 获取镜像 :</span><br><span class="line">docker pull 镜像名:版本号(eg:docker pull ubuntu:12.04)</span><br><span class="line"></span><br><span class="line">- 使用镜像创建容器(--name自定义容器名,hostPort宿主机端口,port容器内部端口) :</span><br><span class="line">docker run --name name -p hostPort:port -d image</span><br><span class="line"></span><br><span class="line">- 查看当前运行的容器(加上-a查看所有容器) :</span><br><span class="line">docker ps</span><br><span class="line"></span><br><span class="line">- 停止NAMES容器 :</span><br><span class="line">docker stop NAMES</span><br><span class="line"></span><br><span class="line">- 删除NAMES容器(-f强制) :</span><br><span class="line">docker rm NAMES</span><br><span class="line"></span><br><span class="line">- 查看容器信息 :</span><br><span class="line">docker container ps</span><br><span class="line"></span><br><span class="line">- 查看容器输出信息 :</span><br><span class="line">docker logs NAMES</span><br><span class="line"></span><br><span class="line">- 进入容器 :</span><br><span class="line">docker attach NAMES</span><br><span class="line"></span><br><span class="line">- 查看镜像 :</span><br><span class="line">docker images</span><br><span class="line"></span><br><span class="line">- 删除NAMES镜像 :</span><br><span class="line">docker rmi NAMES</span><br><span class="line"></span><br><span class="line">- 搜索镜像 :</span><br><span class="line">docker search centos</span><br><span class="line"></span><br><span class="line">- 命令行进行容器 :</span><br><span class="line">docker exec -it [容器名称]  &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">- 运行tomcat容器</span><br><span class="line">docker run --name mytomcat -p 8099:8080 -d tomcat</span><br><span class="line"></span><br><span class="line">- 运行mysql容器并挂载目录</span><br><span class="line">docker run --name mymysql -e MYSQL_ROOT_PASSWORD&#x3D;123456 -d -p 3306:3306 -v &#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql mysql</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-创建镜像"><a href="#3-创建镜像" class="headerlink" title="3.创建镜像"></a>3.创建镜像</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- mkdir sinatra</span><br><span class="line">- cd sinatra</span><br><span class="line">- vi Dockerfile</span><br><span class="line"># This is a comment</span><br><span class="line">FROM ubuntu:14.04</span><br><span class="line">MAINTAINER Docker jkillers &lt;980813351@qq.com&gt;</span><br><span class="line">RUN apt-get -qq update</span><br><span class="line">RUN gem install sinatra</span><br><span class="line"></span><br><span class="line">- docker build -t&#x3D;&quot;playcrab&#x2F;sinatra:v1&quot; .</span><br><span class="line"> -t标记来添加tag,指定新的镜像的用户信息</span><br><span class="line"> &quot;.&quot;是Dockerfile所在目录(当前目录)</span><br><span class="line"> 一个镜像不能超过127层</span><br><span class="line"> ADD命令复制本地文件到镜像</span><br><span class="line"> EXPOSE命令向外部开放端口</span><br><span class="line"> CMD描述容器启动后运行的程序</span><br><span class="line"></span><br><span class="line">ADD myApp &#x2F;var&#x2F;www</span><br><span class="line">EXPOSE httpd port</span><br><span class="line">CMD [&quot;&#x2F;usr&#x2F;sbin&#x2F;apachectl&quot;,&quot;-D&quot;,&quot;FOREGROUND&quot;]</span><br><span class="line"></span><br><span class="line">- docker run -t -i playcrab&#x2F;sinatra:v1 &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-导入导出上传移除镜像"><a href="#4-导入导出上传移除镜像" class="headerlink" title="4.导入导出上传移除镜像"></a>4.导入导出上传移除镜像</h2><blockquote>
<p>导入镜像</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">先下载一个镜像在本地</span><br><span class="line">docker load --input ubuntu_14.04.tar</span><br><span class="line">docker load &lt; ubuntu_14.04.tar</span><br></pre></td></tr></table></figure>
<blockquote>
<p>导出镜像</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker save -o ubuntu_14.04.tar ubuntu:14.04</span><br></pre></td></tr></table></figure>
<blockquote>
<p>上传镜像</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker push playcrab&#x2F;sinatra</span><br></pre></td></tr></table></figure>
<blockquote>
<p>移除镜像</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker rmi playcrab&#x2F;sinatra</span><br><span class="line">在删除镜像之前要先删除依赖于这个镜像的容器</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="5-docker-run容器的创建"><a href="#5-docker-run容器的创建" class="headerlink" title="5.docker run容器的创建"></a>5.docker run容器的创建</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -t -i ubuntu:14.04 &#x2F;bin&#x2F;bash</span><br><span class="line"> -t让Docker分配一个伪终端并绑定到容器的标准输出上</span><br><span class="line"> -i让容器的标准输入保持打开</span><br><span class="line"> 交互模式下,用户可以通过所创建的终端来输入命令</span><br><span class="line"></span><br><span class="line">docker run -d ubuntu:14.04 &#x2F;bin&#x2F;sh -c &quot;while true; do echo hello world; sleep 1; done&quot; </span><br><span class="line"> -d让Docker容器在后台以守护态的形式运行</span><br><span class="line"> - 返回一个唯一ID</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="6-DockerFile"><a href="#6-DockerFile" class="headerlink" title="6.DockerFile"></a>6.DockerFile</h2><blockquote>
<h4 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h4></blockquote>
<ul>
<li>基础镜像信息</li>
<li>维护者信息</li>
<li>镜像操作指令</li>
<li>容器启动时执行指令</li>
</ul>
<blockquote>
<h4 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM:</span><br><span class="line">    第一条指令必须是FROM指令,如果在同一个DockerFile文件中创建多个镜像,可以使用多个FROM指令</span><br><span class="line">MAINTAINER:</span><br><span class="line">    指定维护者信息</span><br><span class="line">RUN:</span><br><span class="line">    每条RUN指令将在当前镜像基础上执行指定命令,并提交为新的镜像.命令较长时可以使用\换行</span><br><span class="line">CMD:</span><br><span class="line">    指定启动容器时执行的命令,每个DockerFile只能有一条CMD命令.如果指定了多条,只有最后一条被执行</span><br><span class="line">EXPOSE:</span><br><span class="line">    Docker主机分配一个端口转发到指定的端口</span><br><span class="line">ENV:</span><br><span class="line">    指定环境变量,会被后续RUN指令使用.并在容器运行时保持</span><br><span class="line">ADD:</span><br><span class="line">    复制指定的&lt;src&gt;到容器的&lt;dest&gt;</span><br><span class="line">COPY:</span><br><span class="line">    复制指定的&lt;src&gt;到容器的&lt;dest&gt;</span><br><span class="line">ENTRYPOINT:</span><br><span class="line">    指定启动容器后执行的命令,每个DockerFile只能有一条ENTRYPOINT命令.如果指定了多条,只有最后一条被执行</span><br><span class="line">VOLUME:</span><br><span class="line">    创建可以从本地主机或其他容器挂载的挂载点,一般用来存放数据库和需要保持的数据等</span><br><span class="line">USER:</span><br><span class="line">    指定运行容器时的用户名或UID,后续的RUN也会使用指定用户</span><br><span class="line">WORKDIR:</span><br><span class="line">    为后续的RUN,CMD,ENTRYPOINT指令配置工作目录.可以使用多个WORKDIR指令,后续命令如果参数是相对路径,则会基于之前命令指定的路径</span><br><span class="line">ONBUILD:</span><br><span class="line">    配置当所创建的镜像作为其它新创建镜像的基础镜像时,所执行的操作指令</span><br></pre></td></tr></table></figure>

<blockquote>
<h4 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h4></blockquote>
<ul>
<li>名字空间(NameSpaces)</li>
<li>控制组(ControlGroups)</li>
<li>Union文件系统(UnionFileSystems)</li>
<li>容器格式(ContainerFormat)</li>
</ul>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Doris的简单介绍</title>
    <url>/2020/11/30/Doris%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<blockquote>
<p>初步了解Doris的原理,与Kylin满足离线多维分析需求相对,Doris满足实时多维分析需求<a href="https://blog.bcmeng.com/post/apache-doris-query.html">传送门</a>.</p>
</blockquote>
<span id="more"></span>

<h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">FE(Frontend)</span><br><span class="line">    功能:</span><br><span class="line">        存储,维护集群元数据</span><br><span class="line">        接收,解析查询请求,规划查询计划,调度查询执行,生成物理计划,返回查询结果</span><br><span class="line">    角色:</span><br><span class="line">        Leader,Follower(高可用)</span><br><span class="line">        Observer(扩展查询节点,只参与读取)</span><br><span class="line">    元数据:</span><br><span class="line">        Paxos协议以及Memory+CheckPoint+Journal机制保证元数据高性能高可靠</span><br><span class="line">        元数据更新(写入磁盘日志文件,写入内存,定期CheckPoint到本地磁盘)</span><br><span class="line">        </span><br><span class="line">BE(Backend)</span><br><span class="line">    功能:</span><br><span class="line">        存储物理数据</span><br><span class="line">        分布式执行查询</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">步骤:</span><br><span class="line">    1.用户通过MySQL(MySQL协议)客户端向Doris发送查询</span><br><span class="line">    2.Doris接收到请求,每个请求封装成一个ConnectContext</span><br><span class="line">    3.ConnectScheduler维护一个线程池,每个ConnectContext在一个线程中由ConnectProcessor(负责查询的处理,审计,返回查询结果给客户端)进程处理</span><br><span class="line">    4.ConnectProcessor进行SQL的Parse(Java CUP Parser: sql_parser.cup),输入是SQL字符串,输出为AST(StatementBase)</span><br><span class="line">        ps:一个SelectStmt由SelectList,FromClause,WherePredicate,GroupByClause,HavingPredicate,OrderByElement,LimitElement组成</span><br><span class="line">    5.StmtExecutor具体负责查询的执行,对AST进行语法和语义分析</span><br><span class="line">        a.检查并绑定Cluster,DataBase,Table,Column等元信息</span><br><span class="line">        b.SQL合法性检查:窗口函数不能distinct,HLL和Bitmap列不能sum/count,<span class="built_in">where</span>中不能有grouping操作</span><br><span class="line">        c.SQL重写</span><br><span class="line">        d.Table和Column的别名处理</span><br><span class="line">        e.Tuple,Slot,表达式分配唯一的ID</span><br><span class="line">        f.函数参数的合法性检测</span><br><span class="line">        g.表达式替换</span><br><span class="line">        h.类型检查,类型替换</span><br><span class="line">    6.ExprRewriter根据ExprRewriteRule进行重写(重写成功会再次触发语法语义分析)</span><br><span class="line">    7.生成单机执行Plan(单机Plan由SingleNodePlanner执行,输入是AST,输出是物理执行Plan)</span><br><span class="line">        a.Slot物化</span><br><span class="line">        b.投影下推: 只会读取必须读取的列</span><br><span class="line">        c.谓词下推: 满足语义的前提下过滤条件尽可能下推到Scan节点</span><br><span class="line">        d.分区,分桶裁剪</span><br><span class="line">        e.Join Reorder: 对于InnerJoin,根据行数调整表的顺序,大表在前</span><br><span class="line">        f.Sort+Limit优化成TopN</span><br><span class="line">        g.MaterializedView选择</span><br><span class="line">    8.DistributedPlanner生成PlanFragment树(1个PlanFragment封装了在一台机器上对同一数据集的操作逻辑)</span><br><span class="line">    9.Coordinator会负责PlanFragment的执行实例生成</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>doris</tag>
      </tags>
  </entry>
  <entry>
    <title>ES压测工具使用esrally</title>
    <url>/2020/06/22/ES%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8esrally/</url>
    <content><![CDATA[<blockquote>
<p>对ES集群的性能进行测试,判断程序是否有优化的空间</p>
</blockquote>
<span id="more"></span>

<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>Git-1.8以后</li>
<li>Python3</li>
</ul>
<hr>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip3 install esrally</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="开始压测"><a href="#开始压测" class="headerlink" title="开始压测"></a>开始压测</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 离线环境对目标ES集群进行压力测试</span></span><br><span class="line">esrally --offline --distribution-version=7.2.0 --pipeline=benchmark-only --target-hosts=hosts:9200,hosts:9200,hosts:9200</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意</span></span><br><span class="line">压测数据过大,下载速度慢可以在网上找对应下载好的数据包</span><br><span class="line">对于本地数据,需要修改/root/.rally/benchmarks/tracks/default/geonames/track.json文件</span><br><span class="line">离线数据包在/root/.rally/benchmarks/data/geonames/documents-2.json</span><br><span class="line">&#123;% import <span class="string">&quot;rally.helpers&quot;</span> as rally with context %&#125;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;version&quot;</span>: 2,</span><br><span class="line">  <span class="string">&quot;description&quot;</span>: <span class="string">&quot;POIs from Geonames&quot;</span>,</span><br><span class="line">  <span class="string">&quot;indices&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;name&quot;</span>: <span class="string">&quot;geonames&quot;</span>,</span><br><span class="line">      <span class="string">&quot;body&quot;</span>: <span class="string">&quot;index.json&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;corpora&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;name&quot;</span>: <span class="string">&quot;geonames&quot;</span>,</span><br><span class="line">      <span class="string">&quot;documents&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="string">&quot;source-file&quot;</span>: <span class="string">&quot;documents-2.json&quot;</span>,</span><br><span class="line">          <span class="string">&quot;document-count&quot;</span>: 11396505,</span><br><span class="line">          <span class="string">&quot;uncompressed-bytes&quot;</span>: 3547614383</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;operations&quot;</span>: [</span><br><span class="line">    &#123;&#123; rally.collect(parts=<span class="string">&quot;operations/*.json&quot;</span>) &#125;&#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;challenges&quot;</span>: [</span><br><span class="line">    &#123;&#123; rally.collect(parts=<span class="string">&quot;challenges/*.json&quot;</span>) &#125;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">压测操作过多,可以修改/root/.rally/benchmarks/tracks/default/geonames/</span><br><span class="line">下challenges和operations文件夹下json文件</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="压测结果"><a href="#压测结果" class="headerlink" title="压测结果"></a>压测结果</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 主要关注Throughput</span><br><span class="line"># 有时会没有该指标,怀疑是写入超过10wdoc&#x2F;s</span><br><span class="line">[WARNING] No throughput metrics available for [index-append]. Likely cause: The benchmark ended already during warmup.</span><br><span class="line">------------------------------------------------------</span><br><span class="line">    _______             __   _____</span><br><span class="line">   &#x2F; ____(_)___  ____ _&#x2F; &#x2F;  &#x2F; ___&#x2F;_________  ________</span><br><span class="line">  &#x2F; &#x2F;_  &#x2F; &#x2F; __ \&#x2F; __ &#96;&#x2F; &#x2F;   \__ \&#x2F; ___&#x2F; __ \&#x2F; ___&#x2F; _ \</span><br><span class="line"> &#x2F; __&#x2F; &#x2F; &#x2F; &#x2F; &#x2F; &#x2F; &#x2F;_&#x2F; &#x2F; &#x2F;   ___&#x2F; &#x2F; &#x2F;__&#x2F; &#x2F;_&#x2F; &#x2F; &#x2F;  &#x2F;  __&#x2F;</span><br><span class="line">&#x2F;_&#x2F;   &#x2F;_&#x2F;_&#x2F; &#x2F;_&#x2F;\__,_&#x2F;_&#x2F;   &#x2F;____&#x2F;\___&#x2F;\____&#x2F;_&#x2F;   \___&#x2F;</span><br><span class="line">------------------------------------------------------</span><br><span class="line">            </span><br><span class="line">|                                                         Metric |         Task |       Value |   Unit |</span><br><span class="line">|---------------------------------------------------------------:|-------------:|------------:|-------:|</span><br><span class="line">|                     Cumulative indexing time of primary shards |              |     185.051 |    min |</span><br><span class="line">|             Min cumulative indexing time across primary shards |              |           0 |    min |</span><br><span class="line">|          Median cumulative indexing time across primary shards |              |           0 |    min |</span><br><span class="line">|             Max cumulative indexing time across primary shards |              |     8.03388 |    min |</span><br><span class="line">|            Cumulative indexing throttle time of primary shards |              |           0 |    min |</span><br><span class="line">|    Min cumulative indexing throttle time across primary shards |              |           0 |    min |</span><br><span class="line">| Median cumulative indexing throttle time across primary shards |              |           0 |    min |</span><br><span class="line">|    Max cumulative indexing throttle time across primary shards |              |           0 |    min |</span><br><span class="line">|                        Cumulative merge time of primary shards |              |     176.086 |    min |</span><br><span class="line">|                       Cumulative merge count of primary shards |              |       77193 |        |</span><br><span class="line">|                Min cumulative merge time across primary shards |              |           0 |    min |</span><br><span class="line">|             Median cumulative merge time across primary shards |              |           0 |    min |</span><br><span class="line">|                Max cumulative merge time across primary shards |              |       7.611 |    min |</span><br><span class="line">|               Cumulative merge throttle time of primary shards |              |     4.00247 |    min |</span><br><span class="line">|       Min cumulative merge throttle time across primary shards |              |           0 |    min |</span><br><span class="line">|    Median cumulative merge throttle time across primary shards |              |           0 |    min |</span><br><span class="line">|       Max cumulative merge throttle time across primary shards |              |    0.718383 |    min |</span><br><span class="line">|                      Cumulative refresh time of primary shards |              |     162.916 |    min |</span><br><span class="line">|                     Cumulative refresh count of primary shards |              |      790587 |        |</span><br><span class="line">|              Min cumulative refresh time across primary shards |              |           0 |    min |</span><br><span class="line">|           Median cumulative refresh time across primary shards |              |           0 |    min |</span><br><span class="line">|              Max cumulative refresh time across primary shards |              |     5.61482 |    min |</span><br><span class="line">|                        Cumulative flush time of primary shards |              |     94.4581 |    min |</span><br><span class="line">|                       Cumulative flush count of primary shards |              | 3.46617e+06 |        |</span><br><span class="line">|                Min cumulative flush time across primary shards |              |           0 |    min |</span><br><span class="line">|             Median cumulative flush time across primary shards |              |     0.00045 |    min |</span><br><span class="line">|                Max cumulative flush time across primary shards |              |      1.5087 |    min |</span><br><span class="line">|                                             Total Young Gen GC |              |      22.105 |      s |</span><br><span class="line">|                                               Total Old Gen GC |              |       0.341 |      s |</span><br><span class="line">|                                                     Store size |              |     82.7575 |     GB |</span><br><span class="line">|                                                  Translog size |              |     6.60301 |     GB |</span><br><span class="line">|                                         Heap used for segments |              |     394.897 |     MB |</span><br><span class="line">|                                       Heap used for doc values |              |      60.712 |     MB |</span><br><span class="line">|                                            Heap used for terms |              |     302.566 |     MB |</span><br><span class="line">|                                            Heap used for norms |              |    0.105835 |     MB |</span><br><span class="line">|                                           Heap used for points |              |     8.01107 |     MB |</span><br><span class="line">|                                    Heap used for stored fields |              |     23.5017 |     MB |</span><br><span class="line">|                                                  Segment count |              |       23151 |        |</span><br><span class="line">|                                                 Min Throughput | index-append |     89289.5 | docs&#x2F;s |</span><br><span class="line">|                                              Median Throughput | index-append |     89984.6 | docs&#x2F;s |</span><br><span class="line">|                                                 Max Throughput | index-append |       90241 | docs&#x2F;s |</span><br><span class="line">|                                        50th percentile latency | index-append |     305.877 |     ms |</span><br><span class="line">|                                        90th percentile latency | index-append |     363.315 |     ms |</span><br><span class="line">|                                        99th percentile latency | index-append |     396.602 |     ms |</span><br><span class="line">|                                       100th percentile latency | index-append |     419.911 |     ms |</span><br><span class="line">|                                   50th percentile service time | index-append |     305.877 |     ms |</span><br><span class="line">|                                   90th percentile service time | index-append |     363.315 |     ms |</span><br><span class="line">|                                   99th percentile service time | index-append |     396.602 |     ms |</span><br><span class="line">|                                  100th percentile service time | index-append |     419.911 |     ms |</span><br><span class="line">|                                                     error rate | index-append |           0 |      % |</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------</span><br><span class="line">[INFO] SUCCESS (took 178 seconds)</span><br><span class="line">---------------------------------</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title>ELK安装搭建以及常见问题</title>
    <url>/2019/02/21/ELK%E5%AE%89%E8%A3%85%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<blockquote>
<p>对于ELK一系列环境搭建</p>
</blockquote>
<span id="more"></span>

<h2 id="安装包准备"><a href="#安装包准备" class="headerlink" title="安装包准备"></a>安装包准备</h2><ul>
<li>elasticsearch-6.6.1.tar.gz</li>
<li>kibana-6.6.1-linux-x86_64.tar.gz</li>
<li>logstash-6.6.1.tar.gz</li>
</ul>
<hr>
<h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建用户</span><br><span class="line">useradd elsearch</span><br><span class="line">tar xf elasticsearch-6.6.1.tar.gz</span><br><span class="line">vi elasticsearch-6.6.1&#x2F;config&#x2F;elasticsearch.yml</span><br><span class="line">node.name: node-n # n变化,每台不一样</span><br><span class="line">bootstrap.memory_lock: false # 不设置会报错</span><br><span class="line">bootstrap.system_call_filter: false</span><br><span class="line">network.host: 192.168.1.128</span><br><span class="line">discovery.zen.ping.unicast.hosts: [&quot;hadoop01&quot;, &quot;hadoop02&quot;, &quot;hadoop03&quot;]</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;security&#x2F;limits.conf</span><br><span class="line">*       soft    nofile  65536</span><br><span class="line">*       hard    nofile  131072</span><br><span class="line">*       soft    nproc   2048</span><br><span class="line">*       hard    nproc   4096</span><br><span class="line">vi &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;90-nproc.conf</span><br><span class="line">*          soft    nproc     4096</span><br><span class="line">root       soft    nproc     unlimited</span><br><span class="line">vi &#x2F;etc&#x2F;sysctl.conf </span><br><span class="line">vm.max_map_count &#x3D; 655360</span><br><span class="line">sysctl -p</span><br><span class="line"># 后台启动</span><br><span class="line">elasticsearch-6.6.1&#x2F;bin&#x2F;elasticsearch -d</span><br><span class="line">http:&#x2F;&#x2F;hadoop01:9200</span><br></pre></td></tr></table></figure>
<h3 id="Kibana"><a href="#Kibana" class="headerlink" title="Kibana"></a>Kibana</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar xf kibana-6.6.1-linux-x86_64.tar.gz</span><br><span class="line">mv kibana-6.6.1-linux-x86_64 kibana-6.6.1</span><br><span class="line">vi kibana-6.6.1&#x2F;config&#x2F;kibana.yml</span><br><span class="line">server.host: &quot;192.168.1.129&quot;</span><br><span class="line">elasticsearch.hosts: [&quot;http:&#x2F;&#x2F;hadoop01:9200&quot;, &quot;http:&#x2F;&#x2F;hadoop02:9200&quot;, &quot;http:&#x2F;&#x2F;hadoop03:9200&quot;]</span><br><span class="line">kibana-6.6.1&#x2F;bin&#x2F;kibana</span><br><span class="line"># 后台启动</span><br><span class="line">nohup kibana-6.6.1&#x2F;bin&#x2F;kibana &amp;</span><br><span class="line">http:&#x2F;&#x2F;hadoop02:5601</span><br></pre></td></tr></table></figure>
<h3 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -zxvf logstash-2.3.1.tar.gz</span><br><span class="line"># 设置采集配置</span><br><span class="line"># 命令行形式</span><br><span class="line">bin&#x2F;logstash -e &#39;input &#123; stdin &#123;&#125; &#125; output &#123; stdout&#123;&#125; &#125;&#39;</span><br><span class="line">bin&#x2F;logstash -e &#39;input &#123; stdin &#123;&#125; &#125; output &#123; stdout&#123;codec &#x3D;&gt; rubydebug&#125; &#125;&#39;</span><br><span class="line">bin&#x2F;logstash -e &#39;input &#123; stdin &#123;&#125; &#125; output &#123; elasticsearch &#123;hosts &#x3D;&gt; [&quot;172.16.0.14:9200&quot;]&#125; stdout&#123;&#125; &#125;&#39;</span><br><span class="line">bin&#x2F;logstash -e &#39;input &#123; stdin &#123;&#125; &#125; output &#123; elasticsearch &#123;hosts &#x3D;&gt; [&quot;172.16.0.15:9200&quot;, &quot;172.16.0.16:9200&quot;]&#125; stdout&#123;&#125; &#125;&#39;</span><br><span class="line"></span><br><span class="line">bin&#x2F;logstash -e &#39;input &#123; stdin &#123;&#125; &#125; output &#123; kafka &#123; topic_id &#x3D;&gt; &quot;test_topic&quot; bootstrap_servers &#x3D;&gt; &quot;172.16.0.11:9092,172.16.0.12:9092,172.16.0.13:9092&quot;&#125; stdout&#123;codec &#x3D;&gt; rubydebug&#125; &#125;&#39;</span><br><span class="line"></span><br><span class="line"># 配置文件形式</span><br><span class="line">vi logstash.conf</span><br><span class="line">input &#123;</span><br><span class="line">	file &#123;</span><br><span class="line">		type &#x3D;&gt; &quot;gamelog&quot;</span><br><span class="line">		path &#x3D;&gt; &quot;&#x2F;log&#x2F;*&#x2F;*.log&quot;</span><br><span class="line">		discover_interval &#x3D;&gt; 10</span><br><span class="line">		start_position &#x3D;&gt; &quot;beginning&quot; </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">		index &#x3D;&gt; &quot;gamelog-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">        hosts &#x3D;&gt; [&quot;172.16.0.14:9200&quot;, &quot;172.16.0.15:9200&quot;, &quot;172.16.0.16:9200&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">bin&#x2F;logstash -f logstash.conf</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
        <category>大数据</category>
        <category>运维</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title>ES常用使用语句记录</title>
    <url>/2019/03/03/ES%E5%B8%B8%E7%94%A8%E4%BD%BF%E7%94%A8%E8%AF%AD%E5%8F%A5%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>记录常用的语句,省得到处找</p>
</blockquote>
<span id="more"></span>

<h1 id="查询数据并写入新Index"><a href="#查询数据并写入新Index" class="headerlink" title="查询数据并写入新Index"></a>查询数据并写入新Index</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;myindex&quot;,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">      &quot;bool&quot;: &#123;</span><br><span class="line">        &quot;must_not&quot;: [</span><br><span class="line">          &#123;&quot;term&quot;: &#123;</span><br><span class="line">            &quot;country&quot;: &#123;</span><br><span class="line">              &quot;value&quot;: &quot;中国&quot;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;&#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;myindexnew&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink1.11.0编译</title>
    <url>/2020/06/01/Flink1.11.0%E7%BC%96%E8%AF%91/</url>
    <content><![CDATA[<blockquote>
<p>等不及官网出包了,自己动手编译</p>
</blockquote>
<span id="more"></span>

<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p><a href="https://github.com/apache/flink/archive/release-1.11.zip">flink-release-1.11.0</a><br><a href="https://archive.apache.org/dist/flink/flink-shaded-11.0/flink-shaded-11.0-src.tgz">flink-shade-1.11.0</a></p>
<hr>
<h2 id="Hadoop版本"><a href="#Hadoop版本" class="headerlink" title="Hadoop版本"></a>Hadoop版本</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">hadoop version</span><br><span class="line">Hadoop 3.0.0-cdh6.2.0</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="文件修改"><a href="#文件修改" class="headerlink" title="文件修改"></a>文件修改</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># flink-shade</span><br><span class="line">修改pom.xml文件增加仓库</span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;mvnrepository&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;mvnrepository.com&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br><span class="line"></span><br><span class="line"># flink-release</span><br><span class="line">修改pom.xml文件增加仓库</span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;mvnrepository&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;mvnrepository.com&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br><span class="line"></span><br><span class="line"># 本地Maven的settings文件</span><br><span class="line">注释&lt;mirror&gt;模块</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># flink-shade</span></span><br><span class="line">mvn clean install -DskipTests -Dhadoop.version=3.0.0-cdh6.2.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># flink-release</span></span><br><span class="line">mvn clean install -DskipTests -Pvendor-repos -Dhadoop.version=3.0.0-cdh6.2.0 -Dmaven.javadoc.skip=<span class="literal">true</span> -Dcheckstyle.skip=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意可能会有报错</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">问题1:</span><br><span class="line">[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.12:check (default) on project flink-parent: Too many files with unapproved license: 2 See RAT report in: F:\test\flink-release-1.11\target\rat.txt -&gt; [Help 1]</span><br><span class="line">解决:</span><br><span class="line">mvn clean install -DskipTests -Pvendor-repos -Dhadoop.version&#x3D;3.0.0-cdh6.2.0 -Dmaven.javadoc.skip&#x3D;true -Dcheckstyle.skip&#x3D;true -Drat.skip&#x3D;true</span><br><span class="line"></span><br><span class="line">问题2:</span><br><span class="line">[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.6:npm (npm install) on project flink-runtime-web_2.11: Failed to run task: &#39;npm ci --cache-max&#x3D;0 --no-save&#39; failed. org.apache.commons.exec.ExecuteException: Process exited with an error: -4048 (Exit value: -4048) -&gt; [Help 1]</span><br><span class="line">解决:</span><br><span class="line">先删除flink-runtime-web\web-dashboard下的node_modules文件夹</span><br><span class="line">使用了淘宝源,删除它</span><br><span class="line">npm install -g mirror-config-china --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org&#x2F;</span><br><span class="line">npm config get registry</span><br><span class="line">npm config rm registry</span><br><span class="line">npm info express</span><br><span class="line">删除node_modules文件夹</span><br><span class="line">cache clean --force</span><br><span class="line">npm update</span><br><span class="line">重新打包</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">打包成功后,在flink-dist目录下会有部署文件</span><br><span class="line">flink-dist&#x2F;target&#x2F;flink-1.11-SNAPSHOT-bin&#x2F;flink-1.11-SNAPSHOT</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink1.11新特性</title>
    <url>/2020/06/08/Flink1.11%E6%96%B0%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<blockquote>
<p>整理下Flink1.11有什么新的功能,以及坑,只有自己用到的</p>
</blockquote>
<span id="more"></span>

<h2 id="Orc格式支持"><a href="#Orc格式支持" class="headerlink" title="Orc格式支持"></a>Orc格式支持</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">OrcBulkWriterFactory</span></span><br><span class="line">使用方式与<span class="type">ParquetAvroWriters</span></span><br><span class="line">但是,目前并没有一个完善的<span class="type">Demo</span></span><br><span class="line">本人在使用时出现有文件生成却没有数据的情况</span><br><span class="line">无法判断是使用问题,还是<span class="type">Flink</span>本身问题</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema: <span class="type">String</span> = <span class="string">&quot;struct&lt;platform:string,event:string,dt:string&gt;&quot;</span></span><br><span class="line"><span class="keyword">val</span> writerProperties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">writerProperties.setProperty(<span class="string">&quot;orc.compress&quot;</span>, <span class="string">&quot;LZ4&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> writerFactory = <span class="keyword">new</span> <span class="type">OrcBulkWriterFactory</span>(<span class="keyword">new</span> <span class="type">DemoVectorizer</span>(schema), writerProperties, <span class="keyword">new</span> <span class="type">Configuration</span>())</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.<span class="type">Path</span></span><br><span class="line"><span class="keyword">val</span> sink = <span class="type">StreamingFileSink</span>.forBulkFormat(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;./resources&quot;</span>),</span><br><span class="line">  writerFactory</span><br><span class="line">).build()</span><br><span class="line"></span><br><span class="line">student.addSink(sink).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="execute的使用"><a href="#execute的使用" class="headerlink" title="execute的使用"></a>execute的使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.11之前TableEnvironmentImpl与StreamExecutionEnvironment的execute方法实现一致</span><br><span class="line">无论用哪一个都可以</span><br><span class="line">1.11修改了TableEnvironmentImpl中execute的实现逻辑</span><br><span class="line">如果代码中涉及了DataStream的操作,则需要使用StreamExecutionEnvironment的execute方法</span><br><span class="line"></span><br><span class="line">简单概述为:</span><br><span class="line">    StreamTableEnvironment.execute() 只能执行 sqlUpdate 和 insertInto 方法执行作业</span><br><span class="line">    Table 转化为 DataStream 后只能通过 StreamExecutionEnvironment.execute() 来执行作业</span><br><span class="line">    新引入的 TableEnvironment.executeSql() 方法是直接执行sql作业 (异步提交作业)，不需要再调用 StreamTableEnvironment.execute() 或 StreamExecutionEnvironment.execute()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>FastDFS搭建</title>
    <url>/2019/05/20/FastDFS%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<blockquote>
<p>FastDFS是一款开源的轻量级分布式文件系统，纯C实现，支持Linux、FreeBSD等Unix系统。</p>
</blockquote>
<span id="more"></span>

<h2 id="安装包需要"><a href="#安装包需要" class="headerlink" title="安装包需要"></a>安装包需要</h2><ul>
<li>fastdfs-5.11.tar.gz</li>
<li>fastdfs-nginx-module-master.zip</li>
<li>libfastcommon-1.0.39.tar.gz</li>
<li>nginx-1.15.8.tar.gz</li>
<li>pcre-8.43.tar.gz</li>
</ul>
<hr>
<h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><p>两台服务器,IP192.168.1.128和IP192.168.1.129</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;home&#x2F;fdfs&#x2F;fastdfs</span><br><span class="line">cd &#x2F;home&#x2F;fdfs&#x2F;fastdfs</span><br><span class="line"></span><br><span class="line">tar xf libfastcommon-1.0.39.tar.gz</span><br><span class="line">cd libfastcommon-1.0.39</span><br><span class="line">.&#x2F;make.sh</span><br><span class="line">.&#x2F;make.sh install</span><br><span class="line"></span><br><span class="line">tar xf fastdfs-5.11.tar.gz </span><br><span class="line">cd fastdfs-5.11</span><br><span class="line">.&#x2F;make.sh</span><br><span class="line">.&#x2F;make.sh install</span><br></pre></td></tr></table></figure>

<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>配置文件默认在/etc/fdfs下<br>默认脚本在/etc/init.d下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建必须的目录,用来存放数据</span><br><span class="line">mkdir -p &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;base</span><br><span class="line">mkdir -p &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;storage0</span><br><span class="line">mkdir -p &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;storage1</span><br><span class="line">mkdir -p &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_tracker</span><br><span class="line"></span><br><span class="line"># 复制配置文件</span><br><span class="line">cd &#x2F;etc&#x2F;fdfs&#x2F;</span><br><span class="line">cp storage.conf.sample storage.conf</span><br><span class="line">cp tracker.conf.sample tracker.conf</span><br><span class="line"></span><br><span class="line"># 配置tracker.conf</span><br><span class="line">vi tracker.conf</span><br><span class="line">base_path&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_tracker</span><br><span class="line">#上传文件时选择group的方法</span><br><span class="line">#0:轮询，1:指定组，2:选择剩余空间最大</span><br><span class="line">store_lookup&#x3D;2</span><br><span class="line">#上传文件时选择server的方法</span><br><span class="line">#0:轮询，1:按IP地址排序，2:通过权重排序</span><br><span class="line">store_server&#x3D;0</span><br><span class="line"></span><br><span class="line"># 配置storage.conf</span><br><span class="line">vi storage.conf</span><br><span class="line">base_path&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;base</span><br><span class="line">store_path_count&#x3D;2</span><br><span class="line">store_path0&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;storage0</span><br><span class="line">store_path1&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;storage1</span><br><span class="line">#跟踪服务器</span><br><span class="line">tracker_server&#x3D;192.168.1.222:22122</span><br><span class="line">tracker_server&#x3D;192.168.1.233:22122</span><br><span class="line"></span><br><span class="line"># 启动服务</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;fdfs_trackerd start</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;fdfs_storaged start</span><br><span class="line"></span><br><span class="line"># 查看服务是否启动</span><br><span class="line">ps -ef|grep fdfs</span><br><span class="line">netstat -nltp</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="测试FastDFS"><a href="#测试FastDFS" class="headerlink" title="测试FastDFS"></a>测试FastDFS</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;client</span><br><span class="line"></span><br><span class="line"># 配置client文件</span><br><span class="line">cd &#x2F;etc&#x2F;fdfs</span><br><span class="line">cp client.conf.sample client.conf</span><br><span class="line">vi client.conf</span><br><span class="line">#存放日志目录</span><br><span class="line">base_path&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;client</span><br><span class="line">#跟踪服务器</span><br><span class="line">tracker_server&#x3D;192.168.1.128:22122</span><br><span class="line">tracker_server&#x3D;192.168.1.129:22122</span><br><span class="line"></span><br><span class="line"># 上传文件</span><br><span class="line">echo &quot;12345678&quot; &gt;&gt; &#x2F;home&#x2F;fdfs&#x2F;1.txt</span><br><span class="line">fdfs_upload_file &#x2F;etc&#x2F;fdfs&#x2F;client.conf &#x2F;home&#x2F;fdfs&#x2F;1.txt</span><br><span class="line">group1&#x2F;M00&#x2F;00&#x2F;00&#x2F;wKgBgF1bnnqAGbnKAAAACZ8EQKA111.txt</span><br><span class="line"></span><br><span class="line"># 下载文件</span><br><span class="line">fdfs_download_file &#x2F;etc&#x2F;fdfs&#x2F;client.conf group1&#x2F;M00&#x2F;00&#x2F;00&#x2F;wKgBgF1bnnqAGbnKAAAACZ8EQKA111.txt</span><br><span class="line"></span><br><span class="line"># 查看文件信息</span><br><span class="line">fdfs_file_info &#x2F;etc&#x2F;fdfs&#x2F;client.conf group1&#x2F;M00&#x2F;00&#x2F;00&#x2F;wKgBgF1bnnqAGbnKAAAACZ8EQKA111.txt</span><br><span class="line"></span><br><span class="line"># 追加文件</span><br><span class="line">echo &quot;hello&quot; &gt;&gt; &#x2F;home&#x2F;fdfs&#x2F;2.txt</span><br><span class="line">fdfs_upload_appender &#x2F;etc&#x2F;fdfs&#x2F;client.conf &#x2F;home&#x2F;fdfs&#x2F;1.txt</span><br><span class="line">group1&#x2F;M01&#x2F;00&#x2F;00&#x2F;wKgBgF1bnwaEcGKzAAAAAJ8EQKA863.txt</span><br><span class="line">fdfs_append_file &#x2F;etc&#x2F;fdfs&#x2F;client.conf group1&#x2F;M01&#x2F;00&#x2F;00&#x2F;wKgBgF1bnwaEcGKzAAAAAJ8EQKA863.txt &#x2F;home&#x2F;fdfs&#x2F;2.txt</span><br><span class="line"></span><br><span class="line"># 删除文件</span><br><span class="line">fdfs_delete_file &#x2F;etc&#x2F;fdfs&#x2F;client.conf group1&#x2F;M01&#x2F;00&#x2F;00&#x2F;wKgBgF1bnwaEcGKzAAAAAJ8EQKA863.txt</span><br><span class="line"></span><br><span class="line"># 查看集群</span><br><span class="line">fdfs_monitor &#x2F;etc&#x2F;fdfs&#x2F;client.conf</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="配置Nginx模块"><a href="#配置Nginx模块" class="headerlink" title="配置Nginx模块"></a>配置Nginx模块</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建用户</span><br><span class="line">useradd -s &#x2F;sbin&#x2F;nologin -M nginx</span><br><span class="line"></span><br><span class="line"># 安装pcre</span><br><span class="line">tar xf pcre-8.43.tar.gz</span><br><span class="line">cd pcre-8.43</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;pcre # 报错则yum install -y gcc-c++</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"></span><br><span class="line"># 安装nginx</span><br><span class="line">yum install zlib-devel openssl-devel</span><br><span class="line">unzip fastdfs-nginx-module-master.zip </span><br><span class="line">tar xf nginx-1.15.8.tar.gz </span><br><span class="line">cd nginx-1.15.8&#x2F;</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;nginx \</span><br><span class="line">--with-pcre&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;pcre-8.43 \</span><br><span class="line">--user&#x3D;nginx \</span><br><span class="line">--group&#x3D;nginx \</span><br><span class="line">--with-http_ssl_module \</span><br><span class="line">--with-http_realip_module \</span><br><span class="line">--with-http_stub_status_module \</span><br><span class="line">--add-module&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;fastdfs-nginx-module-master&#x2F;src</span><br><span class="line"># 执行报错,修改fastdfs-nginx-module-master&#x2F;src&#x2F;config</span><br><span class="line">ngx_module_incs&#x3D;&quot;&#x2F;usr&#x2F;include&#x2F;fastdfs &#x2F;usr&#x2F;include&#x2F;fastcommon&quot;</span><br><span class="line">CORE_INCS&#x3D;&quot;$CORE_INCS &#x2F;usr&#x2F;include&#x2F;fastdfs &#x2F;usr&#x2F;include&#x2F;fastcommon&quot;</span><br><span class="line">make &amp; make install</span><br><span class="line"></span><br><span class="line"># 拷贝配置文件</span><br><span class="line">cd &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;fastdfs-nginx-module-master&#x2F;src</span><br><span class="line">cp mod_fastdfs.conf &#x2F;etc&#x2F;fdfs&#x2F;</span><br><span class="line">cd &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;fastdfs-5.11&#x2F;conf</span><br><span class="line">cp anti-steal.jpg http.conf mime.types &#x2F;etc&#x2F;fdfs&#x2F;</span><br><span class="line"></span><br><span class="line"># 修改nginx.conf</span><br><span class="line">vi &#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;nginx&#x2F;conf&#x2F;nginx.conf</span><br><span class="line">location ~ &#x2F;group[0-9]&#x2F;M00 &#123;</span><br><span class="line">    ngx_fastdfs_module;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 修改mod_fastdfs.conf</span><br><span class="line">vi &#x2F;etc&#x2F;fdfs&#x2F;mod_fastdfs.conf</span><br><span class="line">tracker_server&#x3D;192.168.1.128:22122</span><br><span class="line">tracker_server&#x3D;192.168.1.129:22122</span><br><span class="line">url_have_group_name &#x3D; true</span><br><span class="line">store_path_count&#x3D;2</span><br><span class="line">store_path0&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;storage0</span><br><span class="line">store_path1&#x3D;&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;fdfs_storage&#x2F;storage1</span><br><span class="line"></span><br><span class="line"># 启动nginx</span><br><span class="line">&#x2F;home&#x2F;fdfs&#x2F;fastdfs&#x2F;data&#x2F;nginx&#x2F;sbin&#x2F;nginx</span><br><span class="line"></span><br><span class="line"># 启动报错,查看error.log发现没有权限</span><br><span class="line">chmod 777 -R &#x2F;home</span><br><span class="line"></span><br><span class="line"># 路径不应该放在home底下的,或者说不应该放到其他用户目录下</span><br><span class="line"># 最好是重装,上述修改权限的方式并不可取</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>oss</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink&amp;Spark知识目录</title>
    <url>/2019/08/25/Flink&amp;Spark%E7%9F%A5%E8%AF%86%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>逐步提高自己技术</p>
</blockquote>
<span id="more"></span>

<h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">初识Flink</span><br><span class="line">Flink读取Kafka源码解读</span><br><span class="line">Flink的状态后端</span><br><span class="line">Flink的runtime</span><br><span class="line">Flink系列之数据流编程模型</span><br><span class="line">Flink系列之checkpoint</span><br><span class="line">Flink系列之savepoint</span><br><span class="line">Flink系列之checkpoint和savepoint的区别</span><br><span class="line">Flink系列之部署Standalone模式</span><br><span class="line">Flink系列之部署on yarn模式</span><br><span class="line">Flink系列之state</span><br><span class="line">Flink系列之checkpoint的实现原理</span><br><span class="line">Flink系列之window机制</span><br><span class="line">Flink从checkpoint和savepoint恢复的具体操作</span><br><span class="line">Flink系列之watermark的理解</span><br><span class="line">Flink系列之结合代码分析watermark的过程</span><br><span class="line">Flink系列之自定义Trigger的实现</span><br><span class="line">Flink系列之自定义Evictor的实现</span><br><span class="line">Flink系列之窗口函数的使用</span><br><span class="line">Flink系列之广播状态模式</span><br><span class="line">Flink系列之广播状态模式的具体实现</span><br><span class="line">Flink系列之流的join</span><br><span class="line">Flink系列之动态表和连续查询</span><br><span class="line">Flink系列之客户端测试版</span><br><span class="line">Flink系列之Table和SQL</span><br><span class="line">Flink系列之Table和SQL的一些常用的操作</span><br><span class="line">Flink系列之动态表的查询</span><br><span class="line">Flink系列之JSONKeyValueDeserializationSchema序列化</span><br><span class="line">Flink系列之Side Outputs的使用</span><br><span class="line">Flink系列之Flink如何处理反压</span><br><span class="line">Flink系列之热门商品TopN项目实战</span><br><span class="line">Flink系列之怎么实现exactly-once语义的</span><br><span class="line">Flink系列之窗口函数</span><br><span class="line">Flink系列之ProcessWindowFunction增量聚合</span><br><span class="line">Flink系列之ProcessFunction</span><br><span class="line">Flink系列之ProcessFunction补充说明</span><br><span class="line">Flink系列之KeyedState中ValueState的使用</span><br><span class="line">Flink系列之Operator Chain</span><br><span class="line">Flink系列之End-to-End Exactly-Once的实现</span><br><span class="line">Flink系列之keyby的详解</span><br><span class="line">Flink系列之滚动窗口的使用</span><br><span class="line">Flink系列之滑动窗口的使用</span><br><span class="line">Flink系列之窗口的总结</span><br><span class="line">Flink系列之CEP</span><br><span class="line">Flink系列之CEP</span><br><span class="line">Flink的CEP使用Demo</span><br><span class="line">Flink系列之slot和parallelism的关系</span><br><span class="line">Flink系列之slot和parallelism的测试</span><br><span class="line">Flink系列之动态表和连续查询</span><br><span class="line">Flink系列之流和维表的join实现</span><br><span class="line">Flink系列之空闲状态保留时间</span><br><span class="line">Flink系列是Flink table中的时间属性</span><br><span class="line">Flink系列之Flink SQL实现一个基于processing time</span><br><span class="line">Flink系列之Flink SQL实现一个基于Event time的滑动</span><br><span class="line">Flink系列之UDF函数</span><br><span class="line">Flink系列之UDF使用demo</span><br><span class="line">Flink系列之yarn-session的使用</span><br><span class="line">Flink系列之HDFS连接器</span><br><span class="line">Flink系列之多sink的实现</span><br><span class="line">Flink系列之批量写入mysql实现</span><br><span class="line">Flink系列之窗口聚合和非窗口聚合的区别</span><br><span class="line">Flink系列之Flink消费多个topic的数据sink到不同的</span><br><span class="line">Flink系列之自定义source</span><br><span class="line">Flink系列之测流输出在实际项目中的使用</span><br><span class="line">Flink系列之消费kafka的数据写入elasticsearch</span><br><span class="line">Flink系列之读取arvo格式的数据</span><br><span class="line">Flink系列之early计算的实现</span><br><span class="line">Flink系列之window的start_time和end_time是怎么计算的</span><br><span class="line">Flink的eventtime和watermark的详解和源码分析</span><br><span class="line">Flink指定从某个时间戳开始消费kafka的数据</span><br><span class="line">Flink的Metric的使用</span><br><span class="line">Flink的window源码分析</span><br><span class="line">Flink的广播流使用</span><br><span class="line">Flink的异步IO</span><br><span class="line">Flink使用异步IO查询mysql的数据</span><br><span class="line">Flink的operator chain详细说明</span><br><span class="line">玩Flink没有集群环境还想看到UI界面怎么办呢?</span><br><span class="line">Flink自定义分区sink到kafka,怎么实现呢?</span><br><span class="line">Flink写入hdfs动态路径的实现(Flink streaming的累加器)</span><br><span class="line">Flink的BoundedOutOfOrdernessTimestampExtractor源码解析</span><br><span class="line">Flink的interval join实现</span><br><span class="line">Flink的interval join的API实现</span><br><span class="line">Flink SQL 实现interval </span><br><span class="line">Flink中怎么获取kafka的topic信息</span><br><span class="line">为什么我的Flink任务正常运行,UI上却不显示接收和发送的数据?</span><br><span class="line">怎么从每天的0点开始,实时统计TopN,并且秒级输出呢</span><br><span class="line">Flink使用异步IO查询mysql和redis(scala版本的)</span><br><span class="line">Flink怎么读取hdfs的orc文件,然后用sql分析呢?</span><br><span class="line">Flink的状态清除TTL</span><br><span class="line">Flink批量写入hbase</span><br><span class="line">Flink SQL的Retraction</span><br><span class="line">Flink怎么合理的分配资源?</span><br><span class="line">Flink的apply方法和process方法有什么区别?</span><br><span class="line">Flink的AllWindowFunction源码分析和具体的使用</span><br><span class="line">Flink的operator chain带来的问题分析</span><br><span class="line">Flink的分布式缓存</span><br><span class="line">Flink发生数据倾斜怎么办?(两段聚合的方式)</span><br><span class="line">Flink的AllWindowFunction源码分析和具体的使用</span><br><span class="line">Flink系列之UpsertStreamTableSink的使用</span><br><span class="line">FlinkSQL基于1.8.1实时统计PV,UV</span><br><span class="line">Flink1.9.1最新版本整合kafka使用以及WEB UI的介绍</span><br><span class="line">Flink实时统计用户的点击行为</span><br><span class="line">Flink任务怎么知道某个subtask运行在哪个机器上?</span><br><span class="line">Flink不使用window怎么实现批量操作?</span><br><span class="line">Flink怎么在本地提交任务到远程的集群?</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Spark的核心之RDD: Spark的核心之RDD</span><br><span class="line">sparkstreaming消费kafka(direct方式): sparkstreaming消费kafka(direct方式)</span><br><span class="line">spark算子之combineByKey: spark算子之combineByKey</span><br><span class="line">spark分区个数详解: spark分区个数详解</span><br><span class="line">Spark怎么实现exactly-once的语义: Spark怎么实现exactly-once的语义</span><br><span class="line">spark中RDD的分区是怎么传递的: spark中RDD的分区是怎么传递的</span><br><span class="line">sparkstreaming的window操作: sparkstreaming的window操作</span><br><span class="line">sparkstreaming反压机制的实现原理: sparkstreaming反压机制的实现原理</span><br><span class="line">spark --jars添加第三方jar包: spark --jars添加第三方jar包</span><br><span class="line">sparkstreaming的设计原理: sparkstreaming的设计原理</span><br><span class="line">spark的rdd的理解: spark的rdd的理解</span><br><span class="line">sparkstreaming整合kafka的两种方式: sparkstreaming整合kafka的两种方式</span><br><span class="line">sparkstreaming整合kafka手动维护offest到redis: sparkstreaming整合kafka手动维护offest到redis</span><br><span class="line">sparkstreaming的性能优化: sparkstreaming的性能优化</span><br><span class="line">spark web ui的使用: spark web ui的使用</span><br><span class="line">sparkstreaming结合mysql的事物实现exactly-once的语义: sparkstreaming结合mysql的事物实现exactly-once的语义</span><br><span class="line">spark的dataframe写入mysql或者hive的一个小陷阱: spark的dataframe写入mysql或者hive的一个小陷阱</span><br><span class="line">spark的数据倾斜: spark的数据倾斜</span><br><span class="line">spark的transform算子使用和源码分析: spark的transform算子使用和源码分析</span><br><span class="line">spark和flink的累计器使用: spark和flink的累计器使用</span><br><span class="line">sparkstreaming中StreamingListener的使用: sparkstreaming中StreamingListener的使用</span><br><span class="line">sparkstreaming中怎么获取kafka的topic和timestamp信息: sparkstreaming中怎么获取kafka的topic和timestamp信息</span><br><span class="line">sparkstreaming中使用StreamingListener完成程序的异常报警通知: sparkstreaming中使用StreamingListener完成程序的异常报警通知</span><br><span class="line">spark使用BulkLoad写入hbase时候的排序问题</span><br><span class="line">spark报错OOM一定是executor memory太小了吗?</span><br><span class="line">spark的leftOuterJoin算子的使用和源码解析</span><br><span class="line">如何合理的设计hbase的rowkey?</span><br><span class="line">sparkstreaming的window使用和源码分析以及和Flink的window的区别</span><br><span class="line">spark的4040端口占用问题</span><br><span class="line">集群迁移方案(写的比较随意)</span><br><span class="line">集群迁移(二)</span><br><span class="line">在idea里面怎么远程提交spark任务到yarn集群</span><br><span class="line">spark on yarn动态资源分配</span><br><span class="line">Spark structured streaming的滑动窗口实现</span><br><span class="line">HBase集群的迁移方案</span><br><span class="line">sparkstreaming任务出现堆积怎么办?(流量突然大增资源不够怎么办?)</span><br><span class="line">hbase的集群迁移方案</span><br><span class="line">一次线上SQL的优化记录</span><br><span class="line">hive建表的陷阱</span><br><span class="line">spark sql join的问题</span><br><span class="line">spark的任务怎么合理的分配资源</span><br><span class="line">hive中的数据怎么快速的同步到hbase中</span><br><span class="line">Phoenix整合spark进行查询分析</span><br><span class="line">sparkstreaming实时写hive产生大量的小文件怎么处理比较好?</span><br><span class="line">spark使用bulk load同步数据到hbase的优化</span><br><span class="line">Phoenix的安装和使用</span><br><span class="line">Phoenix整合spark进行查询分析</span><br><span class="line">sparkstreaming实时写hive产生大量的小文件怎么处理比较好?</span><br><span class="line">spark sql读取mysql的数据速度很慢任务长时间卡住怎么优化?</span><br><span class="line">maven环境下java和scala混合开发如何打依赖包?</span><br><span class="line">Spark RDD转Dataframe的时候怎么动态构建schema?</span><br><span class="line">spark的面试题map和MapPartitions有什么区别?</span><br><span class="line">Spark&amp;Flink的面试题</span><br><span class="line">spark on yarn UI上怎么查看日志?</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>FlinkSQL实时写入Hive</title>
    <url>/2020/07/13/FlinkSQL%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5Hive/</url>
    <content><![CDATA[<blockquote>
<p>在新版本中,终于可以不使用StreamingFileSink来写入HDFS了</p>
</blockquote>
<span id="more"></span>

<h2 id="FileSystem连接方式的SQL"><a href="#FileSystem连接方式的SQL" class="headerlink" title="FileSystem连接方式的SQL"></a>FileSystem连接方式的SQL</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sql =</span><br><span class="line">  <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">     |CREATE TABLE fs_table (</span></span><br><span class="line"><span class="string">     |  user_uid  STRING,</span></span><br><span class="line"><span class="string">     |  `ref` BIGINT,</span></span><br><span class="line"><span class="string">     |  reply_attach STRING,</span></span><br><span class="line"><span class="string">     |  dt STRING,</span></span><br><span class="line"><span class="string">     |  h string</span></span><br><span class="line"><span class="string">     |) PARTITIONED BY (dt,h) WITH (</span></span><br><span class="line"><span class="string">     |  &#x27;connector&#x27;=&#x27;filesystem&#x27;,</span></span><br><span class="line"><span class="string">     |  &#x27;path&#x27;=&#x27;hdfs:///tmp/test&#x27;,</span></span><br><span class="line"><span class="string">     |  &#x27;sink.partition-commit.policy.kind&#x27; = &#x27;success-file&#x27;,</span></span><br><span class="line"><span class="string">     |  &#x27;format&#x27;=&#x27;orc&#x27;</span></span><br><span class="line"><span class="string">     |)</span></span><br><span class="line"><span class="string">     |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line"></span><br><span class="line">tableEnv.executeSql(sql);</span><br><span class="line"></span><br><span class="line">tableEnv.executeSql(</span><br><span class="line">  <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">     |insert into fs_table</span></span><br><span class="line"><span class="string">     |select before.user_id,before.`ref`,before.reply_attach,</span></span><br><span class="line"><span class="string">     |DATE_FORMAT(LOCALTIMESTAMP, &#x27;yyyy-MM-dd&#x27;),</span></span><br><span class="line"><span class="string">     |DATE_FORMAT(LOCALTIMESTAMP, &#x27;HH&#x27;)</span></span><br><span class="line"><span class="string">     |FROM test</span></span><br><span class="line"><span class="string">     |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实时Hive"><a href="#实时Hive" class="headerlink" title="实时Hive"></a>实时Hive</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SET</span> table.sql-dialect=hive;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_table (</span><br><span class="line">  user_id <span class="keyword">STRING</span>,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span></span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (dt <span class="keyword">STRING</span>, hr <span class="keyword">STRING</span>) <span class="keyword">STORED</span> <span class="keyword">AS</span> parquet TBLPROPERTIES (</span><br><span class="line">  <span class="string">&#x27;partition.time-extractor.timestamp-pattern&#x27;</span>=<span class="string">&#x27;$dt $hr:00:00&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.trigger&#x27;</span>=<span class="string">&#x27;partition-time&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.delay&#x27;</span>=<span class="string">&#x27;1 h&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;sink.partition-commit.policy.kind&#x27;</span>=<span class="string">&#x27;metastore,success-file&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> table.sql-dialect=<span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table (</span><br><span class="line">  user_id <span class="keyword">STRING</span>,</span><br><span class="line">  order_amount <span class="keyword">DOUBLE</span>,</span><br><span class="line">  log_ts <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> log_ts <span class="keyword">AS</span> log_ts - <span class="built_in">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span></span><br><span class="line">) <span class="keyword">WITH</span> (...);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- streaming sql, insert into hive table</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> hive_table <span class="keyword">SELECT</span> user_id, order_amount, <span class="keyword">DATE_FORMAT</span>(log_ts, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>), <span class="keyword">DATE_FORMAT</span>(log_ts, <span class="string">&#x27;HH&#x27;</span>) <span class="keyword">FROM</span> kafka_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- batch sql, select with partition pruning</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> hive_table <span class="keyword">WHERE</span> dt=<span class="string">&#x27;2020-05-20&#x27;</span> <span class="keyword">and</span> hr=<span class="string">&#x27;12&#x27;</span>;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写入的文件路径</span></span><br><span class="line">path</span><br><span class="line"></span><br><span class="line"><span class="comment"># 动态分区列值为空/空字符串时的默认分区名称</span></span><br><span class="line">partition.default-name</span><br><span class="line"></span><br><span class="line"><span class="comment"># 滚动文件的最大大小,默认128MB</span></span><br><span class="line">sink.rolling-policy.file-size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 滚动文件打开的最长时间,默认30分钟</span></span><br><span class="line">sink.rolling-policy.rollover-interval</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查滚动文件是否应该完成,默认1分钟</span></span><br><span class="line">sink.rolling-policy.check-interval</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否启用按动态分区字段进行shuffle,有可能导致数据倾斜,默认是false</span></span><br><span class="line">sink.shuffle-by-partition.enable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否启动StreamingSource,确保每个分区文件都应以原子方式写入,否则会获取不完整的数据,默认不启用</span></span><br><span class="line">streaming-source.enable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连续监视分区/文件的时间间隔</span></span><br><span class="line">streaming-source.monitor-interval</span><br><span class="line"></span><br><span class="line"><span class="comment"># 流数据源的消费顺序,支持create-time和partition-time</span></span><br><span class="line">streaming-source.consume-order</span><br><span class="line"></span><br><span class="line"><span class="comment"># StreamingSource消耗的起始偏移量,对于create-time和partition-time,应该是时间戳</span></span><br><span class="line">streaming-source.consume-start-offset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从分区值中提取时间的提取器,支持default和custom</span></span><br><span class="line">partition.time-extractor.kind</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从分区值中提取时间的自定义提取器</span></span><br><span class="line">partition.time-extractor.class</span><br><span class="line"></span><br><span class="line"><span class="comment"># default模式时间提取器允许用户使用分区字段来获取合法的时间戳</span></span><br><span class="line"><span class="comment"># 默认支持&#x27;yyyy-mm-dd hh:mm:ss&#x27;</span></span><br><span class="line"><span class="comment"># 可以配置&#x27;$dt&#x27;,&#x27;$year-$month-$day $hour:00:00&#x27;,&#x27;$dt $hour:00:00&#x27;</span></span><br><span class="line">partition.time-extractor.timestamp-pattern</span><br><span class="line"></span><br><span class="line"><span class="comment"># Lookup Join的缓存有效期,默认60分钟</span></span><br><span class="line">lookup.join.cache.ttl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交分区的触发器类型</span></span><br><span class="line"><span class="comment"># process-time: 基于机器时间</span></span><br><span class="line"><span class="comment"># partition-time: 基于分区值中提取的时间,需要生成水印</span></span><br><span class="line">sink.partition-commit.trigger</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分区在延迟时间之前不会提交</span></span><br><span class="line">sink.partition-commit.delay</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入成功后的提交模式,metastore,success-file,custom</span></span><br><span class="line"><span class="comment"># 支持metastore和success-file两者同时进行</span></span><br><span class="line">sink.partition-commit.policy.kind</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于实现PartitionCommitPolicy接口的分区提交策略,用于custom模式</span></span><br><span class="line">sink.partition-commit.policy.class</span><br><span class="line"></span><br><span class="line"><span class="comment"># success-file分区提交策略的文件名,默认是&#x27;_SUCCUESS&#x27;</span></span><br><span class="line">sink.partition-commit.success-file.name</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink1.3版本到Flink1.9不同版本特性</title>
    <url>/2020/01/13/Flink1.3%E7%89%88%E6%9C%AC%E5%88%B0Flink1.9%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7/</url>
    <content><![CDATA[<blockquote>
<p>一年内,公司从flink1.3-&gt; flink1.8 -&gt; flink1.9 不断的迎接不同版本对flink相关业务需求进行满足,代码方面也慢慢的简单化</p>
</blockquote>
<span id="more"></span>

<h3 id="Flink的1-0到1-9特性理论内容"><a href="#Flink的1-0到1-9特性理论内容" class="headerlink" title="Flink的1.0到1.9特性理论内容"></a><a href="https://www.cnblogs.com/zclaude/p/12048429.html">Flink的1.0到1.9特性理论内容</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主要详细讲解</span><br><span class="line">Flink API历史变迁;</span><br><span class="line">Flink High-Level API历史变迁;</span><br><span class="line">Flink Checkpoint &amp; Recovery历史变迁</span><br><span class="line">Flink Runtime历史变迁</span><br></pre></td></tr></table></figure>

<h3 id="flink-历史变迁结合实际业务"><a href="#flink-历史变迁结合实际业务" class="headerlink" title="flink 历史变迁结合实际业务"></a>flink 历史变迁结合实际业务</h3><h4 id="flink1-3版本"><a href="#flink1-3版本" class="headerlink" title="flink1.3版本"></a>flink1.3版本</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># flink1.3版本:基本支持简单的业务的开发</span><br><span class="line">当时对用户做题的行为轨迹进行记录: </span><br><span class="line">Debezium结合Kafka Connect实时捕获Mysql变更事件写入kafka,flink监控kafka,hbase作为中间表进行数据处理,结果存储es</span><br><span class="line">mysql -&gt; kafka -&gt; flink -&gt; hbase -&gt; es</span><br><span class="line"></span><br><span class="line">主要根据flink API结合scala的相关算子, 对es进行增删操作</span><br><span class="line">难点:根据kafka收集到的数据分为增删改以及空数据四块内容,需要分别对这四种数据进行按条件处理,程序大量引用了filter.map操作,代码的可读性比较差</span><br><span class="line"></span><br><span class="line">具体代码实现:</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;hostility-sadan&#x2F;flink-bahavior-trace.git</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="flink1-8版本"><a href="#flink1-8版本" class="headerlink" title="flink1.8版本"></a>flink1.8版本</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink1.8版本:对之前业务逻辑使用新的框架进行优化</span><br><span class="line">主要特征项目:试题画像</span><br><span class="line">需求表述:</span><br><span class="line">1、</span><br><span class="line">维护目标：自建试题-&gt;若干属性（包括status属性，正文+选项+解析 即content+option+analysis 只放ES)</span><br><span class="line">监控：question_info</span><br><span class="line">过滤条件：question_id&lt;0</span><br><span class="line">后续操作：各种属性从after拿出放入ES</span><br><span class="line">2、</span><br><span class="line">维护目标：非引用试题-&gt;自建课列表; 自建课-&gt; 非引用试题列表</span><br><span class="line">监控：tp_j_course_question_info</span><br><span class="line">过滤条件：course_id&lt;0,is_quote&#x3D;0</span><br><span class="line">特殊处理：更新操作里，如果after:local_status&#x3D;2 表示删除，不是更新</span><br><span class="line">后续操作：查询自建课列表更新ES</span><br><span class="line">          如果是有效的且共享课，如果是删除操作，如果非引用试题列表长度为0，从 教材id+schoolid-&gt;共享课列表 移除课</span><br><span class="line">  如果是有效的且共享课，如果是添加操作，如果非引用试题列表长度为1，从 教材id+schoolid-&gt;共享课列表 添加课</span><br><span class="line">3、</span><br><span class="line">维护目标：自建课-&gt;SHARE_TYPE,LOCAL_STATUS,dc_school_id</span><br><span class="line">监控：tp_course_info</span><br><span class="line">过滤条件：course_id&lt;0</span><br><span class="line">后续操作：如果为更新操作，如果SHARE_TYPE从其它值变为1并且非引用试题列表长度不为0，则从 教材id+schoolid-&gt;共享课列表 添加课</span><br><span class="line">                          如果SHARE_TYPE从1变为其它值并且非引用试题列表长度不为0，则从 教材id+schoolid-&gt;共享课列表 移除课</span><br><span class="line">  如果LOCAL_STATUS从1变为2并且非引用试题列表长度不为0，则从 教材id+schoolid-&gt;共享课列表 移除课</span><br><span class="line">                          如果SHARE_TYPE从其它值变为1,用当前时间更新 自建课-&gt;share_time</span><br><span class="line">          如果为插入操作，如SHARE_TYPE&#x3D;1，如非引用试题列表长度不为0，如有教材ID，则从 教材id+schoolid-&gt;共享课列表 添加课</span><br><span class="line">4、</span><br><span class="line">维护目标：自建课-&gt;教材ID</span><br><span class="line">监控:tp_j_course_teaching_material</span><br><span class="line">过滤条件：course_id&lt;0</span><br><span class="line">后续操作：如果为插入操作，如非引用试题列表长度不为0，如SHARE_TYPE&#x3D;1，如LOCAL_STATUS&#x3D;1，则从 教材id+schoolid-&gt;共享课列表 添加课</span><br><span class="line"></span><br><span class="line">难点:多流交互,需要做到流与流处理的无序性</span><br><span class="line">优化内容:构建了图的架构,提高了代码的可读性,以及使用checkpoint进行自动恢复机制</span><br><span class="line"></span><br><span class="line">具体代码实现:</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;hostility-sadan&#x2F;flink-new-ques-all-profile.git</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>FlinkSQL的搭建与使用</title>
    <url>/2019/12/05/FlinkSQL%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>用于Flink1.9以上版本</p>
</blockquote>
<span id="more"></span>

<h2 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h2><ul>
<li>Flink1.9客户端</li>
<li>CDH集群(hive-1.1.0)</li>
</ul>
<hr>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">目前FlinkSQL通过测试的Hive版本只有1.2.1和2.3.4</span><br><span class="line">但是其他版本经过我的测试,发现也是可以使用的</span><br><span class="line">jar包需要导入对应版本的</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">catalogs:</span><br><span class="line"># catalogs 名称</span><br><span class="line">  - name: hive</span><br><span class="line"># catalog连接类型</span><br><span class="line">    type: hive</span><br><span class="line"># hive 安装路径下conf目录路径</span><br><span class="line">    hive-conf-dir: &#x2F;etc&#x2F;hive&#x2F;conf.cloudera.hive</span><br><span class="line"># hive 版本号</span><br><span class="line">    hive-version: 1.2.1</span><br><span class="line">    property-version: 1</span><br><span class="line"># use catalog 后 默认连接的数据库名</span><br><span class="line">    default-database: default</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="拷贝依赖包"><a href="#拷贝依赖包" class="headerlink" title="拷贝依赖包"></a>拷贝依赖包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">需要以下依赖包-我的是CDH的</span><br><span class="line">flink-connector-hive_2.11-1.9.1.jar</span><br><span class="line">flink-hadoop-compatibility_2.11-1.9.1.jar</span><br><span class="line">flink-shaded-hadoop-2-uber-2.6.5-7.0.jar</span><br><span class="line">hadoop-common-2.6.0-cdh5.15.1.jar</span><br><span class="line">hadoop-mapreduce-client-common-2.6.0-cdh5.15.1.jar</span><br><span class="line">hive-common-1.2.1.jar</span><br><span class="line">hive-exec-1.2.1.jar</span><br><span class="line">hive-metastore-1.2.1.jar</span><br><span class="line">libfb303-0.9.3.jar</span><br><span class="line">libthrift-0.9.3.jar</span><br><span class="line">mysql-connector-java-5.1.48-bin.jar</span><br><span class="line">antlr-runtime-3.4.jar</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;sql-client.sh embedded</span><br><span class="line">use catalog hive;</span><br><span class="line">show tables;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TableAPI的使用"><a href="#TableAPI的使用" class="headerlink" title="TableAPI的使用"></a>TableAPI的使用</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"># 记录一下坑</span><br><span class="line"><span class="number">1.</span><span class="type">TableAPI</span>目前不支持<span class="type">HiveStreamTableSink</span>,所以写不进去,可以读</span><br><span class="line"><span class="number">2.</span><span class="type">CDH</span>集群一定要注意引用的pom是<span class="type">CDH</span>版本的</span><br><span class="line"></span><br><span class="line"># 测试代码</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HiveDemoOnTable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ds1 = env.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> hiveCatalog = <span class="keyword">new</span> <span class="type">HiveCatalog</span>(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;default&quot;</span>,</span><br><span class="line">      <span class="string">&quot;hive_conf&quot;</span>, <span class="string">&quot;1.2.1&quot;</span>)</span><br><span class="line">    tEnv.registerCatalog(<span class="string">&quot;test&quot;</span>, hiveCatalog)</span><br><span class="line">    tEnv.useCatalog(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table = tEnv.sqlQuery(<span class="string">&quot;select `topic`,`partition`,`offset`,msg,`c_date` from user_test_orc&quot;</span>)</span><br><span class="line"></span><br><span class="line">    table.insertInto(<span class="string">&quot;user_test_orc&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  case class Order(user: Int, product: String, amount: Int)</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span>(<span class="params">topic: <span class="type">String</span>, partition: <span class="type">Integer</span>, offset: <span class="type">Integer</span>, msg: <span class="type">String</span>, c_date: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="记录出现的问题"><a href="#记录出现的问题" class="headerlink" title="记录出现的问题"></a>记录出现的问题</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">问题1:</span><br><span class="line">Caused by: org.apache.flink.table.client.gateway.SqlExecutionException: Could not create execution context.</span><br><span class="line">修改了默认module</span><br><span class="line">解决:</span><br><span class="line">导入flink-connector-hive_2.11-*.jar</span><br><span class="line"></span><br><span class="line">问题2:</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hive.common.util.HiveVersionInfo</span><br><span class="line">没有找到Hive版本信息</span><br><span class="line">解决:</span><br><span class="line">导入hive-exec-*.jar</span><br><span class="line"></span><br><span class="line">问题3:</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.Writable</span><br><span class="line">没有hadood依赖,导入shaded包或者hadoop依赖包</span><br><span class="line">解决:</span><br><span class="line">导入hadoop-common-*.jar等</span><br><span class="line">或者flink-shaded-hadoop-2-uber-*.jar</span><br><span class="line"></span><br><span class="line">问题4:</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: com.facebook.fb303.FacebookService$Iface</span><br><span class="line">缺少libfb303</span><br><span class="line">解决:</span><br><span class="line">导入libfb303-0.9.3.jar</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>FlinkSQL自定义FORMAT_TYPE</title>
    <url>/2020/06/11/FlinkSQL%E8%87%AA%E5%AE%9A%E4%B9%89FORMAT_TYPE/</url>
    <content><![CDATA[<blockquote>
<p>Flink本身的<code>format.type</code>目前支持<code>json</code>，<code>avro</code>，<code>csv</code>三种格式<br>对于涉及的源码会另开一章进行介绍</p>
</blockquote>
<span id="more"></span>

<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">FlinkSQL</span>创建<span class="type">Kafka</span>源表,数据格式为<span class="type">JSON</span>,但是数据中有一些脏数据</span><br><span class="line">这时候程序会直接报错停掉</span><br><span class="line"><span class="type">Caused</span> by: java.io.<span class="type">IOException</span>: <span class="type">Failed</span> to deserialize <span class="type">JSON</span> <span class="keyword">object</span>.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKafkaTable</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">      |CREATE TABLE demo1 (</span></span><br><span class="line"><span class="string">      |    uid VARCHAR COMMENT &#x27;uid&#x27;,</span></span><br><span class="line"><span class="string">      |    rid VARCHAR COMMENT &#x27;rid&#x27;</span></span><br><span class="line"><span class="string">      |)</span></span><br><span class="line"><span class="string">      |WITH (</span></span><br><span class="line"><span class="string">      |    &#x27;connector.type&#x27; = &#x27;kafka&#x27;, -- 使用 kafka connector</span></span><br><span class="line"><span class="string">      |    &#x27;connector.version&#x27; = &#x27;universal&#x27;,  -- kafka 版本</span></span><br><span class="line"><span class="string">      |    &#x27;connector.topic&#x27; = &#x27;test&#x27;,  -- kafka topic</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.0.key&#x27; = &#x27;zookeeper.connect&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.0.value&#x27; = &#x27;hosts:2181&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.1.key&#x27; = &#x27;bootstrap.servers&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.1.value&#x27; = &#x27;hosts:9092&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.sink-partitioner&#x27; = &#x27;fixed&#x27;,</span></span><br><span class="line"><span class="string">      |    &#x27;update-mode&#x27; = &#x27;append&#x27;,</span></span><br><span class="line"><span class="string">      |    &#x27;format.type&#x27; = &#x27;json&#x27;,  -- 数据源格式为 json</span></span><br><span class="line"><span class="string">      |    &#x27;format.derive-schema&#x27; = &#x27;true&#x27; -- 从 DDL schema 确定 json 解析规则</span></span><br><span class="line"><span class="string">      |)</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><h3 id="自定义Factory"><a href="#自定义Factory" class="headerlink" title="自定义Factory"></a>自定义Factory</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.flink.formats.custom;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.test.flink.CustomJsonRowDeserializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.DeserializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SerializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.typeutils.RowTypeInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.formats.json.JsonRowDeserializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.formats.json.JsonRowSchemaConverter;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.formats.json.JsonRowSerializationSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.DescriptorProperties;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.JsonValidator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.factories.DeserializationSchemaFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.factories.SerializationSchemaFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.factories.TableFormatFactoryBase;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/6/11.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomJsonRowFormatFactory</span> <span class="keyword">extends</span> <span class="title">TableFormatFactoryBase</span>&lt;<span class="title">Row</span>&gt;</span></span><br><span class="line"><span class="class">        <span class="keyword">implements</span> <span class="title">SerializationSchemaFactory</span>&lt;<span class="title">Row</span>&gt;, <span class="title">DeserializationSchemaFactory</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 必须实现一个无参构造器</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CustomJsonRowFormatFactory</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// custom就是你自定义的format.type</span></span><br><span class="line">        <span class="keyword">super</span>(<span class="string">&quot;custom&quot;</span>, <span class="number">1</span>, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将参数转换为DescriptorProperties</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> DescriptorProperties <span class="title">getValidatedProperties</span><span class="params">(Map&lt;String, String&gt; propertiesMap)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> DescriptorProperties descriptorProperties = <span class="keyword">new</span> DescriptorProperties();</span><br><span class="line">        descriptorProperties.putProperties(propertiesMap);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// validate</span></span><br><span class="line">        <span class="keyword">new</span> JsonValidator().validate(descriptorProperties);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> descriptorProperties;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重点: 创建DeserializationSchema,进行反序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DeserializationSchema&lt;Row&gt; <span class="title">createDeserializationSchema</span><span class="params">(Map&lt;String, String&gt; properties)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> DescriptorProperties descriptorProperties = getValidatedProperties(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create and configure</span></span><br><span class="line">        <span class="keyword">final</span> CustomJsonRowDeserializationSchema.Builder schema =</span><br><span class="line">                <span class="keyword">new</span> CustomJsonRowDeserializationSchema.Builder(createTypeInformation(descriptorProperties));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> schema.build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重点: 创建SerializationSchema,进行序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SerializationSchema&lt;Row&gt; <span class="title">createSerializationSchema</span><span class="params">(Map&lt;String, String&gt; properties)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> DescriptorProperties descriptorProperties = getValidatedProperties(properties);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> JsonRowSerializationSchema.Builder(createTypeInformation(descriptorProperties)).build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建TypeInformation</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> TypeInformation&lt;Row&gt; <span class="title">createTypeInformation</span><span class="params">(DescriptorProperties descriptorProperties)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (descriptorProperties.containsKey(JsonValidator.FORMAT_SCHEMA)) &#123;</span><br><span class="line">            <span class="keyword">return</span> (RowTypeInfo) descriptorProperties.getType(JsonValidator.FORMAT_SCHEMA);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (descriptorProperties.containsKey(JsonValidator.FORMAT_JSON_SCHEMA)) &#123;</span><br><span class="line">            <span class="keyword">return</span> JsonRowSchemaConverter.convert(descriptorProperties.getString(JsonValidator.FORMAT_JSON_SCHEMA));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> deriveSchema(descriptorProperties.asMap()).toRowType();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong> 由于是自定义Factory类,所以需要在resources文件夹下进行以下操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">创建文件夹</span><br><span class="line">    META-INF&#x2F;services</span><br><span class="line">创建文件(注意文件名就是下面的字符串)</span><br><span class="line">    org.apache.flink.table.factories.TableFactory</span><br><span class="line">文件内容(自定义Factory类路径)</span><br><span class="line">org.apache.flink.formats.custom.CustomJsonRowFormatFactory</span><br></pre></td></tr></table></figure>
<h3 id="创建自定义的DeSerializationSchema-SerializationSchema"><a href="#创建自定义的DeSerializationSchema-SerializationSchema" class="headerlink" title="创建自定义的DeSerializationSchema/SerializationSchema"></a>创建自定义的DeSerializationSchema/SerializationSchema</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">此处可以参考以下实现</span><br><span class="line">    org.apache.flink.formats.json.JsonRowSerializationSchema</span><br><span class="line">    org.apache.flink.formats.json.JsonRowDeserializationSchema</span><br><span class="line">不再赘述</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKafkaTable</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">      |CREATE TABLE demo1 (</span></span><br><span class="line"><span class="string">      |    uid VARCHAR COMMENT &#x27;uid&#x27;,</span></span><br><span class="line"><span class="string">      |    rid VARCHAR COMMENT &#x27;rid&#x27;</span></span><br><span class="line"><span class="string">      |)</span></span><br><span class="line"><span class="string">      |WITH (</span></span><br><span class="line"><span class="string">      |    &#x27;connector.type&#x27; = &#x27;kafka&#x27;, -- 使用 kafka connector</span></span><br><span class="line"><span class="string">      |    &#x27;connector.version&#x27; = &#x27;universal&#x27;,  -- kafka 版本</span></span><br><span class="line"><span class="string">      |    &#x27;connector.topic&#x27; = &#x27;test&#x27;,  -- kafka topic</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.0.key&#x27; = &#x27;zookeeper.connect&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.0.value&#x27; = &#x27;hosts:2181&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.1.key&#x27; = &#x27;bootstrap.servers&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.1.value&#x27; = &#x27;hosts:9092&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.sink-partitioner&#x27; = &#x27;fixed&#x27;,</span></span><br><span class="line"><span class="string">      |    &#x27;update-mode&#x27; = &#x27;append&#x27;,</span></span><br><span class="line"><span class="string">      |    &#x27;format.type&#x27; = &#x27;custom&#x27;,  -- 数据源格式为解析换为自定义</span></span><br><span class="line"><span class="string">      |    &#x27;format.derive-schema&#x27; = &#x27;true&#x27; -- 从 DDL schema 确定 json 解析规则</span></span><br><span class="line"><span class="string">      |)</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink三流相互关联的例子</title>
    <url>/2020/07/27/Flink%E4%B8%89%E6%B5%81%E7%9B%B8%E4%BA%92%E5%85%B3%E8%81%94%E7%9A%84%E4%BE%8B%E5%AD%90/</url>
    <content><![CDATA[<blockquote>
<p>经典案例:三流影响hbase共享课列表(教材id+schoolId为rowkey)</p>
</blockquote>
<span id="more"></span>
<h4 id="debezium结果kafka-connect实时捕获mysql变更事件-binlog"><a href="#debezium结果kafka-connect实时捕获mysql变更事件-binlog" class="headerlink" title="debezium结果kafka connect实时捕获mysql变更事件(binlog)"></a>debezium结果kafka connect实时捕获mysql变更事件(binlog)</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主要介绍kafka的数据特点:</span><br><span class="line">只有after是插入操作</span><br><span class="line">只有before是删除操作</span><br><span class="line">有before又有after是更改操作,before是修改前的数据after是修改后的数据</span><br><span class="line">为null是debezium捕获mysql的binlog中产生的垃圾数据</span><br></pre></td></tr></table></figure>

<h4 id="流一-tp-j-course-question-info-课与试题的关联表"><a href="#流一-tp-j-course-question-info-课与试题的关联表" class="headerlink" title="流一: tp_j_course_question_info 课与试题的关联表"></a>流一: tp_j_course_question_info 课与试题的关联表</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">字段选取:course_id,question_id,is_quote,local_status</span><br><span class="line">过滤条件: course_id &lt;0 and is_quote&#x3D;0 为非引用课列表内容(一课包含多个试题,一个试题在多个课下)</span><br><span class="line">注意逻辑字段:</span><br><span class="line">更新操作中local_status为2是删除操作</span><br><span class="line">后续操作:</span><br><span class="line">1.如果是有效的且是共享课,是删除操作,非引用试题列表长度为0,从共享课列表中删除</span><br><span class="line">2.如果是有效且共享课,是添加操作,非引用试题列表长度为1,在共享课列表中添加数据</span><br></pre></td></tr></table></figure>

<h4 id="流二-tp-course-info-课信息表"><a href="#流二-tp-course-info-课信息表" class="headerlink" title="流二: tp_course_info 课信息表"></a>流二: tp_course_info 课信息表</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">字段选取:share_type,local_status,dc_school_id</span><br><span class="line">过滤条件:course_id &lt;0</span><br><span class="line">后续操作:</span><br><span class="line">1.如果为更新操作:</span><br><span class="line"> 1).如果share_type从其他值变为1并且非引用试题列表不为0;则在共享课列表添加课</span><br><span class="line"> 2).如果local_status从1变成2并且非引用试题列表长度不为0,则从共享课列表移除课</span><br><span class="line">2.如果是插入操作:如share_type&#x3D;1,如非引用试题列表长度不为0,在共享课列表添加课</span><br></pre></td></tr></table></figure>

<h4 id="流三-tp-j-course-teaching-material-课与教材的关联表"><a href="#流三-tp-j-course-teaching-material-课与教材的关联表" class="headerlink" title="流三: tp_j_course_teaching_material 课与教材的关联表"></a>流三: tp_j_course_teaching_material 课与教材的关联表</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果为插入操作,如非引用试题列表长度不为0,如share_type&#x3D;1,local_status&#x3D;1,则在共享课列表添加课</span><br></pre></td></tr></table></figure>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">三个流相互依赖,但是顺序为无序,每个流过来都需要判断其他两个流的相关字段在hbase中是否有值,如果没有值就只存储自己的数据;如果都有值就进行判断对目标hbase表数据是添加课列表还是删除相关课列表</span><br><span class="line">历史数据:使用hive+spark+hbase架构,离线程序进行生成数据</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink中使用Kryo序列化与反序列化</title>
    <url>/2020/05/25/Flink%E4%B8%AD%E4%BD%BF%E7%94%A8Kryo%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    <content><![CDATA[<blockquote>
<p>并没有涉及Flink的本身使用的序列化器的改动,只是flink使用的kryo依赖等级过低</p>
</blockquote>
<span id="more"></span>

<h2 id="前因"><a href="#前因" class="headerlink" title="前因"></a>前因</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">着手将Spark的实时项目重构成Flink项目</span><br><span class="line">在对于数据的清洗时,原项目是将Json格式的数据转换为实体类</span><br><span class="line">最后对其进行Kryo序列化发送给Kafka</span><br><span class="line">在重构时,为了新老项目能够使用同一个数据源</span><br><span class="line">新项目也采用同样的方式</span><br><span class="line">结果发现Kryo使用的一些冲突</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 使用kryo-shaded,不要使用kryo包 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.esotericsoftware<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kryo-shaded<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.0.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_$&#123;scala.2.11.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">exclusions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">exclusion</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.esotericsoftware.kryo<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kryo<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">exclusion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">exclusions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="冲突"><a href="#冲突" class="headerlink" title="冲突"></a>冲突</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在使用Kryo包时,其实在引用flink-scala-2.11包时,其内部含有Kryo包(等级过低,需要排除)</span><br><span class="line">如果直接引用Kryo包会导致项目编译通过,运行时报错,最终可能导致数据丢失</span><br><span class="line">建议使用kryo-shaded包,很好的兼容了两者的异同</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.代码区别</span><br><span class="line">    由于我是纯scala实现的实体类,并且其中有Map数据结构</span><br><span class="line">    为了使其能够兼顾老项目的Java实体类</span><br><span class="line">    需要应用util.HashMap[lang.String,lang.String]</span><br><span class="line">2.Kryo使用</span><br><span class="line">    setRegistrationRequired(false)</span><br><span class="line">    setReferences(false)</span><br><span class="line">    register(Class)</span><br><span class="line">    序列化分为两种</span><br><span class="line">        一种是序列化后带有ID信息,那么反序列化时就需要提供对应的信息才能进行反序列化(默认这种模式是关闭的)</span><br><span class="line">        一种是不带ID信息,直接进行序列化</span><br><span class="line">3.实体类</span><br><span class="line">    实体类需要保证字段顺序与Hive表中的顺序保持一致</span><br><span class="line">    该bug在flink-1.11.0中有得到修复</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>还有线程池的实现但是因为版本问题,这里没有使用-<a href="https://www.programcreek.com/java-api-examples/index.php?api=com.esotericsoftware.kryo.pool.KryoPool">传送门</a></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.skuld.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">ByteArrayInputStream</span>, <span class="type">ByteArrayOutputStream</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.<span class="type">Kryo</span></span><br><span class="line"><span class="keyword">import</span> com.esotericsoftware.kryo.io.&#123;<span class="type">Input</span>, <span class="type">Output</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.skuld.entry.<span class="type">EventGame</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Kryo序列化</span></span><br><span class="line"><span class="comment">  * @author XiaShuai on 2020/5/14.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KryoUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 30k</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">OUTPUT_BUFFER_SIZE</span> = <span class="number">1024</span> * <span class="number">30</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 60k</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">OUTPUT_MAX_BUFFER_SIZE</span> = <span class="number">1024</span> * <span class="number">60</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> kryo = create</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 线程安全，设计为软引用 softReferences，默认不指定容量</span></span><br><span class="line"><span class="comment">    * 根据实际情况调整</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>: <span class="type">Kryo</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> kryo = <span class="keyword">new</span> <span class="type">Kryo</span></span><br><span class="line">    kryo.setRegistrationRequired(<span class="literal">false</span>)</span><br><span class="line">    kryo.setReferences(<span class="literal">false</span>)</span><br><span class="line">    kryo.register(classOf[util.<span class="type">HashMap</span>[_, _]])</span><br><span class="line">    kryo.register(classOf[<span class="type">Array</span>[_]])</span><br><span class="line">    kryo</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">serialize</span></span>[<span class="type">T</span>](`<span class="class"><span class="keyword">object</span>`</span>: <span class="type">T</span>): <span class="type">Array</span>[<span class="type">Byte</span>] = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> baos = <span class="keyword">new</span> <span class="type">ByteArrayOutputStream</span>()</span><br><span class="line">      <span class="keyword">val</span> output = <span class="keyword">new</span> <span class="type">Output</span>(<span class="type">OUTPUT_BUFFER_SIZE</span>, <span class="type">OUTPUT_MAX_BUFFER_SIZE</span>)</span><br><span class="line">      output.setOutputStream(baos)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        kryo.writeObject(output, `<span class="class"><span class="keyword">object</span>`)</span></span><br><span class="line"><span class="class">        <span class="title">output</span>.<span class="title">flush</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="class">        <span class="title">output</span>.<span class="title">getBuffer</span></span></span><br><span class="line"><span class="class">      &#125; <span class="title">finally</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (output != <span class="literal">null</span>) output.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deserialize</span></span>[<span class="type">T</span>](pvBytes: <span class="type">Array</span>[<span class="type">Byte</span>], tClass: <span class="type">Class</span>[<span class="type">T</span>]): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> byteArrayInputStream = <span class="keyword">new</span> <span class="type">ByteArrayInputStream</span>(pvBytes)</span><br><span class="line">      <span class="keyword">val</span> input = <span class="keyword">new</span> <span class="type">Input</span>(byteArrayInputStream)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        input.setBuffer(pvBytes)</span><br><span class="line">        kryo.readObject(input, tClass)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (input != <span class="literal">null</span>) input.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink业务方向遇到的问题与解决</title>
    <url>/2021/03/17/Flink%E4%B8%9A%E5%8A%A1%E6%96%B9%E5%90%91%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<blockquote>
<p>记录在使用Flink的过程中遇到的一系列问题以及解决方式,来源于工作以及<a href="http://apache-flink.147419.n8.nabble.com/">社区</a>.</p>
</blockquote>
<span id="more"></span>

<h3 id="1-FlinkSQL使用IF-condition-col-null-时出现Illegal-use-of-‘NULL’"><a href="#1-FlinkSQL使用IF-condition-col-null-时出现Illegal-use-of-‘NULL’" class="headerlink" title="1.FlinkSQL使用IF(condition,col,null)时出现Illegal use of ‘NULL’?"></a>1.<a href="http://apache-flink.147419.n8.nabble.com/Flink-sql-NULL-td8495.html">FlinkSQL使用IF(condition,col,null)时出现Illegal use of ‘NULL’?</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IF(condition,col,cast(null as int))</span><br><span class="line">不支持隐式类型,需要手动设置NULL的类型SQL才能通过编译.</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-FlinkSQL的StatementSet执行顺序"><a href="#2-FlinkSQL的StatementSet执行顺序" class="headerlink" title="2.FlinkSQL的StatementSet执行顺序?"></a>2.<a href="http://apache-flink.147419.n8.nabble.com/statement-td11143.html">FlinkSQL的StatementSet执行顺序?</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Source 1,2</span><br><span class="line">statementSet.add(insert1)</span><br><span class="line">statementSet.add(insert2)</span><br><span class="line"></span><br><span class="line">情况1:</span><br><span class="line">Sink1和Sink2通过Chain之后和Source处于同一物理节点</span><br><span class="line">执行时每处理一条数据,先发送给其中1个Sink再发送给另一个,然后才处理下一条数据</span><br><span class="line"></span><br><span class="line">情况2:</span><br><span class="line">Sink1和Sink2,Source处于不同的物理节点,异步,每处理一条数据,通过网络发送给Sink1和Sink2</span><br><span class="line">同时网络有Buffer,所以Sink1和Sink2收到数据的顺序完全不确定.</span><br><span class="line"></span><br><span class="line">以上是基于Edgar调度进行执行,如果基于Lazy调度就是另一种模式,等Source完全处理完数据再发送</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>FlinkTableAPI&amp;SQL用户自定义函数新使用</title>
    <url>/2020/10/16/FlinkTableAPI&amp;SQL%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E6%96%B0%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>在之前的blog里面有对自定义函数做过简单的介绍，这里主要讲下新的功能</p>
</blockquote>
<span id="more"></span>

<h2 id="函数有哪些"><a href="#函数有哪些" class="headerlink" title="函数有哪些"></a>函数有哪些</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ScalarFunction(UDF)</span><br><span class="line">TableFunction(UDTF)</span><br><span class="line">AggregateFunction(UDAGG)</span><br><span class="line">TableAggregateFunction(UDTAGG)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="新功能"><a href="#新功能" class="headerlink" title="新功能"></a>新功能</h2><h3 id="类型推断"><a href="#类型推断" class="headerlink" title="类型推断"></a>类型推断</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">@DataTypeHint</span><br><span class="line">声明输入输出类型</span><br><span class="line">// <span class="keyword">function</span> with overloaded evaluation methods</span><br><span class="line">class OverloadedFunction extends ScalarFunction &#123;</span><br><span class="line"></span><br><span class="line">  // no hint required</span><br><span class="line">  def <span class="built_in">eval</span>(a: Long, b: Long): Long = &#123;</span><br><span class="line">    a + b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // define the precision and scale of a decimal</span><br><span class="line">  @DataTypeHint(<span class="string">&quot;DECIMAL(12, 3)&quot;</span>)</span><br><span class="line">  def <span class="built_in">eval</span>(double a, double b): BigDecimal = &#123;</span><br><span class="line">    java.lang.BigDecimal.valueOf(a + b)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // define a nested data <span class="built_in">type</span></span><br><span class="line">  @DataTypeHint(<span class="string">&quot;ROW&lt;s STRING, t TIMESTAMP(3) WITH LOCAL TIME ZONE&gt;&quot;</span>)</span><br><span class="line">  def <span class="built_in">eval</span>(Int i): Row = &#123;</span><br><span class="line">    Row.of(java.lang.String.valueOf(i), java.time.Instant.ofEpochSecond(i))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // allow wildcard input and customly serialized output</span><br><span class="line">  @DataTypeHint(value = <span class="string">&quot;RAW&quot;</span>, bridgedTo = classOf[java.nio.ByteBuffer])</span><br><span class="line">  // 此处表示输入可以是任何类型</span><br><span class="line">  def <span class="built_in">eval</span>(@DataTypeHint(inputGroup = InputGroup.ANY) Object o): java.nio.ByteBuffer = &#123;</span><br><span class="line">    MyUtils.serializeToByteBuffer(o)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@FunctionHint</span><br><span class="line">声明整个函数的数据类型，如果有多种，代表每种重载函数分别对应一种推断，所有参数都是可选的</span><br><span class="line">@FunctionHint(output = new DataTypeHint(<span class="string">&quot;ROW&lt;s STRING, i INT&gt;&quot;</span>))</span><br><span class="line">class OverloadedFunction extends TableFunction[Row] &#123;</span><br><span class="line"></span><br><span class="line">  def <span class="built_in">eval</span>(a: Int, b: Int): Unit = &#123;</span><br><span class="line">    collect(Row.of(<span class="string">&quot;Sum&quot;</span>, Int.box(a + b)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // overloading of arguments is still possible</span><br><span class="line">  def <span class="built_in">eval</span>(): Unit = &#123;</span><br><span class="line">    collect(Row.of(<span class="string">&quot;Empty args&quot;</span>, Int.box(-1)))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// decouples the <span class="built_in">type</span> inference from evaluation methods,</span><br><span class="line">// the <span class="built_in">type</span> inference is entirely determined by the <span class="keyword">function</span> hints</span><br><span class="line">@FunctionHint(</span><br><span class="line">  input = Array(new DataTypeHint(<span class="string">&quot;INT&quot;</span>), new DataTypeHint(<span class="string">&quot;INT&quot;</span>)),</span><br><span class="line">  output = new DataTypeHint(<span class="string">&quot;INT&quot;</span>)</span><br><span class="line">)</span><br><span class="line">@FunctionHint(</span><br><span class="line">  input = Array(new DataTypeHint(<span class="string">&quot;BIGINT&quot;</span>), new DataTypeHint(<span class="string">&quot;BIGINT&quot;</span>)),</span><br><span class="line">  output = new DataTypeHint(<span class="string">&quot;BIGINT&quot;</span>)</span><br><span class="line">)</span><br><span class="line">@FunctionHint(</span><br><span class="line">  input = Array(),</span><br><span class="line">  output = new DataTypeHint(<span class="string">&quot;BOOLEAN&quot;</span>)</span><br><span class="line">)</span><br><span class="line">class OverloadedFunction extends TableFunction[AnyRef] &#123;</span><br><span class="line"></span><br><span class="line">  // an implementer just needs to make sure that a method exists</span><br><span class="line">  // that can be called by the JVM</span><br><span class="line">  @varargs</span><br><span class="line">  def <span class="built_in">eval</span>(o: AnyRef*) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (o.length == 0) &#123;</span><br><span class="line">      collect(Boolean.box(<span class="literal">false</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    collect(o(0))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">自定义类型推断</span><br><span class="line">public static class LiteralFunction extends ScalarFunction &#123;</span><br><span class="line">  public Object <span class="built_in">eval</span>(String s, String <span class="built_in">type</span>) &#123;</span><br><span class="line">    switch (<span class="built_in">type</span>) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;INT&quot;</span>:</span><br><span class="line">        <span class="built_in">return</span> Integer.valueOf(s);</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;DOUBLE&quot;</span>:</span><br><span class="line">        <span class="built_in">return</span> Double.valueOf(s);</span><br><span class="line">      <span class="keyword">case</span> <span class="string">&quot;STRING&quot;</span>:</span><br><span class="line">      default:</span><br><span class="line">        <span class="built_in">return</span> s;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // the automatic, reflection-based <span class="built_in">type</span> inference is disabled and</span><br><span class="line">  // replaced by the following logic</span><br><span class="line">  @Override</span><br><span class="line">  public TypeInference getTypeInference(DataTypeFactory typeFactory) &#123;</span><br><span class="line">    <span class="built_in">return</span> TypeInference.newBuilder()</span><br><span class="line">      // specify typed arguments</span><br><span class="line">      // parameters will be casted implicitly to those types <span class="keyword">if</span> necessary</span><br><span class="line">      .typedArguments(DataTypes.STRING(), DataTypes.STRING())</span><br><span class="line">      // specify a strategy <span class="keyword">for</span> the result data <span class="built_in">type</span> of the <span class="keyword">function</span></span><br><span class="line">      .outputTypeStrategy(callContext -&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (!callContext.isArgumentLiteral(1) || callContext.isArgumentNull(1)) &#123;</span><br><span class="line">          throw callContext.newValidationError(<span class="string">&quot;Literal expected for second argument.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        // <span class="built_in">return</span> a data <span class="built_in">type</span> based on a literal</span><br><span class="line">        final String literal = callContext.getArgumentValue(1, String.class).orElse(<span class="string">&quot;STRING&quot;</span>);</span><br><span class="line">        switch (literal) &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="string">&quot;INT&quot;</span>:</span><br><span class="line">            <span class="built_in">return</span> Optional.of(DataTypes.INT().notNull());</span><br><span class="line">          <span class="keyword">case</span> <span class="string">&quot;DOUBLE&quot;</span>:</span><br><span class="line">            <span class="built_in">return</span> Optional.of(DataTypes.DOUBLE().notNull());</span><br><span class="line">          <span class="keyword">case</span> <span class="string">&quot;STRING&quot;</span>:</span><br><span class="line">          default:</span><br><span class="line">            <span class="built_in">return</span> Optional.of(DataTypes.STRING());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      .build();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="是否产生确定性结果"><a href="#是否产生确定性结果" class="headerlink" title="是否产生确定性结果"></a>是否产生确定性结果</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">isDeterministic()</span><br><span class="line">如果产生的结果并不是一个确定值(random,date,now),必须返回false</span><br><span class="line">默认情况下,返回true</span><br></pre></td></tr></table></figure>

<h3 id="获取全局信息"><a href="#获取全局信息" class="headerlink" title="获取全局信息"></a>获取全局信息</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">提供open以及close方法获取FunctionContext信息</span><br><span class="line">getMetricGroup()----&gt;此并行子任务的度量标准组</span><br><span class="line">getCachedFile(name)----&gt;分布式缓存文件的本地临时文件副本</span><br><span class="line">getJobParameter(name, defaultValue)----&gt;与给定键关联的全局作业参数值</span><br><span class="line">getExternalResourceInfos(resourceName)----&gt;返回与给定键关联的一组外部资源信息</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ScalarFunction"><a href="#ScalarFunction" class="headerlink" title="ScalarFunction"></a>ScalarFunction</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.<span class="type">InputGroup</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">ScalarFunction</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashFunction</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// take any data type and return INT</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(<span class="meta">@DataTypeHint</span>(inputGroup = <span class="type">InputGroup</span>.<span class="type">ANY</span>) o: <span class="type">AnyRef</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    o.hashCode()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">TableEnvironment</span>.create(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// call function &quot;inline&quot; without registration in Table API</span></span><br><span class="line">env.from(<span class="string">&quot;MyTable&quot;</span>).select(call(classOf[<span class="type">HashFunction</span>], $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// register function</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;HashFunction&quot;</span>, classOf[<span class="type">HashFunction</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">// call registered function in Table API</span></span><br><span class="line">env.from(<span class="string">&quot;MyTable&quot;</span>).select(call(<span class="string">&quot;HashFunction&quot;</span>, $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// call registered function in SQL</span></span><br><span class="line">env.sqlQuery(<span class="string">&quot;SELECT HashFunction(myField) FROM MyTable&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TableFunction"><a href="#TableFunction" class="headerlink" title="TableFunction"></a>TableFunction</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.<span class="type">DataTypeHint</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.annotation.<span class="type">FunctionHint</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">TableFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@FunctionHint</span>(output = <span class="keyword">new</span> <span class="type">DataTypeHint</span>(<span class="string">&quot;ROW&lt;word STRING, length INT&gt;&quot;</span>))</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SplitFunction</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>[<span class="type">Row</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">eval</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// use collect(...) to emit a row</span></span><br><span class="line">    str.split(<span class="string">&quot; &quot;</span>).foreach(s =&gt; collect(<span class="type">Row</span>.of(s, <span class="type">Int</span>.box(s.length))))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">TableEnvironment</span>.create(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// call function &quot;inline&quot; without registration in Table API</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .joinLateral(call(classOf[<span class="type">SplitFunction</span>], $<span class="string">&quot;myField&quot;</span>)</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .leftOuterJoinLateral(call(classOf[<span class="type">SplitFunction</span>], $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// rename fields of the function in Table API</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .leftOuterJoinLateral(call(classOf[<span class="type">SplitFunction</span>], $<span class="string">&quot;myField&quot;</span>).as(<span class="string">&quot;newWord&quot;</span>, <span class="string">&quot;newLength&quot;</span>))</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;newWord&quot;</span>, $<span class="string">&quot;newLength&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// register function</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;SplitFunction&quot;</span>, classOf[<span class="type">SplitFunction</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">// call registered function in Table API</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .joinLateral(call(<span class="string">&quot;SplitFunction&quot;</span>, $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .leftOuterJoinLateral(call(<span class="string">&quot;SplitFunction&quot;</span>, $<span class="string">&quot;myField&quot;</span>))</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;word&quot;</span>, $<span class="string">&quot;length&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// call registered function in SQL</span></span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, word, length &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable, LATERAL TABLE(SplitFunction(myField))&quot;</span>)</span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, word, length &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable &quot;</span> +</span><br><span class="line">  <span class="string">&quot;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) ON TRUE&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// rename fields of the function in SQL</span></span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, newWord, newLength &quot;</span> +</span><br><span class="line">  <span class="string">&quot;FROM MyTable &quot;</span> +</span><br><span class="line">  <span class="string">&quot;LEFT JOIN LATERAL TABLE(SplitFunction(myField)) AS T(newWord, newLength) ON TRUE&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">AggregateFunction</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// mutable accumulator of structured type for the aggregate function</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">WeightedAvgAccumulator</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  var sum: <span class="type">Long</span> = 0,</span></span></span><br><span class="line"><span class="class"><span class="params">  var count: <span class="type">Int</span> = 0</span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// function that takes (value BIGINT, weight INT), stores intermediate results in a structured</span></span></span><br><span class="line"><span class="class"><span class="comment">// type of WeightedAvgAccumulator, and returns the weighted average as BIGINT</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">WeightedAvg</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>[java.lang.<span class="type">Long</span>, <span class="type">WeightedAvgAccumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">WeightedAvgAccumulator</span> = &#123;</span><br><span class="line">    <span class="type">WeightedAvgAccumulator</span>()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>(acc: <span class="type">WeightedAvgAccumulator</span>): java.lang.<span class="type">Long</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (acc.count == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="literal">null</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      acc.sum / acc.count</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">WeightedAvgAccumulator</span>, iValue: java.lang.<span class="type">Long</span>, iWeight: java.lang.<span class="type">Integer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.sum += iValue * iWeight</span><br><span class="line">    acc.count += iWeight</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">retract</span></span>(acc: <span class="type">WeightedAvgAccumulator</span>, iValue: java.lang.<span class="type">Long</span>, iWeight: java.lang.<span class="type">Integer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.sum -= iValue * iWeight</span><br><span class="line">    acc.count -= iWeight</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc: <span class="type">WeightedAvgAccumulator</span>, it: java.lang.<span class="type">Iterable</span>[<span class="type">WeightedAvgAccumulator</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> iter = it.iterator()</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> a = iter.next()</span><br><span class="line">      acc.count += a.count</span><br><span class="line">      acc.sum += a.sum</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetAccumulator</span></span>(acc: <span class="type">WeightedAvgAccumulator</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    acc.count = <span class="number">0</span></span><br><span class="line">    acc.sum = <span class="number">0</span>L</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">TableEnvironment</span>.create(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// call function &quot;inline&quot; without registration in Table API</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .groupBy($<span class="string">&quot;myField&quot;</span>)</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, call(classOf[<span class="type">WeightedAvg</span>], $<span class="string">&quot;value&quot;</span>, $<span class="string">&quot;weight&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// register function</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;WeightedAvg&quot;</span>, classOf[<span class="type">WeightedAvg</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">// call registered function in Table API</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .groupBy($<span class="string">&quot;myField&quot;</span>)</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, call(<span class="string">&quot;WeightedAvg&quot;</span>, $<span class="string">&quot;value&quot;</span>, $<span class="string">&quot;weight&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// call registered function in SQL</span></span><br><span class="line">env.sqlQuery(</span><br><span class="line">  <span class="string">&quot;SELECT myField, WeightedAvg(value, weight) FROM MyTable GROUP BY myField&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="方法说明"><a href="#方法说明" class="headerlink" title="方法说明"></a>方法说明</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">必要</span><br><span class="line">    createAccumulator()</span><br><span class="line">    accumulate()</span><br><span class="line">    getValue()</span><br><span class="line">可选</span><br><span class="line">    retract() 开窗聚合必须实现</span><br><span class="line">    merge() 有界聚合会话聚合必须实现</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TableAggregateFunction"><a href="#TableAggregateFunction" class="headerlink" title="TableAggregateFunction"></a>TableAggregateFunction</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.<span class="type">Integer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.<span class="type">Tuple2</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.<span class="type">TableAggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// mutable accumulator of structured type for the aggregate function</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Top2Accumulator</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  var first: <span class="type">Integer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  var second: <span class="type">Integer</span></span></span></span><br><span class="line"><span class="class"><span class="params"></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// function that takes (value INT), stores intermediate results in a structured</span></span></span><br><span class="line"><span class="class"><span class="comment">// type of Top2Accumulator, and returns the result as a structured type of Tuple2[Integer, Integer]</span></span></span><br><span class="line"><span class="class"><span class="comment">// for value and rank</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Top2</span> <span class="keyword">extends</span> <span class="title">TableAggregateFunction</span>[<span class="type">Tuple2</span>[<span class="type">Integer</span>, <span class="type">Integer</span>], <span class="title">Top2Accumulator</span>] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createAccumulator</span></span>(): <span class="type">Top2Accumulator</span> = &#123;</span><br><span class="line">    <span class="type">Top2Accumulator</span>(</span><br><span class="line">      <span class="type">Integer</span>.<span class="type">MIN_VALUE</span>,</span><br><span class="line">      <span class="type">Integer</span>.<span class="type">MIN_VALUE</span></span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">accumulate</span></span>(acc: <span class="type">Top2Accumulator</span>, value: <span class="type">Integer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (value &gt; acc.first) &#123;</span><br><span class="line">      acc.second = acc.first</span><br><span class="line">      acc.first = value</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (value &gt; acc.second) &#123;</span><br><span class="line">      acc.second = value</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(acc: <span class="type">Top2Accumulator</span>, it: java.lang.<span class="type">Iterable</span>[<span class="type">Top2Accumulator</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> iter = it.iterator()</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> otherAcc = iter.next()</span><br><span class="line">      accumulate(acc, otherAcc.first)</span><br><span class="line">      accumulate(acc, otherAcc.second)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">emitValue</span></span>(acc: <span class="type">Top2Accumulator</span>, out: <span class="type">Collector</span>[<span class="type">Tuple2</span>[<span class="type">Integer</span>, <span class="type">Integer</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// emit the value and rank</span></span><br><span class="line">    <span class="keyword">if</span> (acc.first != <span class="type">Integer</span>.<span class="type">MIN_VALUE</span>) &#123;</span><br><span class="line">      out.collect(<span class="type">Tuple2</span>.of(acc.first, <span class="number">1</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (acc.second != <span class="type">Integer</span>.<span class="type">MIN_VALUE</span>) &#123;</span><br><span class="line">      out.collect(<span class="type">Tuple2</span>.of(acc.second, <span class="number">2</span>))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">TableEnvironment</span>.create(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// call function &quot;inline&quot; without registration in Table API</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .groupBy($<span class="string">&quot;myField&quot;</span>)</span><br><span class="line">  .flatAggregate(call(classOf[<span class="type">Top2</span>], $<span class="string">&quot;value&quot;</span>))</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;f0&quot;</span>, $<span class="string">&quot;f1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// call function &quot;inline&quot; without registration in Table API</span></span><br><span class="line"><span class="comment">// but use an alias for a better naming of Tuple2&#x27;s fields</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .groupBy($<span class="string">&quot;myField&quot;</span>)</span><br><span class="line">  .flatAggregate(call(classOf[<span class="type">Top2</span>], $<span class="string">&quot;value&quot;</span>).as(<span class="string">&quot;value&quot;</span>, <span class="string">&quot;rank&quot;</span>))</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;value&quot;</span>, $<span class="string">&quot;rank&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// register function</span></span><br><span class="line">env.createTemporarySystemFunction(<span class="string">&quot;Top2&quot;</span>, classOf[<span class="type">Top2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">// call registered function in Table API</span></span><br><span class="line">env</span><br><span class="line">  .from(<span class="string">&quot;MyTable&quot;</span>)</span><br><span class="line">  .groupBy($<span class="string">&quot;myField&quot;</span>)</span><br><span class="line">  .flatAggregate(call(<span class="string">&quot;Top2&quot;</span>, $<span class="string">&quot;value&quot;</span>).as(<span class="string">&quot;value&quot;</span>, <span class="string">&quot;rank&quot;</span>))</span><br><span class="line">  .select($<span class="string">&quot;myField&quot;</span>, $<span class="string">&quot;value&quot;</span>, $<span class="string">&quot;rank&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink使用DDL方式注册表</title>
    <url>/2020/01/09/Flink%E4%BD%BF%E7%94%A8DDL%E6%96%B9%E5%BC%8F%E6%B3%A8%E5%86%8C%E8%A1%A8/</url>
    <content><![CDATA[<blockquote>
<p>官网例子实现,主要还是语法与数据类型的使用,官网并没有详细的Demo</p>
</blockquote>
<span id="more"></span>

<h2 id="KafkaSQL代码"><a href="#KafkaSQL代码" class="headerlink" title="KafkaSQL代码"></a>KafkaSQL代码</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-08 17:11</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaSQLExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> bsEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line">    <span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(bsEnv, bsSettings)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">&quot;create table test (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;`business` varchar,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;`ts` bigint&quot;</span> +</span><br><span class="line">      <span class="string">&quot;) with (&quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.type&#x27; = &#x27;kafka&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.version&#x27; = &#x27;0.10&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.topic&#x27; = &#x27;test&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;update-mode&#x27; = &#x27;append&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.properties.0.key&#x27; = &#x27;zookeeper.connect&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.properties.0.value&#x27; = &#x27;hadoop01:2181&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.properties.1.key&#x27; = &#x27;bootstrap.servers&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.properties.1.value&#x27; = &#x27;hadoop01:9092&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.properties.2.key&#x27; = &#x27;group.id&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.properties.2.value&#x27; = &#x27;kafkasql&#x27;, &quot;</span> +</span><br><span class="line">      <span class="comment">//      &quot; &#x27;connector.startup-mode&#x27; = &#x27;earliest-offset&#x27;, &quot; +</span></span><br><span class="line">      <span class="string">&quot; &#x27;connector.startup-mode&#x27; = &#x27;latest-offset&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;format.type&#x27; = &#x27;json&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;format.derive-schema&#x27; = &#x27;true&#x27; &quot;</span> +</span><br><span class="line">      <span class="string">&quot;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.sqlUpdate(sql)</span><br><span class="line"></span><br><span class="line">    tableEnv.toAppendStream[<span class="type">Row</span>](tableEnv.sqlQuery(<span class="string">&quot;select * from test&quot;</span>)).print()</span><br><span class="line"></span><br><span class="line">    tableEnv.execute(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBaseSQL代码"><a href="#HBaseSQL代码" class="headerlink" title="HBaseSQL代码"></a>HBaseSQL代码</h2><p><strong>注意:</strong> 不知道是否是版本问题,HBase-1.2.0版本运行下列代码会报<code>Can&#39;t get the location for replica 0</code>错误</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-08 16:42</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseSQLExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;hdfs&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> bsEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line">    <span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(bsEnv, bsSettings)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">&quot;create table test (&quot;</span> +</span><br><span class="line">      <span class="string">&quot;`name` string,&quot;</span> +</span><br><span class="line">      <span class="string">&quot;`info` ROW&lt;name varchar, age varchar&gt;&quot;</span> +</span><br><span class="line">      <span class="string">&quot;) with (&quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.type&#x27; = &#x27;hbase&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.version&#x27; = &#x27;1.4.3&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.table-name&#x27; = &#x27;user&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.zookeeper.quorum&#x27; = &#x27;hadoop01:2181&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.zookeeper.znode.parent&#x27; = &#x27;/hbase&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.write.buffer-flush.max-size&#x27; = &#x27;1mb&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.write.buffer-flush.max-rows&#x27; = &#x27;1&#x27;, &quot;</span> +</span><br><span class="line">      <span class="string">&quot; &#x27;connector.write.buffer-flush.interval&#x27; = &#x27;2s&#x27; &quot;</span> +</span><br><span class="line">      <span class="string">&quot;)&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.sqlUpdate(sql)</span><br><span class="line"></span><br><span class="line">    tableEnv.toAppendStream[<span class="type">Row</span>](tableEnv.sqlQuery(<span class="string">&quot;select * from test&quot;</span>)).print()</span><br><span class="line"></span><br><span class="line">    tableEnv.execute(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>flink</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink代码编写中的环境配置</title>
    <url>/2020/04/21/Flink%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E4%B8%AD%E7%9A%84%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<blockquote>
<p>在使用Env时可以配置调优的参数</p>
</blockquote>
<span id="more"></span>

<h2 id="OptimizerConfigOptions-优化配置"><a href="#OptimizerConfigOptions-优化配置" class="headerlink" title="OptimizerConfigOptions(优化配置)"></a>OptimizerConfigOptions(优化配置)</h2><h3 id="BATCH-STREAMING"><a href="#BATCH-STREAMING" class="headerlink" title="BATCH_STREAMING"></a>BATCH_STREAMING</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 聚合阶段策略</span></span><br><span class="line">TABLE_OPTIMIZER_AGG_PHASE_STRATEGY 默认值AUTO</span><br><span class="line">    AUTO: 聚合阶段没有特殊的执行器</span><br><span class="line">    TWO_PHASE: 强制使用具有localAggregate和globalAggregate的两阶段聚合;如果聚合调用不支持将优化分为两个阶段，则我们仍将使用一个阶段的聚合</span><br><span class="line">    ONE_PHASE: 强制使用只有CompleteGlobalAggregate的一个阶段聚合</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 重用sub-plans</span></span><br><span class="line">TABLE_OPTIMIZER_REUSE_SUB_PLAN_ENABLED 默认值<span class="literal">true</span></span><br><span class="line">    如果为<span class="literal">true</span>,优化器将尝试找出重复的sub-plans并重用它们</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重用源表</span></span><br><span class="line">TABLE_OPTIMIZER_REUSE_SOURCE_ENABLED 默认值<span class="literal">true</span></span><br><span class="line">    如果为<span class="literal">true</span>,优化器将尝试找出重复的表源并重用它们,只有当TABLE_OPTIMIZER_REUSE_SUB_PLAN_ENABLED为<span class="literal">true</span>工作</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 谓词下推</span></span><br><span class="line">TABLE_OPTIMIZER_SOURCE_PREDICATE_PUSHDOWN_ENABLED 默认值<span class="literal">true</span></span><br><span class="line">    如果为<span class="literal">true</span>,优化器将把谓词下推到FilterableTableSource中</span><br><span class="line"></span><br><span class="line"><span class="comment"># 联接排序</span></span><br><span class="line">TABLE_OPTIMIZER_JOIN_REORDER_ENABLED 默认值<span class="literal">false</span></span><br><span class="line">    在优化器中启用联接重新排序</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="BATCH"><a href="#BATCH" class="headerlink" title="BATCH"></a>BATCH</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 联接广播字节大小</span></span><br><span class="line">TABLE_OPTIMIZER_BROADCAST_JOIN_THRESHOLD 默认值1048576L</span><br><span class="line">    配置执行联接时将广播给所有工作节点的表的最大字节大小.通过将此值设置为-1来禁用广播</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="STREAMING"><a href="#STREAMING" class="headerlink" title="STREAMING"></a>STREAMING</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拆分聚合</span></span><br><span class="line">TABLE_OPTIMIZER_DISTINCT_AGG_SPLIT_ENABLED 默认值<span class="literal">false</span></span><br><span class="line">    告诉优化器是否将不同聚合(例如COUNT(distinct col),SUM(distinct col))拆分为两个级别.</span><br><span class="line">    第一个聚合由一个额外的key进行洗牌,该key使用distinct_key的hashcode和bucket的数量进行计算.</span><br><span class="line">    当不同的聚合中存在数据倾斜时,这种优化非常有用,并提供了扩展作业的能力</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分聚合桶数</span></span><br><span class="line">TABLE_OPTIMIZER_DISTINCT_AGG_SPLIT_BUCKET_NUM 默认值1024</span><br><span class="line">    在拆分不同聚合时配置存储桶数</span><br><span class="line">    该数字在第一级聚合中用于计算一个bucket key <span class="string">&#x27;hash_code（distinct_key）%bucket_NUM&#x27;</span></span><br><span class="line">    它在拆分后用作另一个组key</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ExecutionConfigOptions-执行配置"><a href="#ExecutionConfigOptions-执行配置" class="headerlink" title="ExecutionConfigOptions(执行配置)"></a>ExecutionConfigOptions(执行配置)</h2><h3 id="BATCH-STREAMING-1"><a href="#BATCH-STREAMING-1" class="headerlink" title="BATCH_STREAMING"></a>BATCH_STREAMING</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 最大异步查找JOIN数</span></span><br><span class="line">TABLE_EXEC_ASYNC_LOOKUP_BUFFER_CAPACITY 默认值100</span><br><span class="line">    异步查找联接可以触发的最大异步i/o操作数</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异步操作超时时间</span></span><br><span class="line">TABLE_EXEC_ASYNC_LOOKUP_TIMEOUT 默认值3 min</span><br><span class="line">    异步操作完成的异步超时</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="BATCH-1"><a href="#BATCH-1" class="headerlink" title="BATCH"></a>BATCH</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排序后限制大小</span></span><br><span class="line">TABLE_EXEC_SORT_DEFAULT_LIMIT 默认值-1</span><br><span class="line">    当用户未在order by之后设置限制时的默认限制</span><br><span class="line">    -1表示忽略此配置</span><br><span class="line"></span><br><span class="line"><span class="comment"># 外部合并排序的最大扇入</span></span><br><span class="line">TABLE_EXEC_SORT_MAX_NUM_FILE_HANDLES 默认值128</span><br><span class="line">    外部合并排序的最大扇入</span><br><span class="line">    它限制每个操作员的文件句柄数</span><br><span class="line">    如果太小,可能会导致中间合并</span><br><span class="line">    但如果太大,会造成同时打开的文件太多,消耗内存,导致随机读取</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 合并已排序的溢出文件</span></span><br><span class="line">TABLE_EXEC_SORT_ASYNC_MERGE_ENABLED 默认值<span class="literal">true</span></span><br><span class="line">    是否异步合并已排序的溢出文件</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 压缩溢出数据</span></span><br><span class="line">TABLE_EXEC_SPILL_COMPRESSION_ENABLED 默认值<span class="literal">true</span></span><br><span class="line">    是否压缩溢出的数据</span><br><span class="line">    目前我们只支持sort和<span class="built_in">hash</span> agg以及<span class="built_in">hash</span> join运算符的压缩溢出数据</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 压缩内存</span></span><br><span class="line">TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE 默认值64 kb</span><br><span class="line">    溢出数据时用于压缩的内存大小</span><br><span class="line">    内存越大,压缩比越高,但作业将消耗更多的内存资源</span><br><span class="line">    </span><br><span class="line"><span class="comment"># GroupWindow聚合元素限制</span></span><br><span class="line">TABLE_EXEC_WINDOW_AGG_BUFFER_SIZE_LIMIT 默认值100000</span><br><span class="line">    设置组窗口agg运算符中使用的窗口元素缓冲区大小限制</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用运算符</span></span><br><span class="line">TABLE_EXEC_DISABLED_OPERATORS 无默认值</span><br><span class="line">    主要用于测试,运算符名称的逗号分隔列表,每个名称表示一种禁用的运算符</span><br><span class="line">    可以禁用的运算符包括</span><br><span class="line">        NestedLoopJoin</span><br><span class="line">        ShuffleHashJoin</span><br><span class="line">        BroadcastHashJoin</span><br><span class="line">        SortMergeJoin</span><br><span class="line">        HashAgg</span><br><span class="line">        SortAgg</span><br><span class="line">    默认情况下,不禁用任何运算符</span><br><span class="line"></span><br><span class="line"><span class="comment"># shuffle执行模式</span></span><br><span class="line">TABLE_EXEC_SHUFFLE_MODE 默认值batch</span><br><span class="line">    设置执行shuffle模式,只能设置batch或pipeline</span><br><span class="line">    batch: 作业将逐步运行</span><br><span class="line">    pipeline: 作业将以流模式运行,但当发送方持有资源等待向接收方发送数据时,接收方等待资源启动可能会导致资源死锁</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="STREAMING-1"><a href="#STREAMING-1" class="headerlink" title="STREAMING"></a>STREAMING</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Source空闲超时时间</span></span><br><span class="line">TABLE_EXEC_SOURCE_IDLE_TIMEOUT 默认值-1 ms</span><br><span class="line">    当一个源在超时时间内没有接收到任何元素时,它将被标记为临时空闲</span><br><span class="line">    这允许下游任务在水印空闲时无需等待来自此源的水印就可以推进其水印</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 默认并行度</span></span><br><span class="line">TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM 默认值-1</span><br><span class="line">    为所有要与并行实例一起运行的运算符(如聚合,联接,筛选器)设置默认并行度</span><br><span class="line">    此配置的优先级高于StreamExecutionEnvironment的并行性</span><br><span class="line">    实际上,此配置会覆盖StreamExecutionEnvironment的并行性</span><br><span class="line">    值-1表示未设置默认并行性,然后它将回退以使用StreamExecutionEnvironment的并行性</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 小批量优化</span></span><br><span class="line">TABLE_EXEC_MINIBATCH_ENABLED 默认值<span class="literal">false</span></span><br><span class="line">    指定是否启用小批量优化</span><br><span class="line">    MiniBatch是一种缓冲输入记录以减少状态访问的优化</span><br><span class="line">    这在默认情况下是禁用的</span><br><span class="line">    要启用此功能,用户应将此配置设置为<span class="literal">true</span></span><br><span class="line">    注意: 如果启用了小批量,则必须设置<span class="string">&#x27;table.exec.mini batch.allow latency&#x27;</span>和<span class="string">&#x27;table.exec.mini batch.size&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大延迟</span></span><br><span class="line">TABLE_EXEC_MINIBATCH_ALLOW_LATENCY 默认值-1 ms</span><br><span class="line">    最大延迟可用于小批量缓冲输入记录</span><br><span class="line">    MiniBatch是一种缓冲输入记录以减少状态访问的优化</span><br><span class="line">    当达到最大缓冲记录数时,将以允许的延迟间隔触发MiniBatch</span><br><span class="line">    注意: 如果TABLE_EXEC_MINIBATCH_ENABLED设置为<span class="literal">true</span>,则其值必须大于零</span><br><span class="line"></span><br><span class="line"><span class="comment"># MiniBatch最大输入记录数</span></span><br><span class="line">TABLE_EXEC_MINIBATCH_SIZE 默认值-1L</span><br><span class="line">    可以为MiniBatch缓冲的最大输入记录数</span><br><span class="line">    MiniBatch是一种缓冲输入记录以减少状态访问的优化</span><br><span class="line">    当达到最大缓冲记录数时,将以允许的延迟间隔触发MiniBatch</span><br><span class="line">    注意: MiniBatch当前仅适用于非窗口聚合,如果TABLE_EXEC_MINIBATCH_ENABLED设置为<span class="literal">true</span>,则其值必须为正</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="不被包括的配置"><a href="#不被包括的配置" class="headerlink" title="不被包括的配置"></a>不被包括的配置</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从Flink1.10开始,这被解释为一个权重提示,而不是绝对的内存需求</span></span><br><span class="line"><span class="comment"># 用户不需要更改这些经过仔细调整的权重提示</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 外部缓冲区大小</span></span><br><span class="line">TABLE_EXEC_RESOURCE_EXTERNAL_BUFFER_MEMORY 默认值10 mb</span><br><span class="line">    设置<span class="string">&quot;排序合并联接&quot;</span>,<span class="string">&quot;嵌套联接&quot;</span>和<span class="string">&quot;OVER&quot;</span>窗口中使用的外部缓冲区内存大小</span><br><span class="line">    注意: 内存大小只是一个权重提示,它会影响任务中单个操作员可以应用的内存权重</span><br><span class="line">    实际使用的内存取决于运行环境</span><br><span class="line"></span><br><span class="line"><span class="comment"># 哈希聚合managed内存大小</span></span><br><span class="line">TABLE_EXEC_RESOURCE_HASH_AGG_MEMORY 默认值128 mb</span><br><span class="line">    设置哈希聚合运算符的managed内存大小</span><br><span class="line">    注意: 内存大小只是一个权重提示,它会影响任务中单个操作员可以应用的内存权重</span><br><span class="line">    实际使用的内存取决于运行环境</span><br><span class="line"></span><br><span class="line"><span class="comment"># 哈希联接managed内存大小</span></span><br><span class="line">TABLE_EXEC_RESOURCE_HASH_JOIN_MEMORY 默认值128 mb</span><br><span class="line">    设置哈希联接运算符的managed内存,它定义了下限</span><br><span class="line">    注意: 内存大小只是一个权重提示,它会影响任务中单个操作员可以应用的内存权重</span><br><span class="line">    实际使用的内存取决于运行环境</span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序缓冲区大小</span></span><br><span class="line">TABLE_EXEC_RESOURCE_SORT_MEMORY 默认值128 mb</span><br><span class="line">    设置排序运算符的managed缓冲区内存大小</span><br><span class="line">    注意: 内存大小只是一个权重提示,它会影响任务中单个操作员可以应用的内存权重</span><br><span class="line">    实际使用的内存取决于运行环境</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="常用的配置"><a href="#常用的配置" class="headerlink" title="常用的配置"></a>常用的配置</h2><h3 id="RelNodeBlock"><a href="#RelNodeBlock" class="headerlink" title="RelNodeBlock"></a>RelNodeBlock</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在构造公共子图时禁用union all节点作为断点</span></span><br><span class="line">table.optimizer.union-all-as-breakpoint-disabled 默认<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当为true时,优化器将尝试通过摘要找出重复的子计划来构建优化块(又称公共子图),每个优化块都将独立优化</span></span><br><span class="line">table.optimizer.reuse-optimize-block-with-digest-enabled 默认<span class="literal">false</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="WindowEmitStrategy"><a href="#WindowEmitStrategy" class="headerlink" title="WindowEmitStrategy"></a>WindowEmitStrategy</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># WaterMark到达窗口结束前的发射策略(提前)</span></span><br><span class="line">table.exec.emit.early-fire.enabled 默认<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># WaterMark到达窗口结束前的发射间隔时间</span></span><br><span class="line">table.exec.emit.late-fire.delay</span><br><span class="line"></span><br><span class="line"><span class="comment"># WaterMark到达窗口结束后的发射策略(延迟)</span></span><br><span class="line">table.exec.emit.late-fire.enabled 默认<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># WaterMark到达窗口结束后的发射间隔时间</span></span><br><span class="line">table.exec.emit.late-fire.delay</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// BATCH</span></span><br><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inBatchMode().build()</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.create(settings)</span><br><span class="line"></span><br><span class="line"><span class="comment">// STREAMING</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line"></span><br><span class="line">tEnv.getConfig.getConfiguration.setBoolean(<span class="type">OptimizerConfigOptions</span>.<span class="type">TABLE_OPTIMIZER_JOIN_REORDER_ENABLED</span>, <span class="literal">true</span>)</span><br><span class="line">tEnv.getConfig.getConfiguration.setBoolean(<span class="type">OptimizerConfigOptions</span>.<span class="type">TABLE_OPTIMIZER_REUSE_SOURCE_ENABLED</span>, <span class="literal">false</span>)</span><br><span class="line">tEnv.getConfig.getConfiguration.setLong(<span class="type">OptimizerConfigOptions</span>.<span class="type">TABLE_OPTIMIZER_BROADCAST_JOIN_THRESHOLD</span>, <span class="number">10485760</span>L)</span><br><span class="line">tEnv.getConfig.getConfiguration.setInteger(<span class="type">ExecutionConfigOptions</span>.<span class="type">TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM</span>, <span class="number">1</span>)</span><br><span class="line">tEnv.getConfig.getConfiguration.setInteger(<span class="type">ExecutionConfigOptions</span>.<span class="type">TABLE_EXEC_SORT_DEFAULT_LIMIT</span>, <span class="number">200</span>)</span><br><span class="line">tEnv.getConfig.addConfiguration(<span class="type">GlobalConfiguration</span>.loadConfiguration)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink写入Hive</title>
    <url>/2019/07/02/Flink%E5%86%99%E5%85%A5Hive/</url>
    <content><![CDATA[<blockquote>
<p>虽然说是Flink写入Hive,其实真正的操作是Flink写入Hdfs,Hive进行刷新操作.</p>
</blockquote>
<span id="more"></span>

<h2 id="引用依赖"><a href="#引用依赖" class="headerlink" title="引用依赖"></a>引用依赖</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.12.7<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">flink.version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="name">flink.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-filesystem_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-hcatalog<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.6.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.10_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="Kafka数据格式类"><a href="#Kafka数据格式类" class="headerlink" title="Kafka数据格式类"></a>Kafka数据格式类</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.dev.flink.stream.hive</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.typeinfo.BasicTypeInfo</span><br><span class="line">import org.apache.flink.streaming.util.serialization.KeyedDeserializationSchema</span><br><span class="line">import org.json.JSONObject</span><br><span class="line"></span><br><span class="line">class JsonDeserializationSchema extends KeyedDeserializationSchema[String] &#123;</span><br><span class="line"></span><br><span class="line">  override def isEndOfStream(nextElement: String) &#x3D; false</span><br><span class="line"></span><br><span class="line">  override def deserialize(messageKey: Array[Byte], message: Array[Byte], topic: String, partition: Int, offset: Long) &#x3D; &#123;</span><br><span class="line">    val json &#x3D; new JSONObject()</span><br><span class="line">    json.put(&quot;topic&quot;, topic)</span><br><span class="line">    json.put(&quot;partition&quot;, partition)</span><br><span class="line">    json.put(&quot;offset&quot;, offset)</span><br><span class="line">    json.put(&quot;key&quot;, if (messageKey &#x3D;&#x3D; null) null else new String(messageKey))</span><br><span class="line">    json.put(&quot;value&quot;, if (message &#x3D;&#x3D; null) null else new String(message))</span><br><span class="line">    json.toString()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getProducedType &#x3D; BasicTypeInfo.STRING_TYPE_INFO</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Message信息封装类"><a href="#Message信息封装类" class="headerlink" title="Message信息封装类"></a>Message信息封装类</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.dev.flink.stream.hive;</span><br><span class="line"></span><br><span class="line">public class Message &#123;</span><br><span class="line">    private String topic;</span><br><span class="line">    private int partition;</span><br><span class="line">    private int offset;</span><br><span class="line">    private String msg;</span><br><span class="line"></span><br><span class="line">    public Message(String topic, int partition, int offset, String msg) &#123;</span><br><span class="line">        this.topic &#x3D; topic;</span><br><span class="line">        this.partition &#x3D; partition;</span><br><span class="line">        this.offset &#x3D; offset;</span><br><span class="line">        this.msg &#x3D; msg;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getTopic() &#123;</span><br><span class="line">        return topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setTopic(String topic) &#123;</span><br><span class="line">        this.topic &#x3D; topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int getPartition() &#123;</span><br><span class="line">        return partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setPartition(int partition) &#123;</span><br><span class="line">        this.partition &#x3D; partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int getOffset() &#123;</span><br><span class="line">        return offset;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setOffset(int offset) &#123;</span><br><span class="line">        this.offset &#x3D; offset;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public String getMsg() &#123;</span><br><span class="line">        return msg;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setMsg(String msg) &#123;</span><br><span class="line">        this.msg &#x3D; msg;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;Message&#123;&quot; +</span><br><span class="line">                &quot;topic&#x3D;&#39;&quot; + topic + &#39;\&#39;&#39; +</span><br><span class="line">                &quot;, partition&#x3D;&quot; + partition +</span><br><span class="line">                &quot;, offset&#x3D;&quot; + offset +</span><br><span class="line">                &quot;, msg&#x3D;&#39;&quot; + msg + &#39;\&#39;&#39; +</span><br><span class="line">                &#39;&#125;&#39;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="OrcWriter写入类实现"><a href="#OrcWriter写入类实现" class="headerlink" title="OrcWriter写入类实现"></a>OrcWriter写入类实现</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.dev.flink.stream.hive</span><br><span class="line"></span><br><span class="line">import org.apache.flink.streaming.connectors.fs.Writer</span><br><span class="line">import org.apache.hadoop.fs.&#123;FileSystem, Path&#125;</span><br><span class="line">import org.apache.hadoop.hive.ql.io.orc.OrcFile</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.ObjectInspectorOptions</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.&#123;ObjectInspector, ObjectInspectorFactory&#125;</span><br><span class="line"></span><br><span class="line">import scala.util.Random</span><br><span class="line"></span><br><span class="line">class OrcWriter[T](struct: Class[T]) extends Writer[T] with Serializable &#123;</span><br><span class="line"></span><br><span class="line">  @transient var writer: org.apache.hadoop.hive.ql.io.orc.Writer &#x3D; null</span><br><span class="line">  @transient var inspector: ObjectInspector &#x3D; null</span><br><span class="line">  @transient var basePath: Path &#x3D; null</span><br><span class="line">  @transient var fileSystem: FileSystem &#x3D; null</span><br><span class="line"></span><br><span class="line">  override def duplicate() &#x3D; new OrcWriter(struct)</span><br><span class="line"></span><br><span class="line">  override def open(fs: FileSystem, path: Path) &#x3D; &#123;</span><br><span class="line">    basePath &#x3D; path</span><br><span class="line">    fileSystem &#x3D; fs</span><br><span class="line">    inspector &#x3D; ObjectInspectorFactory.getReflectionObjectInspector(struct, ObjectInspectorOptions.JAVA)</span><br><span class="line">    initWriter</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def initWriter(): Unit &#x3D; &#123;</span><br><span class="line">    val newPath &#x3D; getNewPath()</span><br><span class="line">    writer &#x3D; OrcFile.createWriter(newPath, OrcFile.writerOptions(fileSystem.getConf).inspector(inspector))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def write(element: T) &#x3D; &#123;</span><br><span class="line">    if (writer &#x3D;&#x3D; null)</span><br><span class="line">      initWriter()</span><br><span class="line">    writer.addRow(element)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def flush() &#x3D; &#123;</span><br><span class="line">    if (writer &#x3D;&#x3D; null)</span><br><span class="line">      throw new IllegalStateException(&quot;Writer is not open&quot;)</span><br><span class="line">    val before &#x3D; writer.getRawDataSize</span><br><span class="line">    writer.writeIntermediateFooter()</span><br><span class="line">    val after &#x3D; writer.getRawDataSize</span><br><span class="line">    println(s&quot;###################################$before &#x3D;&#x3D;&gt; $after###################################&quot;)</span><br><span class="line">    writer.getRawDataSize</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getPos &#x3D; flush()</span><br><span class="line"></span><br><span class="line">  override def close() &#x3D; &#123;</span><br><span class="line">    if (writer !&#x3D; null) writer.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def getNewPath(): Path &#x3D; &#123;</span><br><span class="line">    var newPath: Path &#x3D; null</span><br><span class="line">    synchronized &#123;</span><br><span class="line">      newPath &#x3D; new Path(basePath.getParent, getRandomPartName)</span><br><span class="line">      while (fileSystem.exists(newPath)) &#123;</span><br><span class="line">        newPath &#x3D; new Path(basePath.getParent, getRandomPartName)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    newPath</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def getRandomPartName(): String &#x3D; &#123;</span><br><span class="line">    val suffix &#x3D; math.abs(Random.nextLong())</span><br><span class="line">    s&quot;part_$&#123;suffix&#125;.orc&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="执行类实现"><a href="#执行类实现" class="headerlink" title="执行类实现"></a>执行类实现</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.dev.flink.stream.hive</span><br><span class="line"></span><br><span class="line">import java.util.Properties</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.connectors.fs.bucketing.&#123;BucketingSink, DateTimeBucketer&#125;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010</span><br><span class="line">import org.apache.hadoop.conf.Configuration</span><br><span class="line">import org.apache.flink.api.scala._</span><br><span class="line">import org.json.JSONObject</span><br><span class="line">import scala.collection.convert.WrapAsJava._</span><br><span class="line"></span><br><span class="line">object HiveDemoOnSink &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val properties &#x3D; new Properties()</span><br><span class="line">    properties.setProperty(&quot;bootstrap.servers&quot;, &quot;hadoop03:9092&quot;)</span><br><span class="line">    properties.setProperty(&quot;group.id&quot;, &quot;test&quot;)</span><br><span class="line">    properties.setProperty(&quot;auto.offset.reset&quot;, &quot;latest&quot;)</span><br><span class="line"></span><br><span class="line">    val consumer010 &#x3D; new FlinkKafkaConsumer010[String](</span><br><span class="line">      &quot;test&quot;,</span><br><span class="line">      &#x2F;&#x2F;      List(&quot;test1&quot;,&quot;test2&quot;),</span><br><span class="line">      new JsonDeserializationSchema(),</span><br><span class="line">      properties</span><br><span class="line">    ).setStartFromEarliest()</span><br><span class="line"></span><br><span class="line">    val senv &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    senv.enableCheckpointing(500)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    val dataStream &#x3D; senv.addSource(consumer010)</span><br><span class="line">    </span><br><span class="line">    val configuration &#x3D; new Configuration()</span><br><span class="line">    configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs:&#x2F;&#x2F;hadoop01:8020&quot;)</span><br><span class="line">    val bucketingSink &#x3D; new BucketingSink[Message](&quot;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;user_test_orc&quot;).setBucketer(</span><br><span class="line">      new DateTimeBucketer[Message](&quot;&#39;c_date&#x3D;&#39;yyyy-MM-dd&quot;)</span><br><span class="line">    ).setWriter(</span><br><span class="line">      new OrcWriter[Message](classOf[Message])</span><br><span class="line">    ).setBatchSize(1024 * 10).setFSConfig(configuration)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 写入Hdfs</span><br><span class="line">    val ds &#x3D; dataStream.map(data &#x3D;&gt; &#123;</span><br><span class="line">      val json &#x3D; new JSONObject(data.toString)</span><br><span class="line">      val topic &#x3D; json.get(&quot;topic&quot;).toString</span><br><span class="line">      val partition &#x3D; json.get(&quot;partition&quot;).toString.toInt</span><br><span class="line">      val offset &#x3D; json.get(&quot;offset&quot;).toString.toInt</span><br><span class="line">      new Message(topic, partition, offset, json.toString())</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    ds.print()</span><br><span class="line"></span><br><span class="line">    ds.addSink(bucketingSink)</span><br><span class="line">    senv.execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Hive表创建语句"><a href="#Hive表创建语句" class="headerlink" title="Hive表创建语句"></a>Hive表创建语句</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE user_test_orc(</span><br><span class="line">topic string,</span><br><span class="line">partition int,</span><br><span class="line">offset int,</span><br><span class="line">msg string)</span><br><span class="line">PARTITIONEED BY (c_date string)</span><br><span class="line">ROW FORMAT SERDE &#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde&#39;</span><br><span class="line">STORED AS INPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;</span><br><span class="line">OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39;;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><blockquote>
<p>本地IDEA运行无效,打包到集群上运行.</p>
<p>这只是将数据写入HDFS上,相当于往Hive在Hdfs上的目录底下直接上传格式化好的文件,这个时候查询Hive表,是不会出现数据的.</p>
<p>需要执行 <strong>msck repair table tableName;</strong> 命令修复分区.</p>
<p>或者执行 <strong>alter table tableName add partition(分区字段=’值’);</strong> 添加分区</p>
</blockquote>
<hr>
<h2 id="瑕疵"><a href="#瑕疵" class="headerlink" title="瑕疵"></a>瑕疵</h2><blockquote>
<p>需要人为修复或者脚本修复Hive的分区,完美方案应该将这种修复放入代码实现.</p>
</blockquote>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink出现Could not forward element to next operator问题</title>
    <url>/2019/11/29/Flink%E5%87%BA%E7%8E%B0Could-not-forward-element-to-next-operator%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>解决问题 Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator</p>
</blockquote>
<span id="more"></span>

<h2 id="字面意思"><a href="#字面意思" class="headerlink" title="字面意思"></a>字面意思</h2><p>无法将元素转发到下一个运算符</p>
<hr>
<h2 id="出现原因"><a href="#出现原因" class="headerlink" title="出现原因"></a>出现原因</h2><p>这个异常一般是数据源端，出现脏数据，存在null值导致的。</p>
<hr>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">判断空值,赋默认值</span><br><span class="line">case when datetime is null</span><br><span class="line">then timestamp &#39;1970-01-01 00:00:00&#39; </span><br><span class="line">else &#96;datetime&#96; end</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink出现Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created问题</title>
    <url>/2019/11/29/Flink%E5%87%BA%E7%8E%B0Provider-for-class-javax.xml.parsers.DocumentBuilderFactory-cannot-be-created%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>解决问题 Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be created</p>
</blockquote>
<span id="more"></span>

<h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><p>xml-apis冲突问题</p>
<hr>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>去除xml-apis依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.hadoop&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hadoop-client&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;hadoop.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;exclusions&gt;</span><br><span class="line">        &lt;exclusion&gt;</span><br><span class="line">            &lt;groupId&gt;xml-apis&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;xml-apis&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;&#x2F;exclusion&gt;</span><br><span class="line">    &lt;&#x2F;exclusions&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink官网解读</title>
    <url>/2019/12/25/Flink%E5%AE%98%E7%BD%91%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<blockquote>
<p>上次实时维表join的事,觉得自己对于Flink官网并没有去细致了解,单开一篇,记录对Flink官网的解读,局限于scala开发</p>
</blockquote>
<span id="more"></span>

<h1 id="基本API"><a href="#基本API" class="headerlink" title="基本API"></a>基本API</h1><h2 id="数据集与数据流"><a href="#数据集与数据流" class="headerlink" title="数据集与数据流"></a>数据集与数据流</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink使用DataSet和DataStream表示程序中的数据</span><br></pre></td></tr></table></figure>
<h2 id="Flink程序剖析"><a href="#Flink程序剖析" class="headerlink" title="Flink程序剖析"></a>Flink程序剖析</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// a.获得execution environment</span></span><br><span class="line"><span class="comment">// org.apache.flink.api.scala</span></span><br><span class="line"><span class="comment">// ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="comment">// org.apache.flink.streaming.api.scala</span></span><br><span class="line"><span class="comment">// StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment()</span><br><span class="line"></span><br><span class="line"><span class="comment">// b.加载/创建初始数据</span></span><br><span class="line"><span class="keyword">val</span> text = env.readTextFile(<span class="string">&quot;test.log&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// c.指定对此数据的转换</span></span><br><span class="line"><span class="keyword">val</span> counts = text.flatMap(w =&gt; &#123;w.split(<span class="string">&quot; &quot;</span>)&#125;).map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// d.指定将计算结果放在何处</span></span><br><span class="line">counts.writeAsCsv(<span class="string">&quot;output&quot;</span>, <span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>)</span><br><span class="line">counts.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">// e.触发程序执行</span></span><br><span class="line">env.execute(<span class="string">&quot;Scala WordCount Example&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Lazy-Evaluation"><a href="#Lazy-Evaluation" class="headerlink" title="Lazy Evaluation"></a>Lazy Evaluation</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">所有Flink程序都是延迟执行的,执行程序的main方法时,不会进行数据加载和转换,而是先生成执行计划.</span><br><span class="line">当通过execute显示触发时,才会真正执行.</span><br></pre></td></tr></table></figure>
<h2 id="指定键"><a href="#指定键" class="headerlink" title="指定键"></a>指定键</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// join,coGroup,keyBy,groupBy要求定义Key</span></span><br><span class="line"><span class="comment">// reduce,groupReduce,aggregate,windows计算分组数据</span></span><br><span class="line"><span class="keyword">val</span> counts = text</span><br><span class="line">  .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">  .map((_, <span class="number">1</span>))</span><br><span class="line">  .groupBy(<span class="number">0</span>)</span><br><span class="line">  .reduceGroup(<span class="keyword">new</span> <span class="type">GroupReduceFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>), (<span class="type">String</span>, <span class="type">Int</span>)] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(iterable: lang.<span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)], collector: <span class="type">Collector</span>[(<span class="type">String</span>, <span class="type">Int</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> value = iterable.iterator()</span><br><span class="line">      <span class="keyword">var</span> map = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]()</span><br><span class="line">      <span class="keyword">while</span> (value.hasNext) &#123;</span><br><span class="line">        <span class="keyword">val</span> tuple = value.next()</span><br><span class="line">        map += (tuple._1 -&gt; (tuple._2 + map.getOrElse(tuple._1, <span class="number">0</span>)))</span><br><span class="line">      &#125;</span><br><span class="line">      map.foreach(x =&gt; &#123;</span><br><span class="line">        collector.collect((x._1, x._2))</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> counts = text</span><br><span class="line">  .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">  .map((_, <span class="number">1</span>))</span><br><span class="line">  .groupBy(<span class="number">0</span>)</span><br><span class="line">  .reduce((x, y) =&gt; (x._1, x._2 + y._2))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 也可以使用POJO</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span>(<span class="params">word: <span class="type">String</span>, count: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">input</span> </span>= env.fromElements(</span><br><span class="line">    <span class="type">WordCount</span>(<span class="string">&quot;hello&quot;</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="type">WordCount</span>(<span class="string">&quot;world&quot;</span>, <span class="number">2</span>))</span><br><span class="line">input.keyBy(<span class="string">&quot;word&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="指定转换函数"><a href="#指定转换函数" class="headerlink" title="指定转换函数"></a>指定转换函数</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">data.map &#123; x =&gt; x.toInt &#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMapFunction</span> <span class="keyword">extends</span> <span class="title">RichMapFunction</span>[<span class="type">String</span>, <span class="type">Int</span>] </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">map</span></span>(in: <span class="type">String</span>):<span class="type">Int</span> = &#123; in.toInt &#125;</span><br><span class="line">&#125;;</span><br><span class="line">data.map(<span class="keyword">new</span> <span class="type">MyMapFunction</span>())</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Streaming-DataStream-API"><a href="#Streaming-DataStream-API" class="headerlink" title="Streaming(DataStream API)"></a>Streaming(DataStream API)</h1><h2 id="范例"><a href="#范例" class="headerlink" title="范例"></a>范例</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 5秒一个窗口输出计数</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowWordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> text = env.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> counts = text.flatMap &#123; _.toLowerCase.split(<span class="string">&quot;\\W+&quot;</span>) filter &#123; _.nonEmpty &#125; &#125;</span><br><span class="line">      .map &#123; (_, <span class="number">1</span>) &#125;</span><br><span class="line">      .keyBy(<span class="number">0</span>)</span><br><span class="line">      .timeWindow(<span class="type">Time</span>.seconds(<span class="number">5</span>))</span><br><span class="line">      .sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    counts.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;Window Stream WordCount&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 文件</span><br><span class="line">readTextFile(path) &#x2F;&#x2F; 逐行读取文件</span><br><span class="line">readFile(fileInputFormat, path) &#x2F;&#x2F; 根据指定的文件输入格式读取一次文件</span><br><span class="line">readFile(fileInputFormat, path, watchType, interval, pathFilter) &#x2F;&#x2F; watchType可以定期监视路径中的新数据(FileProcessingMode.PROCESS_CONTINUOUSLY),或者处理一次路径中当前数据并退出(FileProcessingMode.PROCESS_ONCE),使用pathFilter,进一步排除文件</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; socket</span><br><span class="line">socketTextStream</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 集合</span><br><span class="line">fromCollection(Seq)</span><br><span class="line">fromCollection(Iterator)</span><br><span class="line">fromElements(elements: _*)</span><br><span class="line">fromParallelCollection(SplittableIterator)</span><br><span class="line">generateSequence(from, to)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 自定义</span><br><span class="line">addSource &#x2F;&#x2F; 比如FlinkKafkaConsumer10</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink复杂事件CEP</title>
    <url>/2020/07/27/Flink%E5%A4%8D%E6%9D%82%E4%BA%8B%E4%BB%B6CEP/</url>
    <content><![CDATA[<blockquote>
<p>flink复杂事件CEP理论篇+小例子</p>
</blockquote>
<span id="more"></span>
<h4 id="什么是复杂事件CEP"><a href="#什么是复杂事件CEP" class="headerlink" title="什么是复杂事件CEP"></a>什么是复杂事件CEP</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个或多个由简单事件构成的事件流通过一定的规则匹配,然后输出用户想得到的数据,满足规则的复杂事件</span><br></pre></td></tr></table></figure>

<h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.目标:从有序的简单事件流中发现一些高阶特征</span><br><span class="line">2.输入:一个或多个由简单事件构成的事件流</span><br><span class="line">3.处理:识别简单事件之间的内在联系,多个符合一定规则的简单事件构成复杂事件</span><br><span class="line">4.输出:满足规则的复杂事件</span><br></pre></td></tr></table></figure>

<h4 id="CEP架构"><a href="#CEP架构" class="headerlink" title="CEP架构"></a>CEP架构</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">多个数据源 -&gt; CEP引擎 -&gt; exactly once.高吞吐,低延迟,高可用,乱序消息处理,规则匹配</span><br></pre></td></tr></table></figure>

<h4 id="CEP-NFA是什么"><a href="#CEP-NFA是什么" class="headerlink" title="CEP-NFA是什么?"></a>CEP-NFA是什么?</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink的每个模式包含多个状态,模式匹配的过程就是状态转换的过程,每个状态(state)可以理解成由Pattern构成,为了从当前的状态转换成下一个状态,用户可以在pattern上指定条件,用于状态的过滤和转换</span><br><span class="line"></span><br><span class="line">实际上Flink CEP首先需要用户创建定义一个个pattern,然后通过链表将由前后逻辑关系的pattern串在一起,构成模式匹配的逻辑表达;然后需要用户利用NFACompiler,将模式进行分拆,创建出NFA(非确定有限自动机)对象,NFA包含了该次模式匹配的各个状态和状态间转换的表达式</span><br></pre></td></tr></table></figure>

<h4 id="CEP-三种状态迁移边"><a href="#CEP-三种状态迁移边" class="headerlink" title="CEP-三种状态迁移边"></a>CEP-三种状态迁移边</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.take:表示事件匹配成功,将当前状态更新到新状态,并前进到&quot;下一个&quot;状态</span><br><span class="line">2.procceed:当事件来到的时候,当前状态不发生变化,在状态转换图中事件直接&quot;前进&quot;到下一个目标状态</span><br><span class="line">3.ignore:当事件来到的时候,如果匹配不成功,忽略当前事件,当前状态不发生任何变化</span><br></pre></td></tr></table></figure>
<h4 id="为了更好的理解上述-举个实际应用例子"><a href="#为了更好的理解上述-举个实际应用例子" class="headerlink" title="为了更好的理解上述,举个实际应用例子"></a>为了更好的理解上述,举个实际应用例子</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.util</span><br><span class="line"></span><br><span class="line">import org.apache.flink.cep.PatternSelectFunction</span><br><span class="line">import org.apache.flink.cep.scala.CEP</span><br><span class="line">import org.apache.flink.cep.scala.pattern.Pattern</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line">import org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;登录样例类</span><br><span class="line">case class LoginEvent(userId:Long,ip:String,eventTpye:String,eventTime:Long)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;输出报警信息样例类</span><br><span class="line">case class Warning(userId: Long,firstFailTime:Long,lastFailTime:Long,warningMSG:String)</span><br><span class="line"></span><br><span class="line">object LoginFailWithCEP &#123;</span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line">    env.setParallelism(1)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;自定义测试数据</span><br><span class="line">    val loginStream &#x3D; env.fromCollection(List(</span><br><span class="line">      LoginEvent(1,&quot;192.168.0.1&quot;,&quot;fail&quot;,1558430842),</span><br><span class="line">      LoginEvent(1,&quot;192.168.0.2&quot;,&quot;success&quot;,1558430843),</span><br><span class="line">      LoginEvent(3,&quot;192.168.0.3&quot;,&quot;fail&quot;,1558430844),</span><br><span class="line">      LoginEvent(3,&quot;192.168.0.3&quot;,&quot;fail&quot;,1558430847),</span><br><span class="line">      LoginEvent(3,&quot;192.168.0.3&quot;,&quot;fail&quot;,1558430848),</span><br><span class="line">      LoginEvent(4,&quot;192.168.0.5&quot;,&quot;fail&quot;,1558430880),</span><br><span class="line">      LoginEvent(2,&quot;192.168.0.10&quot;,&quot;success&quot;,1558430950)</span><br><span class="line">    )).assignAscendingTimestamps(_.eventTime*1000)</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;定义pattern.对事件流进行模式匹配</span><br><span class="line">    val loginFailPattern &#x3D; Pattern.begin[LoginEvent](&quot;begin&quot;)</span><br><span class="line">      .where(_.eventTpye.equals(&quot;fail&quot;))</span><br><span class="line">      .next(&quot;next&quot;)</span><br><span class="line">      .where(_.eventTpye.equals(&quot;fail&quot;))</span><br><span class="line">&#x2F;&#x2F;      .within(Time.seconds(2))</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;在输入流的基础上应用pattern,得到匹配的pattern stream</span><br><span class="line">    val patternStream &#x3D; CEP.pattern(loginStream.keyBy(_.userId),loginFailPattern)</span><br><span class="line"></span><br><span class="line">    val loginFailDataStream &#x3D; patternStream.select(new MySelectFunction())</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;将得到的警告信息流输出sink</span><br><span class="line">    loginFailDataStream.print(&quot;warning&quot;)</span><br><span class="line"></span><br><span class="line">    env.execute(&quot;Login Fail Detect with CEP&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class MySelectFunction() extends PatternSelectFunction[LoginEvent,Warning]&#123;</span><br><span class="line">  override def select(patternEvents: util.Map[String, util.List[LoginEvent]]): Warning &#x3D; &#123;</span><br><span class="line">    val firstFailEvent &#x3D; patternEvents.getOrDefault(&quot;begin&quot;,null).iterator().next()</span><br><span class="line">    val secondEvent &#x3D; patternEvents.getOrDefault(&quot;next&quot;,null).iterator().next()</span><br><span class="line">    Warning(firstFailEvent.userId,firstFailEvent.eventTime,secondEvent.eventTime,&quot;login fail warning&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="引入相关依赖包"><a href="#引入相关依赖包" class="headerlink" title="引入相关依赖包"></a>引入相关依赖包</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;flink-cep-scala_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.10.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink实时写入Hive以ORC格式</title>
    <url>/2020/06/16/Flink%E5%AE%9E%E6%97%B6%E5%86%99%E5%85%A5Hive%E4%BB%A5ORC%E6%A0%BC%E5%BC%8F/</url>
    <content><![CDATA[<blockquote>
<p>请注意版本问题,Flink使用的<code>orc-core</code>过新,对于老版本的hive并不支持,可以通过重写OrcFile类以支持低版本</p>
</blockquote>
<span id="more"></span>

<h2 id="Orc格式"><a href="#Orc格式" class="headerlink" title="Orc格式"></a>Orc格式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Stripe:</span><br><span class="line">    index data</span><br><span class="line">    group of row data</span><br><span class="line">    stripe footer</span><br><span class="line">FileFooter:</span><br><span class="line">    辅助信息,文件中包含的所有Stripe信息</span><br><span class="line">    每个Stripe含有的数据行数,每一行的数据类型</span><br><span class="line">    列级别的聚合操作(count,min,max,sum)</span><br><span class="line">PostScript:</span><br><span class="line">    包含压缩参数和压缩页脚大小</span><br><span class="line">    </span><br><span class="line">Stripe:</span><br><span class="line">    MAGIC</span><br><span class="line">    stripe1&#123;</span><br><span class="line">        data</span><br><span class="line">        index</span><br><span class="line">        footer</span><br><span class="line">    &#125;,</span><br><span class="line">    stripe2&#123;</span><br><span class="line">        data</span><br><span class="line">        index</span><br><span class="line">        footer</span><br><span class="line">    &#125;,</span><br><span class="line">    ...</span><br><span class="line">    metadata</span><br><span class="line">    footer</span><br><span class="line">    PostScript + size(PostScript)</span><br></pre></td></tr></table></figure>
<h3 id="DynamicIntArray和DynamicByteArray"><a href="#DynamicIntArray和DynamicByteArray" class="headerlink" title="DynamicIntArray和DynamicByteArray"></a>DynamicIntArray和DynamicByteArray</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">动态数组,两者一个存Int,一个存Byte</span><br><span class="line">static final int DEFAULT_CHUNKSIZE &#x3D; 8192;</span><br><span class="line">static final int INIT_CHUNKS &#x3D; 128;</span><br><span class="line">chunk初始化128个,每个size大小为8192</span><br><span class="line">增删改查操作需要根据index,计算出对应的chunk和在该chunk内的偏移量来操作数据</span><br><span class="line">public int get(int index) &#123;</span><br><span class="line">    if (index &gt;&#x3D; this.length) &#123;</span><br><span class="line">        throw new IndexOutOfBoundsException(&quot;Index &quot; + index + &quot; is outside of 0..&quot; + (this.length - 1));</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        int i &#x3D; index &#x2F; this.chunkSize;&#x2F;&#x2F; 对应的chunk</span><br><span class="line">        int j &#x3D; index % this.chunkSize;&#x2F;&#x2F; 偏移量</span><br><span class="line">        return this.data[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="OrcFile写入数据"><a href="#OrcFile写入数据" class="headerlink" title="OrcFile写入数据"></a>OrcFile写入数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># WriterImpl.addRowBath入口</span><br><span class="line">TreeWriter 写数据</span><br><span class="line">RowIndexEntry 管理</span><br><span class="line">MemoryManager 内存管理</span><br><span class="line">Stripe 生成</span><br><span class="line"></span><br><span class="line"># useDictionaryEncoding是否使用字典压缩</span><br><span class="line">使用:</span><br><span class="line">    this.dictionary.add(val) 使用红黑树存储当前字符串的bytes值</span><br><span class="line">    this.rows.add(i) 元素存储在dictionary中的offset</span><br><span class="line">不使用:直接写入OutputStream</span><br><span class="line">    this.directStreamOutput.write();</span><br><span class="line">    this.directLengthOutput.write();</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>使用方式与1.10parquet的使用方式类似</p>
<h3 id="Vectorizer"><a href="#Vectorizer" class="headerlink" title="Vectorizer"></a>Vectorizer</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.orc.vector.Vectorizer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/6/15.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DemoVectorizer</span> <span class="keyword">extends</span> <span class="title">Vectorizer</span>&lt;<span class="title">Demo</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DemoVectorizer</span><span class="params">(String schema)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(schema);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">vectorize</span><span class="params">(Demo demo, VectorizedRowBatch vectorizedRowBatch)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> id = vectorizedRowBatch.size++;</span><br><span class="line">        System.out.println(vectorizedRowBatch.size);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i) &#123;</span><br><span class="line">            BytesColumnVector vector = (BytesColumnVector) vectorizedRowBatch.cols[i];</span><br><span class="line">            <span class="keyword">byte</span>[] bytes = demo.platform().getBytes();</span><br><span class="line">            vector.setVal(id, bytes, <span class="number">0</span>, bytes.length);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Main"><a href="#Main" class="headerlink" title="Main"></a>Main</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.nio.<span class="type">ByteBuffer</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.<span class="type">JSON</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.orc.writer.<span class="type">OrcBulkWriterFactory</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.<span class="type">FsStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">CheckpointingMode</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.filesystem.<span class="type">StreamingFileSink</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">StreamExecutionEnvironment</span>, _&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.<span class="type">ProducerConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.core.fs.<span class="type">Path</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * @author XiaShuai on 2020/6/15.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OrcFileWriteDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> <span class="type">READ_TOPIC</span> = <span class="string">&quot;topic&quot;</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.enableCheckpointing(<span class="number">60000</span>L, <span class="type">CheckpointingMode</span>.<span class="type">EXACTLY_ONCE</span>)</span><br><span class="line">    env.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;file:///job/flink/ck/Orc&quot;</span>))</span><br><span class="line">    <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hosts:9092&quot;</span>)</span><br><span class="line">    props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;xs_test3&quot;</span>)</span><br><span class="line">    props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line">    props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> producerProps = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    producerProps.setProperty(<span class="type">ProducerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>, <span class="string">&quot;skuldcdhtest1.ktcs:9092&quot;</span>)</span><br><span class="line">    producerProps.setProperty(<span class="type">ProducerConfig</span>.<span class="type">RETRIES_CONFIG</span>, <span class="string">&quot;3&quot;</span>)</span><br><span class="line">    <span class="comment">// 如果下面配置的是exactly-once的语义 这里必须配置为all</span></span><br><span class="line">    producerProps.setProperty(<span class="type">ProducerConfig</span>.<span class="type">ACKS_CONFIG</span>, <span class="string">&quot;all&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> student = env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer</span>(</span><br><span class="line">      <span class="type">READ_TOPIC</span>, <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>, props).setStartFromLatest()</span><br><span class="line">    ).map(x =&gt; &#123;</span><br><span class="line">      ...</span><br><span class="line">      <span class="type">Demo</span>(<span class="string">&quot;&quot;</span>,<span class="string">&quot;&quot;</span>,<span class="string">&quot;&quot;</span>)</span><br><span class="line">    &#125;).setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> schema: <span class="type">String</span> = <span class="string">&quot;struct&lt;platform:string,event:string,dt:string&gt;&quot;</span></span><br><span class="line">    <span class="keyword">val</span> writerProperties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    writerProperties.setProperty(<span class="string">&quot;orc.compress&quot;</span>, <span class="string">&quot;ZLIB&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> vectorizer = <span class="keyword">new</span> <span class="type">DemoVectorizer</span>(schema)</span><br><span class="line">    <span class="keyword">val</span> writerFactory = <span class="keyword">new</span> <span class="type">CustomOrcBulkWriterFactory</span>(vectorizer, writerProperties, <span class="keyword">new</span> <span class="type">Configuration</span>())</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="type">StreamingFileSink</span>.forBulkFormat(<span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;F:\\test\\Demo\\Flink11\\src\\main\\resources&quot;</span>),</span><br><span class="line">      writerFactory</span><br><span class="line">    ).build()</span><br><span class="line"></span><br><span class="line">    student.addSink(sink).setParallelism(<span class="number">1</span>)</span><br><span class="line">    env.execute(<span class="string">&quot;write hdfs&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span>(<span class="params">platform: <span class="type">String</span>, event: <span class="type">String</span>, dt: <span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决低版本支持问题"><a href="#解决低版本支持问题" class="headerlink" title="解决低版本支持问题"></a>解决低版本支持问题</h2><p><a href="https://github.com/apache/orc/blob/ce4329f396658648796f5b78716f8e1836f139ec/java/core/src/java/org/apache/orc/OrcFile.java#L258">Flink-1.11使用的OrcVersion</a><br><a href="https://github.com/apache/hive/blob/03599216cfc01fc464f1c9a4fa89e81c45327ea5/orc/src/java/org/apache/orc/OrcFile.java#L156">Hive-2.1.1使用的OrcVersion</a></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># 主要原因为Orc在新版本后使用的WriterVersion为ORC_517</span><br><span class="line"># 导致低版本的Hive解析不了</span><br><span class="line"># 自实现OrcFile类,修改回旧版本</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    CURRENT_WRITER = WriterVersion.HIVE_13083;</span><br><span class="line">    memoryManager = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink小扩展之Calcite自定义SQL解析器</title>
    <url>/2020/07/03/Flink%E5%B0%8F%E6%89%A9%E5%B1%95%E4%B9%8BCalcite%E8%87%AA%E5%AE%9A%E4%B9%89SQL%E8%A7%A3%E6%9E%90%E5%99%A8/</url>
    <content><![CDATA[<blockquote>
<p>FlinkSQL其底层的SQL解析流程使用的Calcite框架,参考<a href="https://blog.csdn.net/dafei1288/article/details/102735371">传送门</a></p>
</blockquote>
<span id="more"></span>

<h2 id="前期项目构建"><a href="#前期项目构建" class="headerlink" title="前期项目构建"></a>前期项目构建</h2><h3 id="maven项目"><a href="#maven项目" class="headerlink" title="maven项目"></a>maven项目</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;groupId&gt;org.example&lt;&#x2F;groupId&gt;</span><br><span class="line">&lt;artifactId&gt;calcite_test&lt;&#x2F;artifactId&gt;</span><br><span class="line">&lt;version&gt;1.0&lt;&#x2F;version&gt;</span><br></pre></td></tr></table></figure>
<h3 id="pom-xml"><a href="#pom-xml" class="headerlink" title="pom.xml"></a>pom.xml</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.25<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.java.dev.javacc<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>7.0.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.freemarker<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>freemarker<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.30<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.calcite<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>calcite-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.23.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.codehaus.mojo<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>javacc-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/generated-sources/fmpp<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/Parser.jj<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">lookAhead</span>&gt;</span>2<span class="tag">&lt;/<span class="name">lookAhead</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">isStatic</span>&gt;</span>false<span class="tag">&lt;/<span class="name">isStatic</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>javacc-test<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>generate-test-sources<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>javacc<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/generated-test-sources/fmpp<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>$&#123;project.build.directory&#125;/generated-test-sources/javacc<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/Parser.jj<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">lookAhead</span>&gt;</span>2<span class="tag">&lt;/<span class="name">lookAhead</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">isStatic</span>&gt;</span>false<span class="tag">&lt;/<span class="name">isStatic</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.drill.tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>drill-fmpp-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">config</span>&gt;</span>src/main/codegen/config.fmpp<span class="tag">&lt;/<span class="name">config</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">output</span>&gt;</span>$&#123;project.build.directory&#125;/generated-sources/fmpp<span class="tag">&lt;/<span class="name">output</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">templates</span>&gt;</span>src/main/codegen/templates<span class="tag">&lt;/<span class="name">templates</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>generate-fmpp-sources<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>validate<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>generate<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="模板文件"><a href="#模板文件" class="headerlink" title="模板文件"></a>模板文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 复制Calcite源码</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;calcite.git</span><br><span class="line"># 复制模板文件到自己的工程下</span><br><span class="line">mv calcite\core\src\main\codegen calcite_test\src\main\</span><br><span class="line"></span><br><span class="line"># 可以了解一下模板文件</span><br><span class="line">compoundIdentifier.ftl</span><br><span class="line">parserImpls.ftl</span><br><span class="line">Parser.jj</span><br><span class="line">config.fmpp</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="代码编写"><a href="#代码编写" class="headerlink" title="代码编写"></a>代码编写</h2><h3 id="自定义SqlNode"><a href="#自定义SqlNode" class="headerlink" title="自定义SqlNode"></a>自定义SqlNode</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 需要在org.apache.calcite.sql包内,SqlNode没有public构造函数</span></span><br><span class="line"><span class="keyword">package</span> org.apache.calcite.sql;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.sql.parser.SqlParserPos;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.sql.util.SqlVisitor;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.sql.validate.SqlValidator;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.sql.validate.SqlValidatorScope;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.util.Litmus;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SQL解析树</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/7/3.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SqlExample</span> <span class="keyword">extends</span> <span class="title">SqlNode</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String exampleString;</span><br><span class="line">    <span class="keyword">private</span> SqlParserPos pos;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SqlExample</span><span class="params">(SqlParserPos pos, String exampleString)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(pos);</span><br><span class="line">        <span class="keyword">this</span>.pos = pos;</span><br><span class="line">        <span class="keyword">this</span>.exampleString = exampleString;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getExampleString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;getExampleString&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.exampleString;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SqlNode <span class="title">clone</span><span class="params">(SqlParserPos sqlParserPos)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;clone&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">unparse</span><span class="params">(SqlWriter sqlWriter, <span class="keyword">int</span> i, <span class="keyword">int</span> i1)</span> </span>&#123;</span><br><span class="line">        sqlWriter.keyword(<span class="string">&quot;run&quot;</span>);</span><br><span class="line">        sqlWriter.keyword(<span class="string">&quot;example&quot;</span>);</span><br><span class="line">        sqlWriter.print(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        sqlWriter.keyword(exampleString);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">validate</span><span class="params">(SqlValidator sqlValidator, SqlValidatorScope sqlValidatorScope)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;validate&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> &lt;R&gt; <span class="function">R <span class="title">accept</span><span class="params">(SqlVisitor&lt;R&gt; sqlVisitor)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;validate&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equalsDeep</span><span class="params">(SqlNode sqlNode, Litmus litmus)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;equalsDeep&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="修改config-fmpp"><a href="#修改config-fmpp" class="headerlink" title="修改config.fmpp"></a>修改config.fmpp</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">找到package: &quot;org.apache.calcite.sql.parser.impl&quot;</span><br><span class="line">修改下方class,替换成自己的类名ExampleSqlParserImpl</span><br><span class="line">Flink也是这样处理FlinkSqlParserImpl</span><br><span class="line">class: &quot;ExampleSqlParserImpl&quot;</span><br></pre></td></tr></table></figure>
<h3 id="修改Parser-jj文件"><a href="#修改Parser-jj文件" class="headerlink" title="修改Parser.jj文件"></a>修改Parser.jj文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在import导入处,添加自定义SqlNode解析类的引入</span><br><span class="line">import org.apache.calcite.sql.SQLExample;</span><br><span class="line"></span><br><span class="line">在处理代码中加入解析逻辑,我是加在SqlStmtEof()后</span><br><span class="line">SqlNode SqlExample() :</span><br><span class="line">&#123;</span><br><span class="line">     SqlNode stringNode;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">    &lt;RUN&gt; &lt;EXAMPLE&gt;</span><br><span class="line">    stringNode &#x3D; StringLiteral()</span><br><span class="line">    &#123;</span><br><span class="line">        return new SqlExample(getPos(), token.image);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">找到声明语句的方法SqlNode SqlStmt() :</span><br><span class="line">适当位置加入</span><br><span class="line">|</span><br><span class="line">    stmt &#x3D; SqlExample()</span><br><span class="line"></span><br><span class="line">在&lt;DEFAULT, DQID, BTID&gt; TOKEN :处加入关键字</span><br><span class="line">|   &lt; RUN: &quot;RUN&quot;&gt;</span><br><span class="line">|   &lt; EXAMPLE: &quot;EXAMPLE&quot;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">mvn clean compile</span><br></pre></td></tr></table></figure>
<h3 id="测试代码编写"><a href="#测试代码编写" class="headerlink" title="测试代码编写"></a>测试代码编写</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.devlop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.avatica.util.Casing;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.avatica.util.Quoting;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.sql.SqlNode;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.sql.parser.SqlParser;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.sql.parser.impl.ExampleSqlParserImpl;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.tools.FrameworkConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.tools.Frameworks;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/7/3.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExampleParser</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        FrameworkConfig config = Frameworks.newConfigBuilder()</span><br><span class="line">                .parserConfig(SqlParser.configBuilder()</span><br><span class="line">                        .setParserFactory(ExampleSqlParserImpl.FACTORY)</span><br><span class="line">                        <span class="comment">// 设置大小写是否敏感</span></span><br><span class="line">                        .setCaseSensitive(<span class="keyword">false</span>)</span><br><span class="line">                        <span class="comment">// 设置应用标识,mysql是``</span></span><br><span class="line">                        .setQuoting(Quoting.BACK_TICK)</span><br><span class="line">                        <span class="comment">// Quoting策略,不变,变大写或变小写</span></span><br><span class="line">                        .setQuotedCasing(Casing.TO_UPPER)</span><br><span class="line">                        <span class="comment">// 标识符没有被Quoting后的策略</span></span><br><span class="line">                        .setUnquotedCasing(Casing.TO_UPPER)</span><br><span class="line">                        .build())</span><br><span class="line">                .build();</span><br><span class="line">        String sql = <span class="string">&quot;run example &#x27;select ids, name from test where id &lt; 5&#x27;&quot;</span>;</span><br><span class="line">        SqlParser parser = SqlParser.create(sql, config.getParserConfig());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            SqlNode sqlNode = parser.parseStmt();</span><br><span class="line">            System.out.println(sqlNode.toString());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink实时维表Join之HBase</title>
    <url>/2020/01/06/Flink%E5%AE%9E%E6%97%B6%E7%BB%B4%E8%A1%A8Join%E4%B9%8BHBase/</url>
    <content><![CDATA[<blockquote>
<p>支持HBase注册成表,并异步加载,需要自己实现</p>
</blockquote>
<span id="more"></span>

<p>实现的支持用的还是Java,使用是Scala,等有空的时候实现下scala支持</p>
<h2 id="Java类支持"><a href="#Java类支持" class="headerlink" title="Java类支持"></a>Java类支持</h2><p><strong>HBaseAsyncLookupFunction</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.stumbleupon.async.Callback;</span><br><span class="line"><span class="keyword">import</span> com.stumbleupon.async.Deferred;</span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.RedisClient;</span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.RedisFuture;</span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.api.StatefulRedisConnection;</span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.api.async.RedisAsyncCommands;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.typeutils.RowTypeInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.AsyncTableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.FunctionContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"><span class="keyword">import</span> org.hbase.async.GetRequest;</span><br><span class="line"><span class="keyword">import</span> org.hbase.async.HBaseClient;</span><br><span class="line"><span class="keyword">import</span> org.hbase.async.KeyValue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Collection;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CompletableFuture;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: xs</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020-01-03 17:21</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseAsyncLookupFunction</span> <span class="keyword">extends</span> <span class="title">AsyncTableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String tableName;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String[] fieldNames;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> HBaseClient hBaseClient;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HBaseAsyncLookupFunction</span><span class="params">(String tableName, String[] fieldNames, TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.tableName = tableName;</span><br><span class="line">        <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">        <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> </span>&#123;</span><br><span class="line">        hBaseClient = <span class="keyword">new</span> HBaseClient(<span class="string">&quot;hadoop01,hadoop02,hadoop03&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每一条流数据都会调用此方法进行join</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(CompletableFuture&lt;Collection&lt;Row&gt;&gt; future, Object... paramas)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//表名、主键名、主键值、列名</span></span><br><span class="line">        String[] info = &#123;<span class="string">&quot;userInfo&quot;</span>, <span class="string">&quot;userId&quot;</span>, paramas[<span class="number">0</span>].toString(), <span class="string">&quot;userName&quot;</span>&#125;;</span><br><span class="line">        String key = String.join(<span class="string">&quot;:&quot;</span>, info);</span><br><span class="line">        GetRequest get = <span class="keyword">new</span> GetRequest(tableName, key);</span><br><span class="line">        Deferred&lt;ArrayList&lt;KeyValue&gt;&gt; arrayListDeferred = hBaseClient.get(get);</span><br><span class="line">        arrayListDeferred.addCallbacks(<span class="keyword">new</span> Callback&lt;String, ArrayList&lt;KeyValue&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">(ArrayList&lt;KeyValue&gt; keyValues)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String value;</span><br><span class="line">                <span class="keyword">if</span> (keyValues.size() == <span class="number">0</span>) &#123;</span><br><span class="line">                    value = <span class="keyword">null</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    StringBuilder valueBuilder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">                    <span class="keyword">for</span> (KeyValue keyValue : keyValues) &#123;</span><br><span class="line">                        valueBuilder.append(<span class="keyword">new</span> String(keyValue.value()));</span><br><span class="line">                    &#125;</span><br><span class="line">                    value = valueBuilder.toString();</span><br><span class="line">                &#125;</span><br><span class="line">                future.complete(Collections.singletonList(Row.of(key, value, <span class="string">&quot;aaa&quot;</span>)));</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, <span class="keyword">new</span> Callback&lt;String, Exception&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">(Exception e)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RowTypeInfo(fieldTypes, fieldNames);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String tableName;</span><br><span class="line">        <span class="keyword">private</span> String[] fieldNames;</span><br><span class="line">        <span class="keyword">private</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="title">Builder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Builder <span class="title">getBuilder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Builder();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldNames</span><span class="params">(String[] fieldNames)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldTypes</span><span class="params">(TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withTableName</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.tableName = tableName;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> HBaseAsyncLookupFunction <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> HBaseAsyncLookupFunction(tableName, fieldNames, fieldTypes);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>HBaseAsyncLookupTableSource</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.typeutils.RowTypeInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.AsyncTableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.TableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.sources.LookupableTableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.sources.StreamTableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.types.DataType;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.types.utils.TypeConversions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: xs</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020-01-03 17:23</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseAsyncLookupTableSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">LookupableTableSource</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String tableName;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String[] fieldNames;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HBaseAsyncLookupTableSource</span><span class="params">(String tableName, String[] fieldNames, TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.tableName = tableName;</span><br><span class="line">        <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">        <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//同步方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TableFunction&lt;Row&gt; <span class="title">getLookupFunction</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//异步方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AsyncTableFunction&lt;Row&gt; <span class="title">getAsyncLookupFunction</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> HBaseAsyncLookupFunction.Builder.getBuilder()</span><br><span class="line">                .withTableName(tableName)</span><br><span class="line">                .withFieldNames(fieldNames)</span><br><span class="line">                .withFieldTypes(fieldTypes)</span><br><span class="line">                .build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启异步</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isAsyncEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataType <span class="title">getProducedDataType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TypeConversions.fromLegacyInfoToDataType(<span class="keyword">new</span> RowTypeInfo(fieldTypes, fieldNames));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TableSchema <span class="title">getTableSchema</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TableSchema.builder()</span><br><span class="line">                .fields(fieldNames, TypeConversions.fromLegacyInfoToDataType(fieldTypes))</span><br><span class="line">                .build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment environment)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;do not support getDataStream&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String tableName;</span><br><span class="line">        <span class="keyword">private</span> String[] fieldNames;</span><br><span class="line">        <span class="keyword">private</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="title">Builder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Builder <span class="title">newBuilder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Builder();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldNames</span><span class="params">(String[] fieldNames)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldTypes</span><span class="params">(TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withTableName</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.tableName = tableName;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> HBaseAsyncLookupTableSource <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> HBaseAsyncLookupTableSource(tableName, fieldNames, fieldTypes);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.dim.hbase</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.test.flink.hbase.<span class="type">HBaseAsyncLookupTableSource</span></span><br><span class="line"><span class="keyword">import</span> com.test.flink.redis.<span class="type">RedisAsyncLookupTableSource</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.<span class="type">Types</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.io.jdbc.&#123;<span class="type">JDBCAppendTableSink</span>, <span class="type">JDBCOptions</span>, <span class="type">JDBCUpsertTableSink</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.&#123;<span class="type">StreamTableEnvironment</span>, _&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">DataTypes</span>, <span class="type">EnvironmentSettings</span>, <span class="type">TableSchema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-03 17:15</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DoubleStreamHBaseDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">    <span class="keyword">val</span> ds = env.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line">    <span class="comment">// 1000,good0c,1566375779658</span></span><br><span class="line">    <span class="keyword">val</span> demo = ds.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = x.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      <span class="type">Demo</span>(arr(<span class="number">0</span>), arr(<span class="number">1</span>), arr(<span class="number">2</span>))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    tEnv.registerDataStream(<span class="string">&quot;user_click_name&quot;</span>, demo, <span class="symbol">&#x27;id</span>, <span class="symbol">&#x27;user_click</span>, <span class="symbol">&#x27;time</span>, <span class="symbol">&#x27;proctime</span>.proctime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> hbaseSource = <span class="type">HBaseAsyncLookupTableSource</span>.<span class="type">Builder</span>.newBuilder()</span><br><span class="line">      .withFieldNames(<span class="type">Array</span>(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>))</span><br><span class="line">      .withFieldTypes(<span class="type">Array</span>(<span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>))</span><br><span class="line">      .withTableName(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">      .build()</span><br><span class="line">    tEnv.registerTableSource(<span class="string">&quot;info&quot;</span>, hbaseSource)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">    <span class="comment">//&quot;select t1.id,t1.user_click,t2.name&quot; +</span></span><br><span class="line">      <span class="string">&quot;select * &quot;</span> +</span><br><span class="line">        <span class="string">&quot; from user_click_name as t1&quot;</span> +</span><br><span class="line">        <span class="string">&quot; join info FOR SYSTEM_TIME AS OF t1.proctime as t2&quot;</span> +</span><br><span class="line">        <span class="string">&quot; on t1.id = t2.id&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table = tEnv.sqlQuery(sql)</span><br><span class="line">    <span class="keyword">val</span> tableName = table.toString</span><br><span class="line">    tEnv.toAppendStream[<span class="type">Row</span>](table).print()</span><br><span class="line">    tEnv.execute(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span>(<span class="params">id: <span class="type">String</span>, user_click: <span class="type">String</span>, time: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink和SparkStreaming的区别</title>
    <url>/2020/08/12/Flink%E5%92%8CSparkStreaming%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<blockquote>
<p>一直往前走,但不妨看看曾经与现在不同技术点的闪光点与不足</p>
</blockquote>
<span id="more"></span>
<p>最开始公司使用的是spark streaming的方式来做实时数仓,根据公司对其他方面的需求对时间上的要求更加苛刻,所以开始研究flink,并在不久之后对spark streaming进行替代;对这两个技术点进行总结区分</p>
<h4 id="编程模型对比"><a href="#编程模型对比" class="headerlink" title="编程模型对比"></a>编程模型对比</h4><p>Spark Streaming</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">与kafka的结合主要是两种模型:</span><br><span class="line">1.基于receiver dataStream[kafka高级API的形式]</span><br><span class="line">简单理解是kafka把消息全部封装好,提供给spark去调用,本来kafka的消息分布在不同的partition上面,相当于做了一步数据合并,在发送给spark,效率相对慢一些</span><br><span class="line">接收到的数据存储在executor,会出现数据漏处理或者多处理状况</span><br><span class="line">2.基于direct dataStream[kafka低级API模式]</span><br><span class="line">每次到topic的每个partition依据偏移量进行获取数据,拉取数据以后进行处理,可以实现高可用;效率更快一些,同时偏移量需要自己维护</span><br><span class="line">eg:</span><br><span class="line">val Array(brokers, topics) &#x3D; args&#x2F;&#x2F;    创建一个批处理时间是2s的context    </span><br><span class="line">val sparkConf &#x3D; new SparkConf().setAppName(&quot;DirectKafkaWordCount&quot;)    </span><br><span class="line">val ssc &#x3D; new StreamingContext(sparkConf, Seconds(2))    </span><br><span class="line">&#x2F;&#x2F;    使用broker和topic创建DirectStream    </span><br><span class="line">val topicsSet &#x3D; topics.split(&quot;,&quot;).toSet    </span><br><span class="line">val kafkaParams &#x3D; Map[String, String](&quot;metadata.broker.list&quot; -&gt; brokers)    </span><br><span class="line">val messages &#x3D; KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent,    ConsumerStrategies.Subscribe[String, String](topicsSet, kafkaParams))  </span><br><span class="line">&#x2F;&#x2F; Get the lines, split them into words, count the words and print    </span><br><span class="line">val lines &#x3D; messages.map(_.value)    </span><br><span class="line">val words &#x3D; lines.flatMap(_.split(&quot; &quot;))    </span><br><span class="line">val wordCounts &#x3D; words.map(x &#x3D;&gt; (x, 1L)).reduceByKey(_ + _)   </span><br><span class="line">wordCounts.print()     &#x2F;&#x2F;    启动流    </span><br><span class="line">ssc.start()    </span><br><span class="line">ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">一般代码涉及到的内容有:</span><br><span class="line">1.设置批处理时间</span><br><span class="line">2.创建数据流</span><br><span class="line">3.编写transform</span><br><span class="line">4.编写action</span><br><span class="line">5.启动执行</span><br></pre></td></tr></table></figure>
<p>Flink</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink与kafka是事件驱动,flink内部对poll出来的数据进行了整理,然后逐条emit,形成了时间触发的机制</span><br><span class="line">StreamExecutionEnvironment env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">&#x2F;&#x2F; create a checkpoint every 5 seconds</span><br><span class="line">env.enableCheckpointing(5000); </span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); </span><br><span class="line">DataStream&lt;KafkaEvent&gt; input &#x3D; env</span><br><span class="line">           .addSource(new FlinkKafkaConsumer010&lt;&gt;(</span><br><span class="line">                   parameterTool.getRequired(&quot;input-topic&quot;),  new KafkaEventSchema(),</span><br><span class="line">                   parameterTool.getProperties())</span><br><span class="line">           .assignTimestampsAndWatermarks(new CustomWatermarkExtractor()))</span><br><span class="line">           .setParallelism(1).rebalance()</span><br><span class="line">           .keyBy(&quot;word&quot;)</span><br><span class="line">           .map(new RollingAdditionMapper()).setParallelism(0);</span><br><span class="line">input.addSink(new FlinkKafkaProducer010&lt;&gt;(parameterTool.getRequired(&quot;output-topic&quot;), new KafkaEventSchema(),</span><br><span class="line">        parameterTool.getProperties()));</span><br><span class="line">env.execute(&quot;Kafka 0.10 Example&quot;);</span><br><span class="line">flink的一般代码内容:</span><br><span class="line">1.注册数据source</span><br><span class="line">2.编写运行逻辑</span><br><span class="line">3.注册数据sink</span><br><span class="line">4.调用env.execute相比于spark streaming少了设置批处理时间,还有一个显著的区别是flink的所有算子都是lazy,调用env.execute会构建jobGraph;client端负责jobGraph生成并提交它到集群运行;而spark streaming的操作算子分action和transform,其中仅有transform是lazy形式,而且DAG生成,stage划分,任务调度是在driver端进行的</span><br></pre></td></tr></table></figure>

<h4 id="任务调度原理"><a href="#任务调度原理" class="headerlink" title="任务调度原理"></a>任务调度原理</h4><p>spark 任务调度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Spark Streaming 任务如上文提到的是基于微处理的,实际上每个批次都是一个spark core的任务;对于编码完成的spark core任务在生成到最终执行结束主要包括以下几个部分:</span><br><span class="line">1.构建DAG图</span><br><span class="line">2.划分stage</span><br><span class="line">3.生成taskset</span><br><span class="line">4.调度task</span><br></pre></td></tr></table></figure>
<p>flink 任务调度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对于flink的流任务客户端首先会生成streamGraph,接着生成JobGraph,然后将jobGraph提交给JobManager,由它完成JobGraph到ExecutionGraph的转变,最后由jobManager调度执行</span><br><span class="line">flink的拓扑生成提交执行之后,除非故障,否则拓扑部件执行位置不变,并行度由每一个算子并行度决定;而spark streaming是每个批次都会根据数据本地性和资源情况进行调度,无固定的执行拓扑结构;flink是数据在拓扑结构里流动执行,而spark streaming则是对数据缓存批次并行处理</span><br></pre></td></tr></table></figure>

<h4 id="容错机制及处理语义"><a href="#容错机制及处理语义" class="headerlink" title="容错机制及处理语义"></a>容错机制及处理语义</h4><p>spark streaming保证仅一次处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对于spark streaming任务,可以设置checkpoint,然后假如发生故障并启动,我们可以从上次ck之处恢复,但是这个行为只能使得数据不丢失,可能会重复处理,不能做到恰一次处理语义</span><br><span class="line">与kafka结合的direct stream可以手动维护offset到zookeeper,要保证数据恰一次处理语义,结果输出和offset提交必须在一个事务内完成</span><br><span class="line">1.repartition(1),spark Streaming输出的action变成仅一个partition,这样可以利用事务去做</span><br><span class="line">Dstream.foreachRDD(rdd &#x3D;&gt; &#123; rdd.repartition(1).foreachPartition(partition &#x3D;&gt;&#123;  &#x2F;&#x2F;开启事务</span><br><span class="line">   partition.foreach(each &#x3D;&gt; &#123;  &#x2F;&#x2F;提交数据</span><br><span class="line">   &#125;) &#x2F;&#x2F;提交事务</span><br><span class="line">&#125;)&#125;)</span><br><span class="line">2.将结果和offset一起提交</span><br><span class="line">结果数据包括offset;这样提交结果和提交offset就是一个操作完成,不会数据丢失,也不会重复处理;故障恢复的时候可以利用上次提交结果带的offset</span><br></pre></td></tr></table></figure>
<p>flink与kafka0.11保证仅一次处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">若要sink支持仅一次语义,必须以事务的方式写数据到kafka,这样当提交事务时两次ck间的所有写入操作作为一个事务被提交;这确保了出现故障或崩溃时这些写入操作能够被回滚</span><br><span class="line">一旦ck开始,flink的jobManager向输入流中写入一个ck barrier,将流中所有消息分割成属于本次ck的消息以及属于下次ck的,barrier也会在操作算子间流转,对于每个operator来说,该barrier会触发operator状态后端为该operator状态打快照;data source保存了kafka的offset,之后把ck barrier传递到后续的operator</span><br><span class="line">当barrier在所有算子中传递一遍,并触发的快照写入完成,预提交阶段完成;所有的触发状态快照都被视为ck的一部分,也可以说ck是整个应用程序的状态快照,包括预提交外部状态,出现故障可以从ck恢复;下一步通知所有的操作算子ck成功;该阶段jobmanager会为每个operator发起ck已完成的回调逻辑</span><br></pre></td></tr></table></figure>

<h4 id="Back-pressure"><a href="#Back-pressure" class="headerlink" title="Back pressure"></a>Back pressure</h4><p>消费者消费的速度低于生产者生产的速度,为了使应用正常,消费者会反馈给生产者来调节生产者生产的速度,以使得消费者需要多少,生产者生产多少</p>
<p>spark streaming的背压</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">跟kafka结合是存在背压机制,目标是根据当前job的处理情况来调节后续批次的获取kafka消息的条数;为了达到这个目的,spark streaming在原有的架构上加入了一个RateController,利用的算法是PID,需要的反馈数据是任务处理的结束时间,调度时间,处理时间,消息条数,这些数据是通过sparkListener体系获得,然后通过PIDRateEsimator的compute计算得到一个速率,进而可以计算得到一个offset,然后跟限速设置最大消费条数比较得到一个最终要消费的消息最大offset</span><br><span class="line"></span><br><span class="line">大概算法实现方式:</span><br><span class="line">def compute(time: Long, &#x2F;&#x2F; in milliseconds</span><br><span class="line">              numElements: Long,</span><br><span class="line">              processingDelay: Long, &#x2F;&#x2F; in milliseconds</span><br><span class="line">              schedulingDelay: Long &#x2F;&#x2F; in milliseconds</span><br><span class="line">             ): Option[Double] &#x3D; &#123;</span><br><span class="line">    logTrace(s&quot;\ntime &#x3D; $time, # records &#x3D; $numElements, &quot; +</span><br><span class="line">      s&quot;processing time &#x3D; $processingDelay, scheduling delay &#x3D; $schedulingDelay&quot;)</span><br><span class="line">    this.synchronized &#123;</span><br><span class="line">      if (time &gt; latestTime &amp;&amp; numElements &gt; 0 &amp;&amp; processingDelay &gt; 0) &#123;</span><br><span class="line">        val delaySinceUpdate &#x3D; (time - latestTime).toDouble &#x2F; 1000</span><br><span class="line">        val processingRate &#x3D; numElements.toDouble &#x2F; processingDelay * 1000</span><br><span class="line">        val error &#x3D; latestRate - processingRate</span><br><span class="line">        val historicalError &#x3D; schedulingDelay.toDouble * processingRate &#x2F; batchIntervalMillis</span><br><span class="line">        &#x2F;&#x2F; in elements&#x2F;(second ^ 2)</span><br><span class="line">        val dError &#x3D; (error - latestError) &#x2F; delaySinceUpdate</span><br><span class="line">        val newRate &#x3D; (latestRate - proportional * error -</span><br><span class="line">          integral * historicalError -</span><br><span class="line">          derivative * dError).max(minRate)</span><br><span class="line">        logTrace(s&quot;&quot;&quot;  | latestRate &#x3D; $latestRate, error &#x3D; $error</span><br><span class="line">                    | latestError &#x3D; $latestError, historicalError &#x3D; $historicalError</span><br><span class="line">                    | delaySinceUpdate &#x3D; $delaySinceUpdate, dError &#x3D; $dError</span><br><span class="line">                 &quot;&quot;&quot;.stripMargin)</span><br><span class="line">        latestTime &#x3D; time</span><br><span class="line">        if (firstRun) &#123;</span><br><span class="line">          latestRate &#x3D; processingRate</span><br><span class="line">          latestError &#x3D; 0D</span><br><span class="line">          firstRun &#x3D; false</span><br><span class="line">          logTrace(&quot;First run, rate estimation skipped&quot;)</span><br><span class="line">          None</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          latestRate &#x3D; newRate</span><br><span class="line">          latestError &#x3D; error</span><br><span class="line">          logTrace(s&quot;New rate &#x3D; $newRate&quot;)</span><br><span class="line">          Some(newRate)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        logTrace(&quot;Rate estimation skipped&quot;)</span><br><span class="line">        None</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>flink背压</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink背压是jobmanager针对每一个task每50ms触发100次Thread.getStackTrace()调用,求出阻塞的占比</span><br><span class="line">阻塞占比在web上划分了三个等级</span><br><span class="line">1.ok:0&lt;&#x3D;Ratio&lt;&#x3D;0.10  表示状态良好</span><br><span class="line">2.low:0.10&lt;Ratio&lt;&#x3D;0.5 表示有待观察</span><br><span class="line">3.HIGH: 0.5 &lt;Ratio&lt;&#x3D;1 表示要处理了</span><br></pre></td></tr></table></figure>




















]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink小扩展之Calcite自定义JDBCDriver</title>
    <url>/2020/07/03/Flink%E5%B0%8F%E6%89%A9%E5%B1%95%E4%B9%8BCalcite%E8%87%AA%E5%AE%9A%E4%B9%89JDBCDriver/</url>
    <content><![CDATA[<blockquote>
<p>其实这一部分已经偏离Flink了,但是能够扩展自己知识面-<a href="https://blog.csdn.net/dafei1288/article/details/103485689">传送门</a></p>
</blockquote>
<span id="more"></span>

<h2 id="JDBCURL的组成"><a href="#JDBCURL的组成" class="headerlink" title="JDBCURL的组成"></a>JDBCURL的组成</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">URL: jdbc:json:.&#x2F;src&#x2F;main&#x2F;resource</span><br><span class="line">协议规范: jdbc:json</span><br><span class="line">加载路径: .&#x2F;src&#x2F;main&#x2F;resource</span><br><span class="line">    .&#x2F;user.json</span><br><span class="line">    .&#x2F;order.json</span><br><span class="line">表名:user,order</span><br><span class="line"></span><br><span class="line">数据</span><br><span class="line"># user.json</span><br><span class="line">[&#123;</span><br><span class="line">  &quot;uid&quot;: 1,</span><br><span class="line">  &quot;name&quot;: &quot;dafei1288&quot;,</span><br><span class="line">  &quot;age&quot;: 33,</span><br><span class="line">  &quot;aka&quot;: &quot;+7&quot;</span><br><span class="line">&#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;uid&quot;: 2,</span><br><span class="line">    &quot;name&quot;: &quot;libailu&quot;,</span><br><span class="line">    &quot;age&quot;: 1,</span><br><span class="line">    &quot;aka&quot;: &quot;maimai&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;uid&quot;: 3,</span><br><span class="line">    &quot;name&quot;: &quot;libaitian&quot;,</span><br><span class="line">    &quot;age&quot;: 1,</span><br><span class="line">    &quot;aka&quot;: &quot;doudou&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line"># order.json</span><br><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;oid&quot;: 1,</span><br><span class="line">    &quot;uid&quot;: 1,</span><br><span class="line">    &quot;value&quot;: 11</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;oid&quot;: 2,</span><br><span class="line">    &quot;uid&quot;: 2,</span><br><span class="line">    &quot;value&quot;: 15</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="项目构建"><a href="#项目构建" class="headerlink" title="项目构建"></a>项目构建</h2><h3 id="pom依赖添加"><a href="#pom依赖添加" class="headerlink" title="pom依赖添加"></a>pom依赖添加</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.72<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.guava<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>guava<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>29.0-jre<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="编写自定义Schema类"><a href="#编写自定义Schema类" class="headerlink" title="编写自定义Schema类"></a>编写自定义Schema类</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.jdbc;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.collect.Maps;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.DataContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.linq4j.AbstractEnumerable;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.linq4j.Enumerable;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.linq4j.Enumerator;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.linq4j.Linq4j;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.linq4j.tree.Expression;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.linq4j.tree.Expressions;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.linq4j.tree.Types;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.rel.type.RelDataType;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.rel.type.RelDataTypeFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.ScannableTable;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.SchemaPlus;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.Schemas;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.Statistic;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.Statistics;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.Table;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.impl.AbstractSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.impl.AbstractTable;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.util.BuiltInMethod;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.util.Pair;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSON;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONArray;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/7/3.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonSchema</span> <span class="keyword">extends</span> <span class="title">AbstractSchema</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Map&lt;String, Table&gt; table = Maps.newHashMap();</span><br><span class="line">    <span class="keyword">private</span> String target;</span><br><span class="line">    <span class="keyword">private</span> String topic;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">JsonSchema</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">JsonSchema</span><span class="params">(String topic, String target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        <span class="keyword">this</span>.put(topic, target);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(String topic, String target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">        <span class="keyword">if</span> (!target.startsWith(<span class="string">&quot;[&quot;</span>)) &#123;</span><br><span class="line">            <span class="keyword">this</span>.target = <span class="string">&#x27;[&#x27;</span> + target + <span class="string">&#x27;]&#x27;</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.target = target;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">final</span> Table table = fieldRelation();</span><br><span class="line">        <span class="keyword">if</span> (table != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.table.put(topic, table);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;JsonSchema(topic=&quot;</span> + topic + <span class="string">&quot;:target=&quot;</span> + target + <span class="string">&quot;)&quot;</span> + <span class="keyword">this</span>.table;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getTarget</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> target;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> Map&lt;String, Table&gt; <span class="title">getTableMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> table;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Expression <span class="title">getTargetExpression</span><span class="params">(SchemaPlus parentSchema, String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Types.castIfNecessary(target.getClass(),</span><br><span class="line">                Expressions.call(Schemas.unwrap(getExpression(parentSchema, name), JsonSchema.class),</span><br><span class="line">                        BuiltInMethod.REFLECTIVE_SCHEMA_GET_TARGET.method));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> &lt;T&gt; <span class="function">Table <span class="title">fieldRelation</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        JSONArray jsonarr = JSON.parseArray(target);</span><br><span class="line">        <span class="comment">// final Enumerator&lt;Object&gt; enumerator = Linq4j.enumerator(list);</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> JsonTable(jsonarr);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonTable</span> <span class="keyword">extends</span> <span class="title">AbstractTable</span> <span class="keyword">implements</span> <span class="title">ScannableTable</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> JSONArray jsonarr;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// private final Enumerable&lt;Object&gt; enumerable;</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">JsonTable</span><span class="params">(JSONArray obj)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.jsonarr = obj;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> RelDataType <span class="title">getRowType</span><span class="params">(RelDataTypeFactory typeFactory)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">final</span> List&lt;RelDataType&gt; types = <span class="keyword">new</span> ArrayList&lt;RelDataType&gt;();</span><br><span class="line">            <span class="keyword">final</span> List&lt;String&gt; names = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">            JSONObject jsonobj = jsonarr.getJSONObject(<span class="number">0</span>);</span><br><span class="line">            <span class="keyword">for</span> (String string : jsonobj.keySet()) &#123;</span><br><span class="line">                <span class="keyword">final</span> RelDataType type;</span><br><span class="line">                type = typeFactory.createJavaType(jsonobj.get(string).getClass());</span><br><span class="line">                names.add(string);</span><br><span class="line">                types.add(type);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (names.isEmpty()) &#123;</span><br><span class="line">                names.add(<span class="string">&quot;line&quot;</span>);</span><br><span class="line">                types.add(typeFactory.createJavaType(String.class));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> typeFactory.createStructType(Pair.zip(names, types));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Statistic <span class="title">getStatistic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> Statistics.UNKNOWN;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> Enumerable&lt;Object[]&gt; scan(DataContext root) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> AbstractEnumerable&lt;Object[]&gt;() &#123;</span><br><span class="line">                <span class="keyword">public</span> Enumerator&lt;Object[]&gt; enumerator() &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">new</span> JsonEnumerator(jsonarr);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class JsonEnumerator implements Enumerator&lt;Object[]&gt; &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> Enumerator&lt;Object[]&gt; enumerator;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">JsonEnumerator</span><span class="params">(JSONArray jsonarr)</span> </span>&#123;</span><br><span class="line">            List&lt;Object[]&gt; objs = <span class="keyword">new</span> ArrayList&lt;Object[]&gt;();</span><br><span class="line">            <span class="keyword">for</span> (Object obj : jsonarr) &#123;</span><br><span class="line">                objs.add(((JSONObject) obj).values().toArray());</span><br><span class="line">            &#125;</span><br><span class="line">            enumerator = Linq4j.enumerator(objs);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> Object[] current() &#123;</span><br><span class="line">            <span class="keyword">return</span> (Object[]) enumerator.current();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">moveNext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> enumerator.moveNext();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            enumerator.reset();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            enumerator.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="实现自定义Driver"><a href="#实现自定义Driver" class="headerlink" title="实现自定义Driver"></a>实现自定义Driver</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.jdbc;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.jdbc.CalciteConnection;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.jdbc.Driver;</span><br><span class="line"><span class="keyword">import</span> org.apache.calcite.schema.SchemaPlus;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Files;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Path;</span><br><span class="line"><span class="keyword">import</span> java.nio.file.Paths;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/7/3.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JsonDriver</span> <span class="keyword">extends</span> <span class="title">Driver</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String CONNECT_STRING_PREFIX = <span class="string">&quot;jdbc:json:&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> JsonDriver().register();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> String <span class="title">getConnectStringPrefix</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> CONNECT_STRING_PREFIX;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Connection <span class="title">connect</span><span class="params">(String url, Properties info)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        Connection c = <span class="keyword">super</span>.connect(url, info);</span><br><span class="line">        CalciteConnection optiqConnection = c.unwrap(CalciteConnection.class);</span><br><span class="line">        SchemaPlus rootSchema = optiqConnection.getRootSchema();</span><br><span class="line">        String[] pars = url.split(<span class="string">&quot;:&quot;</span>);</span><br><span class="line">        Path f = Paths.get(pars[<span class="number">2</span>]);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            JsonSchema js = <span class="keyword">new</span> JsonSchema();</span><br><span class="line">            Files.list(f).forEach(it -&gt; &#123;</span><br><span class="line">                File file = it.getName(it.getNameCount() - <span class="number">1</span>).toFile();</span><br><span class="line">                String filename = file.getName();</span><br><span class="line">                filename = filename.substring(<span class="number">0</span>, filename.lastIndexOf(<span class="string">&quot;.&quot;</span>));</span><br><span class="line">                String json = <span class="string">&quot;&quot;</span>;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    json = String.join(<span class="string">&quot;&quot;</span>, Files.readAllLines(it.toAbsolutePath()));<span class="comment">//.forEach(line-&gt;&#123; sb.append(line);</span></span><br><span class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">                js.put(filename, json);</span><br><span class="line">            &#125;);</span><br><span class="line">            rootSchema.add(f.getFileName().toString(), js);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> c;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="测试代码实现"><a href="#测试代码实现" class="headerlink" title="测试代码实现"></a>测试代码实现</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注意:IDEA可能SQL语句会报红,需要修改IDEA的SQL方言</span></span><br><span class="line"><span class="comment">// File-&gt;Settings-&gt;Languages &amp; Frameworks-&gt;SQL Dialects</span></span><br><span class="line"><span class="keyword">package</span> org.example.devlop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/7/3.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExampleJsonRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Class.forName(<span class="string">&quot;org.example.jdbc.JsonDriver&quot;</span>);</span><br><span class="line">        Connection connection = DriverManager.getConnection(<span class="string">&quot;jdbc:json:./src/main/resources/&quot;</span>);</span><br><span class="line">        Statement statement = connection.createStatement();</span><br><span class="line">        ResultSet resultSet = resultSet = statement.executeQuery(</span><br><span class="line">                <span class="string">&quot;select \&quot;user\&quot;.\&quot;uid\&quot; from \&quot;resources\&quot;.\&quot;user\&quot; &quot;</span>);</span><br><span class="line">        printResultSet(resultSet);</span><br><span class="line">        resultSet = statement.executeQuery(</span><br><span class="line">                <span class="string">&quot;select * from \&quot;resources\&quot;.\&quot;order\&quot; &quot;</span>);</span><br><span class="line">        printResultSet(resultSet);</span><br><span class="line">        resultSet = statement.executeQuery(</span><br><span class="line">                <span class="string">&quot;select * from \&quot;resources\&quot;.\&quot;user\&quot; inner join \&quot;resources\&quot;.\&quot;order\&quot;  on \&quot;user\&quot;.\&quot;uid\&quot; = \&quot;order\&quot;.\&quot;uid\&quot;&quot;</span>);</span><br><span class="line">        printResultSet(resultSet);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">printResultSet</span><span class="params">(ResultSet resultSet)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            JSONObject jo = <span class="keyword">new</span> JSONObject();</span><br><span class="line">            <span class="keyword">int</span> n = resultSet.getMetaData().getColumnCount();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++) &#123;</span><br><span class="line">                jo.put(resultSet.getMetaData().getColumnName(i), resultSet.getObject(i));</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(jo.toJSONString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink实时维表Join之Redis</title>
    <url>/2020/01/06/Flink%E5%AE%9E%E6%97%B6%E7%BB%B4%E8%A1%A8Join%E4%B9%8BRedis/</url>
    <content><![CDATA[<blockquote>
<p>支持Redis注册成表,并异步加载,需要自己实现,现在只支持String的keyvalue形式</p>
</blockquote>
<span id="more"></span>

<p>实现的支持用的还是Java,使用是Scala,等有空的时候实现下scala支持</p>
<h2 id="Java类支持"><a href="#Java类支持" class="headerlink" title="Java类支持"></a>Java类支持</h2><p><strong>RedisAsyncLookupFunction</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.redis;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.RedisClient;</span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.RedisFuture;</span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.api.StatefulRedisConnection;</span><br><span class="line"><span class="keyword">import</span> io.lettuce.core.api.async.RedisAsyncCommands;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.typeutils.RowTypeInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.AsyncTableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.FunctionContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Collection;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CompletableFuture;</span><br><span class="line"><span class="keyword">import</span> java.util.function.Consumer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: xs</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020-01-03 17:21</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisAsyncLookupFunction</span> <span class="keyword">extends</span> <span class="title">AsyncTableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String[] fieldNames;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">transient</span> RedisAsyncCommands&lt;String, String&gt; async;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">RedisAsyncLookupFunction</span><span class="params">(String[] fieldNames, TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">        <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(FunctionContext context)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//配置redis异步连接</span></span><br><span class="line">        RedisClient redisClient = RedisClient.create(<span class="string">&quot;redis://hadoop02:6379/0&quot;</span>);</span><br><span class="line">        StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect();</span><br><span class="line">        async = connection.async();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//每一条流数据都会调用此方法进行join</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(CompletableFuture&lt;Collection&lt;Row&gt;&gt; future, Object... paramas)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//表名、主键名、主键值、列名</span></span><br><span class="line">        String[] info = &#123;<span class="string">&quot;userInfo&quot;</span>, <span class="string">&quot;userId&quot;</span>, paramas[<span class="number">0</span>].toString(), <span class="string">&quot;userName&quot;</span>&#125;;</span><br><span class="line">        String key = String.join(<span class="string">&quot;:&quot;</span>, info);</span><br><span class="line">        RedisFuture&lt;String&gt; redisFuture = async.get(key);</span><br><span class="line"></span><br><span class="line">        redisFuture.thenAccept(<span class="keyword">new</span> Consumer&lt;String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(String value)</span> </span>&#123;</span><br><span class="line">                future.complete(Collections.singletonList(Row.of(key, value, <span class="string">&quot;aaa&quot;</span>)));</span><br><span class="line">                <span class="comment">//todo</span></span><br><span class="line">                <span class="comment">// BinaryRow row = new BinaryRow(2);</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RowTypeInfo(fieldTypes, fieldNames);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String[] fieldNames;</span><br><span class="line">        <span class="keyword">private</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="title">Builder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Builder <span class="title">getBuilder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Builder();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldNames</span><span class="params">(String[] fieldNames)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldTypes</span><span class="params">(TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> RedisAsyncLookupFunction <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RedisAsyncLookupFunction(fieldNames, fieldTypes);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>RedisAsyncLookupTableSource</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.redis;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.TypeInformation;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.typeutils.RowTypeInfo;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.AsyncTableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.functions.TableFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.sources.LookupableTableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.sources.StreamTableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.types.DataType;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.types.utils.TypeConversions;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.Row;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: xs</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: 2020-01-03 17:23</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedisAsyncLookupTableSource</span> <span class="keyword">implements</span> <span class="title">StreamTableSource</span>&lt;<span class="title">Row</span>&gt;, <span class="title">LookupableTableSource</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String[] fieldNames;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">RedisAsyncLookupTableSource</span><span class="params">(String[] fieldNames, TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">        <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//同步方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TableFunction&lt;Row&gt; <span class="title">getLookupFunction</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//异步方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AsyncTableFunction&lt;Row&gt; <span class="title">getAsyncLookupFunction</span><span class="params">(String[] strings)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> RedisAsyncLookupFunction.Builder.getBuilder()</span><br><span class="line">                .withFieldNames(fieldNames)</span><br><span class="line">                .withFieldTypes(fieldTypes)</span><br><span class="line">                .build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启异步</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isAsyncEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataType <span class="title">getProducedDataType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TypeConversions.fromLegacyInfoToDataType(<span class="keyword">new</span> RowTypeInfo(fieldTypes, fieldNames));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TableSchema <span class="title">getTableSchema</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> TableSchema.builder()</span><br><span class="line">                .fields(fieldNames, TypeConversions.fromLegacyInfoToDataType(fieldTypes))</span><br><span class="line">                .build();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataStream&lt;Row&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment environment)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;do not support getDataStream&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> String[] fieldNames;</span><br><span class="line">        <span class="keyword">private</span> TypeInformation[] fieldTypes;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="title">Builder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Builder <span class="title">newBuilder</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Builder();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldNames</span><span class="params">(String[] fieldNames)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldNames = fieldNames;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Builder <span class="title">withFieldTypes</span><span class="params">(TypeInformation[] fieldTypes)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.fieldTypes = fieldTypes;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> RedisAsyncLookupTableSource <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RedisAsyncLookupTableSource(fieldNames, fieldTypes);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.dim.redis</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.test.flink.redis.<span class="type">RedisAsyncLookupTableSource</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.&#123;<span class="type">BasicTypeInfo</span>, <span class="type">TypeInformation</span>, <span class="type">Types</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">DataTypes</span>, <span class="type">EnvironmentSettings</span>, <span class="type">TableSchema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.io.jdbc.&#123;<span class="type">JDBCAppendTableSink</span>, <span class="type">JDBCOptions</span>, <span class="type">JDBCUpsertTableSink</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.&#123;<span class="type">StreamTableEnvironment</span>, _&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-03 17:15</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DoubleStreamRedisDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">    <span class="keyword">val</span> ds = env.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line">    <span class="comment">// 1000,good0c,1566375779658</span></span><br><span class="line">    <span class="keyword">val</span> demo = ds.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = x.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      <span class="type">Demo</span>(arr(<span class="number">0</span>), arr(<span class="number">1</span>), arr(<span class="number">2</span>))</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    tEnv.registerDataStream(<span class="string">&quot;user_click_name&quot;</span>, demo, <span class="symbol">&#x27;id</span>, <span class="symbol">&#x27;user_click</span>, <span class="symbol">&#x27;time</span>, <span class="symbol">&#x27;proctime</span>.proctime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> redisSource = <span class="type">RedisAsyncLookupTableSource</span>.<span class="type">Builder</span>.newBuilder().withFieldNames(<span class="type">Array</span>(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>))</span><br><span class="line">      .withFieldTypes(<span class="type">Array</span>(<span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>))</span><br><span class="line">      .build()</span><br><span class="line">    tEnv.registerTableSource(<span class="string">&quot;info&quot;</span>, redisSource)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql =</span><br><span class="line">    <span class="comment">//&quot;select t1.id,t1.user_click,t2.name&quot; +</span></span><br><span class="line">      <span class="string">&quot;select * &quot;</span> +</span><br><span class="line">        <span class="string">&quot; from user_click_name as t1&quot;</span> +</span><br><span class="line">        <span class="string">&quot; join info FOR SYSTEM_TIME AS OF t1.proctime as t2&quot;</span> +</span><br><span class="line">        <span class="string">&quot; on t1.id = t2.id&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> table = tEnv.sqlQuery(sql)</span><br><span class="line">    <span class="keyword">val</span> tableName = table.toString</span><br><span class="line">    tEnv.toAppendStream[<span class="type">Row</span>](table).print()</span><br><span class="line">    <span class="comment">//    val value2 = tEnv.toRetractStream[Row](table).filter(_._1).map(_._2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ----------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// AppendTableSink</span></span><br><span class="line">    <span class="keyword">val</span> sinkA = <span class="type">JDBCAppendTableSink</span>.builder()</span><br><span class="line">      .setDrivername(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">      .setDBUrl(<span class="string">&quot;jdbc:mysql://localhost:3306/world?autoReconnect=true&amp;failOverReadOnly=false&amp;useSSL=false&quot;</span>)</span><br><span class="line">      .setUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">      .setPassword(<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">      .setQuery(<span class="string">&quot;insert into test (uid) values (?)&quot;</span>)</span><br><span class="line">      .setBatchSize(<span class="number">1</span>)</span><br><span class="line">      .setParameterTypes(<span class="type">Types</span>.<span class="type">STRING</span>)</span><br><span class="line">      .build()</span><br><span class="line">    <span class="comment">//    tEnv.registerTableSink(&quot;jdbcOutputTable&quot;, Array[String](&quot;uid&quot;), Array[TypeInformation[_]](BasicTypeInfo.STRING_TYPE_INFO), sinkA)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// UpsertTableSink</span></span><br><span class="line">    <span class="keyword">val</span> sinkB = <span class="type">JDBCUpsertTableSink</span>.builder()</span><br><span class="line">      .setOptions(<span class="type">JDBCOptions</span>.builder()</span><br><span class="line">        .setDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">        .setDBUrl(<span class="string">&quot;jdbc:mysql://localhost:3306/world?autoReconnect=true&amp;failOverReadOnly=false&amp;useSSL=false&quot;</span>)</span><br><span class="line">        .setUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">        .setPassword(<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">        .setTableName(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">        .build())</span><br><span class="line">      .setTableSchema(<span class="type">TableSchema</span>.builder()</span><br><span class="line">        .field(<span class="string">&quot;uid&quot;</span>, <span class="type">DataTypes</span>.<span class="type">STRING</span>())</span><br><span class="line">        .build())</span><br><span class="line">      .setFlushIntervalMills(<span class="number">1</span>)</span><br><span class="line">      .build()</span><br><span class="line">    <span class="comment">// tEnv.registerTableSink(&quot;jdbcOutputTable&quot;, sinkB)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// table.insertInto(&quot;jdbcOutputTable&quot;)</span></span><br><span class="line">    <span class="comment">//    val insertSQL = &quot;insert into jdbcOutputTable (uid) select id from &quot; + tableName</span></span><br><span class="line">    <span class="comment">//    tEnv.sqlUpdate(insertSQL)</span></span><br><span class="line">    tEnv.execute(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span>(<span class="params">id: <span class="type">String</span>, user_click: <span class="type">String</span>, time: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink实时维表Join</title>
    <url>/2019/12/24/Flink%E5%AE%9E%E6%97%B6%E7%BB%B4%E8%A1%A8Join/</url>
    <content><![CDATA[<blockquote>
<p>维表是一张实时变化的表,流表需要去Join维表,如何实现这种需求</p>
</blockquote>
<span id="more"></span>

<p>官网上有这样的例子,只是没有详细去看,还是学的不认真<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/streaming/joins.html">传送门</a></p>
<h2 id="Joins-in-Continuous-Queries-Join连续查询"><a href="#Joins-in-Continuous-Queries-Join连续查询" class="headerlink" title="Joins in Continuous Queries(Join连续查询)"></a>Joins in Continuous Queries(Join连续查询)</h2><h3 id="Regular-Joins-定期Join"><a href="#Regular-Joins-定期Join" class="headerlink" title="Regular Joins(定期Join)"></a>Regular Joins(定期Join)</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Orders</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> Product</span><br><span class="line"><span class="keyword">ON</span> Orders.productId = Product.id</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Time-windowed-Joins-时间窗口Join"><a href="#Time-windowed-Joins-时间窗口Join" class="headerlink" title="Time-windowed Joins(时间窗口Join)"></a>Time-windowed Joins(时间窗口Join)</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  Orders o,</span><br><span class="line">  Shipments s</span><br><span class="line"><span class="keyword">WHERE</span> o.id = s.orderId <span class="keyword">AND</span></span><br><span class="line">      o.ordertime <span class="keyword">BETWEEN</span> s.shiptime - <span class="built_in">INTERVAL</span> <span class="string">&#x27;4&#x27;</span> <span class="keyword">HOUR</span> <span class="keyword">AND</span> s.shiptime</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Join-with-a-Temporal-Table-Function-时间表函数连接"><a href="#Join-with-a-Temporal-Table-Function-时间表函数连接" class="headerlink" title="Join with a Temporal Table Function(时间表函数连接)"></a>Join with a Temporal Table Function(时间表函数连接)</h3><p>重点需要知道TemporalTable如何创建</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Order订单表,RatesHistory汇率表</span><br><span class="line">想要获取不同时间的不同汇率比</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> Orders;</span><br><span class="line"></span><br><span class="line">rowtime amount currency</span><br><span class="line">======= ====== =========</span><br><span class="line">10:15        2 Euro</span><br><span class="line">10:30        1 US Dollar</span><br><span class="line">10:32       50 Yen</span><br><span class="line">10:52        3 Euro</span><br><span class="line">11:04        5 US Dollar</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> RatesHistory;</span><br><span class="line"></span><br><span class="line">rowtime currency   rate</span><br><span class="line">======= ======== ======</span><br><span class="line">09:00   US Dollar   102</span><br><span class="line">09:00   Euro        114</span><br><span class="line">09:00   Yen           1</span><br><span class="line">10:45   Euro        116</span><br><span class="line">11:15   Euro        119</span><br><span class="line">11:49   Pounds      108</span><br></pre></td></tr></table></figure>

<p><strong>时间表函数/时间表创建</strong>:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"># 定义时间表函数</span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ratesHistoryData = <span class="keyword">new</span> mutable.<span class="type">MutableList</span>[(<span class="type">String</span>, <span class="type">Long</span>)]</span><br><span class="line">ratesHistoryData.+=((<span class="string">&quot;US Dollar&quot;</span>, <span class="number">102</span>L))</span><br><span class="line">ratesHistoryData.+=((<span class="string">&quot;Euro&quot;</span>, <span class="number">114</span>L))</span><br><span class="line">ratesHistoryData.+=((<span class="string">&quot;Yen&quot;</span>, <span class="number">1</span>L))</span><br><span class="line">ratesHistoryData.+=((<span class="string">&quot;Euro&quot;</span>, <span class="number">116</span>L))</span><br><span class="line">ratesHistoryData.+=((<span class="string">&quot;Euro&quot;</span>, <span class="number">119</span>L))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ratesHistory = env</span><br><span class="line">  .fromCollection(ratesHistoryData)</span><br><span class="line">  .toTable(tEnv, <span class="symbol">&#x27;r_currency</span>, <span class="symbol">&#x27;r_rate</span>, <span class="symbol">&#x27;r_proctime</span>.proctime)</span><br><span class="line"></span><br><span class="line">tEnv.registerTable(<span class="string">&quot;RatesHistory&quot;</span>, ratesHistory)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在TableAPI中使用</span></span><br><span class="line"><span class="keyword">val</span> rates = ratesHistory.createTemporalTableFunction(<span class="symbol">&#x27;r_proctime</span>, <span class="symbol">&#x27;r_currency</span>) </span><br><span class="line"><span class="comment">// 在SQL中使用</span></span><br><span class="line">tEnv.registerFunction(<span class="string">&quot;Rates&quot;</span>, rates)</span><br><span class="line"></span><br><span class="line"># 定义时间表,只被<span class="type">Blink</span>支持</span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.getTableEnvironment(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rates = <span class="keyword">new</span> <span class="type">HBaseTableSource</span>(conf, <span class="string">&quot;Rates&quot;</span>)</span><br><span class="line">rates.setRowKey(<span class="string">&quot;currency&quot;</span>, <span class="type">String</span>.<span class="keyword">class</span>)</span><br><span class="line">rates.addColumn(<span class="string">&quot;fam1&quot;</span>, <span class="string">&quot;rate&quot;</span>, <span class="type">Double</span>.<span class="keyword">class</span>)</span><br><span class="line"></span><br><span class="line">tEnv.registerTableSource(<span class="string">&quot;Rates&quot;</span>, rates)</span><br></pre></td></tr></table></figure>

<p><strong>实现</strong>:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不使用时间表</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="keyword">SUM</span>(o.amount * r.rate) <span class="keyword">AS</span> amount</span><br><span class="line"><span class="keyword">FROM</span> Orders <span class="keyword">AS</span> o,</span><br><span class="line">  RatesHistory <span class="keyword">AS</span> r</span><br><span class="line"><span class="keyword">WHERE</span> r.currency = o.currency</span><br><span class="line"><span class="keyword">AND</span> r.rowtime = (</span><br><span class="line">  <span class="keyword">SELECT</span> <span class="keyword">MAX</span>(rowtime)</span><br><span class="line">  <span class="keyword">FROM</span> RatesHistory <span class="keyword">AS</span> r2</span><br><span class="line">  <span class="keyword">WHERE</span> r2.currency = o.currency</span><br><span class="line">  <span class="keyword">AND</span> r2.rowtime &lt;= o.rowtime);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用时间表</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  o.amount * r.rate <span class="keyword">AS</span> amount</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  Orders <span class="keyword">AS</span> o,</span><br><span class="line">  <span class="keyword">LATERAL</span> <span class="keyword">TABLE</span> (Rates(o.rowtime)) <span class="keyword">AS</span> r</span><br><span class="line"><span class="keyword">WHERE</span> r.currency = o.currency</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Join-with-a-Temporal-Table-时间表连接"><a href="#Join-with-a-Temporal-Table-时间表连接" class="headerlink" title="Join with a Temporal Table(时间表连接)"></a>Join with a Temporal Table(时间表连接)</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不是任何表都可以作为时间表,作为时间表的只能是LookupableTableSource</span></span><br><span class="line"><span class="comment"># LookupableTableSource只被Blink支持</span></span><br><span class="line"><span class="comment"># 仅在SQL中支持,而在Table API中尚不支持</span></span><br><span class="line"><span class="comment"># Flink当前不支持事件时间时间表连接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  o.amout, o.currency, r.rate, o.amount * r.rate</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  Orders <span class="keyword">AS</span> o</span><br><span class="line">  <span class="keyword">JOIN</span> LatestRates <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> o.proctime <span class="keyword">AS</span> r</span><br><span class="line">  <span class="keyword">ON</span> r.currency = o.currency;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  <span class="keyword">SUM</span>(o_amount * r_rate) <span class="keyword">AS</span> amount</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">  Orders</span><br><span class="line">  <span class="keyword">JOIN</span> LatestRates <span class="keyword">FOR</span> SYSTEM_TIME <span class="keyword">AS</span> <span class="keyword">OF</span> o_proctime</span><br><span class="line">  <span class="keyword">ON</span> r_currency = o_currency;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实际需求"><a href="#实际需求" class="headerlink" title="实际需求"></a>实际需求</h2><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><p>维表实时变化,流表需要进行维表join</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.mysql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.io.jdbc.&#123;<span class="type">JDBCOptions</span>, <span class="type">JDBCTableSource</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">DataTypes</span>, <span class="type">EnvironmentSettings</span>, <span class="type">TableSchema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.&#123;<span class="type">StreamTableEnvironment</span>, _&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2019-12-24 13:41</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DoubleStreamJDBCDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance()</span><br><span class="line">      .useBlinkPlanner()</span><br><span class="line">      .inStreamingMode()</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line">    <span class="keyword">val</span> jdbcOptions = <span class="type">JDBCOptions</span>.builder()</span><br><span class="line">      .setDriverName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">      .setDBUrl(<span class="string">&quot;jdbc:mysql://localhost:3306/world?autoReconnect=true&amp;failOverReadOnly=false&amp;useSSL=false&quot;</span>)</span><br><span class="line">      .setUsername(<span class="string">&quot;root&quot;</span>)</span><br><span class="line">      .setPassword(<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">      .setTableName(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> tableSchema = <span class="type">TableSchema</span>.builder()</span><br><span class="line">      .field(<span class="string">&quot;uid&quot;</span>, <span class="type">DataTypes</span>.<span class="type">INT</span>())</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> jdbcTableSource = <span class="type">JDBCTableSource</span>.builder.setOptions(jdbcOptions).setSchema(tableSchema).build</span><br><span class="line">    tEnv.registerTableSource(<span class="string">&quot;sessions&quot;</span>, jdbcTableSource)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ds = env.socketTextStream(<span class="string">&quot;eva&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line">    <span class="keyword">val</span> demo: <span class="type">DataStream</span>[<span class="type">Demo</span>] = ds.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; &#123;</span><br><span class="line">      <span class="type">Demo</span>(x.toInt, <span class="string">&quot;test&quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">val</span> table = tEnv.sqlQuery(<span class="string">&quot;SELECT * FROM sessions&quot;</span>)</span><br><span class="line"></span><br><span class="line">    tEnv.registerDataStream(<span class="string">&quot;demoTable&quot;</span>, demo, <span class="symbol">&#x27;user</span>, <span class="symbol">&#x27;result</span>, <span class="symbol">&#x27;proctime</span>.proctime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = tEnv.sqlQuery(<span class="string">&quot;select * from demoTable a left join sessions FOR SYSTEM_TIME AS OF a.proctime AS b ON `a`.`user` = `b`.`uid`&quot;</span>)</span><br><span class="line">    tEnv.toRetractStream[<span class="type">Row</span>](result).print</span><br><span class="line">    tEnv.toAppendStream[<span class="type">Order</span>](table).print</span><br><span class="line">    tEnv.execute(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span>(<span class="params">user: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">Demo</span>(<span class="params">user: <span class="type">Int</span>, result: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink开源模块化学习整理</title>
    <url>/2020/09/08/Flink%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9D%97%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>记录Flink模块化学习的整理,不然感觉始终在某一块模块停留,虽然会精于一个领域,但是会乏味</p>
</blockquote>
<span id="more"></span>

<h2 id="flink-annotations"><a href="#flink-annotations" class="headerlink" title="flink-annotations"></a>flink-annotations</h2><hr>
<h2 id="flink-clients"><a href="#flink-clients" class="headerlink" title="flink-clients"></a>flink-clients</h2><hr>
<h2 id="flink-connectors"><a href="#flink-connectors" class="headerlink" title="flink-connectors"></a>flink-connectors</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="flink-container"><a href="#flink-container" class="headerlink" title="flink-container"></a>flink-container</h2><hr>
<h2 id="flink-contrib"><a href="#flink-contrib" class="headerlink" title="flink-contrib"></a>flink-contrib</h2><hr>
<h2 id="flink-core"><a href="#flink-core" class="headerlink" title="flink-core"></a>flink-core</h2><hr>
<h2 id="flink-dist"><a href="#flink-dist" class="headerlink" title="flink-dist"></a>flink-dist</h2><hr>
<h2 id="flink-docs"><a href="#flink-docs" class="headerlink" title="flink-docs"></a>flink-docs</h2><hr>
<h2 id="flink-end-to-end-tests"><a href="#flink-end-to-end-tests" class="headerlink" title="flink-end-to-end-tests"></a>flink-end-to-end-tests</h2><hr>
<h2 id="flink-examples"><a href="#flink-examples" class="headerlink" title="flink-examples"></a>flink-examples</h2><hr>
<h2 id="flink-external-resource"><a href="#flink-external-resource" class="headerlink" title="flink-external-resource"></a>flink-external-resource</h2><hr>
<h2 id="flink-filesystems"><a href="#flink-filesystems" class="headerlink" title="flink-filesystems"></a>flink-filesystems</h2><hr>
<h2 id="flink-formats"><a href="#flink-formats" class="headerlink" title="flink-formats"></a>flink-formats</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink-avro 完成</span><br><span class="line">flink-csv 完成</span><br><span class="line">flink-json 完成</span><br><span class="line">flink-orc 完成</span><br><span class="line">flink-parquet 完成</span><br><span class="line">flink-sql-orc 完成</span><br><span class="line">flink-sql-parquet 完成</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="flink-fs-tests"><a href="#flink-fs-tests" class="headerlink" title="flink-fs-tests"></a>flink-fs-tests</h2><hr>
<h2 id="flink-java"><a href="#flink-java" class="headerlink" title="flink-java"></a>flink-java</h2><hr>
<h2 id="flink-metrics"><a href="#flink-metrics" class="headerlink" title="flink-metrics"></a>flink-metrics</h2>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink批流统一操作外部存储</title>
    <url>/2020/04/18/Flink%E6%89%B9%E6%B5%81%E7%BB%9F%E4%B8%80%E6%93%8D%E4%BD%9C%E5%A4%96%E9%83%A8%E5%AD%98%E5%82%A8/</url>
    <content><![CDATA[<blockquote>
<p>整理公司业务,对于Flink离线批处理进行学习与踩坑</p>
</blockquote>
<span id="more"></span>

<h1 id="环境配置项"><a href="#环境配置项" class="headerlink" title="环境配置项"></a>环境配置项</h1><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">tEnv.getConfig.getConfiguration.setBoolean(<span class="type">OptimizerConfigOptions</span>.<span class="type">TABLE_OPTIMIZER_JOIN_REORDER_ENABLED</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="comment">// 关闭SQL优化重用源表</span></span><br><span class="line">tEnv.getConfig.getConfiguration.setBoolean(<span class="type">OptimizerConfigOptions</span>.<span class="type">TABLE_OPTIMIZER_REUSE_SOURCE_ENABLED</span>, <span class="literal">false</span>)</span><br><span class="line">tEnv.getConfig.getConfiguration.setLong(<span class="type">OptimizerConfigOptions</span>.<span class="type">TABLE_OPTIMIZER_BROADCAST_JOIN_THRESHOLD</span>, <span class="number">10485760</span>L)</span><br><span class="line">tEnv.getConfig.getConfiguration.setInteger(<span class="type">ExecutionConfigOptions</span>.<span class="type">TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM</span>, <span class="number">1</span>)</span><br><span class="line">tEnv.getConfig.getConfiguration.setInteger(<span class="type">ExecutionConfigOptions</span>.<span class="type">TABLE_EXEC_SORT_DEFAULT_LIMIT</span>, <span class="number">200</span>)</span><br><span class="line">tEnv.getConfig.addConfiguration(<span class="type">GlobalConfiguration</span>.loadConfiguration)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h1><h2 id="数据Source"><a href="#数据Source" class="headerlink" title="数据Source"></a>数据Source</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这里我统一使用Hive作为数据源,符合离线数仓实际</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建方式1: StreamTableEnv,写入hive不支持(insert into),只能使用(insert overwrite),可支持MySQL的写入</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建方式2: TableEnv,写入hive(insert into,overwrite皆可),写MySQL不支持</span></span><br><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inBatchMode().build()</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">TableEnvironment</span>.create(settings)</span><br><span class="line"><span class="keyword">val</span> hiveCatalog = <span class="keyword">new</span> <span class="type">HiveCatalog</span>(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;default&quot;</span>,</span><br><span class="line">  <span class="string">&quot;hive_conf&quot;</span>, <span class="string">&quot;2.1.1&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册HiveCatalog</span></span><br><span class="line">tEnv.registerCatalog(<span class="string">&quot;test&quot;</span>, hiveCatalog)</span><br><span class="line">tEnv.useCatalog(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印Catalog中拥有的表</span></span><br><span class="line">tEnv.listTables().foreach(println)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="数据Sink"><a href="#数据Sink" class="headerlink" title="数据Sink"></a>数据Sink</h2><h3 id="写入Hive"><a href="#写入Hive" class="headerlink" title="写入Hive"></a>写入Hive</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建方式1,2皆可,注意两者不同,1不可聚合</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 当结果表为Hive表时</span></span><br><span class="line">tEnv.getConfig.setSqlDialect(<span class="type">SqlDialect</span>.<span class="type">HIVE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意,因为使用的是HiveCatalog,对于Hive上的元数据它都有,所以我们直接使用就行</span></span><br><span class="line"><span class="keyword">val</span> src = tEnv.sqlQuery(<span class="string">&quot;SELECT * FROM default.test&quot;</span>)</span><br><span class="line">tEnv.sqlUpdate(<span class="string">s&quot;INSERT INTO default.test1 SELECT id FROM <span class="subst">$src</span>&quot;</span>)</span><br><span class="line">tEnv.execute(<span class="string">&quot;insert into hive table&quot;</span>)</span><br><span class="line"></span><br><span class="line">注意,可能出现找不到<span class="type">HDFS</span>块的报错</span><br><span class="line">解决方式:</span><br><span class="line">hiveCatalog.getHiveConf.set(<span class="string">&quot;dfs.client.use.datanode.hostname&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="写入Kafka"><a href="#写入Kafka" class="headerlink" title="写入Kafka"></a>写入Kafka</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建方式1,2皆可,1不可以进行聚合操作,2可以</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意,实际上的表是创建在Hive上的,所以不能重复创建</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKafkaTable</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">      |CREATE TABLE demo1 (</span></span><br><span class="line"><span class="string">      |    id VARCHAR COMMENT &#x27;uid&#x27;</span></span><br><span class="line"><span class="string">      |)</span></span><br><span class="line"><span class="string">      |WITH (</span></span><br><span class="line"><span class="string">      |    &#x27;connector.type&#x27; = &#x27;kafka&#x27;, -- 使用 kafka connector</span></span><br><span class="line"><span class="string">      |    &#x27;connector.version&#x27; = &#x27;universal&#x27;,  -- kafka 版本</span></span><br><span class="line"><span class="string">      |    &#x27;connector.topic&#x27; = &#x27;test01&#x27;,  -- kafka topic</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.0.key&#x27; = &#x27;zookeeper.connect&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.0.value&#x27; = &#x27;cdh04:2181&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.1.key&#x27; = &#x27;bootstrap.servers&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.properties.1.value&#x27; = &#x27;cdh04:9092&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">      |    &#x27;connector.sink-partitioner&#x27; = &#x27;fixed&#x27;,</span></span><br><span class="line"><span class="string">      |    &#x27;update-mode&#x27; = &#x27;append&#x27;,</span></span><br><span class="line"><span class="string">      |    &#x27;format.type&#x27; = &#x27;json&#x27;,  -- 数据源格式为 json</span></span><br><span class="line"><span class="string">      |    &#x27;format.derive-schema&#x27; = &#x27;true&#x27; -- 从 DDL schema 确定 json 解析规则</span></span><br><span class="line"><span class="string">      |)</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tEnv.sqlUpdate(createKafkaTable())</span><br><span class="line"><span class="keyword">val</span> src = tEnv.sqlQuery(<span class="string">&quot;SELECT * FROM default.test&quot;</span>)</span><br><span class="line">tEnv.sqlUpdate(<span class="string">s&quot;INSERT INTO demo1 SELECT id FROM <span class="subst">$src</span>&quot;</span>)</span><br><span class="line">tEnv.execute(<span class="string">&quot;insert into kafka table&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// kafka数据</span></span><br><span class="line">kafka-console-consumer --group demo --bootstrap-server cdh05:<span class="number">9092</span> --topic test01 --from-beginning</span><br><span class="line">&#123;<span class="string">&quot;id&quot;</span>:<span class="string">&quot;test&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">注意,可能出现bootstrap.servers <span class="type">URL</span> 不能解析的报错</span><br><span class="line">初步判断是生产环境<span class="type">Kafka</span>内网解析导致的</span><br></pre></td></tr></table></figure>

<h3 id="写入MySQL"><a href="#写入MySQL" class="headerlink" title="写入MySQL"></a>写入MySQL</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 只能使用创建方式1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createMysqlTable</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">       |CREATE TABLE demo2 (</span></span><br><span class="line"><span class="string">       |	`id` VARCHAR</span></span><br><span class="line"><span class="string">       |) WITH (</span></span><br><span class="line"><span class="string">       |	&#x27;connector.type&#x27; = &#x27;jdbc&#x27;,</span></span><br><span class="line"><span class="string">       |	&#x27;connector.url&#x27; = &#x27;jdbc:mysql://localhost:3306/test?useSSL=true&amp;serverTimezone=UTC&#x27;,</span></span><br><span class="line"><span class="string">       |	&#x27;connector.table&#x27; = &#x27;demo&#x27;,</span></span><br><span class="line"><span class="string">       |	&#x27;connector.driver&#x27; = &#x27;com.mysql.cj.jdbc.Driver&#x27;,</span></span><br><span class="line"><span class="string">       |	&#x27;connector.username&#x27; = &#x27;root&#x27;,</span></span><br><span class="line"><span class="string">       |	&#x27;connector.password&#x27; = &#x27;123456&#x27;</span></span><br><span class="line"><span class="string">       |)</span></span><br><span class="line"><span class="string">       |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tEnv.sqlUpdate(createMysqlTable())</span><br><span class="line"><span class="keyword">val</span> src = tEnv.sqlQuery(<span class="string">&quot;SELECT * FROM default.test&quot;</span>)</span><br><span class="line">tEnv.sqlUpdate(<span class="string">s&quot;INSERT INTO demo2 SELECT id FROM <span class="subst">$src</span>&quot;</span>)</span><br><span class="line">tEnv.execute(<span class="string">&quot;insert into mysql table&quot;</span>)</span><br><span class="line"></span><br><span class="line">注意,在进行字段变化,导致字段模糊的情况下</span><br><span class="line">eg:</span><br><span class="line">    <span class="keyword">val</span> table = tEnv.sqlQuery(</span><br><span class="line">        <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">        |SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY index DESC) AS num</span></span><br><span class="line"><span class="string">        |FROM  default.test</span></span><br><span class="line"><span class="string">        |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">    ).filter(<span class="string">&quot;num = 1&quot;</span>).select(</span><br><span class="line">      <span class="string">&quot;id,num&quot;</span></span><br><span class="line">    )</span><br><span class="line">报错<span class="type">Table</span> has a full primary keys <span class="keyword">if</span> it is updated</span><br><span class="line">解决方式:</span><br><span class="line">    在后面加上.groupBy(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;num&quot;</span>).select(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;num&quot;</span>)</span><br><span class="line"></span><br><span class="line">报错<span class="type">Conversion</span> to relational algebra failed to preserve datatypes</span><br><span class="line">解决方式:</span><br><span class="line">    检查类型转换</span><br></pre></td></tr></table></figure>

<h3 id="写入Es"><a href="#写入Es" class="headerlink" title="写入Es"></a>写入Es</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 只支持Streaming Append Mode Sink和Streaming Upsert Mode</span></span><br><span class="line"><span class="comment">// 只能使用创建方式1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createEsTable</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">  <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">     |CREATE TABLE test_xs_01 (</span></span><br><span class="line"><span class="string">     | id VARCHAR</span></span><br><span class="line"><span class="string">     |) WITH (</span></span><br><span class="line"><span class="string">     | &#x27;connector.type&#x27; = &#x27;elasticsearch&#x27;,</span></span><br><span class="line"><span class="string">     | &#x27;connector.version&#x27; = &#x27;7&#x27;,</span></span><br><span class="line"><span class="string">     | &#x27;connector.hosts&#x27; = &#x27;http://hosts01:9200;http://hosts02:9200;http://hosts03:9200&#x27;,</span></span><br><span class="line"><span class="string">     | &#x27;connector.index&#x27; = &#x27;test_xs_01&#x27;,</span></span><br><span class="line"><span class="string">     | &#x27;connector.document-type&#x27; = &#x27;test_xs_01&#x27;,</span></span><br><span class="line"><span class="string">     | &#x27;update-mode&#x27; = &#x27;append&#x27;,</span></span><br><span class="line"><span class="string">     | &#x27;format.type&#x27; = &#x27;json&#x27;</span></span><br><span class="line"><span class="string">     |)</span></span><br><span class="line"><span class="string">     |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">&#125;</span><br><span class="line">注意:update-mode为append时,<span class="type">Es</span>中的id是随机生成的</span><br><span class="line">如果需要指定id,需要使用upsert</span><br><span class="line">并进行group by操作,<span class="type">Es</span>中id为分组字段拼接</span><br><span class="line"></span><br><span class="line">tEnv.sqlUpdate(createEsTable())</span><br><span class="line"><span class="keyword">val</span> src = tEnv.sqlQuery(<span class="string">&quot;SELECT * FROM default.test&quot;</span>)</span><br><span class="line">tEnv.sqlUpdate(<span class="string">s&quot;INSERT INTO test_xs_01 SELECT id FROM <span class="subst">$src</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">tEnv.execute(<span class="string">&quot;insert into es table&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="流处理"><a href="#流处理" class="headerlink" title="流处理"></a>流处理</h1><h2 id="数据Source-1"><a href="#数据Source-1" class="headerlink" title="数据Source"></a>数据Source</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 使用Kafka作为数据源</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">env.setParallelism(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKafkaSourceTable</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    |CREATE TABLE test (</span></span><br><span class="line"><span class="string">    |    business VARCHAR,</span></span><br><span class="line"><span class="string">    |    database VARCHAR,</span></span><br><span class="line"><span class="string">    |    es VARCHAR,</span></span><br><span class="line"><span class="string">    |    ts BIGINT,</span></span><br><span class="line"><span class="string">    |    rowtime as TO_TIMESTAMP(FROM_UNIXTIME(ts / 1000,&#x27;yyyy-MM-dd HH:mm:ss&#x27;)),</span></span><br><span class="line"><span class="string">    |    proctime as PROCTIME(),</span></span><br><span class="line"><span class="string">    |    WATERMARK FOR rowtime as rowtime - INTERVAL &#x27;5&#x27; SECOND</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">    |WITH (</span></span><br><span class="line"><span class="string">    |    &#x27;connector.type&#x27; = &#x27;kafka&#x27;, -- 使用 kafka connector</span></span><br><span class="line"><span class="string">    |    &#x27;connector.version&#x27; = &#x27;universal&#x27;,  -- kafka 版本</span></span><br><span class="line"><span class="string">    |    &#x27;connector.topic&#x27; = &#x27;test01&#x27;,  -- kafka topic</span></span><br><span class="line"><span class="string">    |    &#x27;connector.properties.zookeeper.connect&#x27; = &#x27;cdh04:2181&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">    |    &#x27;connector.properties.bootstrap.servers&#x27; = &#x27;cdh04:9092&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">    |    &#x27;connector.properties.group.id&#x27; = &#x27;kafkasql&#x27;,</span></span><br><span class="line"><span class="string">    |    &#x27;update-mode&#x27; = &#x27;append&#x27;,</span></span><br><span class="line"><span class="string">    |    &#x27;connector.startup-mode&#x27; = &#x27;earliest-offset&#x27;,</span></span><br><span class="line"><span class="string">    |    &#x27;format.type&#x27; = &#x27;json&#x27;,  -- 数据源格式为 json</span></span><br><span class="line"><span class="string">    |    &#x27;format.derive-schema&#x27; = &#x27;true&#x27; -- 从 DDL schema 确定 json 解析规则</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tEnv.sqlUpdate(createKafkaSourceTable)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="数据Sink-1"><a href="#数据Sink-1" class="headerlink" title="数据Sink"></a>数据Sink</h2><h3 id="写入Kakfa"><a href="#写入Kakfa" class="headerlink" title="写入Kakfa"></a>写入Kakfa</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createKafkaSinkTable</span></span>(): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    |CREATE TABLE demo1 (</span></span><br><span class="line"><span class="string">    |    business VARCHAR COMMENT &#x27;uid&#x27;,</span></span><br><span class="line"><span class="string">    |    pv BIGINT COMMENT &#x27;pv&#x27;,</span></span><br><span class="line"><span class="string">    |    t_start TIMESTAMP(3) COMMENT &#x27;开始时间&#x27;,</span></span><br><span class="line"><span class="string">    |    t_end TIMESTAMP(3) COMMENT &#x27;结束时间&#x27;</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">    |WITH (</span></span><br><span class="line"><span class="string">    |    &#x27;connector.type&#x27; = &#x27;kafka&#x27;, -- 使用 kafka connector</span></span><br><span class="line"><span class="string">    |    &#x27;connector.version&#x27; = &#x27;universal&#x27;,  -- kafka 版本</span></span><br><span class="line"><span class="string">    |    &#x27;connector.topic&#x27; = &#x27;test01_sink&#x27;,  -- kafka topic</span></span><br><span class="line"><span class="string">    |    &#x27;connector.properties.0.key&#x27; = &#x27;zookeeper.connect&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">    |    &#x27;connector.properties.0.value&#x27; = &#x27;cdh04:2181&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">    |    &#x27;connector.properties.1.key&#x27; = &#x27;bootstrap.servers&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">    |    &#x27;connector.properties.1.value&#x27; = &#x27;cdh04:9092&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">    |    &#x27;connector.sink-partitioner&#x27; = &#x27;fixed&#x27;,</span></span><br><span class="line"><span class="string">    |    &#x27;update-mode&#x27; = &#x27;append&#x27;,</span></span><br><span class="line"><span class="string">    |    &#x27;format.type&#x27; = &#x27;json&#x27;,  -- 数据源格式为 json</span></span><br><span class="line"><span class="string">    |    &#x27;format.derive-schema&#x27; = &#x27;true&#x27; -- 从 DDL schema 确定 json 解析规则</span></span><br><span class="line"><span class="string">    |)</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> query =</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    |SELECT business,COUNT(1) as pv,</span></span><br><span class="line"><span class="string">    | HOP_START(rowtime, INTERVAL &#x27;5&#x27; second, INTERVAL &#x27;10&#x27; second) as t_start,</span></span><br><span class="line"><span class="string">    | HOP_END(rowtime, INTERVAL &#x27;5&#x27; second, INTERVAL &#x27;10&#x27; second) as t_end</span></span><br><span class="line"><span class="string">    |FROM test</span></span><br><span class="line"><span class="string">    |GROUP BY business,HOP(rowtime, INTERVAL &#x27;5&#x27; second, INTERVAL &#x27;10&#x27; second)</span></span><br><span class="line"><span class="string">        &quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line"><span class="keyword">val</span> res1 = tEnv.sqlQuery(query)</span><br><span class="line"></span><br><span class="line">tEnv.sqlUpdate(createKafkaSinkTable)</span><br><span class="line"></span><br><span class="line">tEnv.sqlUpdate(</span><br><span class="line">    <span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    |INSERT INTO demo1</span></span><br><span class="line"><span class="string">    |SELECT *</span></span><br><span class="line"><span class="string">    |FROM $res1</span></span><br><span class="line"><span class="string">    |&quot;</span><span class="string">&quot;&quot;</span>.stripMargin</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink开源贡献步骤整理</title>
    <url>/2020/06/05/Flink%E5%BC%80%E6%BA%90%E8%B4%A1%E7%8C%AE%E6%AD%A5%E9%AA%A4%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>整理下步骤,以防手忙脚乱</p>
</blockquote>
<span id="more"></span>

<h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.发现问题</span><br><span class="line">    毋庸置疑,得发现问题才能进一步去解决它</span><br><span class="line">b.整理问题说明,发布JIRA</span><br><span class="line">    百度翻译</span><br><span class="line">c.寻找问题所在</span><br><span class="line">    对应着源码模块进行筛查</span><br><span class="line">d.判断问题难度(是否自己可以解决)</span><br><span class="line">    如果自己解决不了,就@大佬解决</span><br><span class="line">e.解决问题</span><br><span class="line">    更新本地源码</span><br><span class="line">    新建JIAR分支(类似FLINK-N,对应自己issue的id)</span><br><span class="line">    解决问题</span><br><span class="line">    commit提交</span><br><span class="line">    提交pr</span><br><span class="line">f.坐等审核</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="更新本地源码项目"><a href="#更新本地源码项目" class="headerlink" title="更新本地源码项目"></a>更新本地源码项目</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br><span class="line">git remote add update  https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;flink.git</span><br><span class="line">git fetch update</span><br><span class="line">git merge update&#x2F;master</span><br><span class="line">git push origin master</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink操作ES</title>
    <url>/2019/07/02/Flink%E6%93%8D%E4%BD%9CES/</url>
    <content><![CDATA[<blockquote>
<p>利用了Flink的Sink功能,将对ES的操作封装在自定义ElasticsearchSinkFunction类中.</p>
</blockquote>
<span id="more"></span>

<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="主实现"><a href="#主实现" class="headerlink" title="主实现"></a>主实现</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> addressList = <span class="keyword">new</span> java.util.<span class="type">ArrayList</span>[<span class="type">HttpHost</span>]()</span><br><span class="line">addressList.add(<span class="keyword">new</span> <span class="type">HttpHost</span>(<span class="type">ES_NAME</span>, <span class="type">ES_PORT</span>))</span><br><span class="line"><span class="keyword">val</span> hbaseDs = kafkaStream.map(x =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> result = <span class="keyword">new</span> <span class="type">JSONObject</span>(x)</span><br><span class="line">  (result.getString(<span class="string">&quot;value&quot;</span>), <span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line">hbaseDs.addSink(<span class="keyword">new</span> <span class="type">ElasticsearchSink</span>.<span class="type">Builder</span>[(<span class="type">String</span>, <span class="type">Int</span>)](addressList, <span class="keyword">new</span> <span class="type">TestElasticsearchSinkFunction</span>).build())</span><br></pre></td></tr></table></figure>
<h3 id="自定义ElasticsearchSinkFunction类"><a href="#自定义ElasticsearchSinkFunction类" class="headerlink" title="自定义ElasticsearchSinkFunction类"></a>自定义ElasticsearchSinkFunction类</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.dev.flink.stream.es.entry</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.dev.flink.stream.es.develop.&#123;<span class="type">ES_INDEX</span>, <span class="type">ES_TYPE</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.<span class="type">RuntimeContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.elasticsearch.&#123;<span class="type">ElasticsearchSinkFunction</span>, <span class="type">RequestIndexer</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.action.update.<span class="type">UpdateRequest</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.common.xcontent.json.<span class="type">JsonXContent</span></span><br><span class="line"><span class="keyword">import</span> org.elasticsearch.script.<span class="type">Script</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestElasticsearchSinkFunction</span> <span class="keyword">extends</span> <span class="title">ElasticsearchSinkFunction</span>[(<span class="type">String</span>, <span class="type">Int</span>)] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(data: (<span class="type">String</span>, <span class="type">Int</span>), runtimeContext: <span class="type">RuntimeContext</span>, requestIndexer: <span class="type">RequestIndexer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> id = data._1</span><br><span class="line">    <span class="keyword">val</span> content = <span class="type">JsonXContent</span>.contentBuilder().startObject()</span><br><span class="line">      .field(<span class="string">&quot;id&quot;</span>, id)</span><br><span class="line">      .field(<span class="string">&quot;word&quot;</span>, data._1)</span><br><span class="line">      .field(<span class="string">&quot;count&quot;</span>, data._2)</span><br><span class="line">      .endObject()</span><br><span class="line">    <span class="comment">//    val indexRequest = new IndexRequest().index(</span></span><br><span class="line">    <span class="comment">//      ES_INDEX</span></span><br><span class="line">    <span class="comment">//    ).`type`(</span></span><br><span class="line">    <span class="comment">//      ES_TYPE</span></span><br><span class="line">    <span class="comment">//    ).id(id).source(content)</span></span><br><span class="line">    <span class="comment">//    requestIndexer.add(indexRequest)</span></span><br><span class="line">    <span class="comment">//    val deleteRequest = new DeleteRequest().index(</span></span><br><span class="line">    <span class="comment">//      ES_INDEX</span></span><br><span class="line">    <span class="comment">//    ).`type`(</span></span><br><span class="line">    <span class="comment">//      ES_TYPE</span></span><br><span class="line">    <span class="comment">//    ).id(id)</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    requestIndexer.add(deleteRequest)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> updateRequest1 = <span class="keyword">new</span> <span class="type">UpdateRequest</span>().index(</span><br><span class="line">      <span class="type">ES_INDEX</span></span><br><span class="line">    ).`<span class="class"><span class="keyword">type</span>`(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">      <span class="type">ES_TYPE</span></span></span></span><br><span class="line"><span class="class"><span class="params">    </span>).<span class="title">id</span>(<span class="params">id</span>)</span></span><br><span class="line"><span class="class">    .<span class="title">docAsUpsert</span>(<span class="params">true</span>).<span class="title">doc</span>(<span class="params">content</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">updateRequest</span> </span>= <span class="keyword">new</span> <span class="type">UpdateRequest</span>().index(</span><br><span class="line">      <span class="type">ES_INDEX</span></span><br><span class="line">    ).`<span class="class"><span class="keyword">type</span>`(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">      <span class="type">ES_TYPE</span></span></span></span><br><span class="line"><span class="class"><span class="params">    </span>).<span class="title">id</span>(<span class="params">id</span>)</span></span><br><span class="line"><span class="class">      .<span class="title">script</span>(<span class="params">new <span class="type">Script</span>(&quot;ctx._source.remove(\&quot;word\&quot;</span>)&quot;)).<span class="title">scriptedUpsert</span>(<span class="params">true</span>)</span></span><br><span class="line"><span class="class">      <span class="comment">//.docAsUpsert(true).doc(content)</span></span></span><br><span class="line"><span class="class">    <span class="comment">// doc对存在的数据进行修改,upsert对不存在的数据进行添加</span></span></span><br><span class="line"><span class="class">    <span class="title">requestIndexer</span>.<span class="title">add</span>(<span class="params">updateRequest1</span>)</span></span><br><span class="line"><span class="class">    <span class="title">requestIndexer</span>.<span class="title">add</span>(<span class="params">updateRequest</span>)</span></span><br><span class="line"><span class="class">  &#125;</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可以看到,对于ES的增删改查都在自定义ElasticsearchSinkFunction类中实现,支持IndexRequest,DeleteRequest,UpdateRequest,GetRequest</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink流表系列之HBase输出表</title>
    <url>/2020/01/06/Flink%E6%B5%81%E8%A1%A8%E7%B3%BB%E5%88%97%E4%B9%8BHBase%E8%BE%93%E5%87%BA%E8%A1%A8/</url>
    <content><![CDATA[<blockquote>
<p>将流表直接插入HBase表中,Source其实也有但是没有找到应用场景</p>
</blockquote>
<span id="more"></span>

<h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.addons.hbase.&#123;<span class="type">HBaseOptions</span>, <span class="type">HBaseTableFactory</span>, <span class="type">HBaseTableSchema</span>, <span class="type">HBaseTableSource</span>, <span class="type">HBaseUpsertTableSink</span>, <span class="type">HBaseWriteOptions</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.&#123;<span class="type">TypeInformation</span>, <span class="type">Types</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">DataTypes</span>, <span class="type">EnvironmentSettings</span>, <span class="type">TableSchema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.<span class="type">DescriptorProperties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.<span class="type">Schema</span>.<span class="type">SCHEMA</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.factories.<span class="type">TableFactoryService</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-06 10:37</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseSinkExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> bsEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line">    <span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(bsEnv, bsSettings)</span><br><span class="line">    <span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">HBaseTableSchema</span>()</span><br><span class="line">    schema.setRowKey(<span class="string">&quot;test&quot;</span>, classOf[<span class="type">String</span>])</span><br><span class="line">    schema.addColumn(<span class="string">&quot;info&quot;</span>, <span class="string">&quot;name&quot;</span>, classOf[<span class="type">String</span>])</span><br><span class="line">    schema.addColumn(<span class="string">&quot;info&quot;</span>, <span class="string">&quot;age&quot;</span>, classOf[<span class="type">String</span>])</span><br><span class="line">    <span class="keyword">val</span> options = <span class="type">HBaseOptions</span>.builder</span><br><span class="line">      .setTableName(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">      .setZkQuorum(<span class="string">&quot;hadoop01:2181&quot;</span>)</span><br><span class="line">      .setZkNodeParent(<span class="string">&quot;/hbase&quot;</span>)</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> writeOptions = <span class="type">HBaseWriteOptions</span>.builder()</span><br><span class="line">      .setBufferFlushIntervalMillis(<span class="number">1000</span>)</span><br><span class="line">      .setBufferFlushMaxRows(<span class="number">1000</span>)</span><br><span class="line">      .setBufferFlushMaxSizeInBytes(<span class="number">10</span> * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line">      .build()</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="keyword">new</span> <span class="type">HBaseUpsertTableSink</span>(schema, options, writeOptions)</span><br><span class="line">    tableEnv.registerTableSink(<span class="string">&quot;hbaseTable&quot;</span>, sink)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ds = bsEnv.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line">    <span class="keyword">val</span> source = ds.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; &#123;</span><br><span class="line">      <span class="type">Source</span>(x, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">    tableEnv.registerDataStream(<span class="string">&quot;demoTable&quot;</span>, source, <span class="symbol">&#x27;user</span>, <span class="symbol">&#x27;result</span>, <span class="symbol">&#x27;age</span>, <span class="symbol">&#x27;proctime</span>.proctime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">&quot;insert into hbaseTable select user, ROW(`result`,age) from demoTable&quot;</span></span><br><span class="line">    </span><br><span class="line">    tableEnv.sqlUpdate(sql)</span><br><span class="line">    tableEnv.execute(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Source</span>(<span class="params">user: <span class="type">String</span>, result: <span class="type">String</span>, age: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TableFactory"><a href="#TableFactory" class="headerlink" title="TableFactory"></a>TableFactory</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">HashMap</span>, <span class="type">Map</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.addons.hbase.&#123;<span class="type">HBaseOptions</span>, <span class="type">HBaseTableFactory</span>, <span class="type">HBaseTableSchema</span>, <span class="type">HBaseTableSource</span>, <span class="type">HBaseUpsertTableSink</span>, <span class="type">HBaseWriteOptions</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.&#123;<span class="type">TypeInformation</span>, <span class="type">Types</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">DataTypes</span>, <span class="type">EnvironmentSettings</span>, <span class="type">TableSchema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.<span class="type">DescriptorProperties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.<span class="type">Schema</span>.<span class="type">SCHEMA</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.factories.<span class="type">TableFactoryService</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-06 10:37</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseSinkExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> bsEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line">    <span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(bsEnv, bsSettings)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> columnNames = <span class="type">Array</span>(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;info&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> f1 = <span class="type">Types</span>.<span class="type">ROW_NAMED</span>(<span class="type">Array</span>[<span class="type">String</span>](<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>), <span class="type">Types</span>.<span class="type">STRING</span>, <span class="type">Types</span>.<span class="type">STRING</span>)</span><br><span class="line">    <span class="keyword">val</span> columnTypes = <span class="type">Array</span>[<span class="type">TypeInformation</span>[_]](<span class="type">Types</span>.<span class="type">STRING</span>, f1)</span><br><span class="line">    <span class="keyword">val</span> tableSchema = <span class="keyword">new</span> <span class="type">TableSchema</span>(columnNames, columnTypes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tableProperties = <span class="keyword">new</span> util.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]</span><br><span class="line">    <span class="comment">// 必须制定connector类型</span></span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.type&quot;</span>, <span class="string">&quot;hbase&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.version&quot;</span>, <span class="string">&quot;1.4.3&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.property-version&quot;</span>, <span class="string">&quot;1&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.table-name&quot;</span>, <span class="string">&quot;user&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop01:2181&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.zookeeper.znode.parent&quot;</span>, <span class="string">&quot;/hbase&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.write.buffer-flush.max-size&quot;</span>, <span class="string">&quot;10mb&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.write.buffer-flush.max-rows&quot;</span>, <span class="string">&quot;1000&quot;</span>)</span><br><span class="line">    tableProperties.put(<span class="string">&quot;connector.write.buffer-flush.interval&quot;</span>, <span class="string">&quot;10s&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> descriptorProperties = <span class="keyword">new</span> <span class="type">DescriptorProperties</span>(<span class="literal">true</span>)</span><br><span class="line">    descriptorProperties.putTableSchema(<span class="type">SCHEMA</span>, tableSchema)</span><br><span class="line">    descriptorProperties.putProperties(tableProperties)</span><br><span class="line">    <span class="keyword">val</span> sink = <span class="type">TableFactoryService</span>.find(classOf[<span class="type">HBaseTableFactory</span>], descriptorProperties.asMap).createTableSink(descriptorProperties.asMap)</span><br><span class="line">    tableEnv.registerTableSink(<span class="string">&quot;hbaseTable&quot;</span>, sink)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ds = bsEnv.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line">    <span class="keyword">val</span> source = ds.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; &#123;</span><br><span class="line">      <span class="type">Source</span>(x, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    tableEnv.registerDataStream(<span class="string">&quot;demoTable&quot;</span>, source, <span class="symbol">&#x27;user</span>, <span class="symbol">&#x27;result</span>, <span class="symbol">&#x27;age</span>, <span class="symbol">&#x27;proctime</span>.proctime)</span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">&quot;insert into hbaseTable select user, ROW(`result`,age) from demoTable&quot;</span></span><br><span class="line">    tableEnv.sqlUpdate(sql)</span><br><span class="line">    tableEnv.execute(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Source</span>(<span class="params">user: <span class="type">String</span>, result: <span class="type">String</span>, age: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink流表系列之Kafka输入输出表</title>
    <url>/2020/01/06/Flink%E6%B5%81%E8%A1%A8%E7%B3%BB%E5%88%97%E4%B9%8BKafka%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E8%A1%A8/</url>
    <content><![CDATA[<blockquote>
<p>记录将kafka注册成流表,进行数据的写入写出</p>
</blockquote>
<span id="more"></span>

<h2 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.&#123;<span class="type">TypeInformation</span>, <span class="type">Types</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.<span class="type">EnvironmentSettings</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.&#123;<span class="type">Json</span>, <span class="type">Kafka</span>, <span class="type">Schema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.types.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2019-12-12 12:38</span></span><br><span class="line"><span class="comment"> * @Description: 将kafkaSource注册成一张表</span></span><br><span class="line"><span class="comment"> * &#123;&quot;topic&quot;:&quot;test&quot;,&quot;partition&quot;:3,&quot;offset&quot;:1,&quot;payload&quot;:[&#123;&quot;col1&quot;:1,&quot;col2&quot;:&quot;2&quot;&#125;,&#123;&quot;col1&quot;:3,&quot;col2&quot;:&quot;4&quot;&#125;]&#125;</span></span><br><span class="line"><span class="comment"> * [1,2, 3,4]</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaSourceExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> map = <span class="type">Map</span>(<span class="string">&quot;payload&quot;</span> -&gt; <span class="type">Types</span>.<span class="type">OBJECT_ARRAY</span>(<span class="type">Types</span>.<span class="type">ROW_NAMED</span>(<span class="type">Array</span>(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>), <span class="type">Types</span>.<span class="type">INT</span>, <span class="type">Types</span>.<span class="type">STRING</span>)))</span><br><span class="line">    <span class="keyword">val</span> bsEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line">    <span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(bsEnv, bsSettings)</span><br><span class="line">    <span class="keyword">val</span> kafka = <span class="keyword">new</span> <span class="type">Kafka</span>()</span><br><span class="line">      .version(<span class="string">&quot;0.10&quot;</span>)</span><br><span class="line">      .topic(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">      .property(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop03:9092&quot;</span>)</span><br><span class="line">      <span class="comment">//      .startFromEarliest()</span></span><br><span class="line">      .startFromLatest()</span><br><span class="line"></span><br><span class="line">    tableEnv.connect(kafka)</span><br><span class="line">      .withFormat(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Json</span>().failOnMissingField(<span class="literal">true</span>).deriveSchema()</span><br><span class="line">      )</span><br><span class="line">      .withSchema(</span><br><span class="line">        registerSchema(map)</span><br><span class="line">      )</span><br><span class="line">      .inAppendMode()</span><br><span class="line">      .registerTableSource(<span class="string">&quot;test&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">&quot;select * from test&quot;</span></span><br><span class="line">    <span class="keyword">val</span> table = tableEnv.sqlQuery(sql)</span><br><span class="line"></span><br><span class="line">    table.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> value = tableEnv.toAppendStream[<span class="type">Row</span>](table)</span><br><span class="line">    value.print()</span><br><span class="line">    bsEnv.execute(<span class="string">&quot;Flink Demo&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">registerSchema</span></span>(map: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">TypeInformation</span>[_]]): <span class="type">Schema</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> schema = <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">    map.map(x =&gt; &#123;</span><br><span class="line">      schema.field(x._1, x._2)</span><br><span class="line">    &#125;)</span><br><span class="line">    schema</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.typeinfo.<span class="type">Types</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">DataStream</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.&#123;<span class="type">DataTypes</span>, <span class="type">EnvironmentSettings</span>, <span class="type">TableSchema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala.<span class="type">StreamTableEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.descriptors.&#123;<span class="type">Json</span>, <span class="type">Kafka</span>, <span class="type">Schema</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-06 10:53</span></span><br><span class="line"><span class="comment"> * @Description: 将流表写入到kafka中,JSON格式</span></span><br><span class="line"><span class="comment"> * &#123;&quot;user&quot;:2,&quot;result&quot;:&quot;test&quot;&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaSinkExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> bsEnv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    <span class="keyword">val</span> bsSettings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line">    <span class="keyword">val</span> tableEnv = <span class="type">StreamTableEnvironment</span>.create(bsEnv, bsSettings)</span><br><span class="line">    <span class="keyword">val</span> topic = <span class="string">&quot;test&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ds = bsEnv.socketTextStream(<span class="string">&quot;hadoop01&quot;</span>, <span class="number">9999</span>, &#x27;\n&#x27;)</span><br><span class="line">    <span class="keyword">val</span> source = ds.flatMap(_.split(<span class="string">&quot; &quot;</span>)).map(x =&gt; &#123;</span><br><span class="line">      <span class="type">Source</span>(x.toInt, <span class="string">&quot;test&quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    tableEnv</span><br><span class="line">      .connect(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Kafka</span>()</span><br><span class="line">          .version(<span class="string">&quot;0.10&quot;</span>)</span><br><span class="line">          .topic(topic)</span><br><span class="line">          .property(<span class="string">&quot;zookeeper.connect&quot;</span>, <span class="string">&quot;hadoop03:2181&quot;</span>)</span><br><span class="line">          .property(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;hadoop03:9092&quot;</span>))</span><br><span class="line">      .withFormat(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Json</span>().deriveSchema)</span><br><span class="line">      .withSchema(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">Schema</span>()</span><br><span class="line">          .field(<span class="string">&quot;user&quot;</span>, <span class="type">Types</span>.<span class="type">INT</span>)</span><br><span class="line">          .field(<span class="string">&quot;result&quot;</span>, <span class="type">Types</span>.<span class="type">STRING</span>)</span><br><span class="line">      )</span><br><span class="line">      .inAppendMode</span><br><span class="line">      .registerTableSink(topic)</span><br><span class="line"></span><br><span class="line">    tableEnv.registerDataStream(<span class="string">&quot;demoTable&quot;</span>, source, <span class="symbol">&#x27;user</span>, <span class="symbol">&#x27;result</span>, <span class="symbol">&#x27;proctime</span>.proctime)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sql = <span class="string">&quot;insert into &quot;</span> + topic + <span class="string">&quot; select user, `result` from demoTable&quot;</span></span><br><span class="line"></span><br><span class="line">    tableEnv.sqlUpdate(sql)</span><br><span class="line">    tableEnv.execute(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Source</span>(<span class="params">user: <span class="type">Int</span>, result: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink消费多Topic时消费不均匀问题</title>
    <url>/2020/12/21/Flink%E6%B6%88%E8%B4%B9%E5%A4%9ATopic%E6%97%B6%E6%B6%88%E8%B4%B9%E4%B8%8D%E5%9D%87%E5%8C%80%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>今天社区有小伙伴提出Flink在消费Kafka多Topic数据时,并行度合理,但还是存在消费不均匀的情况</p>
</blockquote>
<span id="more"></span>

<h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">FlinkKafkaConsumerBase.open()</span><br><span class="line">    AbstractPartitionDiscoverer.discoverPartitions()</span><br><span class="line">        --setAndCheckDiscoveredPartition()</span><br><span class="line">        KafkaTopicPartitionAssigner.assign()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 测试发现,对test[0-4]这5个topic,每个topic都12个分区,最终分配的结果值并不是均匀的</span></span><br><span class="line">    <span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line">    <span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 直接自实现该方案,暴力轮询分配</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之三JobGraph提交</title>
    <url>/2020/05/28/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E4%B8%89JobGraph%E6%8F%90%E4%BA%A4/</url>
    <content><![CDATA[<blockquote>
<p>在Client端生成的两个Graph都已经生成完毕,这时候需要实现客户端到服务端的一个过渡</p>
</blockquote>
<span id="more"></span>

<h2 id="前提介绍"><a href="#前提介绍" class="headerlink" title="前提介绍"></a>前提介绍</h2><h3 id="JobClient"><a href="#JobClient" class="headerlink" title="JobClient"></a>JobClient</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JobClient(接口)</span><br><span class="line">    ClusterClientJobClientAdapter</span><br><span class="line">        AbstractJobClusterExecutor</span><br><span class="line">        ClusterClientJobClientAdapter</span><br><span class="line">        RemoteExecutor</span><br><span class="line">    PerJobMiniClusterJobClient</span><br><span class="line">        LocalExecutor</span><br><span class="line">    </span><br><span class="line">getJobID</span><br><span class="line">getJobStatus</span><br><span class="line">cancel</span><br><span class="line">stopWithSavepoint 待savePoint停止任务</span><br><span class="line">triggerSavepoint 触发savePoint</span><br><span class="line">getAccumulators 获取作业累加器</span><br><span class="line">getJobExecutionResult 获取作业结果</span><br></pre></td></tr></table></figure>
<h3 id="JobManagers"><a href="#JobManagers" class="headerlink" title="JobManagers"></a>JobManagers</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在服务端,也称Master</span><br><span class="line">    协调分布式计算,负责调度任务,协调CK,协调故障恢复等</span><br><span class="line">    每一个Job至少有一个JobManager,高可用部署下会有多个JobManagers,其中一个作为leader,其余处于standby状态</span><br></pre></td></tr></table></figure>
<h3 id="TaskManagers"><a href="#TaskManagers" class="headerlink" title="TaskManagers"></a>TaskManagers</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在服务端,也称Worker</span><br><span class="line">    执行dataflow中的tasks(subtasks),并且缓存和交换数据streams</span><br><span class="line">    每一个Job至少会有一个TaskManager</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="提交过程"><a href="#提交过程" class="headerlink" title="提交过程"></a>提交过程</h2><h3 id="入口"><a href="#入口" class="headerlink" title="入口"></a>入口</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">此处仅分析AbstractSessionClusterExecutor</span><br><span class="line">AbstractSessionClusterExecutor.execute()进行JobGraph生成,并获取Client进行提交</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;JobClient&gt; <span class="title">execute</span><span class="params">(<span class="meta">@Nonnull</span> <span class="keyword">final</span> Pipeline pipeline, <span class="meta">@Nonnull</span> <span class="keyword">final</span> Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 生成JobGraph</span></span><br><span class="line">    <span class="keyword">final</span> JobGraph jobGraph = ExecutorUtils.getJobGraph(pipeline, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成Cluster描述</span></span><br><span class="line">    <span class="keyword">try</span> (<span class="keyword">final</span> ClusterDescriptor&lt;ClusterID&gt; clusterDescriptor = clusterClientFactory.createClusterDescriptor(configuration)) &#123;</span><br><span class="line">        <span class="comment">// 获取ClusterID</span></span><br><span class="line">        <span class="keyword">final</span> ClusterID clusterID = clusterClientFactory.getClusterId(configuration);</span><br><span class="line">        checkState(clusterID != <span class="keyword">null</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 根据ClusterID获取ClusterClient</span></span><br><span class="line">        <span class="keyword">final</span> ClusterClientProvider&lt;ClusterID&gt; clusterClientProvider = clusterDescriptor.retrieve(clusterID);</span><br><span class="line">        </span><br><span class="line">        ClusterClient&lt;ClusterID&gt; clusterClient = clusterClientProvider.getClusterClient();</span><br><span class="line">        <span class="keyword">return</span> clusterClient</span><br><span class="line">                <span class="comment">// 提交任务,此处submitJob对应两种实现</span></span><br><span class="line">                <span class="comment">// MiniClusterClient</span></span><br><span class="line">                <span class="comment">// RestClusterClient</span></span><br><span class="line">                .submitJob(jobGraph)</span><br><span class="line">                .thenApplyAsync(jobID -&gt; (JobClient) <span class="keyword">new</span> ClusterClientJobClientAdapter&lt;&gt;(</span><br><span class="line">                        clusterClientProvider,</span><br><span class="line">                        jobID))</span><br><span class="line">                <span class="comment">// 运行完成</span></span><br><span class="line">                .whenComplete((ignored1, ignored2) -&gt; clusterClient.close());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="MiniClusterClient"><a href="#MiniClusterClient" class="headerlink" title="MiniClusterClient"></a>MiniClusterClient</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">本地微型集群</span><br><span class="line">根据JobGraph的信息,提取jar,生成JobFile</span><br><span class="line">并通过BlobClient上传到Cluster</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;JobID&gt; <span class="title">submitJob</span><span class="params">(<span class="meta">@Nonnull</span> JobGraph jobGraph)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> miniCluster.submitJob(jobGraph).thenApply(JobSubmissionResult::getJobID);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MiniCluster</span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;JobSubmissionResult&gt; <span class="title">submitJob</span><span class="params">(JobGraph jobGraph)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;DispatcherGateway&gt; dispatcherGatewayFuture = getDispatcherGatewayFuture();</span><br><span class="line">    <span class="comment">// 获取Cluster地址信息</span></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;InetSocketAddress&gt; blobServerAddressFuture = createBlobServerAddress(dispatcherGatewayFuture);</span><br><span class="line">    <span class="comment">// 提交jar并设置JobFile</span></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;Void&gt; jarUploadFuture = uploadAndSetJobFiles(blobServerAddressFuture, jobGraph);</span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;Acknowledge&gt; acknowledgeCompletableFuture = jarUploadFuture</span><br><span class="line">        .thenCombine(</span><br><span class="line">            dispatcherGatewayFuture,</span><br><span class="line">            <span class="comment">// 最后交由Dispatcher类进行提交JobGraph</span></span><br><span class="line">            (Void ack, DispatcherGateway dispatcherGateway) -&gt; dispatcherGateway.submitJob(jobGraph, rpcTimeout))</span><br><span class="line">        .thenCompose(Function.identity());</span><br><span class="line">    <span class="keyword">return</span> acknowledgeCompletableFuture.thenApply(</span><br><span class="line">        (Acknowledge ignored) -&gt; <span class="keyword">new</span> JobSubmissionResult(jobGraph.getJobID()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 提交JobFile信息</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;Void&gt; <span class="title">uploadAndSetJobFiles</span><span class="params">(<span class="keyword">final</span> CompletableFuture&lt;InetSocketAddress&gt; blobServerAddressFuture, <span class="keyword">final</span> JobGraph job)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> blobServerAddressFuture.thenAccept(blobServerAddress -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            ClientUtils.extractAndUploadJobGraphFiles(job, () -&gt; <span class="keyword">new</span> BlobClient(blobServerAddress, miniClusterConfiguration.getConfiguration()));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (FlinkException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Dispatcher</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;Acknowledge&gt; <span class="title">submitJob</span><span class="params">(JobGraph jobGraph, Time timeout)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">&quot;Received JobGraph submission &#123;&#125; (&#123;&#125;).&quot;</span>, jobGraph.getJobID(), jobGraph.getName());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (isDuplicateJob(jobGraph.getJobID())) &#123;</span><br><span class="line">            <span class="keyword">return</span> FutureUtils.completedExceptionally(</span><br><span class="line">                <span class="keyword">new</span> DuplicateJobSubmissionException(jobGraph.getJobID()));</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (isPartialResourceConfigured(jobGraph)) &#123;</span><br><span class="line">            <span class="keyword">return</span> FutureUtils.completedExceptionally(</span><br><span class="line">                <span class="keyword">new</span> JobSubmissionException(jobGraph.getJobID(), <span class="string">&quot;Currently jobs is not supported if parts of the vertices have &quot;</span> +</span><br><span class="line">                        <span class="string">&quot;resources configured. The limitation will be removed in future versions.&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 提交JobGraph</span></span><br><span class="line">            <span class="keyword">return</span> internalSubmitJob(jobGraph);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (FlinkException e) &#123;</span><br><span class="line">        <span class="keyword">return</span> FutureUtils.completedExceptionally(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;Acknowledge&gt; <span class="title">internalSubmitJob</span><span class="params">(JobGraph jobGraph)</span> </span>&#123;</span><br><span class="line">    log.info(<span class="string">&quot;Submitting job &#123;&#125; (&#123;&#125;).&quot;</span>, jobGraph.getJobID(), jobGraph.getName());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 完成提交,并启动Job</span></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;Acknowledge&gt; persistAndRunFuture = waitForTerminatingJobManager(jobGraph.getJobID(), jobGraph, <span class="keyword">this</span>::persistAndRunJob)</span><br><span class="line">        .thenApply(ignored -&gt; Acknowledge.get());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> persistAndRunFuture.handleAsync((acknowledge, throwable) -&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (throwable != <span class="keyword">null</span>) &#123;</span><br><span class="line">            cleanUpJobData(jobGraph.getJobID(), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">final</span> Throwable strippedThrowable = ExceptionUtils.stripCompletionException(throwable);</span><br><span class="line">            log.error(<span class="string">&quot;Failed to submit job &#123;&#125;.&quot;</span>, jobGraph.getJobID(), strippedThrowable);</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(</span><br><span class="line">                <span class="keyword">new</span> JobSubmissionException(jobGraph.getJobID(), <span class="string">&quot;Failed to submit job.&quot;</span>, strippedThrowable));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> acknowledge;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, getRpcService().getExecutor());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;Void&gt; <span class="title">persistAndRunJob</span><span class="params">(JobGraph jobGraph)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    jobGraphWriter.putJobGraph(jobGraph);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动Job</span></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;Void&gt; runJobFuture = runJob(jobGraph);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> runJobFuture.whenComplete(BiConsumerWithException.unchecked((Object ignored, Throwable throwable) -&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (throwable != <span class="keyword">null</span>) &#123;</span><br><span class="line">            jobGraphWriter.removeJobGraph(jobGraph.getJobID());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;Void&gt; <span class="title">runJob</span><span class="params">(JobGraph jobGraph)</span> </span>&#123;</span><br><span class="line">    Preconditions.checkState(!jobManagerRunnerFutures.containsKey(jobGraph.getJobID()));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建JobManagerRunner</span></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;JobManagerRunner&gt; jobManagerRunnerFuture = createJobManagerRunner(jobGraph);</span><br><span class="line"></span><br><span class="line">    jobManagerRunnerFutures.put(jobGraph.getJobID(), jobManagerRunnerFuture);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动JobManagerRunner</span></span><br><span class="line">    <span class="keyword">return</span> jobManagerRunnerFuture</span><br><span class="line">        .thenApply(FunctionUtils.uncheckedFunction(<span class="keyword">this</span>::startJobManagerRunner))</span><br><span class="line">        .thenApply(FunctionUtils.nullFn())</span><br><span class="line">        .whenCompleteAsync(</span><br><span class="line">            (ignored, throwable) -&gt; &#123;</span><br><span class="line">                <span class="keyword">if</span> (throwable != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    jobManagerRunnerFutures.remove(jobGraph.getJobID());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            getMainThreadExecutor());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> CompletableFuture&lt;JobManagerRunner&gt; <span class="title">createJobManagerRunner</span><span class="params">(JobGraph jobGraph)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// RPC通信服务</span></span><br><span class="line">    <span class="keyword">final</span> RpcService rpcService = getRpcService();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 调用JobManagerRunnerFactory创建JobManagerRunnerImpl</span></span><br><span class="line">    <span class="keyword">return</span> CompletableFuture.supplyAsync(</span><br><span class="line">        CheckedSupplier.unchecked(() -&gt;</span><br><span class="line">            jobManagerRunnerFactory.createJobManagerRunner(</span><br><span class="line">                jobGraph,</span><br><span class="line">                configuration,</span><br><span class="line">                rpcService,</span><br><span class="line">                highAvailabilityServices,</span><br><span class="line">                heartbeatServices,</span><br><span class="line">                jobManagerSharedServices,</span><br><span class="line">                <span class="keyword">new</span> DefaultJobManagerJobMetricGroupFactory(jobManagerMetricGroup),</span><br><span class="line">                fatalErrorHandler)),</span><br><span class="line">        rpcService.getExecutor());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动JobManagerRunner</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> JobManagerRunner <span class="title">startJobManagerRunner</span><span class="params">(JobManagerRunner jobManagerRunner)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> JobID jobId = jobManagerRunner.getJobID();</span><br><span class="line"></span><br><span class="line">    FutureUtils.assertNoException(</span><br><span class="line">        jobManagerRunner.getResultFuture().handleAsync(</span><br><span class="line">            (ArchivedExecutionGraph archivedExecutionGraph, Throwable throwable) -&gt; &#123;</span><br><span class="line">                <span class="comment">// check if we are still the active JobManagerRunner by checking the identity</span></span><br><span class="line">                <span class="keyword">final</span> JobManagerRunner currentJobManagerRunner = Optional.ofNullable(jobManagerRunnerFutures.get(jobId))</span><br><span class="line">                    .map(future -&gt; future.getNow(<span class="keyword">null</span>))</span><br><span class="line">                    .orElse(<span class="keyword">null</span>);</span><br><span class="line">                <span class="comment">//noinspection ObjectEquality</span></span><br><span class="line">                <span class="keyword">if</span> (jobManagerRunner == currentJobManagerRunner) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (archivedExecutionGraph != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        jobReachedGloballyTerminalState(archivedExecutionGraph);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">final</span> Throwable strippedThrowable = ExceptionUtils.stripCompletionException(throwable);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> (strippedThrowable <span class="keyword">instanceof</span> JobNotFinishedException) &#123;</span><br><span class="line">                            jobNotFinished(jobId);</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            jobMasterFailed(jobId, strippedThrowable);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    log.debug(<span class="string">&quot;There is a newer JobManagerRunner for the job &#123;&#125;.&quot;</span>, jobId);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;, getMainThreadExecutor()));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动,使用选举器去启动JobManagerRunner</span></span><br><span class="line">    jobManagerRunner.start();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jobManagerRunner;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="RestClusterClient"><a href="#RestClusterClient" class="headerlink" title="RestClusterClient"></a>RestClusterClient</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">HTTP REST请求通信</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;JobID&gt; <span class="title">submitJob</span><span class="params">(<span class="meta">@Nonnull</span> JobGraph jobGraph)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// JobGraph落地成JobGraphFile</span></span><br><span class="line">    CompletableFuture&lt;java.nio.file.Path&gt; jobGraphFileFuture = CompletableFuture.supplyAsync(() -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> java.nio.file.Path jobGraphFile = Files.createTempFile(<span class="string">&quot;flink-jobgraph&quot;</span>, <span class="string">&quot;.bin&quot;</span>);</span><br><span class="line">            <span class="keyword">try</span> (ObjectOutputStream objectOut = <span class="keyword">new</span> ObjectOutputStream(Files.newOutputStream(jobGraphFile))) &#123;</span><br><span class="line">                objectOut.writeObject(jobGraph);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> jobGraphFile;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(<span class="keyword">new</span> FlinkException(<span class="string">&quot;Failed to serialize JobGraph.&quot;</span>, e));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, executorService);</span><br><span class="line"></span><br><span class="line">    CompletableFuture&lt;Tuple2&lt;JobSubmitRequestBody, Collection&lt;FileUpload&gt;&gt;&gt; requestFuture = jobGraphFileFuture.thenApply(jobGraphFile -&gt; &#123;</span><br><span class="line">        <span class="comment">// JarFile名称</span></span><br><span class="line">        List&lt;String&gt; jarFileNames = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">8</span>);</span><br><span class="line">        List&lt;JobSubmitRequestBody.DistributedCacheFile&gt; artifactFileNames = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">8</span>);</span><br><span class="line">        <span class="comment">// 需要上传的File集合</span></span><br><span class="line">        Collection&lt;FileUpload&gt; filesToUpload = <span class="keyword">new</span> ArrayList&lt;&gt;(<span class="number">8</span>);</span><br><span class="line"></span><br><span class="line">        filesToUpload.add(<span class="keyword">new</span> FileUpload(jobGraphFile, RestConstants.CONTENT_TYPE_BINARY));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加Jar到FileUpload集合中</span></span><br><span class="line">        <span class="keyword">for</span> (Path jar : jobGraph.getUserJars()) &#123;</span><br><span class="line">            jarFileNames.add(jar.getName());</span><br><span class="line">            filesToUpload.add(<span class="keyword">new</span> FileUpload(Paths.get(jar.toUri()), RestConstants.CONTENT_TYPE_JAR));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加artifacts到FileUpload集合</span></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, DistributedCache.DistributedCacheEntry&gt; artifacts : jobGraph.getUserArtifacts().entrySet()) &#123;</span><br><span class="line">            <span class="keyword">final</span> Path artifactFilePath = <span class="keyword">new</span> Path(artifacts.getValue().filePath);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// Only local artifacts need to be uploaded.</span></span><br><span class="line">                <span class="comment">// 只添加本地的artifacts</span></span><br><span class="line">                <span class="keyword">if</span> (!artifactFilePath.getFileSystem().isDistributedFS()) &#123;</span><br><span class="line">                    artifactFileNames.add(<span class="keyword">new</span> JobSubmitRequestBody.DistributedCacheFile(artifacts.getKey(), artifactFilePath.getName()));</span><br><span class="line">                    filesToUpload.add(<span class="keyword">new</span> FileUpload(Paths.get(artifacts.getValue().filePath), RestConstants.CONTENT_TYPE_BINARY));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(</span><br><span class="line">                    <span class="keyword">new</span> FlinkException(<span class="string">&quot;Failed to get the FileSystem of artifact &quot;</span> + artifactFilePath + <span class="string">&quot;.&quot;</span>, e));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装成requestBody</span></span><br><span class="line">        <span class="keyword">final</span> JobSubmitRequestBody requestBody = <span class="keyword">new</span> JobSubmitRequestBody(</span><br><span class="line">            jobGraphFile.getFileName().toString(),</span><br><span class="line">            jarFileNames,</span><br><span class="line">            artifactFileNames);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 返回(请求主题,FileUpload集合)</span></span><br><span class="line">        <span class="keyword">return</span> Tuple2.of(requestBody, Collections.unmodifiableCollection(filesToUpload));</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> CompletableFuture&lt;JobSubmitResponseBody&gt; submissionFuture = requestFuture.thenCompose(</span><br><span class="line">        <span class="comment">// 发送请求</span></span><br><span class="line">        requestAndFileUploads -&gt; sendRetriableRequest(</span><br><span class="line">            JobSubmitHeaders.getInstance(),</span><br><span class="line">            EmptyMessageParameters.getInstance(),</span><br><span class="line">            requestAndFileUploads.f0,</span><br><span class="line">            requestAndFileUploads.f1,</span><br><span class="line">            isConnectionProblemOrServiceUnavailable())</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除临时文件</span></span><br><span class="line">    submissionFuture</span><br><span class="line">        .thenCombine(jobGraphFileFuture, (ignored, jobGraphFile) -&gt; jobGraphFile)</span><br><span class="line">        .thenAccept(jobGraphFile -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Files.delete(jobGraphFile);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            LOG.warn(<span class="string">&quot;Could not delete temporary file &#123;&#125;.&quot;</span>, jobGraphFile, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回提交结果</span></span><br><span class="line">    <span class="keyword">return</span> submissionFuture</span><br><span class="line">        .thenApply(ignore -&gt; jobGraph.getJobID())</span><br><span class="line">        .exceptionally(</span><br><span class="line">            (Throwable throwable) -&gt; &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(<span class="keyword">new</span> JobSubmissionException(jobGraph.getJobID(), <span class="string">&quot;Failed to submit JobGraph.&quot;</span>, ExceptionUtils.stripCompletionException(throwable)));</span><br><span class="line">            &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送请求</span></span><br><span class="line"><span class="keyword">private</span> &lt;M extends MessageHeaders&lt;R, P, U&gt;, U extends MessageParameters, R extends RequestBody, P extends ResponseBody&gt; CompletableFuture&lt;P&gt;</span><br><span class="line">	sendRetriableRequest(M messageHeaders, U messageParameters, R request, Collection&lt;FileUpload&gt; filesToUpload, Predicate&lt;Throwable&gt; retryPredicate) &#123;</span><br><span class="line">    <span class="keyword">return</span> retry(() -&gt; getWebMonitorBaseUrl().thenCompose(webMonitorBaseUrl -&gt; &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 通过RestClient发送请求</span></span><br><span class="line">            <span class="keyword">return</span> restClient.sendRequest(webMonitorBaseUrl.getHost(), webMonitorBaseUrl.getPort(), messageHeaders, messageParameters, request, filesToUpload);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> CompletionException(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;), retryPredicate);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">可以看到,Flink实际提交JobGraph有两种模式</span><br><span class="line">Mini</span><br><span class="line">    在本地测试运行时是开启了一个BLOB服务端进行对JobGraph信息的接收</span><br><span class="line">    使用BLOBClient进行提交</span><br><span class="line">    提交完之后直接启动该Job</span><br><span class="line">Rest</span><br><span class="line">    而实际部署环境则是通过Rest请求进行提交</span><br><span class="line">    由服务端去响应任务</span><br><span class="line">实际提交的信息则是从JobGraph提取出来的Jars和GraphFiles</span><br><span class="line"></span><br><span class="line">至此Client方面的点已经梳理一遍了,关于对Job的取消,Job状态的获取</span><br><span class="line">可以详细阅读RestClusterClient,同样是发送Rest请求</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之六物理执行图的生成</title>
    <url>/2020/06/12/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E5%85%AD%E7%89%A9%E7%90%86%E6%89%A7%E8%A1%8C%E5%9B%BE%E7%9A%84%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<blockquote>
<p>在前面的文章里StreamGraph,JobGraph以及ExecutionGraph的生成逻辑已经整理的差不多<br>接下来就是如何去真正执行<br>了解用户逻辑代码最终如何被Flink执行</p>
</blockquote>
<span id="more"></span>

<h2 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.任务代码生成Transformation</span><br><span class="line">b.StreamGraphGenerator生成StreamGraph</span><br><span class="line">c.PipelineExecutor调用ExecutorUtils转换为JobGraph</span><br><span class="line">d.JobMasterServiceFactory创建JobMaster,调用createScheduler生成ExecutionGraph</span><br><span class="line">e.JobManagerRunner调用start方法,进行选举,选举完毕后回调grantLeadership方法调用startJobMaster开始运行</span><br><span class="line">f.JobMaster调用start方法,确保RPC通信,调用startJobExecution方法</span><br><span class="line">g.resetAndStartScheduler()</span><br><span class="line">    -&gt;startScheduling()</span><br><span class="line">    -&gt;startSchedulingInternal()选取调度器执行</span><br><span class="line">    获取ExecutionGraph并进行调度executionGraph.scheduleForExecution()</span><br><span class="line">h.调用Scheduling.schedule方法进行调度</span><br><span class="line">    根据配置的调度模式选择调度方法</span><br><span class="line">    scheduleLazy</span><br><span class="line">    scheduleEager</span><br><span class="line">i.为每一个ExecutionVertex分配一个LogicalSlot</span><br><span class="line">j.封装为Execution,进行deploy</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Execution"><a href="#Execution" class="headerlink" title="Execution"></a>Execution</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deploy</span><span class="params">()</span> <span class="keyword">throws</span> JobException </span>&#123;</span><br><span class="line">    assertRunningInJobMasterMainThread();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分配的slot</span></span><br><span class="line">    <span class="keyword">final</span> LogicalSlot slot  = assignedResource;</span><br><span class="line"></span><br><span class="line">    checkNotNull(slot, <span class="string">&quot;In order to deploy the execution we first have to assign a resource via tryAssignResource.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Check if the TaskManager died in the meantime</span></span><br><span class="line">    <span class="comment">// This only speeds up the response to TaskManagers failing concurrently to deployments.</span></span><br><span class="line">    <span class="comment">// The more general check is the rpcTimeout of the deployment call</span></span><br><span class="line">    <span class="keyword">if</span> (!slot.isAlive()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> JobException(<span class="string">&quot;Target slot (TaskManager) for deployment is no longer alive.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// make sure exactly one deployment call happens from the correct state</span></span><br><span class="line">    <span class="comment">// note: the transition from CREATED to DEPLOYING is for testing purposes only</span></span><br><span class="line">    ExecutionState previous = <span class="keyword">this</span>.state;</span><br><span class="line">    <span class="keyword">if</span> (previous == SCHEDULED || previous == CREATED) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!transitionState(previous, DEPLOYING)) &#123;</span><br><span class="line">            <span class="comment">// race condition, someone else beat us to the deploying call.</span></span><br><span class="line">            <span class="comment">// this should actually not happen and indicates a race somewhere else</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Cannot deploy task: Concurrent deployment call race.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// vertex may have been cancelled, or it was already scheduled</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;The vertex must be in CREATED or SCHEDULED state to be deployed. Found state &quot;</span> + previous);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span> != slot.getPayload()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(</span><br><span class="line">            String.format(<span class="string">&quot;The execution %s has not been assigned to the assigned slot.&quot;</span>, <span class="keyword">this</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// race double check, did we fail/cancel and do we need to release the slot?</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.state != DEPLOYING) &#123;</span><br><span class="line">            slot.releaseSlot(<span class="keyword">new</span> FlinkException(<span class="string">&quot;Actual state of execution &quot;</span> + <span class="keyword">this</span> + <span class="string">&quot; (&quot;</span> + state + <span class="string">&quot;) does not match expected state DEPLOYING.&quot;</span>));</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (LOG.isInfoEnabled()) &#123;</span><br><span class="line">            LOG.info(String.format(<span class="string">&quot;Deploying %s (attempt #%d) to %s&quot;</span>, vertex.getTaskNameWithSubtaskIndex(),</span><br><span class="line">                    attemptNumber, getAssignedResourceLocation()));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将ExecutionGraph转换为物理执行图</span></span><br><span class="line">        <span class="comment">// ResultPartition的生成</span></span><br><span class="line">        <span class="comment">// InputGate的前身InputGateDeploymentDescriptor生成</span></span><br><span class="line">        <span class="keyword">final</span> TaskDeploymentDescriptor deployment = TaskDeploymentDescriptorFactory</span><br><span class="line">            .fromExecutionVertex(vertex, attemptNumber)</span><br><span class="line">            .createDeploymentDescriptor(</span><br><span class="line">                slot.getAllocationId(),</span><br><span class="line">                slot.getPhysicalSlotNumber(),</span><br><span class="line">                taskRestore,</span><br><span class="line">                producedPartitions.values());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// null taskRestore to let it be GC&#x27;ed</span></span><br><span class="line">        taskRestore = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> ComponentMainThreadExecutor jobMasterMainThreadExecutor =</span><br><span class="line">            vertex.getExecutionGraph().getJobMasterMainThreadExecutor();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// We run the submission in the future executor so that the serialization of large TDDs does not block</span></span><br><span class="line">        <span class="comment">// the main thread and sync back to the main thread once submission is completed.</span></span><br><span class="line">        <span class="comment">// 进行任务提交,submitTask</span></span><br><span class="line">        CompletableFuture.supplyAsync(() -&gt; taskManagerGateway.submitTask(deployment, rpcTimeout), executor)</span><br><span class="line">            .thenCompose(Function.identity())</span><br><span class="line">            .whenCompleteAsync(</span><br><span class="line">                (ack, failure) -&gt; &#123;</span><br><span class="line">                    <span class="comment">// only respond to the failure case</span></span><br><span class="line">                    <span class="keyword">if</span> (failure != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (failure <span class="keyword">instanceof</span> TimeoutException) &#123;</span><br><span class="line">                            String taskname = vertex.getTaskNameWithSubtaskIndex() + <span class="string">&quot; (&quot;</span> + attemptId + <span class="string">&#x27;)&#x27;</span>;</span><br><span class="line"></span><br><span class="line">                            markFailed(<span class="keyword">new</span> Exception(</span><br><span class="line">                                <span class="string">&quot;Cannot deploy task &quot;</span> + taskname + <span class="string">&quot; - TaskManager (&quot;</span> + getAssignedResourceLocation()</span><br><span class="line">                                    + <span class="string">&quot;) not responding after a rpcTimeout of &quot;</span> + rpcTimeout, failure));</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            markFailed(failure);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                jobMasterMainThreadExecutor);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        markFailed(t);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (isLegacyScheduling()) &#123;</span><br><span class="line">            ExceptionUtils.rethrow(t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TaskDeploymentDescriptorFactory"><a href="#TaskDeploymentDescriptorFactory" class="headerlink" title="TaskDeploymentDescriptorFactory"></a>TaskDeploymentDescriptorFactory</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">将IntermediateResultPartition转换为ResultPartition</span><br><span class="line">getConsumedPartitionShuffleDescriptor()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实际Task"><a href="#实际Task" class="headerlink" title="实际Task"></a>实际Task</h2><h3 id="RPCTaskManagerGateway"><a href="#RPCTaskManagerGateway" class="headerlink" title="RPCTaskManagerGateway"></a>RPCTaskManagerGateway</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// submitTask方法将通过RPC的方法提交Task</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;Acknowledge&gt; <span class="title">submitTask</span><span class="params">(TaskDeploymentDescriptor tdd, Time timeout)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> taskExecutorGateway.submitTask(tdd, jobMasterId, timeout);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="TaskExecutor"><a href="#TaskExecutor" class="headerlink" title="TaskExecutor"></a>TaskExecutor</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;Acknowledge&gt; <span class="title">submitTask</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			TaskDeploymentDescriptor tdd,</span></span></span><br><span class="line"><span class="function"><span class="params">			JobMasterId jobMasterId,</span></span></span><br><span class="line"><span class="function"><span class="params">			Time timeout)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 创建Task</span></span><br><span class="line">        <span class="comment">// InputGate,ResultPartition,ResultPartitionWriter</span></span><br><span class="line">        Task task = <span class="keyword">new</span> Task(</span><br><span class="line">            jobInformation,</span><br><span class="line">            taskInformation,</span><br><span class="line">            tdd.getExecutionAttemptId(),</span><br><span class="line">            tdd.getAllocationId(),</span><br><span class="line">            tdd.getSubtaskIndex(),</span><br><span class="line">            tdd.getAttemptNumber(),</span><br><span class="line">            tdd.getProducedPartitions(),</span><br><span class="line">            tdd.getInputGates(),</span><br><span class="line">            tdd.getTargetSlotNumber(),</span><br><span class="line">            memoryManager,</span><br><span class="line">            taskExecutorServices.getIOManager(),</span><br><span class="line">            taskExecutorServices.getShuffleEnvironment(),</span><br><span class="line">            taskExecutorServices.getKvStateService(),</span><br><span class="line">            taskExecutorServices.getBroadcastVariableManager(),</span><br><span class="line">            taskExecutorServices.getTaskEventDispatcher(),</span><br><span class="line">            taskStateManager,</span><br><span class="line">            taskManagerActions,</span><br><span class="line">            inputSplitProvider,</span><br><span class="line">            checkpointResponder,</span><br><span class="line">            aggregateManager,</span><br><span class="line">            blobCacheService,</span><br><span class="line">            libraryCache,</span><br><span class="line">            fileCache,</span><br><span class="line">            taskManagerConfiguration,</span><br><span class="line">            taskMetricGroup,</span><br><span class="line">            resultPartitionConsumableNotifier,</span><br><span class="line">            partitionStateChecker,</span><br><span class="line">            getRpcService().getExecutor());</span><br><span class="line"></span><br><span class="line">        taskMetricGroup.gauge(MetricNames.IS_BACKPRESSURED, task::isBackPressured);</span><br><span class="line"></span><br><span class="line">        log.info(<span class="string">&quot;Received task &#123;&#125;.&quot;</span>, task.getTaskInfo().getTaskNameWithSubtasks());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> taskAdded;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            taskAdded = taskSlotTable.addTask(task);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SlotNotFoundException | SlotNotActiveException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TaskSubmissionException(<span class="string">&quot;Could not submit task.&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (taskAdded) &#123;</span><br><span class="line">            <span class="comment">// 启动Task</span></span><br><span class="line">            task.startTaskThread();</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">    &#125; <span class="keyword">catch</span> (TaskSubmissionException e) &#123;</span><br><span class="line">        <span class="keyword">return</span> FutureUtils.completedExceptionally(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startTaskThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 调用Task.run()方法</span></span><br><span class="line">	executingThread.start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        doRun();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        terminationFuture.complete(executionState);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 太长,省略点</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doRun</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 用户代码加载</span></span><br><span class="line">    invokable = loadAndInstantiateInvokable(userCodeClassLoader, nameOfInvokableClass, env);</span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 运行invokable,实际上最后会去调用AbstractInvokable派生类的init方法以及runThrowing方法</span></span><br><span class="line">    invokable.invoke();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="AbstractInvokable派生类举例"><a href="#AbstractInvokable派生类举例" class="headerlink" title="AbstractInvokable派生类举例"></a>AbstractInvokable派生类举例</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">其包括Flink中的各种Task类</span><br><span class="line">StreamTask</span><br><span class="line">BatchTask</span><br><span class="line">等等</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; StreamTask</span><br><span class="line">private void initializeStateAndOpen() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">    StreamOperator&lt;?&gt;[] allOperators &#x3D; operatorChain.getAllOperators();</span><br><span class="line"></span><br><span class="line">    for (StreamOperator&lt;?&gt; operator : allOperators) &#123;</span><br><span class="line">        if (null !&#x3D; operator) &#123;</span><br><span class="line">            &#x2F;&#x2F; 运行StreamOperator</span><br><span class="line">            operator.initializeState();</span><br><span class="line">            operator.open();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="用户代码调用"><a href="#用户代码调用" class="headerlink" title="用户代码调用"></a>用户代码调用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">用户写的代码,其实会生成为一个个的Operator,用户代码就是它的userFunction</span><br><span class="line">AbstractInvokable.invoke()</span><br><span class="line">-&gt; StreamTask -&gt;循环调用processInput</span><br><span class="line">-&gt; StreamInputProcessor.processInput[StreamOneInputProcessor]</span><br><span class="line">-&gt; PushingAsyncDataInput.emitNext[StreamTaskNetworkInput]</span><br><span class="line">-&gt; StreamTaskNetworkInput.processElement</span><br><span class="line">-&gt; PushingAsyncDataInput.DataOutput.emitRecord[OneInputStreamTask]</span><br><span class="line">-&gt; OneInputStreamOperator.processElement[StreamFlatMap]</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void processElement(StreamRecord&lt;IN&gt; element) throws Exception &#123;</span><br><span class="line">    collector.setTimestamp(element);</span><br><span class="line">    userFunction.flatMap(element.getValue(), collector);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之五读取Hive流程</title>
    <url>/2020/05/29/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E4%BA%94%E8%AF%BB%E5%8F%96Hive%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<blockquote>
<p>此流程是基于1.10.x版本的,对于1.11.x来说存在许多不足;所以根据问题去看源码</p>
</blockquote>
<span id="more"></span>

<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 将SqlNode去accept一个访问类</span><br><span class="line">FlinkPlannerImpl</span><br><span class="line">    validate()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 获取Table信息</span><br><span class="line">PreValidateReWriter</span><br><span class="line">    visit()</span><br><span class="line">        appendPartitionProjects()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 将Table转换为CatalogTable</span><br><span class="line">FlinkCalciteCatalogReader</span><br><span class="line">    getTable()</span><br><span class="line">        toPreparingTable()</span><br><span class="line">            convertCatalogTable()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 查找并创建TableSource</span><br><span class="line">CatalogSourceTable</span><br><span class="line">    findAndCreateTableSource()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 创建TableSource</span><br><span class="line">HiveTableFactory&lt;-TableSourceFactory</span><br><span class="line">    createTableSource()</span><br><span class="line">        createHiveTableSource()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 从StreamExecutionEnvironment获取DataStream并获取HiveTableInputFormat</span><br><span class="line">HiveTableSource</span><br><span class="line">    getDataStream()</span><br><span class="line">        getInputFormat()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 创建InputSplit并根据HiveTableInputSplit转换为对应的Reader</span><br><span class="line">HiveTableInputFormat</span><br><span class="line">    createInputSplits()</span><br><span class="line">        open()</span><br><span class="line">            nextRecord()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 根据BaseRow添加字段信息转换为GenericRow</span><br><span class="line">HiveMapredSplitReader&#x2F;HiveVectorizedOrcSplitReader</span><br><span class="line">    nextRecord()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 实际运行,通过获取InputSplit循环调用Format执行nextRecord</span><br><span class="line">DataSourceTask</span><br><span class="line">    invoke()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="为什么Source并行度会很高-并且程序内无法控制"><a href="#为什么Source并行度会很高-并且程序内无法控制" class="headerlink" title="为什么Source并行度会很高,并且程序内无法控制?"></a>为什么Source并行度会很高,并且程序内无法控制?</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// HiveOptions</span></span><br><span class="line">TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER 默认值<span class="keyword">false</span></span><br><span class="line">    <span class="keyword">false</span>,使用FlinkNativeVectorizedReader去读取ORC文件</span><br><span class="line">    <span class="keyword">true</span>,使用HadooMapredRecordReader去读取ORC文件</span><br><span class="line">TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM 默认值<span class="keyword">true</span></span><br><span class="line">    <span class="keyword">false</span>,Source的并行度在Config中设置</span><br><span class="line">    <span class="keyword">true</span>,并行度使用split数量</span><br><span class="line">TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM_MAX 默认<span class="number">1000</span></span><br><span class="line">    Source的最大并行度</span><br><span class="line"></span><br><span class="line"><span class="comment">// HiveTableSource.getDataStream</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DataStream&lt;BaseRow&gt; <span class="title">getDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化分区,去查询元数据</span></span><br><span class="line">    List&lt;HiveTablePartition&gt; allHivePartitions = initAllPartitions();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    <span class="comment">// 将字段进行转换</span></span><br><span class="line">    TypeInformation&lt;BaseRow&gt; typeInfo =</span><br><span class="line">            (TypeInformation&lt;BaseRow&gt;) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(getProducedDataType());</span><br><span class="line">    <span class="comment">// 去获取flink-conf.yaml文件中的配置</span></span><br><span class="line">    Configuration conf = GlobalConfiguration.loadConfiguration();</span><br><span class="line">    <span class="comment">// 获取Reader</span></span><br><span class="line">    HiveTableInputFormat inputFormat = getInputFormat(allHivePartitions, conf.getBoolean(HiveOptions.TABLE_EXEC_HIVE_FALLBACK_MAPRED_READER));</span><br><span class="line">    DataStreamSource&lt;BaseRow&gt; source = execEnv.createInput(inputFormat, typeInfo);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 并行度获取,从Env中得到Source的默认并行度</span></span><br><span class="line">    <span class="keyword">int</span> parallelism = conf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);</span><br><span class="line">    <span class="comment">// 默认为true,使用split数量</span></span><br><span class="line">    <span class="keyword">if</span> (conf.getBoolean(HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM)) &#123;</span><br><span class="line">        <span class="keyword">int</span> max = conf.getInteger(HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM_MAX);</span><br><span class="line">        <span class="keyword">if</span> (max &lt; <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalConfigurationException(</span><br><span class="line">                    HiveOptions.TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM_MAX.key() +</span><br><span class="line">                            <span class="string">&quot; cannot be less than 1&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> splitNum;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">long</span> nano1 = System.nanoTime();</span><br><span class="line">            <span class="comment">// 获取split数量</span></span><br><span class="line">            splitNum = inputFormat.createInputSplits(<span class="number">0</span>).length;</span><br><span class="line">            <span class="keyword">long</span> nano2 = System.nanoTime();</span><br><span class="line">            LOG.info(</span><br><span class="line">                    <span class="string">&quot;Hive source(&#123;&#125;&#125;) createInputSplits use time: &#123;&#125; ms&quot;</span>,</span><br><span class="line">                    tablePath,</span><br><span class="line">                    (nano2 - nano1) / <span class="number">1_000_000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> FlinkHiveException(e);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 并行度取两者最小值</span></span><br><span class="line">        parallelism = Math.min(splitNum, max);</span><br><span class="line">    &#125;</span><br><span class="line">    parallelism = limit &gt; <span class="number">0</span> ? Math.min(parallelism, (<span class="keyword">int</span>) limit / <span class="number">1000</span>) : parallelism;</span><br><span class="line">    parallelism = Math.max(<span class="number">1</span>, parallelism);</span><br><span class="line">    source.setParallelism(parallelism);</span><br><span class="line">    <span class="keyword">return</span> source.name(explainSource());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">由于HiveOptions中TABLE_EXEC_HIVE_INFER_SOURCE_PARALLELISM为<span class="keyword">true</span></span><br><span class="line">并且conf的获取为去配置文件中加载</span><br><span class="line">所以在程序内设置并行度并不会生效</span><br></pre></td></tr></table></figure>
<h3 id="为什么写入的Parquet文件无法读取-字段会乱"><a href="#为什么写入的Parquet文件无法读取-字段会乱" class="headerlink" title="为什么写入的Parquet文件无法读取,字段会乱?"></a>为什么写入的Parquet文件无法读取,字段会乱?</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">其实Flink去读取Hive,本质是获取分区数据路径</span></span><br><span class="line"><span class="comment">然后取读取HDFS文件,每一个文件就是一个Split,也对应一个并行度</span></span><br><span class="line"><span class="comment">读取完HDFS文件后,会根据其Parquet文件中的定义,生成一个字段数组</span></span><br><span class="line"><span class="comment">同时,SQL的SELECT操作,也会生成一个字段数组</span></span><br><span class="line"><span class="comment">由于代码的限制性,所以如果顺序错乱就会导致类型不匹配,或者数据错乱的问题</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// HiveMapredSplitReader</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> BaseRow <span class="title">nextRecord</span><span class="params">(BaseRow reuse)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (reachedEnd()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将BaseRow转换为GenericRow</span></span><br><span class="line">    <span class="keyword">final</span> GenericRow row = reuse <span class="keyword">instanceof</span> GenericRow ?</span><br><span class="line">            (GenericRow) reuse : <span class="keyword">new</span> GenericRow(selectedFields.length);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//Use HiveDeserializer to deserialize an object out of a Writable blob</span></span><br><span class="line">        Object hiveRowStruct = deserializer.deserialize(value);</span><br><span class="line">        <span class="comment">// 循环遍历查找字段,注意查找字段是Int数组,代表字段在表中的位置,就是这里不合理</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; selectedFields.length; i++) &#123;</span><br><span class="line">            <span class="comment">// set non-partition columns</span></span><br><span class="line">            <span class="keyword">if</span> (selectedFields[i] &lt; structFields.size()) &#123;</span><br><span class="line">                <span class="comment">// stuctFields是读取split文件解析出来的字段列表,所以可能存在字段顺序不匹配</span></span><br><span class="line">                StructField structField = structFields.get(selectedFields[i]);</span><br><span class="line">                <span class="comment">// 转换字段</span></span><br><span class="line">                Object object = HiveInspectors.toFlinkObject(structField.getFieldObjectInspector(),</span><br><span class="line">                        structObjectInspector.getStructFieldData(hiveRowStruct, structField), hiveShim);</span><br><span class="line">                <span class="comment">// 设置行,字段类型不一致就会报错</span></span><br><span class="line">                row.setField(i, converters[i].toInternal(object));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        LOG.error(<span class="string">&quot;Error happens when converting hive data type to flink data type.&quot;</span>);</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> FlinkHiveException(e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!rowReused) &#123;</span><br><span class="line">        <span class="comment">// set partition columns</span></span><br><span class="line">        <span class="keyword">if</span> (!partitionKeys.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; selectedFields.length; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (selectedFields[i] &gt;= structFields.size()) &#123;</span><br><span class="line">                    String partition = partitionKeys.get(selectedFields[i] - structFields.size());</span><br><span class="line">                    row.setField(i, converters[i].toInternal(hiveTablePartition.getPartitionSpec().get(partition)));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        rowReused = <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span>.fetched = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">return</span> row;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ArrayWritableObjectInspector.getStructFieldData</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">getStructFieldData</span><span class="params">(Object data, StructField fieldRef)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data <span class="keyword">instanceof</span> ArrayWritable) &#123;</span><br><span class="line">        <span class="comment">// 整条数据,对应着数据类型的</span></span><br><span class="line">        ArrayWritable arr = (ArrayWritable)data;</span><br><span class="line">        <span class="comment">// SELECT的字段</span></span><br><span class="line">        ArrayWritableObjectInspector.StructFieldImpl structField = (ArrayWritableObjectInspector.StructFieldImpl)fieldRef;</span><br><span class="line">        <span class="comment">// 获取字段在表中的下标,去arr对应下标获取数据</span></span><br><span class="line">        <span class="comment">// 位置不对,获取到的数据就是错乱的</span></span><br><span class="line">        <span class="keyword">return</span> structField.getIndex() &lt; arr.get().length ? arr.get()[structField.getIndex()] : <span class="keyword">null</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data <span class="keyword">instanceof</span> List) &#123;</span><br><span class="line">        <span class="keyword">return</span> ((List)data).get(((ArrayWritableObjectInspector.StructFieldImpl)fieldRef).getIndex());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;Cannot inspect &quot;</span> + data.getClass().getCanonicalName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之零问题记录</title>
    <url>/2020/05/09/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E9%9B%B6%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>记录描述不清楚的内容,系列完成后进行删除</p>
</blockquote>
<span id="more"></span>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.Operation具体是什么,SqlNode具体怎么转换成它的?</span><br><span class="line">2.Transformation到StreamGraph的过程中,树结构描述生成并不流畅,需要改进</span><br><span class="line">3.StreamEdge的用处并不太清楚,除了联系两个StreamNode还有什么其他的功能?</span><br><span class="line">4.org.apache.flink.streaming.api.operators.Operator和org.apache.flink.table.operations.Operation的区别是什么,只是一个用于Table,一个用于DataStream?</span><br><span class="line">5.SQL是转换为Stream的,那么是在执行什么操作的时候进行转换,转换成Stream是必需操作么?</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink理论总结</title>
    <url>/2020/08/11/Flink%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<blockquote>
<p>平时flink的实际应用太多,根据实际的应用,添加必不可少的相关理论知识</p>
</blockquote>
<span id="more"></span>
<h4 id="flink定义"><a href="#flink定义" class="headerlink" title="flink定义"></a>flink定义</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink 是一个分布式大数据处理引擎,可对有限数据流和无限数据流进行有状态或无状态的计算,能够部署在各种集群环境,对各种规模大小的数据进行快速计算</span><br></pre></td></tr></table></figure>

<h4 id="flink-Application"><a href="#flink-Application" class="headerlink" title="flink Application"></a>flink Application</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">streams流: 分为有限数据流和无限数据流;实际应用的比较多的还是无限数据流的方式</span><br><span class="line">state:状态是计算过程中的数据信息,在容错恢复和ck中有重要的作用,流计算在本质上是increment processing;因此需要不断查询保持状态;为了确保exactly once语义,需要数据能够写入到状态中;而持久化存储,能够保证在整个分布式系统运行失败或者挂掉的情况下做到exactly-once,这是状态的另外一个价值</span><br><span class="line">time:分为event time,ingestion time,processing time;flink的无限数据流是一个持续化的过程,时间是我们判断业务状态是否滞后,数据处理是否及时的重要依据</span><br><span class="line">event time: 相当于事件,它在数据最源头产生时带有时间戳,后面都需要用到事件戳来实现;把对应时间3点到4点的数据放在3点到4点的bucket,然后bucket产生结果</span><br></pre></td></tr></table></figure>

<h4 id="flink-operation"><a href="#flink-operation" class="headerlink" title="flink operation"></a>flink operation</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink具备7*24小时高可用的SOA(面向服务架构),原因是在实现上flink提供了一致性的checkpoint,ck是flink实现容错机制的核心,它周期性的记录计算过程中operator的状态,并生成快照保存到持久化存储;当flink作业发生故障崩溃时,可以有选择的从ck中恢复,保证了计算的一致性</span><br></pre></td></tr></table></figure>

<h4 id="flink有状态分散式流式处理"><a href="#flink有状态分散式流式处理" class="headerlink" title="flink有状态分散式流式处理"></a>flink有状态分散式流式处理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">定义了变数X,X在数据处理过程中会进行读和写,在最后输出结果时,可以依据变数X决定输出的内容,即状态X会影响最终的输出结果;</span><br><span class="line">可以分成两种形式理解:</span><br><span class="line">1.先进行了状态co-partitioned key by,同样的key都会流到computation instance;与使用者出现次数的原理相同,次数即所谓的状态,这个状态一定会跟同一个key的事件累积在同一个computation instance;类似于根据输入流的key重新分区的状态,当分区进入stream之后,这个stream会累积起来的状态也变成coPartition</span><br><span class="line">quesIdDS.map(x &#x3D;&gt; &#123;</span><br><span class="line">      val info &#x3D; x.getMsg.asInstanceOf[Tuple2[String,String]]</span><br><span class="line">      (info._1,1)</span><br><span class="line">    &#125;).keyBy(_._1).timeWindow(Time.of(1,TimeUnit.SECONDS)).sum(1).map(x&#x3D;&gt; (x._1,&quot;&quot;))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2.embedded local state backend;有状态分散式流式处理的引擎,状态可能会累积的非常大,当key非常多时,状态可能就会超出单一节点的memory的负荷量;需要使用到状态后端</span><br><span class="line"></span><br><span class="line">状态后端分为:</span><br><span class="line">1.MemoryStateBackend:直接将state对象存储到taskManager的JVM上,如MapState会被存储为一个HashMap对象.对于远程备份,备份到JobMananger的堆内存上,但是不安全</span><br><span class="line">2.FSStatebackend:跟MemorySatateBackend一样,将state存储到TaskManager的JVM堆上;只是对于远程备份,将state写入到hdfs上</span><br><span class="line">3.RocksDBStateBackend:将state存储到taskManager节点上的RocksDB数据库实力上;对于远程备份,备份到远程的存储系统中</span><br><span class="line"></span><br><span class="line">总结:</span><br><span class="line">MemoryStateBackend和FSStateBackend都是在内存中进行状态管理,所以可以获取较低的读写延迟;但会受限于TM的内存大小,而RocksDBStateBackend直接将state存储到RocksDB数据库中,所以不受JobManager的内存限制,但会有读写延迟的情况</span><br></pre></td></tr></table></figure>

<h4 id="watermarks"><a href="#watermarks" class="headerlink" title="watermarks"></a>watermarks</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一般event-time会搭配watermarks来使用,精髓在于当某个运算值收到带有时间戳&quot;T&quot;的watermarks是就意味着它不会接收到最新的数据了;好处在于可以准确预估收到数据的截止时间;</span><br><span class="line">eg:</span><br><span class="line">假设预期收到数据时间与输出数据时间的时间差延迟5分钟,那么flink中所有的window operator搜索3点到4点的数据,但因为存在延迟需要在多等5分钟直至收集完4点05分数据,判定4点钟的资料收集完毕</span><br></pre></td></tr></table></figure>

<h4 id="运行flink应用"><a href="#运行flink应用" class="headerlink" title="运行flink应用"></a>运行flink应用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">用户用DataStream API 写的一个数据处理程序;</span><br><span class="line">在一个DAG图中不能被chain在一起的operator会被分隔到不同的task中,也就是task是flink中资源调度的最小单位</span><br><span class="line">flink实际运行时包括两类进程:</span><br><span class="line">1.JobManager(JobMaster):协调task的分布式执行,包括调度task,协调创ck以及当job failover时协调各个task从ck中恢复</span><br><span class="line">2.TaskManager(work):执行dataFlow中的tasks,包括内存buffer的分配,dataStream的传递</span><br><span class="line">task slot是一个TM中最小资源分配单位,一个taskmanager中多少个slot意味着能支持多少并发的task处理,一个task slot中可以执行多个operator,一般这些operator是能被chain在一起处理</span><br><span class="line"></span><br><span class="line">基于yarn调度系统:</span><br><span class="line">1.JobManager进程和TaskManager进程都有yarn NodeManager监控</span><br><span class="line">2.如果JobManager进程异常退出,则yarn resourceManager会重新调度JobManager到其他机器</span><br><span class="line">3.如果taskManager进程异常退出,JobManager会收到消息并重新向Yarn ResourceManager申请资源,重新启动TaskManager</span><br><span class="line"></span><br><span class="line">flink作业执行脚本(参考):</span><br><span class="line"></span><br><span class="line">## flink 作业样例</span><br><span class="line"> chk&#x3D;&#96;getConfigChk $BASE&#x2F;$config&#96;</span><br><span class="line"> jarName&#x3D;&#39;flink-action-log-to-hbase-full.jar&#39;</span><br><span class="line"> appName&#x3D;&#39;FlinkActionLogToHbase&#39;</span><br><span class="line"> noRecover&#x3D;false</span><br><span class="line"> for arg in $@; do</span><br><span class="line">   if [ &quot;$arg&quot; &#x3D; &quot;-a&quot; ]; then</span><br><span class="line">    noRecover&#x3D;true</span><br><span class="line">   fi</span><br><span class="line"> done</span><br><span class="line"> if [ &quot;$chk&quot; &#x3D; &quot;hdfs:&#x2F;&#x2F;&quot; -o $noRecover &#x3D; true ]; then  </span><br><span class="line">  echo &quot;##########################################&quot;</span><br><span class="line">  echo &quot;#              初始化运行                #&quot;</span><br><span class="line">  echo &quot;##########################################&quot;</span><br><span class="line">  flink run -m yarn-cluster \</span><br><span class="line">  -ynm $appName \</span><br><span class="line">  -p 7 -ys 2 -ytm 3072 \</span><br><span class="line">  $BASE&#x2F;$jarName \</span><br><span class="line">  $BASE&#x2F;$config</span><br><span class="line"> else</span><br><span class="line">  echo &quot;##########################################&quot;</span><br><span class="line">  echo &quot;# 加载 checkpoint &#x3D;&gt; $chk                 &quot;</span><br><span class="line">  echo &quot;##########################################&quot;</span><br><span class="line">  flink run -m yarn-cluster \</span><br><span class="line">  -ynm $appName&quot;-recover&quot; \</span><br><span class="line">  -p 7 -ys 2 -ytm 3072 \</span><br><span class="line">  -s $chk \</span><br><span class="line">  $BASE&#x2F;$jarName \</span><br><span class="line">  $BASE&#x2F;$config</span><br><span class="line"> fi</span><br></pre></td></tr></table></figure>

<h4 id="理解keyedStream"><a href="#理解keyedStream" class="headerlink" title="理解keyedStream"></a>理解keyedStream</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">为了能够在多个并发实例上并行的对数据进行处理,需要通过keyby将数据进行分组;keyby和window操作都是对数据进行分组,但是keyby是在水平方向对流进行切分,而window是在垂直方向对流进行切分</span><br><span class="line">keyby操作只有当key的数量超过算子的并发实例数才可以较好的工作.由于同一个key对应的所有数据都会发送到同一个实例上,因此如果key的数量比实例数量少时,就会导致部分实例收不到数据,从而导致计算能力不能充分发挥</span><br></pre></td></tr></table></figure>

<h4 id="savepoint和checkpoint的区别"><a href="#savepoint和checkpoint的区别" class="headerlink" title="savepoint和checkpoint的区别"></a>savepoint和checkpoint的区别</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.checkpoint是增量做的,每次的时间较短,数据量较小,只要在程序里面启用后会自动触发,用户无须感知,checkpoint是作业failover的时候自动使用,不需要用户指定</span><br><span class="line">2.savepoint是全量做的,每次的时间较长,数据量较大,需要用户主动去触发;savepoint一般用于程序的版本更新,bug修复,A&#x2F;B Test等场景,需要用户指定</span><br></pre></td></tr></table></figure>

<h4 id="无状态计算"><a href="#无状态计算" class="headerlink" title="无状态计算"></a>无状态计算</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">消费延迟计算</span><br><span class="line">在这种模式的计算中,无论这条输入进来多少次,输出的结果都是一样的,因为单条输入中已经包含了所需的所有信息;消费落后等于生产者减去消费者;生产者的消费在单条数据中可以得到,消费者的数据也可以在单条数据中得到,所以相同输入可以得到相同输出,这就是一个无状态计算</span><br></pre></td></tr></table></figure>

<h4 id="有状态计算"><a href="#有状态计算" class="headerlink" title="有状态计算"></a>有状态计算</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">访问日志统计量</span><br><span class="line">这个计算模式是将数据输入算子中,用来进行各种复杂的计算并输出数据;这个过程中算子会去访问之前存储在里面的状态;另外一个方面,它还会把现在的数据对状态的影响实时更新</span><br></pre></td></tr></table></figure>

<h4 id="状态的应用场景"><a href="#状态的应用场景" class="headerlink" title="状态的应用场景"></a>状态的应用场景</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.去重: 比如上游的系统数据可能会有重复,落到下游系统时希望把重复的数据都去掉;去重需要先了解哪些数据来过,哪些数据还没有来,也就是把所有的主键都记录下来.当一条数据到来后,能够看到在主键当中是否存在</span><br><span class="line">2.窗口计算: 比如统计每分钟nginx日志API被访问了多少次;窗口是一分钟计算一次,在窗口触发前,前59s的数据来了需要先放入内存,即需要把这个窗口之内的数据先保留下来,等到一分钟后,再将整个窗口内触发的数据输出;未触发的窗口数据也是一种状态</span><br><span class="line">3.机器学习&#x2F;深度学习:如训练的模型以及当前模型的参数也是一种状态,机器学习可能每次都用有一个数据集,需要在数据集上进行学习,对模型进行一个反馈</span><br><span class="line">4.访问历史数据: 比如与昨天的数据进行对比,需要访问一些历史数据;如果每次从外部去读,对资源的消耗可能比较大,所以也希望把这些历史数据也放入状态中做对比</span><br></pre></td></tr></table></figure>

<h4 id="理想的状态管理"><a href="#理想的状态管理" class="headerlink" title="理想的状态管理"></a>理想的状态管理</h4><p>最理想的状态管理需要满足易用,高效,可靠</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.易用: flink提供了丰富的数据结构,多样的状态组织形式以及简洁的扩展接口,让状态管理更加易用</span><br><span class="line">2.高效: 实时作业一般需要更低的延迟,一般出现故障,恢复速度也需要更快,当处理能力不够时,可以横向扩展,同时在处理备份时,不影响作业本身处理性能</span><br><span class="line">3.可靠: flink提供了状态持久化,包括不丢不重的语义以及具备自动的容错能力,比如HA,当节点挂掉后会自动拉起,不需要人工介入</span><br></pre></td></tr></table></figure>

<h3 id="Flink状态类型"><a href="#Flink状态类型" class="headerlink" title="Flink状态类型"></a>Flink状态类型</h3><h4 id="Managed-State-amp-Row-State"><a href="#Managed-State-amp-Row-State" class="headerlink" title="Managed State &amp; Row State"></a>Managed State &amp; Row State</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Managed State是Flink自动管理的state,而Raw State是原生态的state;区别在于:</span><br><span class="line">1.从状态管理方式来说,Managed State有flink runtime管理,自动存储,自动恢复,在内存管理上有优化;而raw state需要用户自己管理,需要自己序列化,flink不知道state中存入的数据是什么结构,只有用户自己知道,需要最终序列化为可存储的数据结构</span><br><span class="line">2.从状态数据结构来说,managed state支持已知的数据结构,如value,list,map;而raw state只支持字节数组,所有状态都要转换为二进制字节数组</span><br><span class="line">3.从推荐使用场景,managed state大多数情况下均可使用,而raw state是当managed state不够用时,比如使用自定义operator时,推荐使用raw state</span><br></pre></td></tr></table></figure>

<h4 id="keyed-State-amp-Operator-State"><a href="#keyed-State-amp-Operator-State" class="headerlink" title="keyed State &amp; Operator State"></a>keyed State &amp; Operator State</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Manager State分为两种:keyed State和Opearator State;在Flink Stream模型中,DataStream 经过keyby的操作可以变为keyedStream</span><br><span class="line">Keyed State:</span><br><span class="line">1.只能用在keyStream上的算子中</span><br><span class="line">2.每个key对应一个State</span><br><span class="line">  一个Operator实例处理多个key,访问相应的多个State</span><br><span class="line">3.并发改变,State随着key在实例见迁移</span><br><span class="line">4.通过RuntimeContext访问</span><br><span class="line">5.支持的数据结构: ValueState&#x2F;ListState&#x2F;ReducingState&#x2F;AggregatingState&#x2F;MapState</span><br><span class="line"></span><br><span class="line">Operator State:</span><br><span class="line">1.可以用于所有算子</span><br><span class="line">  常用于source,例如FlinkKafkaConsumer</span><br><span class="line">2.一个Operator实例对应一个State</span><br><span class="line">3.并发改变时有多种重新分配方式可选</span><br><span class="line">  1.均匀分配</span><br><span class="line">  2.合并后每个得到全量</span><br><span class="line">4.实现checkpointedFunction 或 ListCheckpointed接口</span><br><span class="line">5.支持的数据结构:ListState</span><br></pre></td></tr></table></figure>

<h4 id="状态保存以及恢复"><a href="#状态保存以及恢复" class="headerlink" title="状态保存以及恢复"></a>状态保存以及恢复</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">checkpoint:</span><br><span class="line">1.定时制作分布式快照,对程序中的状态进行备份</span><br><span class="line">2.发生故障时</span><br><span class="line">  将整个作业的所有task都回滚到最后一次成功checkpoint中的状态,然后从那个点开始继续处理</span><br><span class="line">3.必要条件</span><br><span class="line">  数据源支持重发</span><br><span class="line">4.一致性语义</span><br><span class="line">   恰好一次,至少一次,至多一次</span><br><span class="line"></span><br><span class="line">在做checkpoint时,可根据Barrier对齐来判断是哪个语义;如果对齐,则为恰好一次,否则没有对齐就是其他两个语义;如果只有一个上游,也就是说Barrier是不需要对齐的;如果只有一个ck在做,不管什么时候从ck恢复,都会恢复到刚才的状态;如果有多个上游,加入一个上游的Barrier到了,另一个Barrier还没有来,如果这个时候对状态进行快照,那么从这个快照恢复的时候,其中一个上游的数据可能会有重复或丢失的情况</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的RestAPI使用</title>
    <url>/2020/01/14/Flink%E7%9A%84RestAPI%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>最近社区在进行Flink平台化开发,提供支持</p>
</blockquote>
<span id="more"></span>

<h2 id="Rest-API"><a href="#Rest-API" class="headerlink" title="Rest API"></a>Rest API</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 默认就是8081,可以自己改</span><br><span class="line">vi flink-conf.yaml</span><br><span class="line">rest.port: 8081</span><br><span class="line"></span><br><span class="line"># Web访问</span><br><span class="line">http:&#x2F;&#x2F;cdh04:8081&#x2F;#&#x2F;overview</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="CRUL操作"><a href="#CRUL操作" class="headerlink" title="CRUL操作"></a>CRUL操作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 关闭集群</span><br><span class="line">curl -X DELETE http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;cluster</span><br><span class="line"></span><br><span class="line"># 查看WebUI配置信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;config</span><br><span class="line"></span><br><span class="line"># 查看通过WebUI上传集群的jar包</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jars</span><br><span class="line"></span><br><span class="line"># 上传jar包</span><br><span class="line">curl -X POST -H &quot;Expect:&quot; -F &quot;jarfile&#x3D;@&#x2F;usr&#x2F;local&#x2F;flink-1.9.1&#x2F;examples&#x2F;streaming&#x2F;SocketWindowWordCount.jar&quot; http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jars&#x2F;upload</span><br><span class="line"></span><br><span class="line"># 删除已上传的jar</span><br><span class="line">curl -X DELETE http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jars&#x2F;f2dc6af3-dabd-46b7-8f79-b0973586182d_SocketWindowWordCount.jar</span><br><span class="line"></span><br><span class="line"># 查看jar的执行计划</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jars&#x2F;24b3d9d6-8c5e-4d1b-a1e3-8609e82e2681_SocketWindowWordCount.jar&#x2F;plan</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jars&#x2F;24b3d9d6-8c5e-4d1b-a1e3-8609e82e2681_SocketWindowWordCount.jar&#x2F;plan?programArg&#x3D;--hostname,cdh04,--port,9999</span><br><span class="line">program-args(用programArg代替)</span><br><span class="line">programArg    programArg&#x3D;--hostname,cdh04,--port,9999</span><br><span class="line">entry-class   org.apache.flink.streaming.examples.socket.SocketWindowWordCount</span><br><span class="line">parallelism   4</span><br><span class="line"></span><br><span class="line"># 启动jar</span><br><span class="line">curl -X POST http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jars&#x2F;24b3d9d6-8c5e-4d1b-a1e3-8609e82e2681_SocketWindowWordCount.jar&#x2F;run?programArg&#x3D;--hostname,cdh04,--port,9999</span><br><span class="line">program-args(用programArg代替)</span><br><span class="line">programArg    programArg&#x3D;--hostname,cdh04,--port,9999</span><br><span class="line">entryClass   org.apache.flink.streaming.examples.socket.SocketWindowWordCount</span><br><span class="line">parallelism   4</span><br><span class="line">allowNonRestoredState(无法从保存点启动作业时是否拒绝提交作业)    true&#x2F;false</span><br><span class="line">savepointPath(保存点)    hdfs:&#x2F;&#x2F;path</span><br><span class="line"></span><br><span class="line"># 查看JM的配置信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobmanager&#x2F;config</span><br><span class="line"></span><br><span class="line"># 查看JM监控指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobmanager&#x2F;metrics</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobmanager&#x2F;metrics?get&#x3D;Status.JVM.ClassLoader.ClassesUnloaded</span><br><span class="line"></span><br><span class="line"># 查看所有的任务以及状态</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs</span><br><span class="line"></span><br><span class="line"># 查看任务监控指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;metrics</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;metrics?get&#x3D;downtime</span><br><span class="line">get</span><br><span class="line">agg</span><br><span class="line">jobs</span><br><span class="line"></span><br><span class="line"># 查看所有的任务概述信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;overview</span><br><span class="line"></span><br><span class="line"># 查看指定任务细节</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c</span><br><span class="line"></span><br><span class="line"># 取消某个任务</span><br><span class="line">curl -X PATCH http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;de67c4a9559ad0328f593da82b7b0819</span><br><span class="line">curl -X PATCH http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;de67c4a9559ad0328f593da82b7b0819?mode&#x3D;cancel</span><br><span class="line">curl -X GET http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;096ba3e7f51889568da14f5bcac2e98a&#x2F;yarn-cancel</span><br><span class="line"></span><br><span class="line"># 返回作业累计器</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;accumulators</span><br><span class="line"></span><br><span class="line"># 返回作业检查点统计信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;checkpoints</span><br><span class="line"></span><br><span class="line"># 返回检查点配置</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;checkpoints&#x2F;config</span><br><span class="line"></span><br><span class="line"># 返回检查点的详细信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;checkpoints&#x2F;details&#x2F;:checkpointid</span><br><span class="line"></span><br><span class="line"># 返回任务及其子任务的检查点统计信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;checkpoints&#x2F;details&#x2F;:checkpointid&#x2F;subtasks&#x2F;:vertexid</span><br><span class="line"></span><br><span class="line"># 返回作业的配置</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;config</span><br><span class="line"></span><br><span class="line"># 返回作业不可恢复的异常</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;exceptions</span><br><span class="line"></span><br><span class="line"># 返回作业执行的结果</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;execution-result</span><br><span class="line"></span><br><span class="line"># 返回作业指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;metrics</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;metrics?get&#x3D;lastCheckpointExternalPath</span><br><span class="line"></span><br><span class="line"># 返回作业的数据流计划</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;plan</span><br><span class="line"></span><br><span class="line"># 触发作业的缩放</span><br><span class="line">curl -X PATCH http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;rescaling</span><br><span class="line">&#123;&quot;errors&quot;:[&quot;Rescaling is temporarily disabled. See FLINK-12312.&quot;]&#125;</span><br><span class="line"></span><br><span class="line"># 返回重新调整操作的状态</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;rescaling&#x2F;:triggerid</span><br><span class="line"></span><br><span class="line"># 触发保存点</span><br><span class="line">curl -X POST http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;savepoints</span><br><span class="line"></span><br><span class="line"># 返回保存点操作的状态</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;savepoints&#x2F;:triggerid</span><br><span class="line"></span><br><span class="line"># 停止具有保存点的作业</span><br><span class="line">curl -X POST http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;stop</span><br><span class="line"></span><br><span class="line"># 返回任务的详细信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid</span><br><span class="line"></span><br><span class="line"># 返回任务的用户定义的累加器</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;accumulators</span><br><span class="line"></span><br><span class="line"># 返回作业的背压信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;backpressure</span><br><span class="line"></span><br><span class="line"># 任务指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;metrics</span><br><span class="line">get</span><br><span class="line"></span><br><span class="line"># 返回任务的所有子任务的所有用户定义的累加器</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;subtasks&#x2F;accumulators</span><br><span class="line"></span><br><span class="line"># 提供对聚合子任务指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;subtasks&#x2F;metrics</span><br><span class="line">get</span><br><span class="line">agg</span><br><span class="line">subtasks</span><br><span class="line"></span><br><span class="line"># 返回子任务当前或最新执行尝试的详细信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;subtasks&#x2F;:subtaskindex</span><br><span class="line"></span><br><span class="line"># 返回子任务执行尝试的详细信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;subtasks&#x2F;:subtaskindex&#x2F;attempts&#x2F;:attempt</span><br><span class="line"></span><br><span class="line"># 返回子任务执行尝试的累加器</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;subtasks&#x2F;:subtaskindex&#x2F;attempts&#x2F;:attempt&#x2F;accumulators</span><br><span class="line"></span><br><span class="line"># 子任务指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;subtasks&#x2F;:subtaskindex&#x2F;metrics</span><br><span class="line">get</span><br><span class="line"></span><br><span class="line"># 返回任务的所有子任务的时间相关信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;subtasktimes</span><br><span class="line"></span><br><span class="line"># 返回任务管理器汇总的任务信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;jobs&#x2F;275bc5b3f2791ea2131fdb87835cb21c&#x2F;vertices&#x2F;:vertexid&#x2F;taskmanagers</span><br><span class="line"></span><br><span class="line"># 返回Flink群集的概述</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;overview</span><br><span class="line"></span><br><span class="line"># 触发对保存点的处置</span><br><span class="line">curl -X POST http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;savepoint-disposal</span><br><span class="line"></span><br><span class="line"># 返回保存点处置操作的状态</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;savepoint-disposal&#x2F;:triggerid</span><br><span class="line"></span><br><span class="line"># 返回所有任务管理器的概述</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;taskmanagers</span><br><span class="line"></span><br><span class="line"># 任务管理器指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;taskmanagers&#x2F;metrics</span><br><span class="line">get</span><br><span class="line">agg</span><br><span class="line">taskmanagers</span><br><span class="line"></span><br><span class="line"># 返回任务管理器的详细信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;taskmanagers&#x2F;26250ba72b84e947dbbb8629f31740bd</span><br><span class="line"></span><br><span class="line"># 指定任务管理器指标信息</span><br><span class="line">curl http:&#x2F;&#x2F;cdh04:8081&#x2F;v1&#x2F;taskmanagers&#x2F;26250ba72b84e947dbbb8629f31740bd&#x2F;metrics</span><br><span class="line">get</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的SavePoint和CheckPoint</title>
    <url>/2019/07/02/Flink%E7%9A%84SavePoint%E5%92%8CCheckPoint/</url>
    <content><![CDATA[<blockquote>
<p>最初的目的是为了处理Flink流程序异常退出,如何恢复数据</p>
</blockquote>
<span id="more"></span>

<h2 id="1-配置文件"><a href="#1-配置文件" class="headerlink" title="1.配置文件"></a>1.配置文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;usr&#x2F;local&#x2F;flink-1.7.2&#x2F;conf&#x2F;flink-conf.yaml</span><br><span class="line"></span><br><span class="line">state.checkpoints.dir: hdfs:&#x2F;&#x2F;namenode-host:port&#x2F;flink-checkpoints</span><br><span class="line">state.checkpoints.dir: hdfs:&#x2F;&#x2F;&#x2F;flink&#x2F;checkpoints</span><br><span class="line"></span><br><span class="line">state.savepoints.dir: hdfs:&#x2F;&#x2F;namenode-host:port&#x2F;flink-checkpoints</span><br><span class="line">state.savepoints.dir: hdfs:&#x2F;&#x2F;&#x2F;flink&#x2F;savepoints</span><br><span class="line"></span><br><span class="line"># 没有指定上述两个目录,执行命令时需要手动指定</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-CheckPoint"><a href="#2-CheckPoint" class="headerlink" title="2.CheckPoint"></a>2.CheckPoint</h2><h3 id="用处"><a href="#用处" class="headerlink" title="用处"></a>用处</h3><ul>
<li>CheckPoint主要用于自动故障恢复.</li>
<li>由Flink自动创建,拥有和发布,不需要用户区交互.</li>
<li>当作业被cancel之后,CheckPoint会被删除,除非设置了<strong>ExternalizedCheckpoint</strong>的保留机制.</li>
</ul>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;usr&#x2F;local&#x2F;flink-1.7.2&#x2F;conf&#x2F;flink-conf.yaml</span><br><span class="line"># 设置CheckPoint默认保留数量</span><br><span class="line">state.checkpoints.num-retained: 20</span><br></pre></td></tr></table></figure>

<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">senv.enableCheckpointing(<span class="number">500</span>)</span><br><span class="line"><span class="comment">// 设置checkpoint保存目录</span></span><br><span class="line">senv.setStateBackend(<span class="keyword">new</span> <span class="type">FsStateBackend</span>(<span class="string">&quot;hdfs:///flink/checkpoints&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> conf = senv.getCheckpointConfig</span><br><span class="line"><span class="comment">// 取消作业时删除检查点.</span></span><br><span class="line">conf.enableExternalizedCheckpoints(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">DELETE_ON_CANCELLATION</span>)</span><br><span class="line"><span class="comment">// 取消作业时保留检查点.</span></span><br><span class="line">conf.enableExternalizedCheckpoints(<span class="type">ExternalizedCheckpointCleanup</span>.<span class="type">RETAIN_ON_CANCELLATION</span>)</span><br><span class="line"></span><br><span class="line">脚本指定对应的checkpoint:</span><br><span class="line">flink1<span class="number">.8</span> run -m yarn-cluster -ynm <span class="type">FlinkBehaviorTrace1</span><span class="number">.8</span> -yn <span class="number">1</span> -ys <span class="number">1</span> -ytm <span class="number">1024</span> -s hdfs:<span class="comment">///flink/checkpoints/data/FlinkBehaviorTrace1.8/check_id/1e87f3f4092026ef36f115f073147c39/chk-2064658/_metadata /home/etiantian/zsd/flink-project/flink1.8-behavior-trace-graphic/flink1.8-behavior-trace/flink1.8-behavior-trace-graphic-full.jar /home/etiantian/zsd/config/flink-config.properties</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-SavePoint"><a href="#3-SavePoint" class="headerlink" title="3.SavePoint"></a>3.SavePoint</h2><h3 id="用处-1"><a href="#用处-1" class="headerlink" title="用处"></a>用处</h3><ul>
<li>SavePoint是通过CheckPoint机制为Streaming Job创建的一致性快照.</li>
<li>需要手动触发,这点与CheckPoint有区别.</li>
<li>SavePoint由用户拥有,创建和删除,在作业停止之后仍然保存.</li>
<li>一般用于Flink版本升级,业务迁移,集群迁移,数据不允许丢失的情况</li>
</ul>
<h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 触发SavePoint</span></span><br><span class="line"><span class="comment">## Flink not on Yarn</span></span><br><span class="line">flink savepoint 1a32cab47537102d70e3a1a885fc431c hdfs:///flink/savepoints</span><br><span class="line"><span class="comment">## Flink on Yarn</span></span><br><span class="line">flink savepoint 1a32cab47537102d70e3a1a885fc431c hdfs:///flink/savepoints  -yid application_1562025913394_0001</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看list</span></span><br><span class="line"><span class="comment">## Flink not on Yarn</span></span><br><span class="line">flink list</span><br><span class="line"><span class="comment">## Flink on Yarn</span></span><br><span class="line">flink list yarn-cluster -yid application_1562025913394_0001</span><br><span class="line"></span><br><span class="line"><span class="comment"># cancel触发savepoint</span></span><br><span class="line"><span class="comment">## Flink not on Yarn</span></span><br><span class="line">flink cancel -s hdfs:///flink/savepoints 1a32cab47537102d70e3a1a885fc431c</span><br><span class="line"><span class="comment">## Flink on Yarn</span></span><br><span class="line">flink cancel -s hdfs:///flink/savepoints 1a32cab47537102d70e3a1a885fc431c yarn-cluster -yid application_1562025913394_0001</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用savepoint</span></span><br><span class="line"><span class="comment">## Flink not on Yarn</span></span><br><span class="line">flink run -s hdfs:///flink/savepoints -m valid1.jar</span><br><span class="line"><span class="comment">## Flink on Yarn</span></span><br><span class="line">flink run -s hdfs:///flink/savepoints -m yarn-cluster valid1.jar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除savepoint</span></span><br><span class="line"><span class="comment">## Flink not on Yarn</span></span><br><span class="line">flink savepoint -d hdfs:///flink/savepoints</span><br><span class="line"><span class="comment">## Flink on Yarn</span></span><br><span class="line">flink savepoint -d hdfs:///flink/savepoints yarn-cluster -yid application_1562025913394_0001</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-疑惑"><a href="#4-疑惑" class="headerlink" title="4.疑惑"></a>4.疑惑</h2><p>如果Flink有自定义的变量值,那么从检查点恢复,这个变量值是初始的,还是程序当前的值.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的Runtime</title>
    <url>/2020/01/13/Flink%E7%9A%84Runtime/</url>
    <content><![CDATA[<blockquote>
<p>介绍Flink Runtime的作业执行的核心机制</p>
</blockquote>
<span id="more"></span>

<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>首先Flink是可以运行在多种环境中的,如Standalone,Yarn,K8S之类;Flink Runtime层采用了标准的Master-Slave架构.</p>
<ul>
<li><code>Client</code>(不属于Runtime)</li>
<li><code>Master</code></li>
<li><ul>
<li>JobManager</li>
</ul>
</li>
<li><ul>
<li>Dispatcher</li>
</ul>
</li>
<li><ul>
<li>ResourceManager </li>
</ul>
</li>
<li><code>Slave</code></li>
<li><ul>
<li>TaskManager</li>
</ul>
</li>
<li><code>Akka</code>(角色通信)</li>
<li><code>Netty</code>(数据传输)</li>
</ul>
<p><code>Dispatcher</code>负责负责接收用户提供的作业，并且负责为这个新提交的作业拉起一个新的<code>JobManager</code>组件。<br><code>ResourceManager</code>负责资源的管理,在整个Flink集群中只有一个<code>ResourceManager</code><br><code>JobManager</code>负责管理作业的执行,在一个Flink集群中可能有多个作业同时执行,每个作业都有自己的<code>JobManager</code>组件</p>
<hr>
<h2 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h2><p>用户提交作业,提交脚本会首先启动一个<code>Client</code>进程负责作业的编译与提交<br>首先将用户编写的代码编译为一个<code>JobGraph</code>(会进行一些检查或优化等工作)<br><code>Client</code>将产生的<code>JobGraph</code>提交到集群中执行<br><strong>两种情况</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Standalone这种Session模式,AM会预先启动,此时Client直接与Dispatcher建立连接并提交作业即可</span><br><span class="line">Per-Job模式,AM不会预先启动,此时Client将首先向资源管理系统&lt;如Yarn,K8S&gt;申请资源来启动AM,然后再向AM中的Dispatcher提交作业</span><br></pre></td></tr></table></figure>
<p>作业到<code>Dispatcher</code>后,<code>Dispatcher</code>会首先启动一个<code>JobManager</code>组件<br><code>JobManager</code>会向<code>ResourceManager</code>申请资源来启动作业中具体的任务<br><strong>两种情况</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">根据Session和Per-Job模式的区别,TaskExecutor可能已经启动或者尚未启动</span><br><span class="line">Session模式,ResourceManager中已有记录了TaskExecutor注册的资源,可以直接选取空闲资源进行分配</span><br><span class="line">Per-Job模式,ResourceManager也需要首先向外部资源管理系统申请资源来启动TaskExecutor,然后等待TaskExecutor注册相应资源后再继续选择空闲资源进程分配</span><br></pre></td></tr></table></figure>
<p><code>TaskExecutor</code>的资源是通过<code>Slot</code>来描述的,一个<code>Slot</code>一般可以执行一个具体的<code>Task</code><br><code>ResourceManager</code>选择到空闲的<code>Slot</code>之后,就会通知相应的<code>TM</code>将该<code>Slot</code>分配分<code>JobManager</code><br><code>TaskExecutor</code>进行相应的记录后,会向<code>JobManager</code>进行注册<br><code>JobManager</code>收到<code>TaskExecutor</code>注册上来的<code>Slot</code>后,就可以实际提交<code>Task</code>了<br><code>TaskExecutor</code>收到<code>JobManager</code>提交的<code>Task</code>之后,会启动一个新的线程来执行该<code>Task</code><br><code>Task</code>启动后就会开始进行预先指定的计算,并通过数据<code>Shuffle</code>模块互相交换数据</p>
<p><strong>注意</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink支持两种不同的模式,Per-job模式与Session模式</span><br><span class="line">Per-job模式下整个Flink集群只执行单个作业,即每个作业会独享Dispatcher和ResourceManager组件</span><br><span class="line">Per-job模式下AppMaster和TaskExecutor都是按需申请的</span><br><span class="line">Per-job模式更适合运行执行时间较长的大作业,这些作业对稳定性要求较高,并且对申请资源的时间不敏感</span><br><span class="line"></span><br><span class="line">Session模式下,Flink预先启动AppMaster以及一组TaskExecutor</span><br><span class="line">然后在整个集群的生命周期中会执行多个作业</span><br><span class="line">Session模式更适合规模小,执行时间短的作业。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="作业调度"><a href="#作业调度" class="headerlink" title="作业调度"></a>作业调度</h2><p>Flink中,资源是由<code>TaskExecutor</code>上的<code>Slot</code>来表示的,每个<code>Slot</code>可以用来执行不同的<code>Task</code><br>任务即<code>Job</code>中实际的<code>Task</code>,它包含了待执行的用户逻辑<br>调度的主要目的就是为了给<code>Task</code>找到匹配的<code>Slot</code></p>
<hr>
<p>在<code>ResourceManager</code>中有一个子组件叫做<code>SlotManager</code>,它维护了当前集群中所有<code>TaskExecutor</code>上的<code>Slot</code>的信息与状态<br>如该<code>Slot</code>在哪个<code>TaskExecutor</code>中,该<code>Slot</code>当前是否空闲等<br>当<code>JobManger</code>来为特定<code>Task</code>申请资源的时候,根据当前是<code>Per-job</code>还是<code>Session</code>模式,<code>ResourceManager</code>可能会去申请资源来启动新的<code>TaskExecutor</code></p>
<hr>
<p>当<code>TaskExecutor</code>启动之后,它会通过服务发现找到当前活跃的<code>ResourceManager</code>并进行注册<br>注册信息中,会包含该<code>TaskExecutor</code>中所有<code>Slot</code>的信息<br><code>ResourceManager</code>收到注册信息后,其中的<code>SlotManager</code>就会记录下相应的<code>Slot</code>信息<br>当<code>JobManager</code>为某个<code>Task</code>来申请资源时,<code>SlotManager</code>就会从当前空闲的<code>Slot</code>中按一定规则选择一个空闲的<code>Slot</code>进行分配<br>当分配完成后<code>RM</code>会首先向<code>TaskManager</code>发送<code>RPC</code>要求将选定的<code>Slot</code>分配给特定的<code>JobManager</code><br><code>TaskManager</code>如果还没有执行过该<code>JobManager</code>的<code>Task</code>的话,它需要首先向相应的<code>JobManager</code>建立连接,然后发送提供<code>Slot</code>的<code>RPC</code>请求<br>在<code>JobManager</code>中,所有<code>Task</code>的请求会缓存到<code>SlotPool</code>中<br>当有<code>Slot</code>被提供之后,<code>SlotPool</code>会从缓存的请求中选择相应的请求并结束相应的请求过程</p>
<hr>
<p>当<code>Task</code>结束之后,无论是正常结束还是异常结束,都会通知<code>JobManager</code>相应的结束状态<br><code>TaskManager</code>端将<code>Slot</code>标记为已占用但未执行任务的状态<br><code>JobManager</code>会首先将相应的<code>Slot</code>缓存到<code>SlotPool</code>中,但不会立即释放<br>这种方式避免了如果将<code>Slot</code>直接还给<code>ResourceManager</code>,在任务异常结束之后需要重启时,需要立刻重新申请<code>Slot</code>的问题<br>通过延时释放,<code>Failover</code>的<code>Task</code>可以尽快调度回原来的<code>TaskManager</code>,从而加快<code>Failover</code>的速度</p>
<hr>
<p>当<code>SlotPool</code>中缓存的<code>Slot</code>超过指定的时间仍未使用时,<code>SlotPool</code>就会发起释放该<code>Slot</code>的过程<br>与申请<code>Slot</code>的过程对应,<code>SlotPool</code>会首先通知<code>TaskManager</code>来释放该<code>Slot</code><br><code>TaskExecutor</code>通知<code>ResourceManager</code>该<code>Slot</code>已经被释放,从而最终完成释放的逻辑<br><strong>注意</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">除了正常的通信逻辑外,在ResourceManager和TaskExecutor之间还存在定时的心跳消息来同步Slot的状态</span><br></pre></td></tr></table></figure>
<p><strong>调度方式</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Eager调度(适用于流作业)</span><br><span class="line">Eager调度如其名子所示,它会在作业启动时申请资源将所有的Task调度起来</span><br><span class="line">这种调度算法主要用来调度可能没有终止的流作业</span><br><span class="line"></span><br><span class="line">Lazy From Source(适用于批作业)</span><br><span class="line">Lazy From Source是从Source开始,按拓扑顺序来进行调度</span><br><span class="line">简单来说,Lazy From Source会先调度没有上游任务的Source任务</span><br><span class="line">当这些任务执行完成时,它会将输出数据缓存到内存或者写入到磁盘中</span><br><span class="line">然后,对于后续的任务,当它的前驱任务全部执行完成后</span><br><span class="line">Flink就会将这些任务调度起来</span><br><span class="line">这些任务会从读取上游缓存的输出数据进行自己的计算</span><br><span class="line">这一过程继续进行直到所有的任务完成计算</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="错误恢复"><a href="#错误恢复" class="headerlink" title="错误恢复"></a>错误恢复</h2><p>整体上来说,错误可能分为两大类:<code>Task</code>执行出现错误或<code>Flink</code>集群的<code>Master</code>出现错误</p>
<p><strong>第一类错误恢复策略</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Restart-all,重启所有的Task</span><br><span class="line">对于Flink的流任务,由于Flink提供Checkpoint机制</span><br><span class="line">因此当任务重启后可以直接从上次的Checkpoint开始继续执行</span><br><span class="line">因此这种方式更适合于流作业</span><br></pre></td></tr></table></figure>

<p><strong>第二类错误恢复策略</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Restart-individual</span><br><span class="line">只适用于Task之间没有数据传输的情况</span><br><span class="line">这种情况下,我们可以直接重启出错的任务</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink的侧输出操作</title>
    <url>/2020/01/13/Flink%E7%9A%84%E4%BE%A7%E8%BE%93%E5%87%BA%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<blockquote>
<p>Filter算子的替代,节省不必要的性能浪费</p>
</blockquote>
<span id="more"></span>

<h2 id="OutputTag"><a href="#OutputTag" class="headerlink" title="OutputTag"></a>OutputTag</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">指定分流的标签,侧输出数据格式可以和主流的数据格式不一致</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="特定的函数发送数据"><a href="#特定的函数发送数据" class="headerlink" title="特定的函数发送数据"></a>特定的函数发送数据</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ProcessFunction</span><br><span class="line">CoProcessFunction</span><br><span class="line">ProcessWindowFunction</span><br><span class="line">ProcessAllWindowFunction</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Demo代码实现"><a href="#Demo代码实现" class="headerlink" title="Demo代码实现"></a>Demo代码实现</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.flink.stream.window</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.test.flink.stream.hive.<span class="type">JsonDeserializationSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.<span class="type">SimpleStringSchema</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.runtime.state.filesystem.<span class="type">FsStateBackend</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.&#123;<span class="type">OutputTag</span>, <span class="type">StreamExecutionEnvironment</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.<span class="type">FlinkKafkaConsumer010</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.convert.<span class="type">WrapAsJava</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.<span class="type">ProcessFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.<span class="type">Collector</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author: xs</span></span><br><span class="line"><span class="comment"> * @Date: 2020-01-13 11:02</span></span><br><span class="line"><span class="comment"> * @Description:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SideOutputExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    properties.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;cdh04:9092&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="string">&quot;auto.offset.reset&quot;</span>, <span class="string">&quot;latest&quot;</span>)</span><br><span class="line">    properties.setProperty(<span class="type">ConsumerConfig</span>.<span class="type">ENABLE_AUTO_COMMIT_CONFIG</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> consumer010 = <span class="keyword">new</span> <span class="type">FlinkKafkaConsumer010</span>[<span class="type">String</span>](</span><br><span class="line">      <span class="type">List</span>(<span class="string">&quot;test&quot;</span>),</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(),</span><br><span class="line">      properties</span><br><span class="line">    ).setStartFromLatest()</span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;hdfs&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> senv = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> delayOutputTag = <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">&quot;delay-side-output&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> ds = senv.addSource(consumer010).map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> arr = x.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">      <span class="type">Demo</span>(arr(<span class="number">0</span>), arr(<span class="number">1</span>).toLong)</span><br><span class="line">    &#125;).process(<span class="keyword">new</span> <span class="type">ProcessFunction</span>[<span class="type">Demo</span>, <span class="type">Demo</span>] &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(value: <span class="type">Demo</span>, ctx: <span class="type">ProcessFunction</span>[<span class="type">Demo</span>, <span class="type">Demo</span>]#<span class="type">Context</span>, out: <span class="type">Collector</span>[<span class="type">Demo</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (value.delayTime &lt; <span class="number">100</span>) &#123;</span><br><span class="line">          out.collect(value)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          ctx.output(delayOutputTag, <span class="string">s&quot;数据 <span class="subst">$&#123;value.toString&#125;</span> 迟到了 ：&quot;</span> + value.delayTime + <span class="string">&quot;秒&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 常规数据处理</span></span><br><span class="line">    ds.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对侧输出的数据处理</span></span><br><span class="line">    ds.getSideOutput(delayOutputTag).print()</span><br><span class="line"></span><br><span class="line">    senv.execute(<span class="string">&quot;Side Outputs Test&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span>(<span class="params">id: <span class="type">String</span>, time: <span class="type">Long</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> delayTime: <span class="type">Long</span> = <span class="type">System</span>.currentTimeMillis() / <span class="number">1000</span> - time</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink窗口源码分析</title>
    <url>/2020/05/04/Flink%E7%AA%97%E5%8F%A3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>深入源码层面,学习Flink窗口操作的原理,这里只挑了各部分的一个进行分析</p>
</blockquote>
<span id="more"></span>

<h2 id="依赖关系"><a href="#依赖关系" class="headerlink" title="依赖关系"></a>依赖关系</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Window</span><br><span class="line">    GlobalWindow(放置所有数据的默认窗口)</span><br><span class="line">    TimeWindow(表示一段时间间隔的窗口)</span><br><span class="line"></span><br><span class="line">WindowAssigner</span><br><span class="line">    -MerginWindowAssigner(窗口是可以合并的)</span><br><span class="line">        -DynamicEventTimeSessionWindows</span><br><span class="line">        -DynamicProcessingTimeSessionWindows</span><br><span class="line">        -EventTimeSessionWindows</span><br><span class="line">        -ProcessingTimeSessionWindows</span><br><span class="line">    -SlidingEventTimeWindows(滑动窗口)</span><br><span class="line">        -SlidingTimeWindows</span><br><span class="line">    -SlidingProcessingTimeWindows(滑动窗口)</span><br><span class="line">    -TumblingEventTimeWindows(滚动窗口)</span><br><span class="line">        -TumblingTimeWindows</span><br><span class="line">    -TumblingProcessionTimeWindows(滚动窗口)</span><br><span class="line">    -GlobalWindwos(将所有元素分配在一个窗口中)</span><br><span class="line">    </span><br><span class="line">Trigger</span><br><span class="line">    -ContinuousEventTimeTrigger(基于给定时间间隔连续触发,计算基于水印)</span><br><span class="line">    -ContinuousProcessingTimeTrigger(基于给定时间间隔连续触发,计算基于ProcessingTime)</span><br><span class="line">    -CountTrigger(每maxCount触发一次计算)</span><br><span class="line">        -用于DataStream</span><br><span class="line">        -用于KeyedStream</span><br><span class="line">    -DeltaTrigger(此触发器计算上次触发的数据点与当前到达的数据点之间的增量。如果增量高于指定的阈值，则会触发。)</span><br><span class="line">        需要用户自己实现DeltaFunction</span><br><span class="line">    -EventTimeTrigger(按照EventTime判断是否触发计算)</span><br><span class="line">        -用于EventTimeWindows</span><br><span class="line">    -NeverTrigger(一个从不触发的触发器，作为GlobalWindows的默认触发器)</span><br><span class="line">        -用于GlobalWindows</span><br><span class="line">    -ProcessingTimeTrigger(按照ProcessingTime判断是否触发计算)</span><br><span class="line">        -用于ProcessingTimeWindows</span><br><span class="line">    -PurgingTrigger(包装类,将TriggerResult为FIRE的改为FIRE_AND_PURGE)</span><br><span class="line">        -用于DataStream</span><br><span class="line">        -用于KeyedStream</span><br><span class="line">        -用于DataStreamGroupWindowAggregateBase</span><br><span class="line">    -StateCleaningCountTrigger(GlobalWindow)(触发清理定时器触发或元素达到maxCount触发)</span><br><span class="line">        -用于DataStreamGroupWindowAggregateBase</span><br><span class="line"></span><br><span class="line">TriggerResult</span><br><span class="line">    CONTINUE(不做任何操作)</span><br><span class="line">    FIRE_AND_PURGE</span><br><span class="line">    FIRE(处理窗口数据)</span><br><span class="line">    PURGE(移除窗口和窗口中的数据)</span><br><span class="line">    </span><br><span class="line">Evictor</span><br><span class="line">    -CountEvictor(以maxCount为判断标准,决定元素是否被移除)</span><br><span class="line">    -DeltaEvictor(计算每个元素与最后一个元素的Delta值,与threshold进行对比,如果大于等于,则移除该元素)</span><br><span class="line">    -TimeEvictor(以时间为判断标准,决定元素是否会被移除)</span><br><span class="line"></span><br><span class="line">Timer</span><br></pre></td></tr></table></figure>

<h2 id="SlidingEventTimeWindows"><a href="#SlidingEventTimeWindows" class="headerlink" title="SlidingEventTimeWindows"></a>SlidingEventTimeWindows</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 成员变量</span><br><span class="line">size:窗口大小,slide窗口步长,offset偏移量</span><br><span class="line"></span><br><span class="line"># 划分窗口</span><br><span class="line">数组结构ArrayList&lt;TimeWindow&gt;:大小为size&#x2F;slide</span><br><span class="line">窗口开始时间:timestamp - (timestamp - offset + slide) % slide;</span><br><span class="line">数组内窗口:new TimeWindow(start, start + size)</span><br><span class="line"></span><br><span class="line"># 默认Trigger</span><br><span class="line">EventTimeTrigger</span><br><span class="line"></span><br><span class="line"># 使用</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; in &#x3D; ...;</span><br><span class="line">KeyedStream&lt;Tuple2&lt;String, Integer&gt;, String&gt; keyed &#x3D; in.keyBy(...);</span><br><span class="line">WindowedStream&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; windowed &#x3D; keyed.window(SlidingEventTimeWindows.of(Time.minutes(1), Time.seconds(10)));</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="EventTimeTrigger"><a href="#EventTimeTrigger" class="headerlink" title="EventTimeTrigger"></a>EventTimeTrigger</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果window中的最大时间戳小于当前水印</span><br><span class="line">FIRE</span><br><span class="line"># 如果window中的最大时间戳大于当前水印</span><br><span class="line">注册Timer定时器</span><br><span class="line">CONTINUE</span><br><span class="line"></span><br><span class="line"># 注册事件时间回调。当当前水印通过时，将使用此处指定的时间调用指定的时间。</span><br><span class="line">Trigger</span><br><span class="line">    TriggerContext.registerEventTimeTimer(long time)</span><br><span class="line"></span><br><span class="line"># WindowOperator</span><br><span class="line">    Context.registerEventTimeTimer(long time)</span><br><span class="line"></span><br><span class="line"># InternalTimerService</span><br><span class="line">    registerEventTimeTimer(N namespace, long time)</span><br><span class="line">    </span><br><span class="line"># 注册事件时间水印超过给定时间时要触发的计时器。计时器触发时，将提供您在此处传递的命名空间。</span><br><span class="line">InternalTimerServiceImpl</span><br><span class="line">    registerEventTimeTimer(N namespace, long time)</span><br><span class="line"></span><br><span class="line"># 当前正在运行的EventTime定时器队列</span><br><span class="line">KeyGroupedInternalPriorityQueue</span><br><span class="line">    add(new TimerHeapInternalTimer&lt;&gt;(time, (K) keyContext.getCurrentKey(), namespace))</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="CountEvictor"><a href="#CountEvictor" class="headerlink" title="CountEvictor"></a>CountEvictor</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">evict方法</span><br><span class="line">如果size小于设置的最大数值,则可以返回</span><br><span class="line">否则将迭代元素,并删除多出的元素</span><br><span class="line">int evictedCount &#x3D; 0;</span><br><span class="line">for (Iterator&lt;TimestampedValue&lt;Object&gt;&gt; iterator &#x3D; elements.iterator(); iterator.hasNext();)&#123;</span><br><span class="line">    iterator.next();</span><br><span class="line">    evictedCount++;</span><br><span class="line">    if (evictedCount &gt; size - maxCount) &#123;</span><br><span class="line">        break;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        iterator.remove();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="WindowOperator工作流程"><a href="#WindowOperator工作流程" class="headerlink" title="WindowOperator工作流程"></a>WindowOperator工作流程</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(StreamRecord&lt;IN&gt; element)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1.获取element归属的windows</span></span><br><span class="line">    <span class="keyword">final</span> Collection&lt;W&gt; elementWindows = windowAssigner.assignWindows(</span><br><span class="line">        element.getValue(), element.getTimestamp(), windowAssignerContext);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果元素不是由指定的元素窗口处理的</span></span><br><span class="line">    <span class="keyword">boolean</span> isSkippedElement = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取element对应的Key</span></span><br><span class="line">    <span class="keyword">final</span> K key = <span class="keyword">this</span>.&lt;K&gt;getKeyedStateBackend().getCurrentKey();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (windowAssigner <span class="keyword">instanceof</span> MergingWindowAssigner) &#123;</span><br><span class="line">        <span class="comment">// 合并窗口</span></span><br><span class="line">        MergingWindowSet&lt;W&gt; mergingWindows = getMergingWindowSet();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 添加新窗口可能会导致合并，在这种情况下，实际窗口是合并的窗口，我们使用它。如果不合并，则实际窗口==窗口</span></span><br><span class="line">            W actualWindow = mergingWindows.addWindow(window, <span class="keyword">new</span> MergingWindowSet.MergeFunction&lt;W&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(W mergeResult,</span></span></span><br><span class="line"><span class="function"><span class="params">                        Collection&lt;W&gt; mergedWindows, W stateWindowResult,</span></span></span><br><span class="line"><span class="function"><span class="params">                        Collection&lt;W&gt; mergedStateWindows)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> ((windowAssigner.isEventTime() &amp;&amp; mergeResult.maxTimestamp() + allowedLateness &lt;= internalTimerService.currentWatermark())) &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;The end timestamp of an &quot;</span> +</span><br><span class="line">                                <span class="string">&quot;event-time window cannot become earlier than the current watermark &quot;</span> +</span><br><span class="line">                                <span class="string">&quot;by merging. Current watermark: &quot;</span> + internalTimerService.currentWatermark() +</span><br><span class="line">                                <span class="string">&quot; window: &quot;</span> + mergeResult);</span><br><span class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!windowAssigner.isEventTime()) &#123;</span><br><span class="line">                        <span class="keyword">long</span> currentProcessingTime = internalTimerService.currentProcessingTime();</span><br><span class="line">                        <span class="keyword">if</span> (mergeResult.maxTimestamp() &lt;= currentProcessingTime) &#123;</span><br><span class="line">                            <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">&quot;The end timestamp of a &quot;</span> +</span><br><span class="line">                                <span class="string">&quot;processing-time window cannot become earlier than the current processing time &quot;</span> +</span><br><span class="line">                                <span class="string">&quot;by merging. Current processing time: &quot;</span> + currentProcessingTime +</span><br><span class="line">                                <span class="string">&quot; window: &quot;</span> + mergeResult);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    triggerContext.key = key;</span><br><span class="line">                    triggerContext.window = mergeResult;</span><br><span class="line"></span><br><span class="line">                    triggerContext.onMerge(mergedWindows);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> (W m: mergedWindows) &#123;</span><br><span class="line">                        triggerContext.window = m;</span><br><span class="line">                        triggerContext.clear();</span><br><span class="line">                        deleteCleanupTimer(m);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 将合并的状态窗口合并到新生成的状态窗口中</span></span><br><span class="line">                    windowMergingState.mergeNamespaces(stateWindowResult, mergedStateWindows);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.如果是延迟窗口,跳过</span></span><br><span class="line">            <span class="keyword">if</span> (isWindowLate(actualWindow)) &#123;</span><br><span class="line">                mergingWindows.retireWindow(actualWindow);</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            isSkippedElement = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">            W stateWindow = mergingWindows.getStateWindow(actualWindow);</span><br><span class="line">            <span class="keyword">if</span> (stateWindow == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Window &quot;</span> + window + <span class="string">&quot; is not in in-flight window set.&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 4.将element存入windowState</span></span><br><span class="line">            windowState.setCurrentNamespace(stateWindow);</span><br><span class="line">            windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 5.判断element是否触发trigger</span></span><br><span class="line">            triggerContext.key = key;</span><br><span class="line">            triggerContext.window = actualWindow;</span><br><span class="line">            TriggerResult triggerResult = triggerContext.onElement(element);</span><br><span class="line">            <span class="keyword">if</span> (triggerResult.isFire()) &#123;</span><br><span class="line">                <span class="comment">// 6.获取windowState,注入windowFunction</span></span><br><span class="line">                ACC contents = windowState.get();</span><br><span class="line">                <span class="keyword">if</span> (contents == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                emitWindowContents(actualWindow, contents);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 7.清除windowState</span></span><br><span class="line">            <span class="keyword">if</span> (triggerResult.isPurge()) &#123;</span><br><span class="line">                windowState.clear();</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 8.注册timer,到窗口结束时清理window</span></span><br><span class="line">            registerCleanupTimer(actualWindow);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 需要确保更新状态中的合并状态</span></span><br><span class="line">        mergingWindows.persist();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 非合并窗口</span></span><br><span class="line">        <span class="keyword">for</span> (W window: elementWindows) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (isWindowLate(window)) &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            isSkippedElement = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">            windowState.setCurrentNamespace(window);</span><br><span class="line">            windowState.add(element.getValue());</span><br><span class="line"></span><br><span class="line">            triggerContext.key = key;</span><br><span class="line">            triggerContext.window = window;</span><br><span class="line"></span><br><span class="line">            TriggerResult triggerResult = triggerContext.onElement(element);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (triggerResult.isFire()) &#123;</span><br><span class="line">                ACC contents = windowState.get();</span><br><span class="line">                <span class="keyword">if</span> (contents == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                emitWindowContents(window, contents);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (triggerResult.isPurge()) &#123;</span><br><span class="line">                windowState.clear();</span><br><span class="line">            &#125;</span><br><span class="line">            registerCleanupTimer(window);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果已设置未由任何窗口延迟到达标记处理的元素，则侧输出输入事件windowAssigner为事件时间和当前时间戳+允许的延迟不小于元素时间戳</span></span><br><span class="line">    <span class="keyword">if</span> (isSkippedElement &amp;&amp; isElementLate(element)) &#123;</span><br><span class="line">        <span class="keyword">if</span> (lateDataOutputTag != <span class="keyword">null</span>)&#123;</span><br><span class="line">            sideOutput(element);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.numLateRecordsDropped.inc();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink读写Hbase之写</title>
    <url>/2019/06/03/Flink%E8%AF%BB%E5%86%99Hbase%E4%B9%8B%E5%86%99/</url>
    <content><![CDATA[<blockquote>
<p>主要对Flink写入HBase数据做一个整理,方便快速进行业务代码开发,只针对于具体的方法操作,并不涉及Flink搭建</p>
</blockquote>
<span id="more"></span>

<hr>
<h2 id="主要方式-4种"><a href="#主要方式-4种" class="headerlink" title="主要方式(4种)"></a>主要方式(4种)</h2><ul>
<li>通过env.addSink(new RichSinkFunction)的形式</li>
<li>通过自定义HBaseUtil操作类的形式</li>
<li>通过env.output(new OutputFormat)的形式</li>
<li>通过env.output(new HadoopOutputFormat)的形式</li>
</ul>
<hr>
<h2 id="RichSinkFunction"><a href="#RichSinkFunction" class="headerlink" title="RichSinkFunction"></a>RichSinkFunction</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">wordCounts.addSink(<span class="keyword">new</span> <span class="type">RichSinkFunction</span>[(<span class="type">String</span>, <span class="type">String</span>)] &#123;</span><br><span class="line">    <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> table: <span class="type">Table</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> mutator: <span class="type">BufferedMutator</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(<span class="type">TABLE_NAME</span>)</span><br><span class="line">        <span class="keyword">val</span> conf: org.apache.hadoop.conf.<span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="type">HBASE_ZOOKEEPER</span>)</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line">        conn = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">        table = conn.getTable(tableName)</span><br><span class="line">        mutator = conn.getBufferedMutator(<span class="keyword">new</span> <span class="type">BufferedMutatorParams</span>(tableName).writeBufferSize(<span class="number">10</span> * <span class="number">1024</span> * <span class="number">1024</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">invoke</span></span>(value: (<span class="type">String</span>, <span class="type">String</span>), context: <span class="type">SinkFunction</span>.<span class="type">Context</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> time1 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(value._1))</span><br><span class="line">        <span class="comment">// put.addColumn(Bytes.toBytes(TABLE_CF), Bytes.toBytes(&quot;count&quot;), Bytes.toBytes(value._2.toString().replace(&quot;---&quot;, &quot;&quot;)))</span></span><br><span class="line">        put.addColumn(<span class="type">Bytes</span>.toBytes(<span class="type">TABLE_CF</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;count&quot;</span>), <span class="type">Bytes</span>.toBytes(value._2 + <span class="string">&quot;---&quot;</span>))</span><br><span class="line">        mutator.mutate(put)</span><br><span class="line">        <span class="comment">// 输出数据</span></span><br><span class="line">        mutator.flush()</span><br><span class="line">        <span class="keyword">val</span> time2 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        println(time2 - time1)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (table != <span class="literal">null</span>) table.close()</span><br><span class="line">            <span class="keyword">if</span> (mutator != <span class="literal">null</span>) mutator.close()</span><br><span class="line">            <span class="keyword">if</span> (conn != <span class="literal">null</span>) conn.close()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; println(e.getMessage)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBaseUtil"><a href="#HBaseUtil" class="headerlink" title="HBaseUtil"></a>HBaseUtil</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HbaseUtil</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> tables: <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Table</span>] = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Table</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initConn</span></span>() &#123;</span><br><span class="line">    <span class="keyword">if</span> (conn == <span class="literal">null</span> || conn.isClosed) &#123;</span><br><span class="line">      println(<span class="string">&quot;----  Init Conn  -----&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">      conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="type">HBASE_ZOOKEEPER</span>)</span><br><span class="line">      conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line">      conn = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getConn</span></span>() = &#123;</span><br><span class="line">    initConn</span><br><span class="line">    conn</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getTable</span></span>(tablename: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!getConn().getAdmin.tableExists(<span class="type">TableName</span>.valueOf(tablename))) &#123;</span><br><span class="line">      conn.getAdmin.createTable(</span><br><span class="line">        <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(</span><br><span class="line">          <span class="type">TableName</span>.valueOf(tablename)</span><br><span class="line">        ).addFamily(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">&quot;info&quot;</span>)</span><br><span class="line">        ))</span><br><span class="line">    &#125;</span><br><span class="line">    tables.getOrElse(tablename, &#123;</span><br><span class="line">      initConn</span><br><span class="line">      conn.getTable(<span class="type">TableName</span>.valueOf(tablename))</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">put</span></span>(tableName: <span class="type">String</span>, p: <span class="type">Put</span>) &#123;</span><br><span class="line">    getTable(tableName)</span><br><span class="line">      .put(p)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(tableName: <span class="type">String</span>, get: <span class="type">Get</span>, cf: <span class="type">String</span>, column: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> r = getTable(tableName)</span><br><span class="line">      .get(get)</span><br><span class="line">    <span class="keyword">if</span> (r != <span class="literal">null</span> &amp;&amp; !r.isEmpty()) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">String</span>(r.getValue(cf.getBytes, column.getBytes))</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//  接受配置文件</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 用于直接建立HBase连接</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param properties</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getConf</span></span>(properties: <span class="type">Properties</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, properties.getProperty(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>))</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, properties.getProperty(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>))</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.master&quot;</span>, properties.getProperty(<span class="string">&quot;hbase.master&quot;</span>))</span><br><span class="line">    conf</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 获取连接</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param conf</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getConn</span></span>(conf: <span class="type">Configuration</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (conn == <span class="literal">null</span> || conn.isClosed) &#123;</span><br><span class="line">      conn = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">    &#125;</span><br><span class="line">    conn</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 获取表,没有表则创建</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param conn</span></span><br><span class="line"><span class="comment">    * @param tableName</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getTable</span></span>(conn: <span class="type">Connection</span>, tableName: <span class="type">String</span>) = &#123;</span><br><span class="line">    createTable(conn, tableName)</span><br><span class="line">    conn.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 创建表</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param conn</span></span><br><span class="line"><span class="comment">    * @param tableName</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createTable</span></span>(conn: <span class="type">Connection</span>, tableName: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!conn.getAdmin.tableExists(<span class="type">TableName</span>.valueOf(tableName))) &#123;</span><br><span class="line">      <span class="keyword">val</span> tableDescriptor = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">      tableDescriptor.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">&quot;info&quot;</span>.getBytes()))</span><br><span class="line">      conn.getAdmin.createTable(tableDescriptor)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 提交数据</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param conn</span></span><br><span class="line"><span class="comment">    * @param tableName</span></span><br><span class="line"><span class="comment">    * @param data</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">putData</span></span>(conn: <span class="type">Connection</span>, tableName: <span class="type">String</span>, data: <span class="type">Put</span>) = &#123;</span><br><span class="line">    getTable(conn, tableName).put(data)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 对表直接进行批量写入时使用</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param conf</span></span><br><span class="line"><span class="comment">    * @param tableName</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getNewJobConf</span></span>(conf: <span class="type">Configuration</span>, tableName: <span class="type">String</span>) = &#123;</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.defaults.for.version.skip&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    conf.set(org.apache.hadoop.hbase.mapreduce.<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, tableName)</span><br><span class="line">    conf.setClass(<span class="string">&quot;mapreduce.job.outputformat.class&quot;</span>, classOf[org.apache.hadoop.hbase.mapreduce.<span class="type">TableOutputFormat</span>[<span class="type">String</span>]], classOf[org.apache.hadoop.mapreduce.<span class="type">OutputFormat</span>[<span class="type">String</span>, <span class="type">Mutation</span>]])</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JobConf</span>(conf)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// main</span></span><br><span class="line"><span class="comment">// 输入数据</span></span><br><span class="line">wordCounts.map(x =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>((x._1).getBytes)</span><br><span class="line">  put.addColumn(<span class="string">&quot;info&quot;</span>.getBytes, <span class="string">&quot;count&quot;</span>.getBytes, x._2.toString.getBytes)</span><br><span class="line">  <span class="comment">// 输出数据</span></span><br><span class="line">  <span class="type">HbaseUtil</span>.put(<span class="string">&quot;test.demo_&quot;</span>, put)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="OutputFormat"><a href="#OutputFormat" class="headerlink" title="OutputFormat"></a>OutputFormat</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> tableOuputFormat = <span class="keyword">new</span> <span class="type">OutputFormat</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>]] &#123;</span><br><span class="line">  <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">configure</span></span>(configuration: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(i: <span class="type">Int</span>, i1: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: org.apache.hadoop.conf.<span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="type">HBASE_ZOOKEEPER</span>)</span><br><span class="line">    conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line">    conn = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">writeRecord</span></span>(it: <span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(<span class="type">TABLE_NAME</span>)</span><br><span class="line">    <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(it.f0))</span><br><span class="line">    put.addColumn(<span class="type">Bytes</span>.toBytes(<span class="type">TABLE_CF</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;count&quot;</span>), <span class="type">Bytes</span>.toBytes(it.f1 + <span class="string">&quot;小猪猪&quot;</span>))</span><br><span class="line">    conn.getTable(tableName).put(put)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (conn != <span class="literal">null</span>) conn.close()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; println(e.getMessage)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 输入数据</span></span><br><span class="line"><span class="keyword">val</span> hbaseDs = env.createInput(tableInputFormat)</span><br><span class="line"><span class="comment">// 输出数据</span></span><br><span class="line">hbaseDs.output(tableOutputFormat)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HadoopOutputFormat"><a href="#HadoopOutputFormat" class="headerlink" title="HadoopOutputFormat"></a>HadoopOutputFormat</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 输入数据</span></span><br><span class="line"><span class="keyword">val</span> hbaseDs = env.createInput(tableInputFormat)</span><br><span class="line"><span class="keyword">val</span> hadoopOF = <span class="keyword">new</span> <span class="type">HadoopOutputFormat</span>[<span class="type">String</span>, <span class="type">Mutation</span>](<span class="keyword">new</span> <span class="type">TableOutputFormat</span>(), job)</span><br><span class="line">println(hbaseDs.collect().length)</span><br><span class="line"><span class="keyword">val</span> ds = hbaseDs.map(x =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(x.f0.getBytes())</span><br><span class="line">  put.addColumn(<span class="type">Bytes</span>.toBytes(<span class="type">TABLE_CF</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;count&quot;</span>), <span class="type">Bytes</span>.toBytes(x.f1 + <span class="string">&quot;小猪猪&quot;</span>))</span><br><span class="line">  (x.f0, put.asInstanceOf[<span class="type">Mutation</span>])</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">// 输出数据</span></span><br><span class="line">ds.output(hadoopOF)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="优劣"><a href="#优劣" class="headerlink" title="优劣"></a>优劣</h2><blockquote>
<ul>
<li>对于上述的四种方式,比较常见的是前三种,也是我在网上搜索Flink操作HBase数据出现次数较多的结果.</li>
<li>但是根据实际的操作会发现,前三种方式写入HBase的速度,速度远远不如SparkRDD.saveAsNewAPIHadoopDataset操作.</li>
<li>第四种方式是总结了Flink将数据转换成HFile的文件,然后进行Bulk Load操作.</li>
</ul>
</blockquote>
<p>** 如果你有更好的方式,欢迎和我联系 **</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink读取Hive表并行度过高</title>
    <url>/2020/05/26/Flink%E8%AF%BB%E5%8F%96Hive%E8%A1%A8%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%BF%87%E9%AB%98/</url>
    <content><![CDATA[<blockquote>
<p>基于Flink-1.10.0,此版本只能在flink-conf.yaml中控制source并行度</p>
</blockquote>
<span id="more"></span>

<h2 id="出现情况"><a href="#出现情况" class="headerlink" title="出现情况"></a>出现情况</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">读取Hive表,存在大量小文件,这时Source端的并行度不受全局并行度影响</span><br><span class="line">并行度等于文件数量导致并行度过高</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在flink-conf.yaml中配置以下参数</span><br><span class="line">    table.exec.hive.infer-source-parallelism&#x3D;true (默认使用自动推断)</span><br><span class="line">    table.exec.hive.infer-source-parallelism.max&#x3D;1000 (自动推断的最大并发) </span><br><span class="line"></span><br><span class="line">Sink的并行度默认和上游的并行度相同,如果有Shuffle,使用配置的统一并行度</span><br><span class="line"></span><br><span class="line">静等flink-1.11发布</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink读写Hbase之读</title>
    <url>/2019/06/03/Flink%E8%AF%BB%E5%86%99Hbase%E4%B9%8B%E8%AF%BB/</url>
    <content><![CDATA[<blockquote>
<p>主要对Flink读取HBase数据做一个整理,方便快速进行业务代码开发,只针对于具体的方法操作,并不涉及Flink搭建</p>
</blockquote>
<span id="more"></span>

<hr>
<h2 id="主要方式-3种"><a href="#主要方式-3种" class="headerlink" title="主要方式(3种)"></a>主要方式(3种)</h2><ul>
<li>通过env.addSource(new RichSourceFunction)的形式</li>
<li>通过env.createInput(new TableInputFormat)的形式</li>
<li>通过env.createInput(new HadoopInputFormat)的形式</li>
</ul>
<hr>
<h2 id="RichSourceFunction"><a href="#RichSourceFunction" class="headerlink" title="RichSourceFunction"></a>RichSourceFunction</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> dataStream = env.addSource(<span class="keyword">new</span> <span class="type">RichSourceFunction</span>[(<span class="type">String</span>, <span class="type">String</span>)] &#123;</span><br><span class="line">    <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> table: <span class="type">Table</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> scan: <span class="type">Scan</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(<span class="type">TABLE_NAME</span>)</span><br><span class="line">        <span class="keyword">val</span> conf: org.apache.hadoop.conf.<span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="type">HBASE_ZOOKEEPER</span>)</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line">        conn = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">        table = conn.getTable(tableName)</span><br><span class="line">        scan = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">        scan.addFamily(<span class="type">Bytes</span>.toBytes(<span class="type">TABLE_CF</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(sourceContext: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[(<span class="type">String</span>, <span class="type">String</span>)]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> rs = table.getScanner(scan)</span><br><span class="line">        <span class="keyword">val</span> iterator = rs.iterator()</span><br><span class="line">        <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">            <span class="keyword">val</span> result = iterator.next()</span><br><span class="line">            <span class="keyword">val</span> rowKey = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">            <span class="keyword">val</span> value = <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(<span class="type">TABLE_CF</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;count&quot;</span>)))</span><br><span class="line">            sourceContext.collect((rowKey, value))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (table != <span class="literal">null</span>) table.close()</span><br><span class="line">            <span class="keyword">if</span> (conn != <span class="literal">null</span>) conn.close()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; println(e.getMessage)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TableInputFormat"><a href="#TableInputFormat" class="headerlink" title="TableInputFormat"></a>TableInputFormat</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> tableInputFormat = <span class="keyword">new</span> <span class="type">TableInputFormat</span>[<span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>]] &#123;</span><br><span class="line">    <span class="keyword">val</span> tuple2 = <span class="keyword">new</span> <span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>]</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getScanner</span></span>: <span class="type">Scan</span> = &#123;</span><br><span class="line">        scan</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getTableName</span></span>: <span class="type">String</span> = <span class="type">TABLE_NAME</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">mapResultToTuple</span></span>(result: <span class="type">Result</span>): <span class="type">Tuple2</span>[<span class="type">String</span>, <span class="type">String</span>] = &#123;</span><br><span class="line">        <span class="keyword">val</span> key = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">        <span class="keyword">val</span> value = <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(<span class="type">TABLE_CF</span>), <span class="type">Bytes</span>.toBytes(<span class="string">&quot;count&quot;</span>)))</span><br><span class="line">        tuple2.setField(key, <span class="number">0</span>)</span><br><span class="line">        tuple2.setField(value, <span class="number">1</span>)</span><br><span class="line">        tuple2</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">configure</span></span>(parameters: <span class="type">Configuration</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> tableName = <span class="type">TableName</span>.valueOf(<span class="type">TABLE_NAME</span>)</span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">val</span> conf: org.apache.hadoop.conf.<span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="type">HBASE_ZOOKEEPER</span>)</span><br><span class="line">        conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line">        conn = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br><span class="line">        table = conn.getTable(tableName).asInstanceOf[<span class="type">HTable</span>]</span><br><span class="line">        scan = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">        scan.addFamily(<span class="type">Bytes</span>.toBytes(<span class="type">TABLE_CF</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hbaseDs = env.createInput(tableInputFormat)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HadoopInputFormat"><a href="#HadoopInputFormat" class="headerlink" title="HadoopInputFormat"></a>HadoopInputFormat</h2><blockquote>
<p>对于TableInputFormat的优化,但是有一定的缺点,只能是全量的读取HBase表,不能指定rowKey去读取</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">conf.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="type">HBASE_ZOOKEEPER</span>)</span><br><span class="line">conf.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>)</span><br><span class="line">conf.set(<span class="string">&quot;hbase.defaults.for.version.skip&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">conf.set(<span class="string">&quot;mapred.output.dir&quot;</span>, <span class="string">&quot;hdfs://hadoop01:8020/demo&quot;</span>)</span><br><span class="line">conf.set(org.apache.hadoop.hbase.mapreduce.<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, <span class="string">&quot;test1&quot;</span>)</span><br><span class="line">conf.set(org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">&quot;test&quot;</span>)</span><br><span class="line">conf.setClass(<span class="string">&quot;mapreduce.job.outputformat.class&quot;</span>,</span><br><span class="line">  classOf[org.apache.hadoop.hbase.mapreduce.<span class="type">TableOutputFormat</span>[<span class="type">String</span>]],</span><br><span class="line">  classOf[org.apache.hadoop.mapreduce.<span class="type">OutputFormat</span>[<span class="type">String</span>, <span class="type">Mutation</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> job = <span class="type">Job</span>.getInstance(conf)</span><br><span class="line"><span class="keyword">val</span> hadoopIF = <span class="keyword">new</span> <span class="type">HadoopInputFormat</span>(<span class="keyword">new</span> <span class="type">TableInputFormat</span>(), classOf[<span class="type">ImmutableBytesWritable</span>], classOf[<span class="type">Result</span>], job)</span><br><span class="line"><span class="keyword">val</span> value = env.createInput(hadoopIF)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="1-在使用第三种方式HadoopInputFormat时-本地Idea运行没有问题-打包到Flink集群上运行会出现问题"><a href="#1-在使用第三种方式HadoopInputFormat时-本地Idea运行没有问题-打包到Flink集群上运行会出现问题" class="headerlink" title="1.在使用第三种方式HadoopInputFormat时,本地Idea运行没有问题,打包到Flink集群上运行会出现问题"></a>1.在使用第三种方式HadoopInputFormat时,本地Idea运行没有问题,打包到Flink集群上运行会出现问题</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">org.apache.flink.client.program.ProgramInvocationException: The main method caused an error.</span><br><span class="line">	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:546)</span><br><span class="line">	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:421)</span><br><span class="line">	at org.apache.flink.client.program.ClusterClient.run(ClusterClient.java:427)</span><br><span class="line">	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:813)</span><br><span class="line">	at org.apache.flink.client.cli.CliFrontend.runProgram(CliFrontend.java:287)</span><br><span class="line">	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:213)</span><br><span class="line">	at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:1050)</span><br><span class="line">	at org.apache.flink.client.cli.CliFrontend.lambda$main$11(CliFrontend.java:1126)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)</span><br><span class="line">	at org.apache.flink.runtime.security.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)</span><br><span class="line">	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1126)</span><br><span class="line">Caused by: java.lang.RuntimeException: Could not load the TypeInformation for the class &#39;org.apache.hadoop.io.Writable&#39;. You may be missing the &#39;flink-hadoop-compatibility&#39; dependency.</span><br><span class="line">	at org.apache.flink.api.java.typeutils.TypeExtractor.createHadoopWritableTypeInfo(TypeExtractor.java:2082)</span><br><span class="line">	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1701)</span><br><span class="line">	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:1643)</span><br><span class="line">	at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:921)</span><br><span class="line">	at org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:781)</span><br><span class="line">	at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:735)</span><br><span class="line">	at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfo(TypeExtractor.java:731)</span><br><span class="line">	at com.dev.flink.stream.hbase.develop.HBaseDemoOnFormat$$anon$3.&lt;init&gt;(HBaseDemoOnFormat.scala:66)</span><br><span class="line">	at com.dev.flink.stream.hbase.develop.HBaseDemoOnFormat$.main(HBaseDemoOnFormat.scala:66)</span><br><span class="line">	at com.dev.flink.stream.hbase.develop.HBaseDemoOnFormat.main(HBaseDemoOnFormat.scala)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:529)</span><br><span class="line">	... 12 more</span><br></pre></td></tr></table></figure>
<p><strong>解决方式:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将依赖包flink-hadoop-compatibility复制到Flink集群lib目录下</span><br><span class="line">mv flink-hadoop-compatibility_2.11-1.6.4.jar  &#x2F;usr&#x2F;local&#x2F;flink-1.7.2&#x2F;lib&#x2F;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink连接器解析UpsertKafka</title>
    <url>/2021/05/13/Flink%E8%BF%9E%E6%8E%A5%E5%99%A8%E8%A7%A3%E6%9E%90UpsertKafka/</url>
    <content><![CDATA[<blockquote>
<p>Flink的RowKind和Kafka的ChangeLog消息互转</p>
</blockquote>
<span id="more"></span>

<h2 id="消息类型"><a href="#消息类型" class="headerlink" title="消息类型"></a>消息类型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Flink</span><br><span class="line">INSERT</span><br><span class="line">UPDATE-BEFORE</span><br><span class="line">UPDATE-AFTER</span><br><span class="line">DELETE</span><br><span class="line"></span><br><span class="line"># Kafka</span><br><span class="line">UPSERT</span><br><span class="line">DELETE(Tombstone)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="顺序性"><a href="#顺序性" class="headerlink" title="顺序性"></a>顺序性</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">确保具有相同Key的消息写入到一个Partition之中</span><br><span class="line">    Flink内部分发不允许使用非Keyby的Partitioner</span><br><span class="line">    Flink-&gt;Kafka按照id &#x3D; mod(hash(key),num)的分区策略,num为Kafka分区数</span><br><span class="line"></span><br><span class="line"># 注意(并行度修改没有问题)</span><br><span class="line">UpsertKafka不允许修改分片策略</span><br><span class="line">不允许修改分区数量</span><br><span class="line">确保查询的Key和Sink表的Key是一致的</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/fsk119/flink-pageviews-demo</span></span><br><span class="line"><span class="comment"># 添加依赖项</span></span><br><span class="line"><span class="comment"># MySQL添加测试表数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> flink;</span><br><span class="line"><span class="keyword">USE</span> flink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">users</span> (</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  user_name <span class="built_in">VARCHAR</span>(<span class="number">1000</span>),</span><br><span class="line">  region <span class="built_in">VARCHAR</span>(<span class="number">1000</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">users</span> <span class="keyword">VALUES</span> </span><br><span class="line">(<span class="number">1</span>, <span class="string">&#x27;Timo&#x27;</span>, <span class="string">&#x27;Berlin&#x27;</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Beijing&#x27;</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="string">&#x27;Apple&#x27;</span>, <span class="string">&#x27;Beijing&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># SqlClient创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">users</span> (</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  user_name <span class="keyword">STRING</span>,</span><br><span class="line">  region <span class="keyword">STRING</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;mysql-cdc&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;hostname&#x27;</span> = <span class="string">&#x27;localhost&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;database-name&#x27;</span> = <span class="string">&#x27;flink&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;table-name&#x27;</span> = <span class="string">&#x27;users&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;username&#x27;</span> = <span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;password&#x27;</span> = <span class="string">&#x27;123456&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews (</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  page_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  view_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> = <span class="string">&#x27;pageviews&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> = <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;scan.startup.mode&#x27;</span> = <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;format&#x27;</span> = <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> pageviews <span class="keyword">VALUES</span></span><br><span class="line">  (<span class="number">1</span>, <span class="number">101</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>)),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">104</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:01.00&#x27;</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 案例一,创建Sink表,灌入关联数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> enriched_pageviews (</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  user_region <span class="keyword">STRING</span>,</span><br><span class="line">  page_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  view_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  WATERMARK <span class="keyword">FOR</span> view_time <span class="keyword">as</span> view_time - <span class="built_in">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">SECOND</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (user_id, page_id) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;upsert-kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> = <span class="string">&#x27;enriched_pageviews&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> = <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> = <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> = <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> enriched_pageviews</span><br><span class="line"><span class="keyword">SELECT</span> pageviews.user_id, region, pageviews.page_id, pageviews.view_time</span><br><span class="line"><span class="keyword">FROM</span> pageviews</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> <span class="keyword">users</span> <span class="keyword">ON</span> pageviews.user_id = users.user_id;</span><br><span class="line"></span><br><span class="line">kafka-console-consumer <span class="comment">--bootstrap-server mac:9092 --topic &quot;enriched_pageviews&quot; --from-beginning --property print.key=true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 案例二,聚合数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews_per_region (</span><br><span class="line">  user_region <span class="keyword">STRING</span>,</span><br><span class="line">  cnt <span class="built_in">BIGINT</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (user_region) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;upsert-kafka&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;topic&#x27;</span> = <span class="string">&#x27;pageviews_per_region&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> = <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;key.format&#x27;</span> = <span class="string">&#x27;json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;value.format&#x27;</span> = <span class="string">&#x27;json&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> pageviews_per_region</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  user_region,</span><br><span class="line">  <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> enriched_pageviews</span><br><span class="line"><span class="keyword">WHERE</span> user_region <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> user_region;</span><br><span class="line"></span><br><span class="line">kafka-console-consumer <span class="comment">--bootstrap-server mac:9092 --topic &quot;pageviews_per_region&quot; --from-beginning --property print.key=true</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="源码项"><a href="#源码项" class="headerlink" title="源码项"></a>源码项</h2><h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># 主要是将RowKind进行合并</span><br><span class="line"># BufferedUpsertSinkFunction</span><br><span class="line">invoke()-&gt;addToBuffer()-&gt;changeFlag()</span><br><span class="line"><span class="function"><span class="keyword">private</span> RowData <span class="title">changeFlag</span><span class="params">(RowData value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (value.getRowKind()) &#123;</span><br><span class="line">        <span class="keyword">case</span> INSERT:</span><br><span class="line">        <span class="keyword">case</span> UPDATE_AFTER:</span><br><span class="line">            value.setRowKind(UPDATE_AFTER);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> UPDATE_BEFORE:</span><br><span class="line">        <span class="keyword">case</span> DELETE:</span><br><span class="line">            value.setRowKind(DELETE);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># 将读取的数据转换为对应的格式</span><br><span class="line"># DynamicKafkaDeserializationSchema</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deserialize</span><span class="params">(ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record, Collector&lt;RowData&gt; collector)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 没有Key并且没有MetaData</span></span><br><span class="line">    <span class="keyword">if</span> (keyDeserialization == <span class="keyword">null</span> &amp;&amp; !hasMetadata) &#123;</span><br><span class="line">        valueDeserialization.deserialize(record.value(), collector);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// key</span></span><br><span class="line">    <span class="keyword">if</span> (keyDeserialization != <span class="keyword">null</span>) &#123;</span><br><span class="line">        keyDeserialization.deserialize(record.key(), keyCollector);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// project output while emitting values</span></span><br><span class="line">    outputCollector.inputRecord = record;</span><br><span class="line">    outputCollector.physicalKeyRows = keyCollector.buffer;</span><br><span class="line">    outputCollector.outputCollector = collector;</span><br><span class="line">    <span class="keyword">if</span> (record.value() == <span class="keyword">null</span> &amp;&amp; upsertMode) &#123;</span><br><span class="line">        <span class="comment">// Kafka的墓碑信息,value为null</span></span><br><span class="line">        outputCollector.collect(<span class="keyword">null</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        valueDeserialization.deserialize(record.value(), outputCollector);</span><br><span class="line">    &#125;</span><br><span class="line">    keyCollector.buffer.clear();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Value的emit在OutputProjectionCollector</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">collect</span><span class="params">(RowData physicalValueRow)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// no key defined</span></span><br><span class="line">    <span class="keyword">if</span> (keyProjection.length == <span class="number">0</span>) &#123;</span><br><span class="line">        emitRow(<span class="keyword">null</span>, (GenericRowData) physicalValueRow);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// otherwise emit a value for each key</span></span><br><span class="line">    <span class="keyword">for</span> (RowData physicalKeyRow : physicalKeyRows) &#123;</span><br><span class="line">        emitRow((GenericRowData) physicalKeyRow, (GenericRowData) physicalValueRow);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">emitRow</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="meta">@Nullable</span> GenericRowData physicalKeyRow,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="meta">@Nullable</span> GenericRowData physicalValueRow)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> RowKind rowKind;</span><br><span class="line">    <span class="keyword">if</span> (physicalValueRow == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (upsertMode) &#123;</span><br><span class="line">            rowKind = RowKind.DELETE;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> DeserializationException(</span><br><span class="line">                    <span class="string">&quot;Invalid null value received in non-upsert mode. Could not to set row kind for output record.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        rowKind = physicalValueRow.getRowKind();</span><br><span class="line">    &#125;</span><br><span class="line">    ......</span><br><span class="line">    outputCollector.collect(producedRow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink集成Prometheus与Grafana</title>
    <url>/2020/01/07/Flink%E9%9B%86%E6%88%90Prometheus%E4%B8%8EGrafana/</url>
    <content><![CDATA[<blockquote>
<p>监控Flink任务情况</p>
</blockquote>
<span id="more"></span>

<h2 id="下载软件"><a href="#下载软件" class="headerlink" title="下载软件"></a>下载软件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grafana</span><br><span class="line">node_exporter</span><br><span class="line">prometheus</span><br><span class="line">pushgateway</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink方面修改"><a href="#Flink方面修改" class="headerlink" title="Flink方面修改"></a>Flink方面修改</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 复制opt&#x2F;flink-metrics-prometheus到lib目录</span><br><span class="line"># 修改conf&#x2F;flink-conf.yaml</span><br><span class="line">metrics.reporter.promgateway.class: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter</span><br><span class="line">metrics.reporter.promgateway.host: hadoop01</span><br><span class="line">metrics.reporter.promgateway.port: 9091</span><br><span class="line">metrics.reporter.promgateway.jobName: myJob</span><br><span class="line">metrics.reporter.promgateway.randomJobNameSuffix: true</span><br><span class="line">metrics.reporter.promgateway.deleteOnShutdown: false</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="prometheus-yml"><a href="#prometheus-yml" class="headerlink" title="prometheus.yml"></a>prometheus.yml</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#39;prometheus&#39;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;hadoop01:9090&#39;]</span><br><span class="line">        labels:</span><br><span class="line">          instance: &#39;prometheus&#39;</span><br><span class="line">  - job_name: &#39;linux&#39;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;hadoop01:9100&#39;]</span><br><span class="line">        labels:</span><br><span class="line">          instance: &#39;hadoop01&#39;</span><br><span class="line">  - job_name: &#39;pushgateway&#39;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;hadoop01:9091&#39;]</span><br><span class="line">        labels:</span><br><span class="line">          instance: &#39;pushgateway&#39;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># pushgateway</span><br><span class="line">.&#x2F;pushgateway &amp;</span><br><span class="line">.&#x2F;pushgateway --web.enable-lifecycle --web.enable-admin-api &amp;</span><br><span class="line"></span><br><span class="line"># node_exporter</span><br><span class="line">.&#x2F;node_exporter &amp;</span><br><span class="line"></span><br><span class="line"># prometheus</span><br><span class="line">.&#x2F;prometheus --config.file&#x3D;.&#x2F;conf&#x2F;prometheus.yml &amp;</span><br><span class="line"></span><br><span class="line"># grafana</span><br><span class="line">.&#x2F;bin&#x2F;grafana-server web &amp;</span><br><span class="line">username&#x2F;password: admin&#x2F;admin</span><br><span class="line"></span><br><span class="line"># 注意版本问题</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="针对pushgateway的优化"><a href="#针对pushgateway的优化" class="headerlink" title="针对pushgateway的优化"></a>针对pushgateway的优化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 版本为1.0.1,低版本并不会主动去清除group信息</span><br><span class="line"># 哪怕是很久没有进行push数据了,也不会清除</span><br><span class="line"># 需要自己写脚本定时去清除所有的group信息</span><br><span class="line">curl -X PUT http:&#x2F;&#x2F;hadoop01:9091&#x2F;api&#x2F;v1&#x2F;admin&#x2F;wipe</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="图表的设置"><a href="#图表的设置" class="headerlink" title="图表的设置"></a>图表的设置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">可以直接去Grafana官网导入,也可以自己写</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>prometheus</tag>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink面试基础</title>
    <url>/2020/02/27/Flink%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>记录面试中有关Flink的概念知识</p>
</blockquote>
<span id="more"></span>

<h1 id="Flink面试基础"><a href="#Flink面试基础" class="headerlink" title="Flink面试基础"></a>Flink面试基础</h1><h1 id="Flink时间类型有哪些？"><a href="#Flink时间类型有哪些？" class="headerlink" title="Flink时间类型有哪些？"></a>Flink时间类型有哪些？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">三种时间类型</span><br><span class="line">Event Time事件生成时间，事件实际产生的时间</span><br><span class="line">Ingestion Time事件接入时间，进入Flink系统的时间</span><br><span class="line">Processing Time事件处理时间，Flink处理的时间</span><br><span class="line"></span><br><span class="line">事件处理时间处理Window时，使用机器自身时间</span><br><span class="line">可能因为机器时钟不同步导致乱序，适用于时间精确度不高的计算</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink的窗口类型有哪些？"><a href="#Flink的窗口类型有哪些？" class="headerlink" title="Flink的窗口类型有哪些？"></a>Flink的窗口类型有哪些？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两种窗口类型</span><br><span class="line">    基于时间的Time Window</span><br><span class="line">    基于数量的Count Window</span><br><span class="line">窗口的操作又可以分为</span><br><span class="line">Tumbling Window滚动窗口</span><br><span class="line">Sliding Window滑动窗口</span><br><span class="line">Session Window会话窗口</span><br><span class="line">Global Window全局窗口</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink定义窗口的组件？"><a href="#Flink定义窗口的组件？" class="headerlink" title="Flink定义窗口的组件？"></a>Flink定义窗口的组件？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Assigner 决定某个元素被分配到哪个&#x2F;哪些窗口中去</span><br><span class="line">Trigger 决定一个窗口何时能够被计算或清除，每个窗口都会拥有一个自己的Trigger</span><br><span class="line">Evictor 在Trigger触发之后，窗口处理之前，Evictor会用来剔除窗口中不需要的元素，相当于一个filter</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink窗口执行过程？"><a href="#Flink窗口执行过程？" class="headerlink" title="Flink窗口执行过程？"></a>Flink窗口执行过程？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Assigner，Trigger，Evictor都位于一个算子(Window Operator)</span><br><span class="line">数据流进入算子，每一个到达的元素都交给Assigner</span><br><span class="line">Assigner决定元素被放到哪个&#x2F;些窗口，可能会创建新窗口</span><br><span class="line">一个元素可以被放入多个窗口中，所以存在多个窗口是可能的</span><br><span class="line">每个窗口都拥有自己的Trigger</span><br><span class="line">Trigger有定时器，决定窗口何时被计算或清除</span><br><span class="line">每当有元素加入该窗口或之前注册的定时器超时了，Trigger都会被调用</span><br><span class="line">Trigger的返回结果可以是Continue(不做任何操作)，Fire(处理窗口数据)，Purge(移除窗口和窗口中数据)，或者Fire+Purge</span><br><span class="line">一个Trigger的调用结果只是Fire的话，会计算窗口并保留窗口原样，窗口数据不变，等待下次Fire时再计算</span><br><span class="line">一个窗口可以被重复计算多次直到被Purge</span><br><span class="line">在Purge之前，窗口会一直占用内存</span><br><span class="line">当Trigger Fire了，窗口中元素集合交给Evictor(如果有)</span><br><span class="line">Evictor主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要移除</span><br><span class="line">剩余元素会交给用户指定函数进行窗口的计算</span><br><span class="line">如果没有Evictor，窗口中所有元素会一起交给函数进行计算</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink的状态后端？"><a href="#Flink的状态后端？" class="headerlink" title="Flink的状态后端？"></a>Flink的状态后端？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">3种状态后端</span><br><span class="line">内存 MemoryStateBackend</span><br><span class="line">文件系统 FsStateBackend</span><br><span class="line">RocksDB RocksDBStateBackend</span><br><span class="line">只有RocksDB状态后端支持增量检查点，默认关闭</span><br><span class="line">state.backend: rocksdb</span><br><span class="line">state.backend.incremental: true</span><br><span class="line">RocksDBStateBackend rocksDBStateBackend &#x3D; new RocksDBStateBackend(&quot;hdfs:&#x2F;&#x2F;path&#x2F;to&#x2F;flink-checkpoints&quot;, true);</span><br><span class="line">env.setStateBackend(rocksDBStateBackend);</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink中RocksDB的优缺点？"><a href="#Flink中RocksDB的优缺点？" class="headerlink" title="Flink中RocksDB的优缺点？"></a>Flink中RocksDB的优缺点？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优:</span><br><span class="line">    如果状态空间超大，增量检查点只包含本次ck与上次ck之间的差异</span><br><span class="line">    而不是所有状态，变得轻量级，解决了大状态CK问题</span><br><span class="line">    上一个检查点已经存在的文件可以直接引用，不被引用的文件可以及时删除</span><br><span class="line">缺:</span><br><span class="line">    程序出现问题，TM需要从多个检查点加载状态数据，这些数据可能包括被删除的状态</span><br><span class="line">    旧检查点文件不能随便删除，因为新检查点仍然会引用他们，贸然删除，程序可能无法恢复</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink的状态类型有哪些？"><a href="#Flink的状态类型有哪些？" class="headerlink" title="Flink的状态类型有哪些？"></a>Flink的状态类型有哪些？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基本的有OperatorState和KeyedState</span><br><span class="line">特殊的有BroadcastState</span><br><span class="line">OperatorState跟一个特定Operator的并发实例绑定，整个Operator只对应一个State，有多少状态由并行度决定</span><br><span class="line">KeyedState是基于KeyStream上的状态，KeyBy之后的OperatorState，与并行度无关了，有多少状态由KeyBy之后有多少Key决定</span><br><span class="line">BroadcastState是1.5开始出现的，用于以特定方式组合和联合处理两个事件流。</span><br><span class="line">    A流的事件被广播到一个算子的所有并行实例，该算子将它们保存为状态。</span><br><span class="line">    B流的事件不广播，发送给同一个算子的单个实例，并与广播流的事件一起处理。</span><br><span class="line">    BroadcastState保存在内存中</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink如何处理延迟数据？"><a href="#Flink如何处理延迟数据？" class="headerlink" title="Flink如何处理延迟数据？"></a>Flink如何处理延迟数据？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink窗口处理流式数据虽然提供了基础EventTime的WaterMark机制，但是只能在一定程序上解决，如果数据延迟超过了WaterMark设置的时间，数据会被Flink丢弃。</span><br><span class="line">可以通过设置AllowedLateness+OutputTag来处理这些严重迟到的数据。默认是永不超时。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink中ManagedState和RawState区别？"><a href="#Flink中ManagedState和RawState区别？" class="headerlink" title="Flink中ManagedState和RawState区别？"></a>Flink中ManagedState和RawState区别？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是KeyedState和OperatorState的两种存在形式</span><br><span class="line">ManagedState，托管状态，由Flink框架管理的状态</span><br><span class="line">通过框架提供的接口来更行和管理状态的值</span><br><span class="line">不需要序列化</span><br><span class="line">RawState，原始状态，由用户自行管理的具体数据结构</span><br><span class="line">Flink在做CK时，使用byte[]来读写状态内容，对其内部数据结构一无所知</span><br><span class="line">需要序列化</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink的KeyedState和OperatorState的区别？"><a href="#Flink的KeyedState和OperatorState的区别？" class="headerlink" title="Flink的KeyedState和OperatorState的区别？"></a>Flink的KeyedState和OperatorState的区别？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KeyedState</span><br><span class="line">    只适用于KeyedStream上的算子</span><br><span class="line">    每个Key对应一个状态</span><br><span class="line">    重写RichFunction，通过里面的RuntimeContext访问</span><br><span class="line">    状态随着Key自动在多个算子子任务上迁移</span><br><span class="line">    支持ValueState，ListState，MapSate等数据结构</span><br><span class="line">OperatorState</span><br><span class="line">    可以用于所有算子</span><br><span class="line">    一个算子子任务对应一个状态</span><br><span class="line">    通过实现CheckpointedFunction等接口访问</span><br><span class="line">    有多种状态重新分配方式</span><br><span class="line">    支持ListState，BroadcastSate等数据结构</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink的Join优化？"><a href="#Flink的Join优化？" class="headerlink" title="Flink的Join优化？"></a>Flink的Join优化？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink处理数据时，每台机器只是存储了集群的部分数据。</span><br><span class="line">为了执行Join，Flink需要找到两个数据集的所有满足Join条件的数据。</span><br><span class="line">Flink需要将两个数据集有相同Key的数据发送到同一台机器上</span><br><span class="line">两种策略：</span><br><span class="line">repartition-repartition strategy</span><br><span class="line">    两个数据集都会使用Key进行重分区并通过网络传输</span><br><span class="line">broadcast-forward strategy</span><br><span class="line">    一个数据集不动，另一个数据集会复制到有第一个数据集部分数据的所有机器上</span><br><span class="line">    ds1.join(ds2,JoinHint.BROADCAST_HASH_FIRST)</span><br><span class="line">    第二个参数就是提示</span><br><span class="line">    BROADCAST_HASH_FIRST：第一个数据集是较小的数据集</span><br><span class="line">    BROADCAST_HASH_SECOND：第二个数据集是较小的数据集</span><br><span class="line">    REPARTITION_HASH_FIRST：第一个数据集是较小的数据集</span><br><span class="line">    REPARTITION_HASH_SECOND：第二个数据集是较小的数据集</span><br><span class="line">    REPARTITION_SORT_MERGE：对数据集进行重分区，同时使用sort和merge策略</span><br><span class="line">    OPTIMIZER_CHOOSES：Flink的优化器决定两个数据集如何join</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink自定义Sink和Source写法？"><a href="#Flink自定义Sink和Source写法？" class="headerlink" title="Flink自定义Sink和Source写法？"></a>Flink自定义Sink和Source写法？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">自定义Sink，继承RichSinkFunction，实现数据写入</span><br><span class="line">自定义Source，继承RichSourceFunction，实现数据读取</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink自定义UDF函数写法？"><a href="#Flink自定义UDF函数写法？" class="headerlink" title="Flink自定义UDF函数写法？"></a>Flink自定义UDF函数写法？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">UDF:继承ScalarFunction</span><br><span class="line">UDAF:继承AggregateFunction</span><br><span class="line">UDTF:继承TableFunction</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink如何保证exactly-once？"><a href="#Flink如何保证exactly-once？" class="headerlink" title="Flink如何保证exactly-once？"></a>Flink如何保证exactly-once？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">通过Flink的CK机制保证唯一</span><br><span class="line">Barrier插入到数据流中，作为数据流的一部分和数据一起向下流动</span><br><span class="line">Barrier不会干扰正常数据，每个Barrier都带有快照ID</span><br><span class="line">多个不同快照的多个Barrier会在流中同时出现</span><br><span class="line">当所有的OperatorTask成功存储了它们的状态，一个检查点才算完成</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink反压如何解决的？"><a href="#Flink反压如何解决的？" class="headerlink" title="Flink反压如何解决的？"></a>Flink反压如何解决的？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">示例：</span><br><span class="line">    A进入FLink，被Task1处理</span><br><span class="line">    记录被序列化进缓冲区</span><br><span class="line">    缓冲区的数据被移动到Task2，Task2会从缓冲区内读取记录</span><br><span class="line">    Task1在其输出端分配了一个缓冲区，Task2在其输入端也有一个</span><br><span class="line">本地传输：</span><br><span class="line">    如果Task1和Task2在同一个TM，缓冲区可以直接共享</span><br><span class="line">    一旦Task2消费了数据它会被回收</span><br><span class="line">    如果Task2比Task1慢，缓冲区会以比Task1填充速度更慢的速度进行回收</span><br><span class="line">    从而使Task1降速</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink异步IO读写的情况？"><a href="#Flink异步IO读写的情况？" class="headerlink" title="Flink异步IO读写的情况？"></a>Flink异步IO读写的情况？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink的Async I&#x2F;O允许用户将异步请求客户端与数据流一起使用</span><br><span class="line">操作:</span><br><span class="line">    实现AsyncFunction调度请求</span><br><span class="line">    一个结果ResultFuture的回调</span><br><span class="line">    在数据流上应用异步IO操作作为转换</span><br><span class="line">&#x2F;**</span><br><span class="line"> * AsyncFunction的一个实现，它发送请求并设置回调</span><br><span class="line"> *&#x2F;</span><br><span class="line">class AsyncDatabaseRequest extends AsyncFunction[String, (String, String)] &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;** 可以发出带有回调的并发请求的特定数据库的客户端 *&#x2F;</span><br><span class="line">    lazy val client: DatabaseClient &#x3D; new DatabaseClient(host, post, credentials)</span><br><span class="line"></span><br><span class="line">    &#x2F;** 用于将来回调的上下文 *&#x2F;</span><br><span class="line">    implicit lazy val executor: ExecutionContext &#x3D; ExecutionContext.fromExecutor(Executors.directExecutor())</span><br><span class="line"></span><br><span class="line">    override def asyncInvoke(str: String, resultFuture: ResultFuture[(String, String)]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 发出异步请求，接收结果的Future</span><br><span class="line">        val resultFutureRequested: Future[String] &#x3D; client.query(str)</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 将回调设置为在客户端请求完成后执行</span><br><span class="line">        &#x2F;&#x2F; 回调只是将结果转发到结果Future</span><br><span class="line">        resultFutureRequested.onSuccess &#123;</span><br><span class="line">            case result: String &#x3D;&gt; resultFuture.complete(Iterable((str, result)))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 创建原始流</span><br><span class="line">val stream: DataStream[String] &#x3D; ...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 应用异步I&#x2F;O转换</span><br><span class="line">val resultStream: DataStream[(String, String)] &#x3D;</span><br><span class="line">    AsyncDataStream.unorderedWait(stream, new AsyncDatabaseRequest(), 1000, TimeUnit.MILLISECONDS, 100)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink的CK和SP的区别是什么？"><a href="#Flink的CK和SP的区别是什么？" class="headerlink" title="Flink的CK和SP的区别是什么？"></a>Flink的CK和SP的区别是什么？</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CK的生命周期由Flink管理，即Flink创建，拥有和发布CK，无需用户交互，轻量级，快速恢复</span><br><span class="line">SP由用户创建，拥有和删除，是计划的，手动备份的，可作为恢复。像Flink版本更新，更改JobGraph，更改并行度</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>GP数据加载工具应用</title>
    <url>/2021/04/14/GP%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%B7%A5%E5%85%B7%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>主要介绍gpload进行数据导入操作流程</p>
</blockquote>
<span id="more"></span>

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gpload是对gpfdist的封装,使用前必须开启gpfdist服务</span><br><span class="line"></span><br><span class="line"># 创建加载数据的表,用于将数据导入到该表</span><br><span class="line">CREATE TABLE demo_load(</span><br><span class="line">    id int,</span><br><span class="line">    name text</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用gpload"><a href="#使用gpload" class="headerlink" title="使用gpload"></a>使用gpload</h2><h3 id="执行命令"><a href="#执行命令" class="headerlink" title="执行命令"></a>执行命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi load.yml</span><br><span class="line">VERSION: 1.0.0.1</span><br><span class="line">DATABASE: gpdb</span><br><span class="line">USER: gpadmin</span><br><span class="line">HOST: 192.168.157.128</span><br><span class="line">PORT: 5432</span><br><span class="line">GPLOAD:</span><br><span class="line">   INPUT:</span><br><span class="line">    - SOURCE:</span><br><span class="line">         LOCAL_HOSTNAME:</span><br><span class="line">           - master</span><br><span class="line">         PORT: 5432</span><br><span class="line">         FILE:</span><br><span class="line">           - &#x2F;home&#x2F;gpadmin&#x2F;load_files&#x2F;demo.txt</span><br><span class="line">    - COLUMNS:</span><br><span class="line">               - id: int</span><br><span class="line">               - name: text</span><br><span class="line">    - FORMAT: text</span><br><span class="line">    - DELIMITER: &#39;|&#39;</span><br><span class="line">    - QUOTE: &#39;&quot;&#39;</span><br><span class="line">    - HEADER: false</span><br><span class="line">    - ERROR_LIMIT: 25</span><br><span class="line">    - ERROR_TABLE: public.gpload_err</span><br><span class="line">   OUTPUT:</span><br><span class="line">    - TABLE: public.demo_load</span><br><span class="line">    - MODE: INSERT</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">gpload -f load.yml</span><br></pre></td></tr></table></figure>

<h3 id="参数含义"><a href="#参数含义" class="headerlink" title="参数含义"></a>参数含义</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">VERSION: 1.0.0.1            --指定控制文件schema的版本</span><br><span class="line">DATABASE: db_name           --指定连接数据库的名字,如果没有指定,由环境变量$PGDATABASE,或者通过gpload参数-d指定</span><br><span class="line">USER: db_username           --指定连接目标数据库的用户名,如果不使用超级管理员,服务参数gp_external_grant_privileges必须设置成on</span><br><span class="line">HOST: master_hostname       --指定master主机名,也可以通过gpload的-h选项,或者环境变量$PGHOST指定</span><br><span class="line">PORT: master_port           --指定master的连接端口号,默认是5432,或者通过gpload命令的-p选项或者环境变量$PGPORT指定</span><br><span class="line">GPLOAD:                     --必须指定,表示装载设置部分在它下面必须定义INPUT:和OUTPUT:两个部分</span><br><span class="line">INPUT:                      --必须指定,这部分指定装载数据的格式和位置</span><br><span class="line">- SOURCE:                   --必须指定,定义source文件的位置,每个输入部分可以定义多个source部分, windows路径的指定比较特别,比如c:\要写成c:&#x2F;</span><br><span class="line">LOCAL_HOSTNAME:             --指定gpload运行的主机名称和ip地址,如果有多块网卡,可以同时使用它们,提高装载速度.默认只使用首选主机名和IP</span><br><span class="line">- hostname_or_ip</span><br><span class="line">PORT: http_port             --指定gpfdist使用的端口,也可以选择端口范围,由系统选择,如果同时指定,port设置优先级高</span><br><span class="line">| PORT_RANGE: [start_port_range, end_port_range]</span><br><span class="line">FILE:                       --指定装载数据文件的位置,目录或者命名管道.如果文件使用gpzip或者bzip2进行了压缩,它可以自动解压.可以使用通配符*和C语言风格的关系匹配模式指定多个文件</span><br><span class="line">- &#x2F;path&#x2F;to&#x2F;input_file</span><br><span class="line">- COLUMNS:                    --指定数据源的数据格式,如果没有指定这部分,source表的列顺序,数量,以及数据类型必须与目标表一致</span><br><span class="line">- field_name: data_type</span><br><span class="line">- FORMAT: text | csv          --指定文件格式是text还是csv</span><br><span class="line">- DELIMITER: &#39;delimiter_character&#39;  --指定文本数据域(列)之间的分割符,默认是|</span><br><span class="line">- ESCAPE: &#39;escape_character&#39; | &#39;OFF&#39;  --text定义转义字符,text格式默认是\,在text格式中可以选择off关掉转义字符(web log处理时比较有用)</span><br><span class="line">- NULL_AS: &#39;null_string&#39;       --指定描述空值的字符串,text格式默认是\N,csv格式不使用转义符号的空值</span><br><span class="line">- FORCE_NOT_NULL: true | false --csv格式,强制所有字符默认都用&quot;&quot;括起,因此不能有空值,如果两个分割符之间没有值,被当做0长度字符串,认为值已经丢失</span><br><span class="line">- QUOTE: &#39;csv_quote_character&#39;  --csv指定转义字符,默认是&quot;</span><br><span class="line">- HEADER: true | false          --是否跳过数据文件第一行,当做表头</span><br><span class="line">- ENCODING: database_encoding   --指定数据源的字符集</span><br><span class="line">- ERROR_LIMIT: integer          --指定由于不符合格式数据记录的上限,如果超过该上限,gpload停止装载,否则正确记录可以被装载,错误记录抛出写入错误表.但它仅支持数据格式错误,不支持违背约束的问题</span><br><span class="line">- ERROR_TABLE: schema.table_name --指定不符合格式要求记录的错误表.如果指定的表不存在系统自动创建</span><br><span class="line"></span><br><span class="line">OUTPUT:</span><br><span class="line">- TABLE: schema.table_name       --指定装载的目标表</span><br><span class="line">- MODE: insert | update | merge  --指定操作模式，默认是insert.merge操作不支持使用随机分布策略的表</span><br><span class="line">- MATCH_COLUMNS:                 --为update操作和merge操作指定匹配条件</span><br><span class="line">     - target_column_name            </span><br><span class="line">- UPDATE_COLUMNS:                 --为update操作和merge操作指定更新的列</span><br><span class="line">     - target_column_name</span><br><span class="line">- UPDATE_CONDITION: &#39;boolean_condition&#39;  --指定where条件,目标表中只有满足条件的记录才能更改,(merge情况下,只有满足条件的记录才能insert)</span><br><span class="line">- MAPPING:                        --指定source列和目标列的映射关系</span><br><span class="line">target_column_name: source_column_name | &#39;expression&#39;</span><br><span class="line">PRELOAD:                          --指定load之前的操作</span><br><span class="line">- TRUNCATE: true | false          --如果设置成true,装载之前先删除目标表中所有记录,再装载</span><br><span class="line">- REUSE_TABLES: true | false     --设置成true，不会删除外部表对象会这中间表对象。从而提升性能</span><br><span class="line">SQL:</span><br><span class="line">- BEFORE: &quot;sql_command&quot;         --装载操作开始前执行的SQL，比如写日志表</span><br><span class="line">- AFTER: &quot;sql_command&quot;          --装载操作之后执行的SQL</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">每次gpload之后都会产生一个外部表,查看表是又连接也会失效</span><br><span class="line">did not find an external table to reuse.</span><br><span class="line">需要手动指定新的gpfdist文件服务目录</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume的使用</title>
    <url>/2017/11/18/Flume%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>Flume配置文件与使用介绍</p>
</blockquote>
<span id="more"></span>

<h2 id="端口监听"><a href="#端口监听" class="headerlink" title="端口监听"></a>端口监听</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建配置文件</span><br><span class="line">vi netcat-logger.conf</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line">a1.sources.r1.type &#x3D; netcat</span><br><span class="line">a1.sources.r1.bind &#x3D; localhost</span><br><span class="line">a1.sources.r1.port &#x3D; 44444</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 启动服务</span><br><span class="line">bin&#x2F;flume-ng agent --conf conf --conf-file conf&#x2F;netcat-logger.conf --name a1 -Dflume.root.logger&#x3D;INFO,console</span><br><span class="line"></span><br><span class="line"># 启动监听端口</span><br><span class="line">telnet localhost 44444</span><br><span class="line"></span><br><span class="line"># 传入数据</span><br><span class="line">$ telnet localhost 44444</span><br><span class="line">Trying 127.0.0.1...</span><br><span class="line">Connected to localhost.localdomain (127.0.0.1).</span><br><span class="line">Escape character is &#39;^]&#39;.</span><br><span class="line">Hello world! &lt;ENTER&gt;</span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="文件夹-lt-往文件夹内放文件-不要生成文件-gt"><a href="#文件夹-lt-往文件夹内放文件-不要生成文件-gt" class="headerlink" title="文件夹&lt;往文件夹内放文件,不要生成文件&gt;"></a>文件夹&lt;往文件夹内放文件,不要生成文件&gt;</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建配置文件</span><br><span class="line">vi spool-logger.conf</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; spooldir</span><br><span class="line">#被监视的文件夹</span><br><span class="line">a1.sources.r1.spoolDir &#x3D; &#x2F;home&#x2F;hadoop&#x2F;flumespool</span><br><span class="line">a1.sources.r1.fileHeader &#x3D; true</span><br><span class="line">a1.sinks.k1.type &#x3D; logger</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 启动服务</span><br><span class="line">bin&#x2F;flume-ng agent -c .&#x2F;conf -f .&#x2F;conf&#x2F;spool-logger.conf -n a1 -Dflume.root.logger&#x3D;INFO,console</span><br><span class="line"></span><br><span class="line"># 传入数据</span><br><span class="line">mv xxxFile.log &#x2F;home&#x2F;hadoop&#x2F;flumeSpool&#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="tail命令toHDFS"><a href="#tail命令toHDFS" class="headerlink" title="tail命令toHDFS"></a>tail命令toHDFS</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建配置文件</span><br><span class="line">vi tail-hdfs.conf</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line">#exec 指的是命令</span><br><span class="line"># Describe&#x2F;configure the source</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">#F根据文件名追中, f根据文件的nodeid追中</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;home&#x2F;hadoop&#x2F;log&#x2F;test.log</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">#下沉目标</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line">#指定目录, flum帮做目的替换</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; &#x2F;flume&#x2F;events&#x2F;%y-%m-%d&#x2F;%H%M&#x2F;</span><br><span class="line">#文件的命名, 前缀</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; events-</span><br><span class="line">#10 分钟就改目录</span><br><span class="line">a1.sinks.k1.hdfs.round &#x3D; true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue &#x3D; 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit &#x3D; minute</span><br><span class="line">#文件滚动之前的等待时间(秒)</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3</span><br><span class="line">#文件滚动的大小限制(bytes)</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 500</span><br><span class="line">#写入多少个event数据后滚动文件(事件个数)</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 20</span><br><span class="line">#5个事件就往里面写入</span><br><span class="line">a1.sinks.k1.hdfs.batchSize &#x3D; 5</span><br><span class="line">#用本地时间格式化目录</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#下沉后, 生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 启动服务</span><br><span class="line">bin&#x2F;flume-ng agent -c conf -f conf&#x2F;tail-hdfs.conf -n a1</span><br><span class="line"></span><br><span class="line"># 传入数据</span><br><span class="line">mkdir &#x2F;home&#x2F;hadoop&#x2F;log</span><br><span class="line">while true</span><br><span class="line">do</span><br><span class="line">echo 111111 &gt;&gt; &#x2F;home&#x2F;hadoop&#x2F;log&#x2F;test.log</span><br><span class="line">sleep 0.5</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="tail命令toAVRO"><a href="#tail命令toAVRO" class="headerlink" title="tail命令toAVRO"></a>tail命令toAVRO</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 第一台服务器</span><br><span class="line"># 创建配置文件</span><br><span class="line">vi tail-avro.conf</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line">a1.sources.r1.type &#x3D; exec</span><br><span class="line">a1.sources.r1.command &#x3D; tail -F &#x2F;home&#x2F;hadoop&#x2F;flumelog&#x2F;test.log</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line"># 绑定的不是本机, 是另外一台机器的服务地址, sink端的avro是一个发送端, avro的客户端, 往Hatsune-01这个机器上发</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.sinks.k1.type &#x3D; avro</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line">a1.sinks.k1.hostname &#x3D; 主机名</span><br><span class="line">a1.sinks.k1.port &#x3D; 4141</span><br><span class="line">a1.sinks.k1.batch-size &#x3D; 2</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 启动服务</span><br><span class="line">bin&#x2F;flume-ng agent -c conf -f conf&#x2F;tail-avro.conf -n a1</span><br><span class="line"></span><br><span class="line"># 第二台服务器</span><br><span class="line"># 创建配置文件</span><br><span class="line">vi avro-hdfs.conf</span><br><span class="line">a1.sources &#x3D; r1</span><br><span class="line">a1.sinks &#x3D; k1</span><br><span class="line">a1.channels &#x3D; c1</span><br><span class="line"># source中的avro组件是接收者服务, 绑定本机</span><br><span class="line">a1.sources.r1.type &#x3D; avro</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sources.r1.bind &#x3D; 0.0.0.0</span><br><span class="line">a1.sources.r1.port &#x3D; 4141</span><br><span class="line">a1.sinks.k1.type &#x3D; hdfs</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line">#指定目录, flum帮做目的替换</span><br><span class="line">a1.sinks.k1.hdfs.path &#x3D; &#x2F;flume&#x2F;events&#x2F;%y-%m-%d&#x2F;%H%M&#x2F;</span><br><span class="line">#文件的命名, 前缀</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix &#x3D; events-</span><br><span class="line">#10 分钟就改目录</span><br><span class="line">a1.sinks.k1.hdfs.round &#x3D; true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue &#x3D; 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit &#x3D; minute</span><br><span class="line">#文件滚动之前的等待时间(秒)</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval &#x3D; 3</span><br><span class="line">#文件滚动的大小限制(bytes)</span><br><span class="line">a1.sinks.k1.hdfs.rollSize &#x3D; 500</span><br><span class="line">#写入多少个event数据后滚动文件(事件个数)</span><br><span class="line">a1.sinks.k1.hdfs.rollCount &#x3D; 20</span><br><span class="line">#5个事件就往里面写入</span><br><span class="line">a1.sinks.k1.hdfs.batchSize &#x3D; 5</span><br><span class="line">#用本地时间格式化目录</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp &#x3D; true</span><br><span class="line">#下沉后, 生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType &#x3D; DataStream</span><br><span class="line">a1.channels.c1.type &#x3D; memory</span><br><span class="line">a1.channels.c1.capacity &#x3D; 1000</span><br><span class="line">a1.channels.c1.transactionCapacity &#x3D; 100</span><br><span class="line">a1.sources.r1.channels &#x3D; c1</span><br><span class="line">a1.sinks.k1.channel &#x3D; c1</span><br><span class="line"></span><br><span class="line"># 启动服务</span><br><span class="line">bin&#x2F;flume-ng agent -c conf -f conf&#x2F;avro-hdfs.conf -n a1</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title>GP中使用dblink操作</title>
    <url>/2021/04/15/GP%E4%B8%AD%E4%BD%BF%E7%94%A8dblink%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<blockquote>
<p>使用dblink操作GP数据库</p>
</blockquote>
<span id="more"></span>

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看Greenplum的psql版本</span><br><span class="line">select version();</span><br><span class="line">PostgreSQL 9.4.24 (Greenplum Database 6.0.0-beta.1 build dev)</span><br><span class="line"></span><br><span class="line">--- 我这个版本GP自带了dblink</span><br><span class="line"># 对于自带dblink的高版本GP执行下面命令即可使用</span><br><span class="line">psql</span><br><span class="line">CREATE EXTENSION dblink;</span><br><span class="line"></span><br><span class="line">--- 低版本操作</span><br><span class="line"># 官网下载postgresql源码(如果没有带)</span><br><span class="line">https:&#x2F;&#x2F;www.postgresql.org&#x2F;ftp&#x2F;source&#x2F;</span><br><span class="line"></span><br><span class="line"># 编译</span><br><span class="line">cd &#x2F;opt</span><br><span class="line">tar -zxvf postgresql-*.tar.gz</span><br><span class="line">cd postgresql-*&#x2F;contrib&#x2F;dblink</span><br><span class="line"># 修改Makefile(忽略警告)</span><br><span class="line">PG_CPPFLAGS &#x3D; -I$(libpq_srcdir) -w</span><br><span class="line"># 开始编译</span><br><span class="line">make USE_PGXS&#x3D;1 install</span><br><span class="line"></span><br><span class="line"># 将生成的dblink.so文件复制到各个节点下</span><br><span class="line">cp dblink.so &#x2F;usr&#x2F;local&#x2F;gpdb&#x2F;lib&#x2F;postgresql&#x2F;</span><br><span class="line"></span><br><span class="line"># 加载dblink方法</span><br><span class="line">psql -f dblink.sql gpdb</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用操作"><a href="#使用操作" class="headerlink" title="使用操作"></a>使用操作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置连接</span><br><span class="line">--- 本地可以简写,不需要hostaddr,port,user,password</span><br><span class="line">select dblink_connect(&#39;localconn&#39;,&#39;dbname&#x3D;gpdb&#39;);</span><br><span class="line">--- 远程必须写全</span><br><span class="line">select dblink_connect(&#39;disconn&#39;,&#39;hostaddr&#x3D;192.168.157.128 port&#x3D;5432 dbname&#x3D;gpdb user&#x3D;gpadmin password&#x3D;123456&#39;);</span><br><span class="line"></span><br><span class="line"># 执行查询操作</span><br><span class="line">select * from dblink(&#39;localconn&#39;,&#39;select * from public.demo&#39;) as a(a int,b text);</span><br><span class="line">select * from dblink(&#39;dbname&#x3D;gpdb&#39;,&#39;select * from public.demo&#39;) as a(a int,b text);</span><br><span class="line">insert into public.demo_load select * from dblink(&#39;dbname&#x3D;gpdb&#39;,&#39;select * from public.demo&#39;) as a(a int,b text);</span><br><span class="line">insert into public.demo_load select * from dblink(&#39;hostaddr&#x3D;192.168.157.128 port&#x3D;5432 dbname&#x3D;gpdb user&#x3D;gpadmin password&#x3D;123456&#39;,&#39;select * from public.demo&#39;) as a(a int,b text);</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">使用insert * from select时不能只使用dblink_connect设置好的连接,需要数据库连接串</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>Github经常性连接不上解决方式</title>
    <url>/2021/03/17/Github%E7%BB%8F%E5%B8%B8%E6%80%A7%E8%BF%9E%E6%8E%A5%E4%B8%8D%E4%B8%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<blockquote>
<p>平常开发经常性需要访问github代码,频繁连接不上github</p>
</blockquote>
<span id="more"></span>

<h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 访问https://www.ipaddress.com/或者https://site.ip138.com/</span></span><br><span class="line"><span class="comment"># 找到目前github的IP映射</span></span><br><span class="line">输入域名查询IP地址</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改本机的hosts文件</span></span><br><span class="line">140.82.114.4	github.com</span><br><span class="line">199.232.69.194	github.global.ssl.fastly.net</span><br><span class="line">185.199.108.133	raw.githubusercontent.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># ps.如果使用的是Mac本,可能需要添加更多的映射关系</span></span><br><span class="line">---Mac本直接别用Safari浏览器,换Google浏览器就可以访问了</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>GreenPlum编译安装单机版</title>
    <url>/2021/04/12/GreenPlum%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%E5%8D%95%E6%9C%BA%E7%89%88/</url>
    <content><![CDATA[<blockquote>
<p>网上教程过于零散,而且有不正确的地方</p>
</blockquote>
<span id="more"></span>

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="基本情况"><a href="#基本情况" class="headerlink" title="基本情况"></a>基本情况</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CentOS7</span><br><span class="line">GP_6X_STABLE</span><br><span class="line">1 master 1 primary 1 mirror</span><br></pre></td></tr></table></figure>

<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y net-tools # 需要ifconfig与netstat命令</span><br><span class="line"></span><br><span class="line">systemctl stop firewalld # 关闭防火墙</span><br><span class="line"></span><br><span class="line">systemctl disable firewalld # 禁用防火墙</span><br><span class="line"></span><br><span class="line">hostnamectl set-hostname master # 修改主机名</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;hosts # 配置主机域名</span><br><span class="line">192.168.157.168    master</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;selinux&#x2F;config # 关闭selinux</span><br><span class="line">SELINUX&#x3D;disabled</span><br><span class="line"></span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;sysctl.conf # 修改内核(可不修改) </span><br><span class="line">net.ipv4.ip_forward &#x3D; 0 </span><br><span class="line">net.ipv4.conf.default.accept_source_route &#x3D; 0 </span><br><span class="line">net.ipv4.tcp_syncookies &#x3D; 1</span><br><span class="line">net.ipv4.tcp_tw_recycle &#x3D; 1 </span><br><span class="line">net.ipv4.tcp_max_syn_backlog &#x3D; 4096 </span><br><span class="line">net.ipv4.conf.all.arp_filter &#x3D; 1</span><br><span class="line">net.ipv4.ip_local_port_range &#x3D; 1025 65535</span><br><span class="line">net.core.netdev_max_backlog&#x3D; 10000 </span><br><span class="line">net.core.rmem_max &#x3D; 2097152</span><br><span class="line">net.core.wmem_max &#x3D; 2097152</span><br><span class="line">net.core.somaxconn &#x3D; 2048</span><br><span class="line">kernel.sysrq &#x3D; 1 </span><br><span class="line">kernel.core_uses_pid &#x3D; 1 </span><br><span class="line">kernel.msgmni &#x3D; 2048 </span><br><span class="line">kernel.msgmax &#x3D; 65536</span><br><span class="line">kernel.msgmnb &#x3D; 65536 </span><br><span class="line">kernel.shmmni &#x3D; 4096 </span><br><span class="line">kernel.shmmax &#x3D; 500000000 </span><br><span class="line">kernel.shmall &#x3D; 4000000000 </span><br><span class="line">kernel.sem &#x3D; 250 64000 100 512 </span><br><span class="line">vm.overcommit_memory &#x3D; 2</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;security&#x2F;limits.conf # 修改文件描述符文件(可不修改)</span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 65536</span><br><span class="line">* soft nproc 131072</span><br><span class="line">* hard nproc 131072</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="编译安装启动"><a href="#编译安装启动" class="headerlink" title="编译安装启动"></a>编译安装启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;gpdb-6X_STABLE</span><br><span class="line">.&#x2F;README.CentOS.bash</span><br><span class="line">sudo ln -sf &#x2F;usr&#x2F;bin&#x2F;cmake3 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cmake</span><br><span class="line">sudo yum install -y centos-release-scl</span><br><span class="line">sudo yum install -y devtoolset-7-toolchain</span><br><span class="line">echo &#39;source scl_source enable devtoolset-7&#39; &gt;&gt; ~&#x2F;.bashrc</span><br><span class="line">.&#x2F;configure --with-perl --with-python --with-libxml --with-gssapi --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;gpdb</span><br><span class="line">make -j8</span><br><span class="line">make -j8 install</span><br><span class="line">## 编译完成</span><br><span class="line"></span><br><span class="line">useradd gpadmin # 创建gpadmin用户并授权</span><br><span class="line">passwd gpadmin</span><br><span class="line">chown -R gpadmin &#x2F;usr&#x2F;local&#x2F;gpdb</span><br><span class="line">chgrp -R gpadmin &#x2F;usr&#x2F;local&#x2F;gpdb</span><br><span class="line"></span><br><span class="line">su gpadmin # 切换gpadmin用户,创建数据目录</span><br><span class="line">mkdir -p &#x2F;data&#x2F;gpdata&#x2F;master </span><br><span class="line">mkdir -p &#x2F;data&#x2F;gpdata&#x2F;primary</span><br><span class="line">mkdir -p &#x2F;data&#x2F;gpdata&#x2F;mirror </span><br><span class="line"></span><br><span class="line">vi .bashrc # 设置gpadmin用户的环境变量</span><br><span class="line">source &#x2F;usr&#x2F;local&#x2F;gpdb&#x2F;greenplum_path.sh</span><br><span class="line">export MASTER_DATA_DIRECTORY&#x3D;&#x2F;data&#x2F;gpdata&#x2F;master&#x2F;gpseg-1</span><br><span class="line">export PGPORT&#x3D;5432</span><br><span class="line">export PGUSER&#x3D;gpadmin</span><br><span class="line">export PGDATABASE&#x3D;gpdb</span><br><span class="line"></span><br><span class="line">source .bashrc # 使环境变量生效</span><br><span class="line"></span><br><span class="line">vi &#x2F;home&#x2F;gpadmin&#x2F;seg_hosts # 添加节点服务器文件</span><br><span class="line">master</span><br><span class="line"></span><br><span class="line">ssh-keygen # 配置免密</span><br><span class="line">ssh-copy-id master</span><br><span class="line">gpssh-exkeys -f &#x2F;home&#x2F;gpadmin&#x2F;seg_hosts</span><br><span class="line"></span><br><span class="line">cp &#x2F;usr&#x2F;local&#x2F;gpdb&#x2F;docs&#x2F;cli_help&#x2F;gpconfigs&#x2F;gpinitsystem_config &#x2F;home&#x2F;gpadmin&#x2F;initGreenplum # 复制配置文件</span><br><span class="line"></span><br><span class="line">vi initGreenplum 修改配置文件</span><br><span class="line">declare -a DATA_DIRECTORY&#x3D;(&#x2F;data&#x2F;gpdata&#x2F;primary)</span><br><span class="line">MASTER_HOSTNAME&#x3D;master</span><br><span class="line">MASTER_DIRECTORY&#x3D;&#x2F;data&#x2F;gpdata&#x2F;master</span><br><span class="line">MASTER_PORT&#x3D;5432</span><br><span class="line">MIRROR_PORT_BASE&#x3D;7000</span><br><span class="line">DATABASE_NAME&#x3D;gpdb</span><br><span class="line">declare -a MIRROR_DATA_DIRECTORY&#x3D;(&#x2F;data&#x2F;gpdata&#x2F;mirror)</span><br><span class="line">MACHINE_LIST_FILE&#x3D;&#x2F;home&#x2F;gpadmin&#x2F;seg_hosts</span><br><span class="line"></span><br><span class="line">gpinitsystem -c &#x2F;home&#x2F;gpadmin&#x2F;initGreenplum # 初始化GP数据库</span><br><span class="line">gpstart -a # 启动GP</span><br><span class="line">psql # 进入命令行</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="远程连接GP"><a href="#远程连接GP" class="headerlink" title="远程连接GP"></a>远程连接GP</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;data&#x2F;gpdata&#x2F;master&#x2F;gpseg-1&#x2F;pg_hba.conf</span><br><span class="line"># 添加对应的IP地址即可</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>GP与Kafka联动应用</title>
    <url>/2021/04/16/GP%E4%B8%8EKafka%E8%81%94%E5%8A%A8%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>利用Greenplum Streaming Server(GPSS)实现kafka到gp的过程</p>
</blockquote>
<span id="more"></span>

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="安装GPSS"><a href="#安装GPSS" class="headerlink" title="安装GPSS"></a>安装<a href="https://network.pivotal.io/products/greenplum-streaming-server#/releases/866955/file_groups/3395">GPSS</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装GPSS插件(GP5.10之后支持)</span><br><span class="line"># 下载GPSS安装包</span><br><span class="line">https:&#x2F;&#x2F;network.pivotal.io&#x2F;products&#x2F;greenplum-streaming-server#&#x2F;releases&#x2F;866955&#x2F;file_groups&#x2F;3395</span><br><span class="line"></span><br><span class="line"># 保证GP数据库启动(下列安装方式3选1)</span><br><span class="line"># 安装GPSS gppkg(安装升级GP数据库集群中所有主机的GPSS)</span><br><span class="line">su gpadmin</span><br><span class="line">gppkg -i gpss-gpdb6-1.3.6-rhel7-x86_64.gppkg</span><br><span class="line"></span><br><span class="line"># 安装GPSS Tarball(用于安装包括GP数据库的单个ETL服务器上安装升级GPSS)</span><br><span class="line">tar xzvf gpss-gpdb6-1.3.6-rhel7-x86_64.tar.gz</span><br><span class="line">cd gpss-gpdb6-1.3.6-rhel7-x86_64</span><br><span class="line">.&#x2F;install_gpdb_component</span><br><span class="line"></span><br><span class="line"># 安装GPSS ETL RPM(用于未安装GP数据库专用ETL服务器上安装升级GPSS)</span><br><span class="line">sudo yum install gpss-gpdb6-1.3.6-rhel7-x86_64.rpm</span><br><span class="line">psql</span><br><span class="line">.&#x2F;usr&#x2F;local&#x2F;gpss&#x2F;gpss_path.sh</span><br><span class="line"></span><br><span class="line"># 加载组件</span><br><span class="line">psql</span><br><span class="line">CREATE EXTENSION gpss;</span><br><span class="line"></span><br><span class="line"># 注意:会报libstdc++.so.6: version &#96;CXXABI_1.3.9&#39; not found</span><br><span class="line">网上下载libstdc++.so.6.0.26,映射到libstdc++.so.6</span><br><span class="line">cp libstdc++.so.6.0.26 &#x2F;usr&#x2F;lib64&#x2F;</span><br><span class="line">cd &#x2F;usr&#x2F;lib64&#x2F;</span><br><span class="line">chmod 755 libstdc++.so.6.0.26</span><br><span class="line">rm -rf libstdc++.so.6</span><br><span class="line">ln -s libstdc++.so.6.0.26 libstdc++.so.6</span><br></pre></td></tr></table></figure>

<h3 id="配置加载文件"><a href="#配置加载文件" class="headerlink" title="配置加载文件"></a>配置加载文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 加载Kafka CSV数据</span><br><span class="line"># &quot;123&quot;,&quot;09&quot;,&quot;456.78&quot;</span><br><span class="line"></span><br><span class="line">vi firstload_cfg.yaml</span><br><span class="line">DATABASE: testdb</span><br><span class="line">USER: gpadmin</span><br><span class="line">HOST: gpmaster</span><br><span class="line">PORT: 5432</span><br><span class="line">KAFKA:</span><br><span class="line">   INPUT:</span><br><span class="line">     SOURCE:</span><br><span class="line">        BROKERS: localhost:9092</span><br><span class="line">        TOPIC: topic_for_gpkafka</span><br><span class="line">     COLUMNS:</span><br><span class="line">        - NAME: cust_id</span><br><span class="line">          TYPE: int</span><br><span class="line">        - NAME: __IGNORED__</span><br><span class="line">          TYPE: int</span><br><span class="line">        - NAME: expenses</span><br><span class="line">          TYPE: decimal(9,2)</span><br><span class="line">     FORMAT: csv</span><br><span class="line">     ERROR_LIMIT: 125</span><br><span class="line">   OUTPUT:</span><br><span class="line">     TABLE: data_from_kafka</span><br><span class="line">     MAPPING:</span><br><span class="line">        - NAME: customer_id</span><br><span class="line">          EXPRESSION: cust_id</span><br><span class="line">        - NAME: expenses</span><br><span class="line">          EXPRESSION: expenses</span><br><span class="line">        - NAME: tax_due</span><br><span class="line">          EXPRESSION: expenses * .0725</span><br><span class="line">   COMMIT:</span><br><span class="line">     MINIMAL_INTERVAL: 2000</span><br><span class="line"></span><br><span class="line"># 加载Kafka JSON数据</span><br><span class="line"># &#123; &quot;cust_id&quot;: 123, &quot;month&quot;: 9, &quot;amount_paid&quot;:456.78 &#125;</span><br><span class="line"></span><br><span class="line">vi simple_jsonload_cfg.yaml</span><br><span class="line">DATABASE: testdb</span><br><span class="line">USER: gpadmin</span><br><span class="line">HOST: gpmaster</span><br><span class="line">PORT: 5432</span><br><span class="line">KAFKA:</span><br><span class="line">   INPUT:</span><br><span class="line">     SOURCE:</span><br><span class="line">        BROKERS: localhost:9092</span><br><span class="line">        TOPIC: topic_json</span><br><span class="line">     FORMAT: json</span><br><span class="line">     ERROR_LIMIT: 10</span><br><span class="line">   OUTPUT:</span><br><span class="line">     TABLE: single_json_column</span><br><span class="line">   COMMIT:</span><br><span class="line">     MINIMAL_INTERVAL: 1000</span><br><span class="line"></span><br><span class="line"># 加载Kafka JSON数据(带映射)</span><br><span class="line"># &#123; &quot;cust_id&quot;: 123, &quot;month&quot;: 9, &quot;amount_paid&quot;:456.78 &#125;</span><br><span class="line"></span><br><span class="line">vi jsonload_cfg.yaml</span><br><span class="line">DATABASE: testdb</span><br><span class="line">USER: gpadmin</span><br><span class="line">HOST: gpmaster</span><br><span class="line">PORT: 5432</span><br><span class="line">KAFKA:</span><br><span class="line">   INPUT:</span><br><span class="line">     SOURCE:</span><br><span class="line">        BROKERS: localhost:9092</span><br><span class="line">        TOPIC: topic_json_gpkafka</span><br><span class="line">     COLUMNS:</span><br><span class="line">        - NAME: jdata</span><br><span class="line">          TYPE: json</span><br><span class="line">     FORMAT: json</span><br><span class="line">     ERROR_LIMIT: 10</span><br><span class="line">   OUTPUT:</span><br><span class="line">     TABLE: json_from_kafka</span><br><span class="line">     MAPPING:</span><br><span class="line">        - NAME: customer_id</span><br><span class="line">          EXPRESSION: (jdata-&gt;&gt;&#39;cust_id&#39;)::int</span><br><span class="line">        - NAME: month</span><br><span class="line">          EXPRESSION: (jdata-&gt;&gt;&#39;month&#39;)::int</span><br><span class="line">        - NAME: amount_paid</span><br><span class="line">          EXPRESSION: (jdata-&gt;&gt;&#39;expenses&#39;)::decimal</span><br><span class="line">   COMMIT:</span><br><span class="line">     MINIMAL_INTERVAL: 2000</span><br><span class="line">     </span><br><span class="line"># 加载Kafka Avro数据</span><br><span class="line"># 1    &#123; &quot;cust_id&quot;: 123, &quot;year&quot;: 1997, &quot;expenses&quot;:[456.78, 67.89] &#125;</span><br><span class="line"># Avro数据生产者</span><br><span class="line">kafka-avro-console-producer \</span><br><span class="line">    --broker-list localhost:9092 \</span><br><span class="line">    --topic topic_avrokv \</span><br><span class="line">    --property parse.key&#x3D;true --property key.schema&#x3D;&#39;&#123;&quot;type&quot; : &quot;int&quot;, &quot;name&quot; : &quot;id&quot;&#125;&#39; \</span><br><span class="line">    --property value.schema&#x3D;&#39;&#123; &quot;type&quot; : &quot;record&quot;, &quot;name&quot; : &quot;example_schema&quot;, &quot;namespace&quot; : &quot;com.example&quot;, &quot;fields&quot; : [ &#123; &quot;name&quot; : &quot;cust_id&quot;, &quot;type&quot; : &quot;int&quot;, &quot;doc&quot; : &quot;Id of the customer account&quot; &#125;, &#123; &quot;name&quot; : &quot;year&quot;, &quot;type&quot; : &quot;int&quot;, &quot;doc&quot; : &quot;year of expense&quot; &#125;, &#123; &quot;name&quot; : &quot;expenses&quot;, &quot;type&quot; : &#123;&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;float&quot;&#125;, &quot;doc&quot; : &quot;Expenses for the year&quot; &#125; ], &quot;doc:&quot; : &quot;A basic schema for storing messages&quot; &#125;&#39;</span><br><span class="line"></span><br><span class="line">vi avrokvload_cfg.yaml</span><br><span class="line">DATABASE: testdb</span><br><span class="line">USER: gpadmin</span><br><span class="line">HOST: gpmaster</span><br><span class="line">PORT: 5432</span><br><span class="line">VERSION: 2</span><br><span class="line">KAFKA:</span><br><span class="line">   INPUT:</span><br><span class="line">     SOURCE:</span><br><span class="line">        BROKERS: localhost:9092</span><br><span class="line">        TOPIC: topic_avrokv</span><br><span class="line">     VALUE:</span><br><span class="line">        COLUMNS:</span><br><span class="line">          - NAME: c1</span><br><span class="line">            TYPE: json</span><br><span class="line">        FORMAT: avro</span><br><span class="line">        AVRO_OPTION:</span><br><span class="line">          SCHEMA_REGISTRY_ADDR: http:&#x2F;&#x2F;localhost:8081</span><br><span class="line">     KEY:</span><br><span class="line">        COLUMNS:</span><br><span class="line">          - NAME: id</span><br><span class="line">            TYPE: json</span><br><span class="line">        FORMAT: avro</span><br><span class="line">        AVRO_OPTION:</span><br><span class="line">          SCHEMA_REGISTRY_ADDR: http:&#x2F;&#x2F;localhost:8081</span><br><span class="line">     ERROR_LIMIT: 0</span><br><span class="line">   OUTPUT:</span><br><span class="line">     TABLE: avrokv_from_kafka</span><br><span class="line">     MAPPING:</span><br><span class="line">        - NAME: id</span><br><span class="line">          EXPRESSION: id</span><br><span class="line">        - NAME: customer_id</span><br><span class="line">          EXPRESSION: (c1-&gt;&gt;&#39;cust_id&#39;)::int</span><br><span class="line">        - NAME: year</span><br><span class="line">          EXPRESSION: (c1-&gt;&gt;&#39;year&#39;)::int</span><br><span class="line">        - NAME: expenses</span><br><span class="line">          EXPRESSION: array(select json_array_elements(c1-&gt;&#39;expenses&#39;)::text::float)</span><br><span class="line">   COMMIT:</span><br><span class="line">     MINIMAL_INTERVAL: 2000</span><br></pre></td></tr></table></figure>

<h3 id="Kafka命令"><a href="#Kafka命令" class="headerlink" title="Kafka命令"></a>Kafka命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建Topic</span><br><span class="line">kafka-topics.sh --create \</span><br><span class="line">    --zookeeper localhost:2181 --replication-factor 1 --partitions 1 \</span><br><span class="line">    --topic topic_json_gpkafka</span><br><span class="line"></span><br><span class="line"># 启动生产者生产数据</span><br><span class="line">kafka-console-producer.sh \</span><br><span class="line">    --broker-list localhost:9092 \</span><br><span class="line">    --topic topic_json_gpkafka &lt; sample_data.json</span><br><span class="line"></span><br><span class="line"># 启动消费者消费数据</span><br><span class="line">kafka-console-consumer.sh \</span><br><span class="line">    --bootstrap-server localhost:9092 --topic topic_json_gpkafka \</span><br><span class="line">    --from-beginning</span><br></pre></td></tr></table></figure>

<h3 id="创建目标表"><a href="#创建目标表" class="headerlink" title="创建目标表"></a>创建目标表</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># CSV</span><br><span class="line">CREATE TABLE data_from_kafka(customer_id int8, expenses decimal(9,2),tax_due decimal(7,2));</span><br><span class="line"></span><br><span class="line"># JSON</span><br><span class="line">CREATE TABLE single_json_column(value json);</span><br><span class="line"></span><br><span class="line"># JSON映射</span><br><span class="line">CREATE TABLE json_from_kafka(customer_id int8,month int4,amount_paid decimal(9,2));</span><br><span class="line"></span><br><span class="line"># Avro</span><br><span class="line">CREATE TABLE avrokv_from_kafka(id json,customer_id int,year int,expenses decimal(9,2)[]);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="一次性使用"><a href="#一次性使用" class="headerlink" title="一次性使用"></a>一次性使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建好加载配置文件,以及目标表</span><br><span class="line">gpkafka load --quit-at-eof custom_load_cfg.yml</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">GP-Kafka集成要求Kafka版本0.11或以上,确保exactly-once</span><br><span class="line">可以利用下面代码,使用低版本Kafka,但是会失去exactly-once</span><br><span class="line">PROPERTIES:</span><br><span class="line">      api.version.request: false</span><br><span class="line">      broker.version.fallback: 0.8.2.1</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启动常驻任务"><a href="#启动常驻任务" class="headerlink" title="启动常驻任务"></a>启动常驻任务</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动GPSS侦听端口和文件服务端口</span><br><span class="line">vi gpsscfg_ex.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;ListenAddress&quot;: &#123;</span><br><span class="line">        &quot;Host&quot;: &quot;localhost&quot;,</span><br><span class="line">        &quot;Port&quot;: 5019</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;Gpfdist&quot;: &#123;</span><br><span class="line">        &quot;Host&quot;: &quot;localhost&quot;,</span><br><span class="line">        &quot;Port&quot;: 8319</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">gpss gpsscfg_ex.json --log-dir .&#x2F;gpsslogs &amp; </span><br><span class="line"></span><br><span class="line"># 将Kafka数据加载作业提交到在端口号5019上运行的GPSS实例</span><br><span class="line">gpsscli submit --name kafkademo --gpss-port 5019 .&#x2F;firstload_cfg.yaml</span><br><span class="line"></span><br><span class="line"># 列出所有GPSS作业</span><br><span class="line">gpsscli list --all --gpss-port 5019</span><br><span class="line"></span><br><span class="line"># 开启kafademo任务</span><br><span class="line">gpsscli start kafkademo --gpss-port 5019</span><br><span class="line"></span><br><span class="line"># 停止kafademo任务</span><br><span class="line">gpsscli stop orders1 --gpss-port 5019</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>GP查询计划怎么阅读</title>
    <url>/2021/04/16/GP%E6%9F%A5%E8%AF%A2%E8%AE%A1%E5%88%92%E6%80%8E%E4%B9%88%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<blockquote>
<p>对EXPLAIN执行计划的阅读解释</p>
</blockquote>
<span id="more"></span>

<h2 id="查询计划"><a href="#查询计划" class="headerlink" title="查询计划"></a>查询计划</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">描述了GP数据库优化器执行查询时遵循的执行步骤</span><br><span class="line">整体是一个树,阅读时从底向上阅读,每一个节点都会将其结果传递给其直接上层节点</span><br><span class="line">每个节点表示计划的一个步骤,每个节点对应那一行标识了该步骤的执行操作</span><br><span class="line">并且标识了用于执行该操作的方法</span><br><span class="line"></span><br><span class="line">例子:</span><br><span class="line">gpdb&#x3D;# explain select id,count(1) from demo_load group by id;</span><br><span class="line"> Gather Motion 1:1  (slice1; segments: 1)  (cost&#x3D;0.00..431.00 rows&#x3D;1 width&#x3D;12)</span><br><span class="line">   -&gt;  GroupAggregate  (cost&#x3D;0.00..431.00 rows&#x3D;1 width&#x3D;12)</span><br><span class="line">         Group Key: id</span><br><span class="line">         -&gt;  Sort  (cost&#x3D;0.00..431.00 rows&#x3D;1 width&#x3D;4)</span><br><span class="line">               Sort Key: id</span><br><span class="line">               -&gt;  Seq Scan on demo_load  (cost&#x3D;0.00..431.00 rows&#x3D;1 width&#x3D;4)</span><br><span class="line"> Optimizer: Pivotal Optimizer (GPORCA)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="代价估计"><a href="#代价估计" class="headerlink" title="代价估计"></a>代价估计</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cost - 读取的顺序页面</span><br><span class="line">    一次顺序磁盘页面读取,前面为读取第一行的代价,后面为得到所有行的代价</span><br><span class="line">    </span><br><span class="line">rows - 行数</span><br><span class="line">    计划节点输出的行数,该值可能会小于计划节点实际处理或扫描的行数</span><br><span class="line">    </span><br><span class="line">width - 行宽度</span><br><span class="line">    计划节点输出所有列以字节表示的总宽度</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="操作解释"><a href="#操作解释" class="headerlink" title="操作解释"></a>操作解释</h2><h3 id="扫描操作"><a href="#扫描操作" class="headerlink" title="扫描操作"></a>扫描操作</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对表上的Seq Scan — 扫描表中的所有行</span><br><span class="line">Append-only Scan — 扫描行存追加优化表</span><br><span class="line">Append-only Columnar Scan — 扫描列存追加优化表中的行</span><br><span class="line">Index Scan — 遍历一个B-树索引以从表中取得行</span><br><span class="line">Bitmap Append-only Row-oriented Scan — 从索引中收集仅追加表中行的指针并且按照磁盘上的位置进行排序</span><br><span class="line">Dynamic Table Scan — 使用一个分区选择函数来选择分区</span><br><span class="line">    Function Scan节点包含分区选择函数的名称,可以是下列之一:</span><br><span class="line">        gp_partition_expansion — 选择表中的所有分区.不会有分区被消除</span><br><span class="line">        gp_partition_selection — 基于一个等值表达式选择一个分区</span><br><span class="line">        gp_partition_inversion — 基于一个范围表达式选择分区</span><br><span class="line">    Function Scan节点将动态选择的分区列表传递给Result节点,该节点又会被传递给Sequence节点</span><br></pre></td></tr></table></figure>

<h3 id="Join操作"><a href="#Join操作" class="headerlink" title="Join操作"></a>Join操作</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Hash Join – 从较小的表构建一个哈希表,用连接列作为哈希键</span><br><span class="line">    然后扫描较大的表,为连接列计算哈希键并且探索哈希表寻找具有相同哈希键的行</span><br><span class="line">    哈希连接通常是Greenplum数据库中最快的连接</span><br><span class="line">    解释计划中的Hash Cond标识要被连接的列</span><br><span class="line">Nested Loop – 在较大数据集的行上迭代,在每次迭代时于较小的数据集中扫描行</span><br><span class="line">    嵌套循环连接要求广播其中的一个表,这样一个表中的所有行才能与其他表中的所有行进行比较</span><br><span class="line">    它在较小的表或者通过使用索引约束的表上执行得不错</span><br><span class="line">    它还被用于笛卡尔积和范围连接</span><br><span class="line">    在使用Nested Loop连接大型表时会有性能影响</span><br><span class="line">    对于包含Nested Loop连接操作符的计划节点,应该验证SQL并且确保结果是想要的结果</span><br><span class="line">    设置服务器配置参数enable_nestloop为OFF(默认)能够让优化器更偏爱Hash Join</span><br><span class="line">Merge Join – 排序两个数据集并且将它们合并起来</span><br><span class="line">    归并连接对预排序好的数据很快,但是在现实世界中很少见</span><br><span class="line">    为了更偏爱Merge Join,可把系统配置参数enable_mergejoin设置为ON</span><br></pre></td></tr></table></figure>

<h3 id="Motion操作"><a href="#Motion操作" class="headerlink" title="Motion操作"></a>Motion操作</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Broadcast motion – 每一个Segment将自己的行发送给所有其他Segment,这样每一个Segment实例都有表的一份完整的本地拷贝</span><br><span class="line">    Broadcast motion可能不如Redistribute motion那么好,因此优化器通常只为小型表选择Broadcast motion</span><br><span class="line">    对大型表来说,Broadcast motion是不可接受的</span><br><span class="line">    在数据没有按照连接键分布的情况下,将把一个表中所需的行动态重分布到另一个Segment</span><br><span class="line">Redistribute motion – 每一个Segment重新哈希数据并且把行发送到对应于哈希键的合适Segment上</span><br><span class="line">Gather motion – 来自所有Segment的结果数据被组装成一个单一的流</span><br></pre></td></tr></table></figure>

<h3 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Materialize – 规划器将一个子查询物化一次,这样就不用为顶层行重复该工作</span><br><span class="line">InitPlan – 一个预查询,被用在动态分区消除中,当执行时还不知道规划器需要用来标识要扫描分区的值时,会执行这个预查询</span><br><span class="line">Sort – 为另一个要求排序数据的操作(例如Aggregation或者Merge Join)准备排序数据</span><br><span class="line">Group By – 通过一个或者更多列分组行</span><br><span class="line">Group&#x2F;Hash Aggregate – 使用哈希聚集行</span><br><span class="line">Append – 串接数据集,例如在整合从分区表中各分区扫描的行时会用到</span><br><span class="line">Filter – 使用来自于一个WHERE子句的条件选择行</span><br><span class="line">Limit – 限制返回的行数</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>GreenPlum日常整理</title>
    <url>/2021/03/11/GreenPlum%E6%97%A5%E5%B8%B8%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>新知识结构，记录下日常使用</p>
</blockquote>
<span id="more"></span>

<h2 id="GP的函数创建"><a href="#GP的函数创建" class="headerlink" title="GP的函数创建"></a>GP的函数创建</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- GP实现REGEXP_LIKE函数</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OR</span> <span class="keyword">REPLACE</span> <span class="keyword">FUNCTION</span> <span class="keyword">regexp_like</span>(<span class="keyword">str</span> <span class="built_in">character</span>, reg <span class="built_in">character</span>)</span><br><span class="line">  <span class="keyword">RETURNS</span> <span class="built_in">boolean</span> <span class="keyword">AS</span></span><br><span class="line">$<span class="keyword">BODY</span>$</span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">v_match  <span class="built_in">text</span>;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">select</span> regexp_matches(<span class="keyword">str</span>,reg) <span class="keyword">into</span> v_match;</span><br><span class="line">if v_match is not NULL then</span><br><span class="line">return true;</span><br><span class="line">else</span><br><span class="line">return false;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">if</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line">$BODY$</span><br><span class="line">  LANGUAGE plpgsql VOLATILE;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="GP解锁操作"><a href="#GP解锁操作" class="headerlink" title="GP解锁操作"></a>GP解锁操作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看segment锁情况</span><br><span class="line">select gp_execution_dbid(),pid,relation::regclass,locktype,mode,granted</span><br><span class="line">from gp_dist_random(&#39;pg_locks&#39;);</span><br><span class="line"></span><br><span class="line"># 查看具体什么语句持有的锁</span><br><span class="line">select gp_execution_dbid() dbid,procpid,current_query</span><br><span class="line">from gp_dist_random(&#39;pg_stat_activity&#39;)  </span><br><span class="line">where procpid in (</span><br><span class="line">    select pid </span><br><span class="line">    from gp_dist_random(&#39;pg_locks&#39;)</span><br><span class="line">    where locktype&#x3D;&#39;relation&#39; and mode&#x3D;&#39;ExclusiveLock&#39;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"># 连接相关segment</span><br><span class="line">PGOPTIONS&#x3D;&quot;-c gp_session_role&#x3D;utility&quot; psql -h localhost -p 5432 -d gpdb</span><br><span class="line"></span><br><span class="line"># 在segment查询相关锁情况</span><br><span class="line">select</span><br><span class="line">    w.current_query as waiting_query,</span><br><span class="line">    w.procpid as w_pid,</span><br><span class="line">    w.usename as w_user,</span><br><span class="line">    l.current_query as locking_query,</span><br><span class="line">    l.procpid as l_pid,</span><br><span class="line">    l.usename as l_user,</span><br><span class="line">    t.schemaname || &#39;.&#39; || t.relname as tablename</span><br><span class="line">from pg_stat_activity w</span><br><span class="line">join pg_locks l1 </span><br><span class="line">on w.procpid &#x3D; l1.pid </span><br><span class="line">and not l1.granted</span><br><span class="line">join pg_locks l2 </span><br><span class="line">on l1.relation &#x3D; l2.relation </span><br><span class="line">and l2.granted</span><br><span class="line">join pg_stat_activity l </span><br><span class="line">on l2.pid &#x3D; l.procpid</span><br><span class="line">join pg_stat_user_tables t </span><br><span class="line">on l1.relation &#x3D; t.relid</span><br><span class="line">where w.waiting;</span><br><span class="line"></span><br><span class="line"># 处理持有锁的pid</span><br><span class="line">select pg_terminate_backend(&#39;procpid&#39;);</span><br><span class="line"></span><br><span class="line"># GP查看锁</span><br><span class="line">select pid,rolname,rsqname,granted,current_query,datname</span><br><span class="line">from pg_roles,gp_toolkit.gp_resqueue_status,pg_locks,pg_stat_activity</span><br><span class="line">WHERE pg_roles.rolresqueue&#x3D;pg_locks.objid</span><br><span class="line">AND pg_locks.objid&#x3D;gp_toolkit.gp_resqueue_status.queueid</span><br><span class="line">AND pg_stat_activity.procpid&#x3D;pg_locks.pid;</span><br><span class="line"> </span><br><span class="line"># GP解除锁定</span><br><span class="line">pg_cancel_backend(#pid)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase的基本命令</title>
    <url>/2017/11/23/HBase%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<blockquote>
<p>记录Hbase常用的命令以及对性能帮助大的命令.</p>
</blockquote>
<span id="more"></span>

<h2 id="基本Shell命令"><a href="#基本Shell命令" class="headerlink" title="基本Shell命令"></a>基本Shell命令</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 进入hbase命令行</span><br><span class="line">.&#x2F;hbase shell </span><br><span class="line"></span><br><span class="line"># 显示hbase中的表</span><br><span class="line">list </span><br><span class="line"></span><br><span class="line"># 创建user表，包含info、data两个列族</span><br><span class="line">create &#39;user&#39;, &#39;info1&#39;, &#39;data1&#39;</span><br><span class="line">create &#39;user&#39;, &#123;NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; &#39;3&#39;&#125; </span><br><span class="line"></span><br><span class="line"># 向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsan</span><br><span class="line">put &#39;user&#39;, &#39;rk0001&#39;, &#39;info:name&#39;, &#39;zhangsan&#39; </span><br><span class="line"></span><br><span class="line"># 向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为female</span><br><span class="line">put &#39;user&#39;, &#39;rk0001&#39;, &#39;info:gender&#39;, &#39;female&#39; </span><br><span class="line"></span><br><span class="line"># 向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20</span><br><span class="line">put &#39;user&#39;, &#39;rk0001&#39;, &#39;info:age&#39;, 20 </span><br><span class="line"></span><br><span class="line"># 向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为picture</span><br><span class="line">put &#39;user&#39;, &#39;rk0001&#39;, &#39;data:pic&#39;, &#39;picture&#39; </span><br><span class="line"></span><br><span class="line"># 获取user表中row key为rk0001的所有信息</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39; </span><br><span class="line"></span><br><span class="line"># 获取user表中row key为rk0001，info列族的所有信息</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#39;info&#39; </span><br><span class="line"></span><br><span class="line"># 获取user表中row key为rk0001，info列族的name、age列标示符的信息</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#39;info:name&#39;, &#39;info:age&#39; </span><br><span class="line"></span><br><span class="line"># 获取user表中row key为rk0001，info、data列族的信息</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#39;info&#39;, &#39;data&#39;</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#123;COLUMN &#x3D;&gt; [&#39;info&#39;, &#39;data&#39;]&#125; </span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#123;COLUMN &#x3D;&gt; [&#39;info:name&#39;, &#39;data:pic&#39;]&#125; </span><br><span class="line"></span><br><span class="line"># 获取user表中row key为rk0001，列族为info，版本号最新5个的信息</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#123;COLUMN &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; 2&#125;</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#123;COLUMN &#x3D;&gt; &#39;info:name&#39;, VERSIONS &#x3D;&gt; 5&#125;</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#123;COLUMN &#x3D;&gt; &#39;info:name&#39;, VERSIONS &#x3D;&gt; 5, TIMERANGE &#x3D;&gt; [1392368783980, 1392380169184]&#125; </span><br><span class="line"></span><br><span class="line"># 获取user表中row key为rk0001，cell的值为zhangsan的信息</span><br><span class="line">get &#39;user&#39;, &#39;rk0001&#39;, &#123;FILTER &#x3D;&gt; &quot;ValueFilter(&#x3D;, &#39;binary:zhangsan&#39;)&quot;&#125; </span><br><span class="line"></span><br><span class="line"># 获取user表中row key为rk0001，列标示符中含有a的信息get &#39;user&#39;, &#39;rk0001&#39;, &#123;FILTER &#x3D;&gt; &quot;(QualifierFilter(&#x3D;,&#39;substring:a&#39;))&quot;&#125; </span><br><span class="line">put &#39;user&#39;, &#39;rk0002&#39;, &#39;info:name&#39;, &#39;fanbingbing&#39;</span><br><span class="line">put &#39;user&#39;, &#39;rk0002&#39;, &#39;info:gender&#39;, &#39;female&#39;</span><br><span class="line">put &#39;user&#39;, &#39;rk0002&#39;, &#39;info:nationality&#39;, &#39;中国&#39;</span><br><span class="line">get &#39;user&#39;, &#39;rk0002&#39;, &#123;FILTER &#x3D;&gt; &quot;ValueFilter(&#x3D;, &#39;binary:中国&#39;)&quot;&#125;  </span><br><span class="line"></span><br><span class="line"># 查询user表中的所有信息</span><br><span class="line">scan &#39;user&#39; </span><br><span class="line"></span><br><span class="line"># 查询user表中列族为info的信息</span><br><span class="line">scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; &#39;info&#39;&#125;</span><br><span class="line">scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; &#39;info&#39;, RAW &#x3D;&gt; true, VERSIONS &#x3D;&gt; 5&#125;</span><br><span class="line">scan &#39;persion&#39;, &#123;COLUMNS &#x3D;&gt; &#39;info&#39;, RAW &#x3D;&gt; true, VERSIONS &#x3D;&gt; 3&#125;</span><br><span class="line"></span><br><span class="line"># 查询user表中列族为info和data的信息</span><br><span class="line">scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; [&#39;info&#39;, &#39;data&#39;]&#125;</span><br><span class="line">scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; [&#39;info:name&#39;, &#39;data:pic&#39;]&#125;  </span><br><span class="line"></span><br><span class="line"># 查询user表中列族为info、列标示符为name的信息</span><br><span class="line">scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; &#39;info:name&#39;&#125; </span><br><span class="line"></span><br><span class="line"># 查询user表中列族为info、列标示符为name的信息,并且版本最新的5个</span><br><span class="line">scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; &#39;info:name&#39;, VERSIONS &#x3D;&gt; 5&#125; </span><br><span class="line"></span><br><span class="line"># 查询user表中列族为info和data且列标示符中含有a字符的信息scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; [&#39;info&#39;, &#39;data&#39;], FILTER &#x3D;&gt; &quot;(QualifierFilter(&#x3D;,&#39;substring:a&#39;))&quot;&#125; </span><br><span class="line"></span><br><span class="line"># 查询user表中列族为info，rk范围是[rk0001, rk0003)的数据</span><br><span class="line">scan &#39;user&#39;, &#123;COLUMNS &#x3D;&gt; &#39;info&#39;, STARTROW &#x3D;&gt; &#39;rk0001&#39;, ENDROW &#x3D;&gt; &#39;rk0003&#39;&#125; </span><br><span class="line"></span><br><span class="line"># 查询user表中row key以rk字符开头的</span><br><span class="line">scan &#39;user&#39;,&#123;FILTER&#x3D;&gt;&quot;PrefixFilter(&#39;rk&#39;)&quot;&#125; </span><br><span class="line"></span><br><span class="line"># 查询user表中指定范围的数据</span><br><span class="line">scan &#39;user&#39;, &#123;TIMERANGE &#x3D;&gt; [1392368783980, 1392380169184]&#125; </span><br><span class="line"></span><br><span class="line"># 删除数据删除user表row key为rk0001，列标示符为info:name的数据</span><br><span class="line">delete &#39;people&#39;, &#39;rk0001&#39;, &#39;info:name&#39;</span><br><span class="line"></span><br><span class="line"># 删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据</span><br><span class="line">delete &#39;user&#39;, &#39;rk0001&#39;, &#39;info:name&#39;, 1392383705316  </span><br><span class="line"></span><br><span class="line"># 清空user表中的数据</span><br><span class="line">truncate &#39;user&#39;  </span><br><span class="line"></span><br><span class="line"># 修改表结构首先停用user表（新版本不用）</span><br><span class="line">disable &#39;user&#39; </span><br><span class="line"></span><br><span class="line"># 添加两个列族f1和f2</span><br><span class="line">alter &#39;people&#39;, NAME &#x3D;&gt; &#39;f1&#39;</span><br><span class="line">alter &#39;user&#39;, NAME &#x3D;&gt; &#39;f2&#39;</span><br><span class="line"></span><br><span class="line"># 启用表</span><br><span class="line">enable &#39;user&#39;  </span><br><span class="line"></span><br><span class="line"># 删除一个列族</span><br><span class="line">alter &#39;user&#39;, NAME &#x3D;&gt; &#39;f1&#39;, METHOD &#x3D;&gt; &#39;delete&#39; </span><br><span class="line">alter &#39;user&#39;, &#39;delete&#39; &#x3D;&gt; &#39;f1&#39; </span><br><span class="line"></span><br><span class="line"># 添加列族f1同时删除列族f2</span><br><span class="line">alter &#39;user&#39;, &#123;NAME &#x3D;&gt; &#39;f1&#39;&#125;, &#123;NAME &#x3D;&gt; &#39;f2&#39;, METHOD &#x3D;&gt; &#39;delete&#39;&#125; </span><br><span class="line"></span><br><span class="line"># 将user表的f1列族版本号改为5</span><br><span class="line">alter &#39;people&#39;, NAME &#x3D;&gt; &#39;info&#39;, VERSIONS &#x3D;&gt; 5</span><br><span class="line"></span><br><span class="line"># 删除表</span><br><span class="line">disable &#39;user&#39;</span><br><span class="line">drop &#39;user&#39;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="major-compact"><a href="#major-compact" class="headerlink" title="major_compact"></a>major_compact</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 合并文件,清除删除,过期,多余版本的数据,提高读写数据的效率</span><br><span class="line"># HBase中实现了两种Compaction的方式</span><br><span class="line">Minor: 操作只用来做部分文件的合并操作以及包括minVersion&#x3D;0并且设置ttl的过期版本清理，不做任何删除数据、多版本数据的清理工作。</span><br><span class="line">Major: 操作是对Region下的HStore下的所有StoreFile执行合并操作，最终的结果是整理合并出一个文件。</span><br><span class="line"></span><br><span class="line"># 使用的时机&lt;major_compact是很重的后台操作&gt;</span><br><span class="line">业务低峰时段执行</span><br><span class="line">优先考虑含有TTL的表</span><br><span class="line">storefile短期内增加比较多</span><br><span class="line">表中storefile平均大小比较小</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>GP的外部表应用</title>
    <url>/2021/04/13/GP%E7%9A%84%E5%A4%96%E9%83%A8%E8%A1%A8%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>对GP外部表使用做一些整理,以及基于gpfdist,gpfdists以及gphdfs进行演示</p>
</blockquote>
<span id="more"></span>

<h2 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">允许用户像访问标准数据库表一样的访问外部表</span><br><span class="line"></span><br><span class="line">GP提供两种类型的外部表</span><br><span class="line">    可读外部表:数据装载,不允许数据修改</span><br><span class="line">    可写外部表:数据卸载,从数据库表中选择记录输出到文件&#x2F;命令管道&#x2F;可执行程序(MR),只允许INSERT操作</span><br><span class="line"></span><br><span class="line">可读外部表分类</span><br><span class="line">    常规:访问静态文件</span><br><span class="line">    WEB:访问动态数据源</span><br><span class="line"></span><br><span class="line">创建外部表定义时,必须指定文件格式和文件位置</span><br><span class="line">    TEXT类型对所有协议有效。</span><br><span class="line">    逗号分隔的CSV对于gpfdist和file协议有效</span><br><span class="line">    自定义格式适合于gphdfs</span><br></pre></td></tr></table></figure>

<h3 id="外部表创建"><a href="#外部表创建" class="headerlink" title="外部表创建"></a>外部表创建</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建单文件服务的可读外部表</span><br><span class="line">CREATE EXTERNAL TABLE demo (id int,name text)</span><br><span class="line">LOCATION(&#39;gpfdist:&#x2F;&#x2F;hostname:port&#x2F;demo.txt&#39;)</span><br><span class="line">FORMAT &#39;TEXT&#39; (DELIMITER &#39;|&#39; NULL &#39;&#39;);</span><br><span class="line"></span><br><span class="line"># 创建多文件服务的可读外部表</span><br><span class="line">CREATE EXTERNAL TABLE demo (id int,name text)</span><br><span class="line">LOCATION(&#39;gpfdist:&#x2F;&#x2F;hostname:port1&#x2F;demo.txt&#39;,&#39;gpfdist:&#x2F;&#x2F;hostname:port2&#x2F;demo.txt&#39;)</span><br><span class="line">FORMAT &#39;TEXT&#39; (DELIMITER &#39;|&#39; NULL &#39;&#39;);</span><br><span class="line"></span><br><span class="line"># 带错误数据日期的多文件服务</span><br><span class="line">CREATE EXTERNAL TABLE demo (id int,name text)</span><br><span class="line">LOCATION(&#39;gpfdist:&#x2F;&#x2F;hostname:port1&#x2F;demo.txt&#39;,&#39;gpfdist:&#x2F;&#x2F;hostname:port2&#x2F;demo.txt&#39;)</span><br><span class="line">FORMAT &#39;CSV&#39; (DELIMITER &#39;,&#39; )</span><br><span class="line">LOG ERRORS INTO err_customer SEGMENT REJECT LIMIT 2;</span><br><span class="line"># 查看错误日志</span><br><span class="line">select * from err_customer;</span><br><span class="line"></span><br><span class="line"># 创建可写外部表</span><br><span class="line">CREATE WRITABLE EXTERNAL TABLE output (LIKE input)</span><br><span class="line">LOCATION(&#39;gpfdist:&#x2F;&#x2F;localhost:port&#x2F;output.out&#39;)</span><br><span class="line">FORMAT &#39;TEXT&#39; (DELIMITER &#39;|&#39; NULL &#39;&#39;)</span><br><span class="line">DISTRIBUTED BY (id);</span><br><span class="line">insert into output select * from input ;</span><br><span class="line"></span><br><span class="line"># 将外部表装载到数据表</span><br><span class="line">CREATE TABLE new AS SELECT * FROM demo；</span><br><span class="line"></span><br><span class="line"># 创建WEB外部表(有两种方式URL和OS)</span><br><span class="line">查询优化器不允许重复扫描WEB表的数据</span><br><span class="line"></span><br><span class="line"># URL(URL的数量对应并行访问WEB表的segment实例)</span><br><span class="line">CREATE EXTERNAL WEB TABLE demo (name text,date date,amount float4,category text,description text )</span><br><span class="line">LOCATION(</span><br><span class="line">&#39;http:&#x2F;&#x2F;WEB_URL&#x2F;file1.csv&#39;,</span><br><span class="line">&#39;http:&#x2F;&#x2F;WEB_URL&#x2F;file2.csv&#39;</span><br><span class="line">)</span><br><span class="line">FORMAT &#39;CSV&#39; (HEADER);</span><br><span class="line"># OS(在一个或多个segment上指定执行SHELL命令或脚本,输出结果作为WEB表访问的数据)</span><br><span class="line">CREATE EXTERNAL WEB TABLE tb_ext_wb01 (output text)</span><br><span class="line">EXECUTE &#39;hostname&#39;</span><br><span class="line">FORMAT &#39;TEXT&#39;;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="GPFDIST"><a href="#GPFDIST" class="headerlink" title="GPFDIST"></a>GPFDIST</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在外部表指定文件的所有主机上运行GP文件分发程序</span><br><span class="line">指向一个给定的目录,并行的为所有segment实例提供外部数据文件服务</span><br><span class="line">如果文件使用了gzip或者bzip2压缩,gpfdist会自动解压</span><br><span class="line">可以使用多个gpfdist来提升外部表的扫描性能</span><br><span class="line">可以使用通配符或者C风格的模式匹配多个文件</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">    实际应用中,一般会把gpfdist部署在ETL文件服务器上,在这个服务器上启动一个或者多个gpfdist</span><br><span class="line">    一般指定文件数据的父目录,因为大部分是很多数据文件使用同一个gpfdist,路径细写就不能使用同一个gpfdist(开启gpfdist进程时指定文件根目录,定义外部表时指定子目录)</span><br><span class="line">    gpfdist进程取决于网络带宽</span><br></pre></td></tr></table></figure>

<h3 id="配置参数"><a href="#配置参数" class="headerlink" title="配置参数"></a>配置参数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 控制节点并行度</span><br><span class="line">gp_external_max_segs(最大多少segment实例访问同一个gpfdist文件分发程序)</span><br></pre></td></tr></table></figure>

<h3 id="启动与停止"><a href="#启动与停止" class="headerlink" title="启动与停止"></a>启动与停止</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动gpfdist,必须指定其提供文件服务的目录以及运行的端口(默认8080)</span><br><span class="line">gpfdist -d &#x2F;var&#x2F;load_files -p 9190 -l &#x2F;home&#x2F;gpadmin&#x2F;log &amp;</span><br><span class="line"></span><br><span class="line"># 同一台主机启动多个gpfdist服务,只需要指定不同的目录和端口即可</span><br><span class="line">gpfdist -d &#x2F;var&#x2F;load_files1 -p 9191 -l &#x2F;home&#x2F;gpadmin&#x2F;log &amp;</span><br><span class="line">gpfdist -d &#x2F;var&#x2F;load_files2 -p 9192 -l &#x2F;home&#x2F;gpadmin&#x2F;log2 &amp;</span><br><span class="line"></span><br><span class="line"># 停止gpfdist(通过kill命令)</span><br></pre></td></tr></table></figure>

<h3 id="故障诊断"><a href="#故障诊断" class="headerlink" title="故障诊断"></a>故障诊断</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 确保segment可以访问gpfdist网络(利用wget命令测试连接性)</span><br><span class="line">wget http:&#x2F;&#x2F;hostname:post&#x2F;filename</span><br><span class="line"># 需要确保CREATE EXTERNAL TABLE定义了hostname,port以及gpfdist的文件名</span><br></pre></td></tr></table></figure>

<h3 id="使用操作"><a href="#使用操作" class="headerlink" title="使用操作"></a>使用操作</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建文件服务目录</span><br><span class="line">su gpadmin</span><br><span class="line">cd ~</span><br><span class="line">mkdir load_files</span><br><span class="line"># 启动文件服务</span><br><span class="line">gpfdist -d &#x2F;home&#x2F;gpadmin&#x2F;load_files -p 9190 -l &#x2F;home&#x2F;gpadmin&#x2F;log &amp;</span><br><span class="line"></span><br><span class="line"># 准备外部数据</span><br><span class="line">cd &#x2F;home&#x2F;gpadmin&#x2F;load_files</span><br><span class="line">vi demo.txt</span><br><span class="line">1|XS</span><br><span class="line">2|JKS</span><br><span class="line">3|JF</span><br><span class="line"></span><br><span class="line"># 创建单文件服务的可读外部表</span><br><span class="line">psql</span><br><span class="line">CREATE EXTERNAL TABLE demo (id int,name text)</span><br><span class="line">LOCATION(&#39;gpfdist:&#x2F;&#x2F;master:9190&#x2F;demo.txt&#39;)</span><br><span class="line">FORMAT &#39;TEXT&#39; (DELIMITER &#39;|&#39; NULL &#39;&#39;);</span><br><span class="line"></span><br><span class="line"># 查看数据</span><br><span class="line">SELECT * FROM demo;</span><br><span class="line"> id | name</span><br><span class="line">----+------</span><br><span class="line">  1 | XS</span><br><span class="line">  2 | JKS</span><br><span class="line">  3 | JF</span><br><span class="line">(3 rows)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="GPFDISTS"><a href="#GPFDISTS" class="headerlink" title="GPFDISTS"></a>GPFDISTS</h2><h3 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gpfdists是gpfdist的安全版本</span><br><span class="line">开启的加密通信并确保文件与GP之间的安全认证</span><br><span class="line"></span><br><span class="line">使用file:&#x2F;&#x2F;协议,外部文件必须存放在segment主机上</span><br><span class="line">指定符合segment实例数量的URL将并行工作来访问外部表</span><br><span class="line">每个segment主机外部文件数量不能超过segment实例数量</span><br><span class="line">pg_max_external_files用来确定每个外部表中允许有多少个外部文件</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="GPHDFS"><a href="#GPHDFS" class="headerlink" title="GPHDFS"></a>GPHDFS</h2><h3 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">该协议指定一个HDFS包含通配符的路径</span><br><span class="line">在GP连接到HDFS文件时,所有数据将从HDFS数据节点被并行读取到GP的segment实例快速处理</span><br><span class="line">每个segment实例只读取一组Hadoop数据块</span><br><span class="line">对于写,每个segment实例只写giant实例包含的数据</span><br></pre></td></tr></table></figure>

<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 保证gpadmin用户可以访问hdfs</span><br><span class="line"># 修改master配置参数</span><br><span class="line">gpconfig -c gp_hadoop_target_version -v &quot;hadoop2&quot;</span><br><span class="line">gpconfig -c gp_hadoop_home -v &quot;&#x2F;home&#x2F;hadoop&#x2F;hadoop&quot;</span><br><span class="line"># 重启后检查配置参数</span><br><span class="line">gpstop -M fast -ra</span><br><span class="line">gpconfig -s gp_hadoop_target_version</span><br><span class="line">gpconfig -s gp_hadoop_home</span><br><span class="line"></span><br><span class="line"># 验证</span><br><span class="line">hdfs dfs -ls &#x2F;</span><br><span class="line"></span><br><span class="line"># 设置权限</span><br><span class="line">psql gpdb</span><br><span class="line">#写权限</span><br><span class="line">GRANT INSERT ON PROTOCOL gphdfs TO gpadmin;</span><br><span class="line">#读权限</span><br><span class="line">GRANT SELECT ON PROTOCOL gphdfs TO gpadmin;</span><br><span class="line">#所有权限</span><br><span class="line">GRANT ALL ON PROTOCOL gphdfs TO gpadmin;</span><br><span class="line"></span><br><span class="line">create external table test</span><br><span class="line">(</span><br><span class="line">       id int,</span><br><span class="line">       name text</span><br><span class="line">)</span><br><span class="line">LOCATION (&#39;gphdfs:&#x2F;&#x2F;master:9000&#x2F;test.txt&#39;)</span><br><span class="line">FORMAT &#39;TEXT&#39; (delimiter &#39;\t&#39;);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase知识点整理</title>
    <url>/2018/01/31/HBase%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>整理一下HBase经常会问到的知识性的问题</p>
</blockquote>
<span id="more"></span>

<h2 id="HBase的特点是什么"><a href="#HBase的特点是什么" class="headerlink" title="HBase的特点是什么?"></a>HBase的特点是什么?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HBase一个分布式的基于列式存储的数据库，基于Hadoop的 hdfs存储，zookeeper进行管理。</span><br><span class="line"></span><br><span class="line">HBase适合存储半结构化或非结构化数据，对于数据结构字段不够确定或者杂乱无章很难按一个概念去抽取的数据。</span><br><span class="line"></span><br><span class="line">HBase中值为null的记录不会被存储。</span><br><span class="line"></span><br><span class="line">基于的表包含rowkey，时间戳，和列族。新写入数据时，时间戳更新，同时可以查询到以前的版本。</span><br><span class="line"></span><br><span class="line">HBase是主从架构。HMaster作为主节点，HRegionServer 作为从节点。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBase如何导入-导出数据"><a href="#HBase如何导入-导出数据" class="headerlink" title="HBase如何导入,导出数据?"></a>HBase如何导入,导出数据?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用MapReduce Job方式，根据HbaseAPI编写java脚本，将文本文件用文件流的方式截取，然后存储到多个字符串数组中.</span><br><span class="line">在put方法下，通过对表中的列族进行for循环遍历列名，用if判断列名后进行for循环.</span><br><span class="line">调用put.add的方法对列族下每一个列进行设值，每个列族下有几个了就赋值几次！没有表先对先创建表。</span><br><span class="line"></span><br><span class="line"># 导入数据</span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Driver import tablename hdfspath</span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import tablename hdfspath</span><br><span class="line"></span><br><span class="line"># 导出数据</span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Driver export tablename hdfspath</span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export tablename hdfspath</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBase的存储结构"><a href="#HBase的存储结构" class="headerlink" title="HBase的存储结构?"></a>HBase的存储结构?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HBase中的每张表都通过行键(rowkey)按照一定的范围被分割成多个子表（HRegion）.</span><br><span class="line"></span><br><span class="line">默认一个HRegion超过256M就要被分割成两个，由HRegionServer管理，管理哪些HRegion由Hmaster分配。</span><br><span class="line"></span><br><span class="line">HRegion存取一个子表时，会创建一个HRegion对象，然后对表的每个列族（Column Family）创建一个store实例.</span><br><span class="line"></span><br><span class="line">每个store都会有0个或多个StoreFile与之对应，每个StoreFile都会对应一个HFile.</span><br><span class="line"></span><br><span class="line">HFile就是实际的存储文件，因此，一个HRegion还拥有一个MemStore实例。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBase和Hive有什么区别？Hive与HBase的底层存储是什么？Hive是产生的原因是什么？HBase是为了弥补Hadoop的什么缺陷"><a href="#HBase和Hive有什么区别？Hive与HBase的底层存储是什么？Hive是产生的原因是什么？HBase是为了弥补Hadoop的什么缺陷" class="headerlink" title="HBase和Hive有什么区别？Hive与HBase的底层存储是什么？Hive是产生的原因是什么？HBase是为了弥补Hadoop的什么缺陷?"></a>HBase和Hive有什么区别？Hive与HBase的底层存储是什么？Hive是产生的原因是什么？HBase是为了弥补Hadoop的什么缺陷?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.共同点</span><br><span class="line">hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储.</span><br><span class="line"></span><br><span class="line">b.区别</span><br><span class="line">Hive是建立在Hadoop之上为了减少MapReducejobs编写工作的批处理系统</span><br><span class="line">HBase是为了支持弥补Hadoop对实时操作的缺陷的项目</span><br><span class="line"></span><br><span class="line">想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop，如果是索引访问，就用HBase+Hadoop。</span><br><span class="line"></span><br><span class="line">Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。</span><br><span class="line"></span><br><span class="line">Hive本身不存储和计算数据，它完全依赖于HDFS和 MapReduce，Hive中的表纯逻辑。</span><br><span class="line"></span><br><span class="line">Hive借用Hadoop的MapReduce来完成一些Hive中的命令的执行.</span><br><span class="line"></span><br><span class="line">HBase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。</span><br><span class="line"></span><br><span class="line">HBase是列存储。</span><br><span class="line"></span><br><span class="line">HDFS作为底层存储，HDFS是存放文件的系统，而HBase负责组织文件。</span><br><span class="line"></span><br><span class="line">Hive需要用到HDFS存储文件，需要用到MapReduce计算框架。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解释下HBase实时查询的原理"><a href="#解释下HBase实时查询的原理" class="headerlink" title="解释下HBase实时查询的原理?"></a>解释下HBase实时查询的原理?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">实时查询，可以认为是从内存中查询，一般响应时间在1秒内。</span><br><span class="line"></span><br><span class="line">HBase的机制是数据先写入到内存中，当数据量达到一定的量（如 128M），再写入磁盘中.</span><br><span class="line"></span><br><span class="line">在内存中，是不进行数据的更新或合并操作的，只增加数据.</span><br><span class="line"></span><br><span class="line">这使得用户的写操作只要进入内存中就可以立即返回，保证了HBase I&#x2F;O的高性能。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="列簇怎么创建比较好"><a href="#列簇怎么创建比较好" class="headerlink" title="列簇怎么创建比较好?"></a>列簇怎么创建比较好?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rowKey最好要创建有规则的rowKey，即最好是有序的。</span><br><span class="line"></span><br><span class="line">HBase中一张表最好只创建一到两个列族比较好，因为HBase不能很好的处理多个列族。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="描述HBase的rowKey的设计原则"><a href="#描述HBase的rowKey的设计原则" class="headerlink" title="描述HBase的rowKey的设计原则"></a>描述HBase的rowKey的设计原则</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.rowkey长度原则</span><br><span class="line">rowkey是一个二进制码流，可以是任意字符串，最大长度64kb.</span><br><span class="line">实际应用中一般为10-100bytes，以 byte[]形式保存，一般设计成定长。</span><br><span class="line">建议越短越好，不要超过16个字节， 原因如下：</span><br><span class="line">    数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长会极大影响HFile的存储效率.</span><br><span class="line">    MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低.</span><br><span class="line">    系统不能缓存更多的数据，这样会降低检索效率.</span><br><span class="line"></span><br><span class="line">b.rowkey散列原则</span><br><span class="line">如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面.</span><br><span class="line">建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段.</span><br><span class="line">这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。</span><br><span class="line">如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上.</span><br><span class="line">这样在数据检索的时候负载会集中在个别的RegionServer 上，造成热点问题，会降低查询效率。</span><br><span class="line"></span><br><span class="line">c.rowkey唯一原则</span><br><span class="line">必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的.</span><br><span class="line">因此，设计rowkey的时候，要充分利用这个排序的特点.</span><br><span class="line">将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="描述-Hbase-中-scan-和-get-的功能以及实现的异同"><a href="#描述-Hbase-中-scan-和-get-的功能以及实现的异同" class="headerlink" title="描述 Hbase 中 scan 和 get 的功能以及实现的异同"></a>描述 Hbase 中 scan 和 get 的功能以及实现的异同</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.按指定RowKey获取唯一一条记录，get方法( org.apache.hadoop.hbase.client.Get)</span><br><span class="line">Get的方法处理分两种:</span><br><span class="line">    设置了ClosestRowBefore和没有设置的rowlock主要是用来保证行的事务性，即每个get是以一个row来标记的.</span><br><span class="line">    一个row中可以有很多family和column。</span><br><span class="line"></span><br><span class="line">b.按指定的条件获取一批记录，scan方法(org.apache.Hadoop.hbase.client.Scan)</span><br><span class="line">实现条件查询功能使用的就是scan方式:</span><br><span class="line">    scan可以通过setCaching与setBatch方法提高速度(以空间换时间);</span><br><span class="line">    scan可以通过setStartRow与setEndRow来限定范围([start，end]start? 是闭区间，end 是开区间)。范围越小，性能越高;</span><br><span class="line">    scan可以通过setFilter方法添加过滤器，这也是分页、多条件查询的基础。 </span><br><span class="line">    </span><br><span class="line">c.全表扫描，即直接扫描整张表中所有行记录。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="请详细描述HBase中一个Cell的结构"><a href="#请详细描述HBase中一个Cell的结构" class="headerlink" title="请详细描述HBase中一个Cell的结构"></a>请详细描述HBase中一个Cell的结构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HBase中通过row和columns确定的为一个存贮单元称为cell。</span><br><span class="line">Cell：由&#123;row key，column(&#x3D;&lt;family&gt; + &lt;label&gt;)，version&#125;是唯一确定的单元.</span><br><span class="line">Cell中的数据是没有类型的，全部是字节码形式存贮。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="简述HBase中compact用途是什么，什么时候触发，分为哪两种，有什么区别，有哪些相关配置参数"><a href="#简述HBase中compact用途是什么，什么时候触发，分为哪两种，有什么区别，有哪些相关配置参数" class="headerlink" title="简述HBase中compact用途是什么，什么时候触发，分为哪两种，有什么区别，有哪些相关配置参数?"></a>简述HBase中compact用途是什么，什么时候触发，分为哪两种，有什么区别，有哪些相关配置参数?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.compact的用途</span><br><span class="line">在HBase中每当有memstore数据flush到磁盘之后，就形成一个storeFile.</span><br><span class="line">当storeFile的数量达到一定程度后，就需要将storeFile文件来进行compaction操作。</span><br><span class="line">Compact的作用：</span><br><span class="line">    合并文件</span><br><span class="line">    清除过期，多余版本的数据</span><br><span class="line">    提高读写数据的效率</span><br><span class="line"></span><br><span class="line">b.种类与区别</span><br><span class="line">种类:</span><br><span class="line">    minor</span><br><span class="line">    major</span><br><span class="line">区别:</span><br><span class="line">    Minor操作只用来做部分文件的合并操作以及包括minVersion&#x3D;0并且设置ttl的过期版本清理，不做任何删除数据、多版本数据的清理工作。</span><br><span class="line">    Major操作是对Region下的HStore下的所有StoreFile执行合并操作，最终的结果是整理合并出一个文件。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="简述HBase-Filter的实现原理是什么？结合实际项目经验，写出几个使用Filter的场景。"><a href="#简述HBase-Filter的实现原理是什么？结合实际项目经验，写出几个使用Filter的场景。" class="headerlink" title="简述HBase Filter的实现原理是什么？结合实际项目经验，写出几个使用Filter的场景。"></a>简述HBase Filter的实现原理是什么？结合实际项目经验，写出几个使用Filter的场景。</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HBase为筛选数据提供了一组过滤器，通过这个过滤器可以在 HBase中的数据的多个维度（行，列，数据版本）上进行对数据的筛选操作，也就是说过滤器最终能够筛选的数据能够细化到具体的一个存储单元格上（由行键，列名，时间戳定位）。</span><br><span class="line">RowFilter、PrefixFilter。HBase的filter是通过scan设置的，所以是基于scan的查询结果进行过滤。</span><br><span class="line">过滤器的类型很多，但是可以分为两大类&lt;比较过滤器，专用过滤器&gt;。</span><br><span class="line">过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端。</span><br><span class="line">如在进行订单开发的时候，我们使用rowkeyfilter过滤出某个用户的所有订单。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBase内部是什么机制？"><a href="#HBase内部是什么机制？" class="headerlink" title="HBase内部是什么机制？"></a>HBase内部是什么机制？</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在HBase中无论是增加新行还是修改已有的行，其内部流程都是相同的。</span><br><span class="line">HBase接到命令后存下变化信息，或者写入失败抛出异常。默认情况下，执行写入时会写到两个地方：预写式日志（write-ahead log，也称HLog）和MemStore。</span><br><span class="line">HBase的默认方式是把写入动作记录在这两个地方，以保证数据持久化。</span><br><span class="line">只有当这两个地方的变化信息都写入并确认后，才认为写动作完成。</span><br><span class="line"></span><br><span class="line">MemStore是内存里的写入缓冲区，HBase中数据在永久写入硬盘之前在这里累积。</span><br><span class="line">当MemStore填满后，其中的数据会刷写到硬盘，生成一个HFile。HFile是HBase使用的底层存储格式。</span><br><span class="line">HFile对应于列族，一个列族可以有多个HFile，但一个HFile不能存储多个列族的数据。</span><br><span class="line">在集群的每个节点上，每个列族有一个MemStore。</span><br><span class="line">大型分布式系统中硬件故障很常见，HBase也不例外。</span><br><span class="line"></span><br><span class="line">设想一下，如果MemStore还没有刷写，服务器就崩溃了，内存中没有写入硬盘的数据就会丢失。</span><br><span class="line">HBase的应对办法是在写动作完成之前先写入WAL。</span><br><span class="line">HBase集群中每台服务器维护一个WAL来记录发生的变化。</span><br><span class="line">WAL是底层文件系统上的一个文件。</span><br><span class="line">直到WAL新记录成功写入后，写动作才被认为成功完成。</span><br><span class="line">这可以保证HBase和支撑它的文件系统满足持久性。</span><br><span class="line"></span><br><span class="line">大多数情况下，HBase使用Hadoop分布式文件系统（HDFS）来作为底层文件系统。</span><br><span class="line">如果HBase服务器宕机，没有从MemStore里刷写到HFile的数据将可以通过回放WAL来恢复。</span><br><span class="line">你不需要手工执行。Hbase的内部机制中有恢复流程部分来处理。</span><br><span class="line">每台HBase服务器有一个WAL，这台服务器上的所有表（和它们的列族）共享这个WAL。</span><br><span class="line">你可能想到，写入时跳过WAL应该会提升写性能。但我们不建议禁用WAL，除非你愿意在出问题时丢失数据。</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">    不写入WAL会在RegionServer故障时增加丢失数据的风险。</span><br><span class="line">    关闭WAL，出现故障时HBase可能无法恢复数据，没有刷写到硬盘的所有写入数据都会丢失。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBase宕机如何处理？"><a href="#HBase宕机如何处理？" class="headerlink" title="HBase宕机如何处理？"></a>HBase宕机如何处理？</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">宕机分为HMaster宕机和HRegisoner宕机.</span><br><span class="line">如果是HRegisoner宕机，HMaster会将其所管理的region重新分布到其他活动的RegionServer上，由于数据和日志都持久在HDFS中，该操作不会导致数据丢失。</span><br><span class="line">所以数据的一致性和安全性是有保障的。</span><br><span class="line"></span><br><span class="line">如果是HMaster宕机，HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。</span><br><span class="line">即ZooKeeper会保证总会有一个HMaster在对外提供服务。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo设置Blog的分类与标签</title>
    <url>/2016/04/29/Hexo%E8%AE%BE%E7%BD%AEBlog%E7%9A%84%E5%88%86%E7%B1%BB%E4%B8%8E%E6%A0%87%E7%AD%BE/</url>
    <content><![CDATA[<blockquote>
<p>Hexo的简单使用</p>
</blockquote>
<span id="more"></span>

<h2 id="1-添加关于页面"><a href="#1-添加关于页面" class="headerlink" title="1.添加关于页面"></a>1.添加关于页面</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new page &quot;about&quot;</span><br><span class="line"># 修改themes&#x2F;chan&#x2F;_config.yml</span><br><span class="line">nav:</span><br><span class="line">    name: 关于</span><br><span class="line">    url: &#x2F;about</span><br></pre></td></tr></table></figure>

<h2 id="2-添加分类-标签页面"><a href="#2-添加分类-标签页面" class="headerlink" title="2.添加分类-标签页面"></a>2.添加分类-标签页面</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 标题</span><br><span class="line">date: 时间</span><br><span class="line">categories: 分类</span><br><span class="line">tags: 标签</span><br><span class="line">---</span><br></pre></td></tr></table></figure>

<h2 id="3-设置阅读全文"><a href="#3-设置阅读全文" class="headerlink" title="3.设置阅读全文"></a>3.设置阅读全文</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将下面语句写在需要出现的位置，自定义预览长度</span><br><span class="line">&lt;!-- more --&gt;</span><br></pre></td></tr></table></figure>

<h2 id="4-设置文档模板"><a href="#4-设置文档模板" class="headerlink" title="4.设置文档模板"></a>4.设置文档模板</h2><p>修改scaffolds目录下的post文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">categories:</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase的工作原理</title>
    <url>/2017/11/23/HBase%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>Hbase工作原理介绍</p>
</blockquote>
<span id="more"></span>

<h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Client</span><br><span class="line">包含访问HBase的接口，Client维护着一些cache来加快对HBase的访问，比如Region的位置信息。</span><br><span class="line"></span><br><span class="line"># Zookeeper</span><br><span class="line">保证任何时候，集群中只有一个Master</span><br><span class="line">存贮所有Region的寻址入口，root表在哪台服务器上。</span><br><span class="line">实时监控Region Server的状态，将Region server的上线和下线信息实时通知给Master</span><br><span class="line">存储HBase的schema,包括有哪些table，每个table有哪些column family</span><br><span class="line"></span><br><span class="line"># Master</span><br><span class="line">为Region Server分配Region</span><br><span class="line">负责Region Server的负载均衡</span><br><span class="line">发现失效的Region Server并重新分配其上的Region</span><br><span class="line">HDFS上的垃圾文件回收</span><br><span class="line">处理schema更新请求</span><br><span class="line"></span><br><span class="line"># Region Server</span><br><span class="line">Region Server维护Master分配给它的Region，处理对这些Region的IO请求</span><br><span class="line">Region Server负责切分在运行过程中变得过大的Region</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">可以看到，Client访问HBase上数据的过程并不需要Master参与（寻址访问Zookeeper和Region Server，数据读写访问Region Server），Master仅仅维护者table和region的元数据信息，负载很低。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="物理存储"><a href="#物理存储" class="headerlink" title="物理存储"></a>物理存储</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Table的存储方式</span><br><span class="line">Table中的所有行都按照row key的字典序排列。</span><br><span class="line">Table在行的方向上分割为多个Region。</span><br><span class="line">Region按大小分割的，每个表一开始只有一个Region，随着数据不断插入表，Region不断增大，当增大到一个阀值的时候，Region就会等分会两个新的Region。当Table中的行不断增多，就会有越来越多的Region。</span><br><span class="line">Region是Hbase中分布式存储和负载均衡的最小单元。最小单元就表示不同的Region可以分布在不同的Region Server上。但一个Region是不会拆分到多个Region Server上的。</span><br><span class="line">Region虽然是分布式存储的最小单元，但并不是物理存储的最小单元。</span><br><span class="line">事实上，Region由一个或者多个Store组成，每个Store保存一个column family。</span><br><span class="line">每个Strore又由一个MemStore和0至多个StoreFile组成。如上图</span><br><span class="line"></span><br><span class="line"># MemStore &amp; StoreFile</span><br><span class="line">一个Region由多个Store组成，每个Store包含一个列族的所有数据</span><br><span class="line">Store包括位于内存的MemStore和位于硬盘的StoreFile</span><br><span class="line">写操作先写入MemStore,当MemStore中的数据量达到某个阈值，Region Server启动flashcache进程写入StoreFile,每次写入形成单独一个StoreFile</span><br><span class="line">当StoreFile大小超过一定阈值后，会把当前的Region分割成两个，并由Master分配给相应的Region Server，实现负载均衡</span><br><span class="line">客户端检索数据时，先在MemStore找，找不到再找StoreFile</span><br><span class="line">StoreFile以HFile格式保存在HDFS上。</span><br><span class="line"></span><br><span class="line"># HFile</span><br><span class="line">Data Block段: 保存表中的数据，这部分可以被压缩</span><br><span class="line">Meta Block段(可选的): 保存用户自定义的kv对，可以被压缩。</span><br><span class="line">File Info段: Hfile的元信息，不被压缩，用户也可以在这一部分添加自己的元信息。</span><br><span class="line">Data Block Index段: Data Block的索引。每条索引的key是被索引的block的第一条记录的key。</span><br><span class="line">Meta Block Index段(可选的): Meta Block的索引。</span><br><span class="line">Trailer: </span><br><span class="line">    这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先 读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check).</span><br><span class="line">    然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。</span><br><span class="line">    DataBlock Index采用LRU机制淘汰。</span><br><span class="line">HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。</span><br><span class="line">目标HFile的压缩支持两种方式：Gzip，Lzo。</span><br><span class="line"></span><br><span class="line"># HLog(WAL Log)</span><br><span class="line">HLog记录数据的所有变更,一旦数据修改，就可以从log中进行恢复</span><br><span class="line">每个Region Server维护一个HLog,而不是每个Region一个。这样不同Region(来自不同table)的日志会混在一起.</span><br><span class="line">这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，因此可以提高对table的写性能。</span><br><span class="line">带来的麻烦是，如果一台Region Server下线，为了恢复其上的Region，需要将Region Server上的log进行拆分，然后分发到其它Region Server上进行恢复。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="寻址机制"><a href="#寻址机制" class="headerlink" title="寻址机制"></a>寻址机制</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 两个关键表</span><br><span class="line">ROOT表 &amp; META表</span><br><span class="line"></span><br><span class="line"># 假设我们要从Table里面插寻一条RowKey是RK10000的数据</span><br><span class="line">a.从META表里面查询哪个Region包含这条数据</span><br><span class="line">b.获取管理这个Region的RegionServer地址</span><br><span class="line">c.连接这个RegionServer, 查到这条数据</span><br><span class="line"></span><br><span class="line"># 系统如何找到某个RK</span><br><span class="line">a.第一层是保存Zookeeper里面的文件，它持有ROOT Region的位置</span><br><span class="line">b.ROOT Region是META表的第一个Region,其中保存了META表其它Region的位置,通过ROOT Region，我们就可以访问META表的数据</span><br><span class="line">c.META是第三层，它是一个特殊的表，保存了HBase中所有数据表的Region位置信息</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">ROOT Region永远不会被split，保证了最需要三次跳转，就能定位到任意Region</span><br><span class="line">META表每行保存一个Region的位置信息，RK采用表名加表的最后一行编码而成。</span><br><span class="line">为了加快访问，META表的全部Region都保存在内存中。</span><br><span class="line">Client会将查询过的位置信息保存缓存起来，缓存不会主动失效，因此如果Client上的缓存全部失效，则需要进行最多6次网络来回，才能定位到正确的Region(其中三次用来发现缓存失效，另外三次用来获取位置信息)。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="读写过程"><a href="#读写过程" class="headerlink" title="读写过程"></a>读写过程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 读请求过程</span><br><span class="line">客户端通过Zookeeper以及ROOT表和META表找到目标数据所在的Region Server</span><br><span class="line">联系Region Server查询目标数据</span><br><span class="line">Region Server定位到目标数据所在的Region，发出查询请求</span><br><span class="line">Region先在MemStore中查找，命中则返回</span><br><span class="line">如果在MemStore中找不到，则在StoreFile中扫描（可能会扫描到很多的StoreFile--bloomfilter）</span><br><span class="line"></span><br><span class="line"># 写请求过程</span><br><span class="line">Client向Region Server提交写请求</span><br><span class="line">Region Server找到目标Region</span><br><span class="line">Region检查数据是否与schema一致</span><br><span class="line">如果客户端没有指定版本，则获取当前系统时间作为数据版本</span><br><span class="line">将更新写入WAL log</span><br><span class="line">将更新写入MemStore</span><br><span class="line">判断MemStore的是否需要flush为StoreFile</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">数据在更新时首先写入HLog(WAL log)和内存(MemStore)中，MemStore中的数据是排序的，当MemStore累计到一定阈值时，就会创建一个新的MemStore，并 且将老的MemStore添加到flush队列，由单独的线程flush到磁盘上，成为一个StoreFile。</span><br><span class="line">于此同时，系统会在Zookeeper中记录一个redo point，表示这个时刻之前的变更已经持久化了。</span><br><span class="line">当系统出现意外时，可能导致内存(MemStore)中的数据丢失，此时使用Log(WAL log)来恢复checkpoint之后的数据。</span><br><span class="line">StoreFile是只读的，一旦创建后就不可以再修改。因此Hbase的更新其实是不断追加的操作。</span><br><span class="line">当一个Store中的StoreFile达到一定的阈值后，就会进行一次合并(minor_compact, major_compact),将对同一个key的修改合并到一起，形成一个大的StoreFile，当StoreFile的大小达到一定阈值后，又会对 StoreFile进行split，等分为两个StoreFile。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Region管理"><a href="#Region管理" class="headerlink" title="Region管理"></a>Region管理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Region分配</span><br><span class="line">任何时刻，一个Region只能分配给一个Region Server。Master记录了当前有哪些可用的Region Server。以及当前哪些Region分配给了哪些Region Server，哪些Region还没有分配。</span><br><span class="line">当需要分配的新的Region，并且有一个Region Server上有可用空间时，Master就给这个Region Server发送一个装载请求，把Region分配给这个Region Server。Region Server得到请求后，就开始对此Region提供服务。</span><br><span class="line"></span><br><span class="line"># Region Server上线</span><br><span class="line">Master使用Zookeeper来跟踪Region Server状态。当某个Region Server启动时，会首先在Zookeeper上的server目录下建立代表自己的znode，并获得该znode的独占锁。</span><br><span class="line">由于Master订阅了server目录上的变更消息，当server目录下的文件出现新增或删除操作时，Master可以得到来自Zookeeper的实时通知。因此一旦Region Server上线，Master能马上得到消息。</span><br><span class="line"></span><br><span class="line"># Region Server下线</span><br><span class="line">当Region Server下线时，它和Zookeeper的会话断开，Zookeeper而自动释放代表这台server的文件上的独占锁。而Master不断轮询server目录下文件的锁状态。</span><br><span class="line">如果Master发现某个Region Server丢失了它自己的独占锁，(或者Master连续几次和Region Server通信都无法成功),Master就是尝试去获取代表这个Region Server的读写锁，一旦获取成功，就可以确定：</span><br><span class="line">    Region Server和Zookeeper之间的网络断开了。</span><br><span class="line">    Region Server挂了。</span><br><span class="line">无论哪种情况，Region Server都无法继续为它的Region提供服务了，此时Master会删除server目录下代表这台Region Server的znode数据，并将这台Region Server的Region分配给其它还活着的同志。</span><br><span class="line">如果网络短暂出现问题导致Region Server丢失了它的锁，那么Region Server重新连接到zookeeper之后，只要代表它的文件还在，它就会不断尝试获取这个文件上的锁，一旦获取到了，就可以继续提供服务。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Master工作机制"><a href="#Master工作机制" class="headerlink" title="Master工作机制"></a>Master工作机制</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Master上线</span><br><span class="line">Master启动进行以下步骤:</span><br><span class="line">    从Zookeeper上获取唯一一个代表Active Master的锁，用来阻止其它Master成为Master。</span><br><span class="line">    扫描Zookeeper上的server父节点，获得当前可用的Region Server列表。</span><br><span class="line">    和每个Region Server通信，获得当前已分配的Region和Region Server的对应关系。</span><br><span class="line">    扫描META Region的集合，计算得到当前还未分配的Region，将他们放入待分配Region列表。</span><br><span class="line"> </span><br><span class="line"># Master下线</span><br><span class="line">由于Master只维护表和Region的元数据，而不参与表数据IO的过程，Master下线仅导致所有元数据的修改被冻结(无法创建删除表，无法修改表的schema，无法进行Region的负载均衡，无法处理Region 上下线，无法进行Region的合并，唯一例外的是Region的split可以正常进行，因为只有Region Server参与)，表的数据读写还可以正常进行。</span><br><span class="line">因此Master下线短时间内对整个NBase集群没有影响。从上线过程可以看到，Master保存的信息全是可以冗余信息（都可以从系统其它地方收集到或者计算出来），因此，一般HBase集群中总是有一个Master在提供服务，还有一个以上 的Master在等待时机抢占它的位置。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive基础语法整理</title>
    <url>/2017/08/30/Hive%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>对于hive的一些常见使用</p>
</blockquote>
<span id="more"></span>

<h2 id="进入hive"><a href="#进入hive" class="headerlink" title="进入hive"></a>进入hive</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive</span><br><span class="line"># 或者</span><br><span class="line">beeline</span><br><span class="line">!connect jdbc:hive2:&#x2F;&#x2F;localhost:10000</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看表信息</span><br><span class="line">desc formatted u1;</span><br><span class="line">desc extended u1;</span><br><span class="line">show create table u1;</span><br><span class="line"></span><br><span class="line"># 创建内部表</span><br><span class="line">create table data(id int,name string)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;;</span><br><span class="line">create table data(id int,name string)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;;</span><br><span class="line"></span><br><span class="line"># 创建外部表</span><br><span class="line">create table data(id int,name string)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">stored as textfile</span><br><span class="line">location &#39;&#x2F;root&#x2F;class.txt&#39;;</span><br><span class="line"></span><br><span class="line"># 修改表名</span><br><span class="line">alter table data rename to data1;</span><br><span class="line"></span><br><span class="line"># 删除表</span><br><span class="line"># 删除外部表,会删除元数据,数据内容还存在</span><br><span class="line"># 删除内部表,会删除元数据,数据内容也被删除</span><br><span class="line">drop table data;</span><br><span class="line"></span><br><span class="line"># 清空表数据</span><br><span class="line"># truncate不能删除外部表</span><br><span class="line">truncate table data;</span><br><span class="line"></span><br><span class="line"># 添加字段</span><br><span class="line">alter table data add columns(age int,add String);</span><br><span class="line"> </span><br><span class="line"># 删除字段---只留下age,add字段</span><br><span class="line">alter table data replace columns(age int,add String);</span><br><span class="line"></span><br><span class="line"># 导入数据</span><br><span class="line">insert into 导入到多张表 from aa7 insert into aa10 select id where id&gt;1 insert into aa11 select name;</span><br><span class="line"></span><br><span class="line"># 导出数据</span><br><span class="line"># 从hive表导出到本地目录</span><br><span class="line">insert overwrite local directory &#39;&#x2F;root&#x2F;hivedata&#x2F;exp1&#39; row format delimited fields terminated by &#39;\t&#39; select * from aa7;</span><br><span class="line"># 从hive表导出到hdfs目录</span><br><span class="line">insert overwrite directory &#39;&#x2F;exp1&#39; row format delimited fields terminated by &#39;\t&#39; select * from aa7;</span><br><span class="line"># &gt;重定向到文件中</span><br><span class="line">hive -e &quot;use uu;select * from aa7&quot; &gt; &#x2F;root&#x2F;exp1;</span><br><span class="line"></span><br><span class="line"># 分区表</span><br><span class="line">create table data(id int,name string)</span><br><span class="line">partitioned by (country string)</span><br><span class="line">row format delimited fields terminated by &#39;,&#39;;</span><br><span class="line"></span><br><span class="line"># 加载文件到分区表</span><br><span class="line">load data local inpath &#39;&#x2F;root&#x2F;class.txt&#39; into table data partition(country&#x3D;&#39;china&#39;);</span><br><span class="line"></span><br><span class="line"># 添加分区</span><br><span class="line">alter table data add partition(country&#x3D;&#39;america&#39;);</span><br><span class="line"></span><br><span class="line"># 添加分区时指定数据</span><br><span class="line">alter table data add partition (year&#x3D;&#39;2018&#39;) location &#39;&#x2F;数据&#39;;</span><br><span class="line"></span><br><span class="line">#指定分区对应到已有的数据</span><br><span class="line">alter table test partition (year&#x3D;&#39;2020&#39;) set location &#39;hdfs:&#x2F;&#x2F;hadoop01:9000&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;qf.db&#x2F;comm&#x2F;year&#x3D;2021&#39;</span><br><span class="line"></span><br><span class="line"># 显示分区</span><br><span class="line">show partitions test;</span><br><span class="line"></span><br><span class="line"># 删除分区</span><br><span class="line">alter table data drop partition(country&#x3D;&#39;america&#39;);</span><br><span class="line"></span><br><span class="line"># 重命名分区</span><br><span class="line">alter table data partition(year&#x3D;&#39;2017&#39;) rename to partition(year&#x3D;&#39;2021&#39;);</span><br><span class="line"></span><br><span class="line"># 修改字段(字段名,类型,位置,注释)--必须指定列的类型</span><br><span class="line">alter table data change column id myid int comment &#39;注释&#39;; </span><br><span class="line">alter table data change column id myid int after name; </span><br><span class="line">alter table data change column id myid int frist;</span><br><span class="line"></span><br><span class="line"># 将内部表改成外部表--true必须大写</span><br><span class="line">alter table test set tblproperties(&#39;EXTERNAL&#39;&#x3D;&#39;TRUE&#39;);</span><br><span class="line"></span><br><span class="line"># 将外部表改成内部表--false小写大写都可以</span><br><span class="line">alter table test set tblproperties(&#39;EXTERNAL&#39;&#x3D;&#39;false&#39;);</span><br><span class="line"></span><br><span class="line"># 创建分桶表</span><br><span class="line"># 分桶字段对其Hash值,然后mod总的桶数,得到的余数就是一个桶</span><br><span class="line">create table if not exists data(</span><br><span class="line">    id int,</span><br><span class="line">    comment string</span><br><span class="line">)</span><br><span class="line">clustered by(id) into 4 buckets</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"># (load方式加载分桶数据,并没有真正的划分数据)</span><br><span class="line">load data local inpath &#39;&#x2F;root&#x2F;class.txt&#39; into table data;</span><br><span class="line"></span><br><span class="line"># 在使用insert into 加载数据时,要开启自动分桶</span><br><span class="line">set hive.enforce.bucketing&#x3D;true;</span><br><span class="line">insert into data select id,comment from tmp3;</span><br><span class="line"></span><br><span class="line"># 查询分桶数据</span><br><span class="line">select * from data;</span><br><span class="line"></span><br><span class="line"># 查询第一桶tablesample(bucket x out of y on id);</span><br><span class="line"># x:从第几桶开始</span><br><span class="line"># y:总的桶数</span><br><span class="line"># x不能大于y</span><br><span class="line"># y总的桶数尽量是源总桶数的倍数</span><br><span class="line"># x x+分桶数&#x2F;(分桶数&#x2F;y)&#x3D;下一桶</span><br><span class="line">select * from data tablesample(bucket 1 out of 4 on id);</span><br><span class="line">select * from tmp4 order by rand() limit 3; </span><br><span class="line">select * from tmp4 tablesample(1 rows);</span><br><span class="line">select * from tmp4 tablesample(2G);</span><br><span class="line">select * from tmp4 tablesample(2M);</span><br><span class="line">select * from tmp4 tablesample(2K);</span><br><span class="line">select * from tmp4 tablesample(2B);</span><br><span class="line">select * from tmp4 tablesample(20 percent);</span><br><span class="line"></span><br><span class="line"># 大表标识</span><br><span class="line">select &#x2F;*+STREAMTABLE(d)*&#x2F; d.deptno from data d left join emp e on d.deptno &#x3D; e.deptno where e.deptno is null;</span><br><span class="line"></span><br><span class="line"># sort by</span><br><span class="line"># 局部排序,指单个reducer结果集排序,整个job是不是有序,他不管</span><br><span class="line"># order by</span><br><span class="line"># 全局排序,整个job的所有reducer中的数据都会排序</span><br><span class="line"># 当reducer数量为1时,两者都一样.</span><br><span class="line"># distribute by</span><br><span class="line"># 对distribute by后的字段进行分发,相同的分发到同一个reduce,当distribute by 和 sort by同时存在时,distribute by在sort by之前</span><br><span class="line"># cluster by</span><br><span class="line"># 兼有distribute by 和 sort by 的功能,但是sort by需要是升序</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="复杂的数据类型"><a href="#复杂的数据类型" class="headerlink" title="复杂的数据类型"></a>复杂的数据类型</h2><ul>
<li>array:数组</li>
<li>map:集合</li>
<li>struct:结构体</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lucey 90,80,99</span><br><span class="line">biman 10,100,99</span><br><span class="line">create table if not exists data(</span><br><span class="line">    name string,</span><br><span class="line">    score Array&lt;double&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line"></span><br><span class="line">select a.name,a.score[1],a.score[2] from data a where a.score[1] &gt; 60;</span><br><span class="line"></span><br><span class="line">lucey Chinese:90,Math:80,English:99</span><br><span class="line">biman Chinese:10,Math:100,English:99</span><br><span class="line">create table if not exists data(</span><br><span class="line">    name string,</span><br><span class="line">    score map&lt;string,double&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;</span><br><span class="line">map keys terminated by &#39;:&#39;;</span><br><span class="line"> </span><br><span class="line">select m.name,m.score[&#39;Chinese&#39;],m.score[&#39;Math&#39;] from data m where m.score[&#39;Math&#39;] &gt; 60;</span><br><span class="line"> </span><br><span class="line">lucey 90,80,99</span><br><span class="line">biman 10,100,99</span><br><span class="line">create table if not exists data(</span><br><span class="line">    name string,</span><br><span class="line">    score struct&lt;chinese:double,math:double,englist:double&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#39;\t&#39;</span><br><span class="line">collection items terminated by &#39;,&#39;;</span><br><span class="line"></span><br><span class="line">select str.name,str.score.chinese,str.score.math from data str where str.score.math &gt; 60;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="内部函数"><a href="#内部函数" class="headerlink" title="内部函数"></a>内部函数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select rand();</span><br><span class="line">select split(rand()*100,&quot;\\.&quot;)[0];</span><br><span class="line">select substring(rand()*100,2,2);</span><br><span class="line">select regexp_replace(&quot;a.jsp&quot;,&quot;jsp&quot;,&quot;html&quot;);</span><br><span class="line">select cast(1 as double);</span><br><span class="line">select case when 1&#x3D;1 then &quot;man&quot; when 1&#x3D;2 then &quot;woman&quot; else &quot;yao&quot; end;</span><br><span class="line">select concat(&quot;1&quot;,&quot;2&quot;);</span><br><span class="line">select concat_ws(&quot;|&quot;,&quot;1&quot;,&quot;2&quot;); # 连接符</span><br><span class="line">select length(&quot;asb&quot;);</span><br><span class="line"></span><br><span class="line"># 时间戳转字符串</span><br><span class="line">from_unixtime(cast(f.finish_time as int),&#39;yyyy-MM-dd HH:mm:ss&#39;)</span><br><span class="line"># 字符串转时间戳</span><br><span class="line">unix_timestamp(&#39;2018-02-06 00:00:00&#39;,&#39;yyyy-MM-dd HH:mm:ss&#39;)</span><br><span class="line"></span><br><span class="line">row_number():没有相同名次,名次不空位</span><br><span class="line">rank():有并列名次,并列名次后将空位</span><br><span class="line">dense_rank():有并列名次,无空位</span><br><span class="line"></span><br><span class="line"># 查询每个班级的前三</span><br><span class="line">select tmp.c,tmp.s </span><br><span class="line">from (</span><br><span class="line">select r.class c,r.score s,row_number() over (distribute by r.class sort by r.score desc) rr from classinfo r</span><br><span class="line">) tmp </span><br><span class="line">where rr&lt;4;</span><br><span class="line"></span><br><span class="line">select class,score,</span><br><span class="line">rank() over(distribute by class sort by score desc) rank,</span><br><span class="line">dense_rank() over(distribute by class sort by score desc) dense_rank,</span><br><span class="line">row_number() over(distribute by class sort by score desc) row_number</span><br><span class="line">from classinfo;</span><br><span class="line"></span><br><span class="line"># 一行变多行,将split之后转化的array替换成一个虚表</span><br><span class="line"># 关键词lateral view explode</span><br><span class="line">select id, test.context from demo lateral view explode(split(context, &#39;,&#39;)) test as context;</span><br><span class="line"></span><br><span class="line"># 多行变一行,会去重,将分组之后的context放入集合中,并以&#39;,&#39;进行分割</span><br><span class="line"># 如果不使用concat_ws,结果就是集合</span><br><span class="line">select id,concat_ws(&#39;,&#39;,collect_set(context)) from demo group by id;</span><br><span class="line"></span><br><span class="line"># 多行多列变一行,先进行字符串拼接,在存入集合中</span><br><span class="line">select a.id,concat_ws(&#39;|&#39;,collect_set(a.info))</span><br><span class="line">from (</span><br><span class="line">	select id,concat_ws(&#39;,&#39;,context,dt) as info</span><br><span class="line">	from demo</span><br><span class="line">) a</span><br><span class="line">group by a.id;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="执行计划"><a href="#执行计划" class="headerlink" title="执行计划"></a>执行计划</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 通过EXPLAIN,EXPLAIN EXTENDED或explain DEPENDENCY来查看执行计划和依赖情况</span><br><span class="line">explain select * from aa7;</span><br><span class="line">explain extended select * from aa7;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="命令行执行"><a href="#命令行执行" class="headerlink" title="命令行执行"></a>命令行执行</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive -e &quot;sql&quot;</span><br><span class="line"># 显示表名列名</span><br><span class="line">set hive.cli.print.header&#x3D;true;</span><br><span class="line"># 不显示表名</span><br><span class="line">set  hive.resultset.use.unique.column.names&#x3D;false;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive整合Hbase</title>
    <url>/2017/08/30/Hive%E6%95%B4%E5%90%88Hbase/</url>
    <content><![CDATA[<blockquote>
<p>hive表数据依赖于hbase数据</p>
</blockquote>
<span id="more"></span>

<h2 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># hbase已存在的表,需要使用外部表的形式创建</span><br><span class="line">CREATE EXTERNAL TABLE hbase_hive_1(key int, value string)   </span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;   </span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,cf1:val&quot;)   </span><br><span class="line">TBLPROPERTIES (&quot;hbase.table.name&quot; &#x3D; &quot;xyz&quot;);  </span><br><span class="line"></span><br><span class="line"># 复杂数据类型,hbase表test2中的字段为user:gid,user:sid,info:uid,info:level</span><br><span class="line">CREATE EXTERNAL TABLE hive_test_2(key int,user map&lt;string,string&gt;,info map&lt;string,string&gt;)   </span><br><span class="line">STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39;   </span><br><span class="line">WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; &#x3D;&quot;user:,info:&quot;)    </span><br><span class="line">TBLPROPERTIES  (&quot;hbase.table.name&quot; &#x3D; &quot;test2&quot;);  </span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+Github搭建个人Blog</title>
    <url>/2016/04/29/Hexo+Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BABlog/</url>
    <content><![CDATA[<blockquote>
<p>博客搭建</p>
</blockquote>
<span id="more"></span>

<h2 id="1-环境安装"><a href="#1-环境安装" class="headerlink" title="1.环境安装"></a>1.环境安装</h2><ul>
<li>下载Git</li>
<li>下载Node.js</li>
<li>下载HexoEditor</li>
<li><strong>注意：</strong>Git，Node.js和HexoEditor都已经下载好在<a href="https://github.com/jxeditor/jxeditor.github.io">Github</a>上</li>
<li>并且需要注意版本Node.js使用12.13.1</li>
</ul>
<hr>
<h2 id="2-Hexo安装命令与免密配置"><a href="#2-Hexo安装命令与免密配置" class="headerlink" title="2.Hexo安装命令与免密配置"></a>2.Hexo安装命令与免密配置</h2><ul>
<li>进入blog的文件夹</li>
<li>右键Git Bash Here</li>
<li>执行下列命令</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据具体node的安装目录而定，设置全局module和缓存</span></span><br><span class="line">npm config <span class="built_in">set</span> prefix <span class="string">&quot;C:/Program Files/nodejs/npm_global&quot;</span></span><br><span class="line">npm config <span class="built_in">set</span> cache <span class="string">&quot;C:/Program Files/nodejs/npm_cache&quot;</span></span><br><span class="line"><span class="comment"># 设置系统变量和用户变量（Windows）</span></span><br><span class="line">系统变量NODE_PATH=C:/Program Files/nodejs/npm_global/node_modules</span><br><span class="line">用户变量PATH=C:/Program Files/nodejs/npm_global</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果默认存放在C盘，可以没有权限，需要管理员权限运行</span></span><br><span class="line">npm install -g hexo-cli</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入Blog目录，下载hexo</span></span><br><span class="line">npm install hexo --save</span><br><span class="line"></span><br><span class="line"><span class="comment"># 免密配置</span></span><br><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;账号&quot;</span></span><br><span class="line"><span class="comment"># 复制~/.ssh/id_rsa.pub内容</span></span><br><span class="line"><span class="comment"># 添加到GitHub的ssh key</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-启动Hexo"><a href="#3-启动Hexo" class="headerlink" title="3.启动Hexo"></a>3.启动Hexo</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改目录配置文件_config.yml的deploy</span></span><br><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repository: git@github.com:jxeditor/jxeditor.github.io.git</span><br><span class="line">  branch: master</span><br><span class="line">hexo g <span class="comment"># 生成静态页面</span></span><br><span class="line">hexo s <span class="comment"># 启动本地服务</span></span><br><span class="line">hexo d <span class="comment"># 部署到远程</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-Git初始化并将项目推送到分支"><a href="#4-Git初始化并将项目推送到分支" class="headerlink" title="4.Git初始化并将项目推送到分支"></a>4.Git初始化并将项目推送到分支</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将项目推送到GitHub的分支</span></span><br><span class="line">git config --global user.email <span class="string">&quot;you@example.com&quot;</span></span><br><span class="line">git config --global user.name <span class="string">&quot;Your Name&quot;</span></span><br><span class="line">git init</span><br><span class="line">git remote add origin https://github.com/jxeditor/jxeditor.github.io.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改git remote的模式</span></span><br><span class="line">git remote -v</span><br><span class="line"><span class="comment"># 将https方式修改成主机:仓库的形式</span></span><br><span class="line">git remote add origin https://github.com/jxeditor/jxeditor.github.io.git</span><br><span class="line">git remote rm origin</span><br><span class="line">git remote add origin git@github.com:jxeditor/jxeditor.github.io.git</span><br><span class="line"></span><br><span class="line">git pull origin 远程分支 <span class="comment"># 远程分支没有可以不进行拉取</span></span><br><span class="line"><span class="comment"># 进行一系列操作</span></span><br><span class="line">git add *</span><br><span class="line">git commit -m <span class="string">&quot;注释&quot;</span></span><br><span class="line">git push origin 本地分支:远程分支</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="5-Git进行push时项目含有子项目"><a href="#5-Git进行push时项目含有子项目" class="headerlink" title="5.Git进行push时项目含有子项目"></a>5.Git进行push时项目含有子项目</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除子项目的.git文件夹</span></span><br><span class="line">git rm --cached file_path</span><br><span class="line">git add file_path</span><br><span class="line">git commit -m <span class="string">&quot;注释&quot;</span></span><br><span class="line">git push origin 分支</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="6-Git新建切换分支"><a href="#6-Git新建切换分支" class="headerlink" title="6.Git新建切换分支"></a>6.Git新建切换分支</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git branch 分支</span><br><span class="line">git checkout 分支</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="7-Git出现error-failed-to-push-some-refs-…"><a href="#7-Git出现error-failed-to-push-some-refs-…" class="headerlink" title="7.Git出现error:failed to push some refs …"></a>7.Git出现error:failed to push some refs …</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 原因是远程分支有文件，但是本地进行文件操作时，并没有去拉去远程文件</span><br><span class="line">git pull origin 分支</span><br><span class="line"># 进行一系列修改</span><br><span class="line">git add *</span><br><span class="line">git commit -m &quot;注释&quot;</span><br><span class="line">git push origin 分支</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="8-Git大文件上传"><a href="#8-Git大文件上传" class="headerlink" title="8.Git大文件上传"></a>8.Git大文件上传</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git lfs install</span><br><span class="line">git lfs track <span class="string">&quot;*.psd&quot;</span> <span class="comment"># 追踪规则，后缀psd文件超过限制</span></span><br><span class="line">git add .gitattributes</span><br><span class="line">git add file.psd</span><br><span class="line">git commit -m <span class="string">&quot;Add design file&quot;</span></span><br><span class="line">git push origin 本地分支:远程分支</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive自连接与窗口分析函数的妙用</title>
    <url>/2019/07/22/Hive%E8%87%AA%E8%BF%9E%E6%8E%A5%E4%B8%8E%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0%E7%9A%84%E5%A6%99%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>对于HQL的窗口函数,大家可能都不陌生,尤其一些求topN等等.本人在网上就看到一些很神奇的列子.</p>
</blockquote>
<span id="more"></span>

<h1 id="基本需求"><a href="#基本需求" class="headerlink" title="基本需求"></a>基本需求</h1><blockquote>
<p>有一张demo表,有id(用户),date(月份),pv(访问量)三个字段,针对表中数据实现相对应的业务需求</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name    date        pv</span><br><span class="line">A       2015-01     5</span><br><span class="line">A       2015-01     15</span><br><span class="line">B       2015-01     5</span><br><span class="line">A       2015-01     8</span><br><span class="line">B       2015-01     25</span><br><span class="line">A       2015-01     5</span><br><span class="line">A       2015-02     4</span><br><span class="line">A       2015-02     6</span><br><span class="line">A       2015-02     4</span><br><span class="line">B       2015-02     10</span><br><span class="line">B       2015-02     5</span><br><span class="line">A       2015-03     16</span><br><span class="line">A       2015-03     22</span><br><span class="line">B       2015-03     23</span><br><span class="line">B       2015-03     10</span><br><span class="line">B       2015-03     11</span><br></pre></td></tr></table></figure>
<p><strong>需求:</strong><br>求每个用户截止每月为止的最大单月访问次数和累计到该月的总访问次数</p>
<hr>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="自连接方式"><a href="#自连接方式" class="headerlink" title="自连接方式"></a>自连接方式</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.name,a.date,a.pv,<span class="keyword">max</span>(b.pv),<span class="keyword">sum</span>(b.pv)</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,<span class="built_in">date</span>,<span class="keyword">sum</span>(pv) pv</span><br><span class="line">    <span class="keyword">from</span> demo</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>,<span class="built_in">date</span></span><br><span class="line">) a</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,<span class="built_in">date</span>,<span class="keyword">sum</span>(pv) pv</span><br><span class="line">    <span class="keyword">from</span> demo</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>,<span class="built_in">date</span></span><br><span class="line">) b</span><br><span class="line"><span class="keyword">on</span> a.name=b.name</span><br><span class="line"><span class="keyword">where</span> a.date &gt;= b.date</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> a.name,a.date,a.pv;</span><br></pre></td></tr></table></figure>
<h2 id="窗口函数方式"><a href="#窗口函数方式" class="headerlink" title="窗口函数方式"></a>窗口函数方式</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> a.name,a.date,a.pv,</span><br><span class="line"><span class="keyword">sum</span>(a.pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> a.name <span class="keyword">order</span> <span class="keyword">by</span> a.date) sumpv,</span><br><span class="line"><span class="keyword">max</span>(a.pv) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> a.name <span class="keyword">order</span> <span class="keyword">by</span> a.date) maxpv</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,<span class="built_in">date</span>,<span class="keyword">sum</span>(pv) pv</span><br><span class="line">    <span class="keyword">from</span> demo</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span>,<span class="built_in">date</span></span><br><span class="line">) a;</span><br></pre></td></tr></table></figure>
<p>**实测: **<br>两种方式对应的job数为6:1</p>
<hr>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive压缩效果不明显踩坑记录</title>
    <url>/2020/06/10/Hive%E5%8E%8B%E7%BC%A9%E6%95%88%E6%9E%9C%E4%B8%8D%E6%98%8E%E6%98%BE%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>记录一下Hive的配置参数漏配的严重后果</p>
</blockquote>
<span id="more"></span>

<h2 id="前因"><a href="#前因" class="headerlink" title="前因"></a>前因</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">项目逻辑</span><br><span class="line">    1.Spark实时任务每五分钟生成parquet格式的snappy压缩文件</span><br><span class="line">    2.另有一个Spark离线任务对前一天生成的小文件进行合并</span><br><span class="line"></span><br><span class="line">问题</span><br><span class="line">    发现合并前小文件总大小要远远小于合并后的文件总大小</span><br><span class="line">    足有两倍的差值</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">建表时漏配置</span><br><span class="line">    parquet.page.size</span><br><span class="line">    parquet.dictionary.page.size</span><br><span class="line">导致仍使用默认值1M,压缩效果极其不理想</span><br><span class="line"></span><br><span class="line">由于表是外部表,删除表重新创建表,或修改表属性</span><br><span class="line">ALTER TABLE tableName set TBLPROPERTIES (&#39;parquet.page.size&#39;&#x3D;&#39;33554432&#39;);</span><br><span class="line">ALTER TABLE tableName set TBLPROPERTIES (&#39;parquet.dictionary.page.size&#39;&#x3D;&#39;33554432&#39;);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hue-Hive权限设置</title>
    <url>/2020/05/03/Hue-Hive%E6%9D%83%E9%99%90%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<blockquote>
<p>设置Hive-Hue权限,库,表以及分区字段</p>
</blockquote>
<span id="more"></span>

<h3 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一般在metastore库中</span><br><span class="line"></span><br><span class="line"># 库权限</span><br><span class="line">db_privs</span><br><span class="line"></span><br><span class="line"># 表权限</span><br><span class="line">tbl_privs</span><br><span class="line"></span><br><span class="line"># 分区字段权限</span><br><span class="line">part_col_privs</span><br></pre></td></tr></table></figure>

<h3 id="开启Hive权限控制"><a href="#开启Hive权限控制" class="headerlink" title="开启Hive权限控制"></a>开启Hive权限控制</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"># 修改Hive配置项</span><br><span class="line">CDH-Hive-配置-&lt;Hive客户端高级配置代码段(安全阀),HiveServer2高级配置代码段(安全阀)&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启权限控制 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 创建该表的用户对这个表拥有的权限,设置的是所有 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.createtable.owner.grants<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ALL<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.task.factory<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 超级管理员,设置成hive用户 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.users.in.admin.role<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Hue创建非Admin账户"><a href="#Hue创建非Admin账户" class="headerlink" title="Hue创建非Admin账户"></a>Hue创建非Admin账户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 用户添加</span><br><span class="line">管理用户-添加组-设置权限-添加用户-设置组</span><br><span class="line"></span><br><span class="line"># 权限设置</span><br><span class="line">beeswax.access,rdbms.access,impala.access,pig.access</span><br><span class="line"></span><br><span class="line"># 用户</span><br><span class="line">guest</span><br><span class="line"></span><br><span class="line"># 组</span><br><span class="line">guest</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Hive设置用户角色以及权限"><a href="#Hive设置用户角色以及权限" class="headerlink" title="Hive设置用户角色以及权限"></a>Hive设置用户角色以及权限</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建Linux用户</span><br><span class="line">useradd guest</span><br><span class="line">passwd guest</span><br><span class="line">gpasswd -a guest guest</span><br><span class="line"></span><br><span class="line"># 需要使用超级管理员-hive</span><br><span class="line">su hive</span><br><span class="line"></span><br><span class="line"># 默认角色为public</span><br><span class="line">set role admin;</span><br><span class="line"></span><br><span class="line"># 查看当前角色</span><br><span class="line">show current roles;</span><br><span class="line"></span><br><span class="line"># 查看所有角色</span><br><span class="line">show roles;</span><br><span class="line"></span><br><span class="line"># 新建一个角色</span><br><span class="line">create role guest;</span><br><span class="line"></span><br><span class="line"># 将角色赋给Linux用户</span><br><span class="line">grant role guest to user guest;</span><br><span class="line"></span><br><span class="line"># 赋予用户查询default库的权限</span><br><span class="line">grant select on database default to user guest;</span><br><span class="line"></span><br><span class="line"># 赋予角色查询default库的权限</span><br><span class="line"># 按理来说拥有这个角色的用户都会获取到权限才对</span><br><span class="line"># 但是没有反应</span><br><span class="line">grant select on database default to role guest;</span><br><span class="line"></span><br><span class="line"># 收回权限</span><br><span class="line">revoke select on database default from role guest;</span><br><span class="line"></span><br><span class="line"># 查看权限</span><br><span class="line">show grant user guest;</span><br><span class="line">show grant user guest on database default;</span><br><span class="line">show grant user guest on table default.test;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>在使用权限控制后,如果发现权限设置无效,可能是需要指定角色导致,默认角色是<code>public</code><br>并且,对用户或角色赋予数据库的权限,并没有用,只能设置表权限才能生效</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>JdbcRowDataLookupFunction缓存逻辑问题</title>
    <url>/2021/02/19/JdbcRowDataLookupFunction%E7%BC%93%E5%AD%98%E9%80%BB%E8%BE%91%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>修复下设置缓存导致关联不上数据的问题<a href="https://issues.apache.org/jira/projects/FLINK/issues/FLINK-21415">issue</a></p>
</blockquote>
<span id="more"></span>

<h2 id="问题再现"><a href="#问题再现" class="headerlink" title="问题再现"></a>问题再现</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">不太能理解TTL期间对于未命中的key,为什么保留,虽然减少了IO,也增加了数据的不准确性</span><br><span class="line"></span><br><span class="line">使用FlinkSQL创建MySQL维表时,使用了缓存</span><br><span class="line">lookup.cache.ttl</span><br><span class="line">lookup.max-retries</span><br><span class="line">lookup.cache.max-rows</span><br><span class="line">发现和不使用缓存得到的数据存在明显差异</span><br><span class="line">同一订单,不同状态,假如产生2条数据</span><br><span class="line">不使用缓存,当关联维表时因为操作延迟原因可能出现关联不上的情况,等到第二次关联时,基本上可以关联上</span><br><span class="line">使用缓存,第一次关联不上,之后的数据也关联不上</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">使用缓存后,对于查询不到的数据也进行了缓存</span><br><span class="line"><span class="comment">// JdbcRowDataLookupFunction</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(Object... keys)</span> </span>&#123;</span><br><span class="line">    RowData keyRow = GenericRowData.of(keys);</span><br><span class="line">    <span class="keyword">if</span> (cache != <span class="keyword">null</span>) &#123;</span><br><span class="line">        List&lt;RowData&gt; cachedRows = cache.getIfPresent(keyRow);</span><br><span class="line">        <span class="keyword">if</span> (cachedRows != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (RowData cachedRow : cachedRows) &#123;</span><br><span class="line">                collect(cachedRow);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> retry = <span class="number">0</span>; retry &lt;= maxRetryTimes; retry++) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            statement.clearParameters();</span><br><span class="line">            statement = lookupKeyRowConverter.toExternal(keyRow, statement);</span><br><span class="line">            <span class="keyword">try</span> (ResultSet resultSet = statement.executeQuery()) &#123;</span><br><span class="line">                <span class="keyword">if</span> (cache == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">                        collect(jdbcRowConverter.toInternal(resultSet));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    ArrayList&lt;RowData&gt; rows = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                    <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">                        RowData row = jdbcRowConverter.toInternal(resultSet);</span><br><span class="line">                        rows.add(row);</span><br><span class="line">                        collect(row);</span><br><span class="line">                    &#125;</span><br><span class="line">                    rows.trimToSize();</span><br><span class="line">                    <span class="comment">// 即便resultSet没有查询到数据,也会将rows放入缓存</span></span><br><span class="line">                    cache.put(keyRow, rows);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (SQLException e) &#123;</span><br><span class="line">            ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(rows.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    cache.put(keyRow, rows);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>IceBerg初探</title>
    <url>/2021/04/27/IceBerg%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<blockquote>
<p>整活,去年就有听过之信大佬讲过iceberg,不过当时认为iceberg只是做了一层数据编排的操作,认识还是太过于浅薄</p>
</blockquote>
<span id="more"></span>

<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Flink和Zeppelin环境需要提前部署起来</span><br><span class="line"># 下载Iceberg源码</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;iceberg.git</span><br><span class="line">cd iceberg</span><br><span class="line"># 开始编译</span><br><span class="line">.&#x2F;gradlew build -x test</span><br><span class="line"># 对应的运行时依赖包</span><br><span class="line">spark-runtime&#x2F;build&#x2F;libs</span><br><span class="line">spark-runtime&#x2F;build&#x2F;libs</span><br><span class="line">flink-runtime&#x2F;build&#x2F;libs</span><br><span class="line">hive-runtime&#x2F;build&#x2F;libs</span><br><span class="line"></span><br><span class="line"># 在Zeppelin配置Iceberg依赖包</span><br><span class="line">%flink.conf</span><br><span class="line">flink.execution.jars &#x2F;opt&#x2F;iceberg&#x2F;flink-runtime&#x2F;build&#x2F;libs&#x2F;iceberg-flink-runtime-dae6c49.jar</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="创建新Catalog"><a href="#创建新Catalog" class="headerlink" title="创建新Catalog"></a>创建新Catalog</h2><h3 id="HiveCatalog"><a href="#HiveCatalog" class="headerlink" title="HiveCatalog"></a>HiveCatalog</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">%flink.ssql</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">CATALOG</span> hive_catalog <span class="keyword">WITH</span> (</span><br><span class="line"><span class="string">&#x27;type&#x27;</span>=<span class="string">&#x27;iceberg&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;catalog-type&#x27;</span>=<span class="string">&#x27;hive&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;uri&#x27;</span>=<span class="string">&#x27;thrift://mac:9083&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;clients&#x27;</span>=<span class="string">&#x27;5&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;property-version&#x27;</span>=<span class="string">&#x27;1&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;warehouse&#x27;</span>=<span class="string">&#x27;hdfs://user/hive/warehouse&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以我的理解,这里使用的还是Hive的元数据,相当于一个软连接</span></span><br><span class="line"><span class="comment"># 对hive_catalog的操作就是对Hive的元数据进行操作</span></span><br></pre></td></tr></table></figure>
<h3 id="HadoopCatalog"><a href="#HadoopCatalog" class="headerlink" title="HadoopCatalog"></a>HadoopCatalog</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">CATALOG</span> hadoop_catalog <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;type&#x27;</span>=<span class="string">&#x27;iceberg&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;catalog-type&#x27;</span>=<span class="string">&#x27;hadoop&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;warehouse&#x27;</span>=<span class="string">&#x27;hdfs://mac:9000/warehouse/test&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;property-version&#x27;</span>=<span class="string">&#x27;1&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<h3 id="CustomCatalog"><a href="#CustomCatalog" class="headerlink" title="CustomCatalog"></a>CustomCatalog</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">CATALOG</span> my_catalog <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;type&#x27;</span>=<span class="string">&#x27;iceberg&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;catalog-impl&#x27;</span>=<span class="string">&#x27;com.my.custom.CatalogImpl&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;my-additional-catalog-config&#x27;</span>=<span class="string">&#x27;my-value&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="目前支持的SQL操作"><a href="#目前支持的SQL操作" class="headerlink" title="目前支持的SQL操作"></a>目前支持的SQL操作</h2><h3 id="DDL操作"><a href="#DDL操作" class="headerlink" title="DDL操作"></a>DDL操作</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建DataBase</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> iceberg_db;</span><br><span class="line"><span class="keyword">USE</span> iceberg_db;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_catalog.default.sample (</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">&#x27;unique id&#x27;</span>,</span><br><span class="line">    <span class="keyword">data</span> <span class="keyword">STRING</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- 不支持计算列,水印,主键</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 分区</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_catalog.default.sample (</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">&#x27;unique id&#x27;</span>,</span><br><span class="line">    <span class="keyword">data</span> <span class="keyword">STRING</span></span><br><span class="line">) PARTITIONED <span class="keyword">BY</span> (<span class="keyword">data</span>);</span><br><span class="line"><span class="comment">-- Iceberg支持隐藏分区,但是FlinkSQL暂不支持</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- Like建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive_catalog.default.sample (</span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">&#x27;unique id&#x27;</span>,</span><br><span class="line">    <span class="keyword">data</span> <span class="keyword">STRING</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span>  hive_catalog.default.sample_like <span class="keyword">LIKE</span> hive_catalog.default.sample;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 修改表</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> hive_catalog.default.sample <span class="keyword">SET</span> (<span class="string">&#x27;write.format.default&#x27;</span>=<span class="string">&#x27;avro&#x27;</span>)</span><br><span class="line"><span class="comment">-- 仅支持flink-1.11</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 修改表名</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> hive_catalog.default.sample <span class="keyword">RENAME</span> <span class="keyword">TO</span> hive_catalog.default.new_sample;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除表</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> hive_catalog.default.sample;</span><br></pre></td></tr></table></figure>
<h3 id="查询操作"><a href="#查询操作" class="headerlink" title="查询操作"></a>查询操作</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 支持流\批两种模式</span></span><br><span class="line"><span class="keyword">SET</span> execution.type = streaming</span><br><span class="line"><span class="keyword">SET</span> table.dynamic-<span class="keyword">table</span>-options.enabled=<span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">sample</span> <span class="comment">/*+ OPTIONS(&#x27;streaming&#x27;=&#x27;true&#x27;, &#x27;monitor-interval&#x27;=&#x27;1s&#x27;)*/</span> ;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">sample</span> <span class="comment">/*+ OPTIONS(&#x27;streaming&#x27;=&#x27;true&#x27;, &#x27;monitor-interval&#x27;=&#x27;1s&#x27;, &#x27;start-snapshot-id&#x27;=&#x27;3821550127947089987&#x27;)*/</span> ;</span><br><span class="line"><span class="comment">-- monitor-interval 连续监视新提交的数据文件的时间间隔(默认值:&quot;1s&quot;)</span></span><br><span class="line"><span class="comment">-- start-snapshot-id 流作业开始的快照ID</span></span><br><span class="line"><span class="keyword">SET</span> execution.type = batch</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> <span class="keyword">sample</span>       ;</span><br></pre></td></tr></table></figure>
<h3 id="插入操作"><a href="#插入操作" class="headerlink" title="插入操作"></a>插入操作</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- Append</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> hive_catalog.default.sample <span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> hive_catalog.default.sample <span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="keyword">data</span> <span class="keyword">from</span> other_kafka_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Overwrite(流作业不支持)</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">sample</span> <span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE hive_catalog.default.sample <span class="keyword">PARTITION</span>(<span class="keyword">data</span>=<span class="string">&#x27;a&#x27;</span>) <span class="keyword">SELECT</span> <span class="number">6</span>;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="API操作"><a href="#API操作" class="headerlink" title="API操作"></a>API操作</h2><h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">--- Batch</span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadooptable(<span class="string">&quot;hdfs://nn:8020/warehouse/path&quot;</span>);</span><br><span class="line">DataStream&lt;RowData&gt; batch = FlinkSource.forRowData()</span><br><span class="line">     .env(env)</span><br><span class="line">     .tableLoader(loader)</span><br><span class="line">     .streaming(<span class="keyword">false</span>)</span><br><span class="line">     .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print all records to stdout.</span></span><br><span class="line">batch.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Submit and execute this batch read job.</span></span><br><span class="line">env.execute(<span class="string">&quot;Test Iceberg Batch Read&quot;</span>);</span><br><span class="line"></span><br><span class="line">--- Stream</span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();</span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadooptable(<span class="string">&quot;hdfs://nn:8020/warehouse/path&quot;</span>);</span><br><span class="line">DataStream&lt;RowData&gt; stream = FlinkSource.forRowData()</span><br><span class="line">     .env(env)</span><br><span class="line">     .tableLoader(loader)</span><br><span class="line">     .streaming(<span class="keyword">true</span>)</span><br><span class="line">     .startSnapshotId(<span class="number">3821550127947089987</span>)</span><br><span class="line">     .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print all records to stdout.</span></span><br><span class="line">stream.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Submit and execute this streaming read job.</span></span><br><span class="line">env.execute(<span class="string">&quot;Test Iceberg Batch Read&quot;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="数据写入"><a href="#数据写入" class="headerlink" title="数据写入"></a>数据写入</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">--- 追加</span><br><span class="line">StreamExecutionEnvironment env = ...;</span><br><span class="line"></span><br><span class="line">DataStream&lt;RowData&gt; input = ... ;</span><br><span class="line">Configuration hadoopConf = <span class="keyword">new</span> Configuration();</span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadooptable(<span class="string">&quot;hdfs://nn:8020/warehouse/path&quot;</span>);</span><br><span class="line"></span><br><span class="line">FlinkSink.forRowData(input)</span><br><span class="line">    .tableLoader(tableLoader)</span><br><span class="line">    .hadoopConf(hadoopConf)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">&quot;Test Iceberg DataStream&quot;</span>);</span><br><span class="line"></span><br><span class="line">--- 覆写</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="合并小文件"><a href="#合并小文件" class="headerlink" title="合并小文件"></a>合并小文件</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 原理和Spark的rewriteDataFiles相同</span></span><br><span class="line"><span class="keyword">import</span> org.apache.iceberg.flink.actions.Actions;</span><br><span class="line"></span><br><span class="line">TableLoader tableLoader = TableLoader.fromHadooptable(<span class="string">&quot;hdfs://nn:8020/warehouse/path&quot;</span>);</span><br><span class="line">Table table = tableLoader.loadTable();</span><br><span class="line">RewriteDataFilesActionResult result = Actions.forTable(table)</span><br><span class="line">        .rewriteDataFiles()</span><br><span class="line">        .execute();</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 由于我使用的版本较高,iceberg目前好像支持到flink-1.11.*,hadoop-2.7.3,hive-2.3.8</span><br><span class="line"># 编译之后会报错,不让强转,翻看源码就有点懵圈了</span><br><span class="line"># 明明HadoopCatalog就是继承BaseMetastoreCatalog抽象类,然后BaseMetastoreCatalog实现Catalog接口</span><br><span class="line">Cannot initialize Catalog, org.apache.iceberg.hadoop.HadoopCatalog does not implement Catalog.</span><br><span class="line"># 通过flink版本降级到1.11.3之后可以使用</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="更多操作传送门"><a href="#更多操作传送门" class="headerlink" title="更多操作传送门"></a>更多操作<a href="https://iceberg.apache.org/flink/">传送门</a></h2>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>iceberg</tag>
      </tags>
  </entry>
  <entry>
    <title>KMeans算法实现逻辑及代码</title>
    <url>/2019/11/29/KMeans%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91%E5%8F%8A%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<blockquote>
<p>将KMeans算法整个逻辑梳理出来,容易理解,并实现代码</p>
</blockquote>
<span id="more"></span>

<h2 id="什么是KMeans"><a href="#什么是KMeans" class="headerlink" title="什么是KMeans"></a>什么是KMeans</h2><p><strong>注意：</strong>需要与KNN区分开来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kmeans是聚类算法的一种,是一种迭代求解的聚类分析算法。</span><br><span class="line">其步骤是随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。</span><br><span class="line">聚类中心以及分配给它们的对象就代表一个聚类。</span><br><span class="line">每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。</span><br><span class="line">这个过程将不断重复直到满足某个终止条件。</span><br><span class="line">终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.选取K个类中心</span><br><span class="line">迭代&#123;</span><br><span class="line">    2.计算每个点到K个类中心的距离</span><br><span class="line">    3.把数据点分配给距离最近的一个类中心</span><br><span class="line">    4.计算新的类中心(对该类中的所有点取均值)</span><br><span class="line">&#125;</span><br><span class="line">5.满足终止条件后终止迭代</span><br><span class="line">    a.不再有重新分配</span><br><span class="line">    b.最大迭代数</span><br><span class="line">    c.所有类中心移动小于某一值</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h2><h3 id="Point类-数据点"><a href="#Point类-数据点" class="headerlink" title="Point类(数据点)"></a>Point类(数据点)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.algorithm.kmeans;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Point</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">float</span>[] localArray;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> clusterId;  <span class="comment">// 类中心的ID(记录最近的类中心)</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">float</span> dist;     <span class="comment">// 到类中心的距离(记录最近的类中心)</span></span><br><span class="line">    <span class="keyword">private</span> Point clusterPoint;<span class="comment">//中心点信息</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Point <span class="title">getClusterPoint</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> clusterPoint;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setClusterPoint</span><span class="params">(Point clusterPoint)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.clusterPoint = clusterPoint;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">float</span>[] getLocalArray() &#123;</span><br><span class="line">        <span class="keyword">return</span> localArray;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setLocalArray</span><span class="params">(<span class="keyword">float</span>[] localArray)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.localArray = localArray;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getClusterId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> clusterId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Point</span><span class="params">(<span class="keyword">int</span> id, <span class="keyword">float</span>[] localArray)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.localArray = localArray;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Point</span><span class="params">(<span class="keyword">float</span>[] localArray)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = -<span class="number">1</span>; <span class="comment">//表示不属于任意一个类</span></span><br><span class="line">        <span class="keyword">this</span>.localArray = localArray;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">float</span>[] getlocalArray() &#123;</span><br><span class="line">        <span class="keyword">return</span> localArray;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setClusterId</span><span class="params">(<span class="keyword">int</span> clusterId)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.clusterId = clusterId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getClusterid</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> clusterId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getDist</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> dist;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDist</span><span class="params">(<span class="keyword">float</span> dist)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.dist = dist;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> JSONObject.toJSONString(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(<span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (obj == <span class="keyword">null</span> || getClass() != obj.getClass())</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">        Point point = (Point) obj;</span><br><span class="line">        <span class="keyword">if</span> (point.localArray.length != localArray.length)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; localArray.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (Float.compare(point.localArray[i], localArray[i]) != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">float</span> x = localArray[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">float</span> y = localArray[localArray.length - <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">long</span> temp = x != +<span class="number">0.0d</span> ? Double.doubleToLongBits(x) : <span class="number">0L</span>;</span><br><span class="line">        <span class="keyword">int</span> result = (<span class="keyword">int</span>) (temp ^ (temp &gt;&gt;&gt; <span class="number">32</span>));</span><br><span class="line">        temp = y != +<span class="number">0.0d</span> ? Double.doubleToLongBits(y) : <span class="number">0L</span>;</span><br><span class="line">        result = <span class="number">31</span> * result + (<span class="keyword">int</span>) (temp ^ (temp &gt;&gt;&gt; <span class="number">32</span>));</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Cluster类-类中心"><a href="#Cluster类-类中心" class="headerlink" title="Cluster类(类中心)"></a>Cluster类(类中心)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.algorithm.kmeans;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Cluster</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> id;<span class="comment">// 标识</span></span><br><span class="line">    <span class="keyword">private</span> Point center;<span class="comment">// 中心</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Point&gt; members = <span class="keyword">new</span> ArrayList&lt;Point&gt;();<span class="comment">// 成员</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cluster</span><span class="params">(<span class="keyword">int</span> id, Point center)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.center = center;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Cluster</span><span class="params">(<span class="keyword">int</span> id, Point center, List&lt;Point&gt; members)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.center = center;</span><br><span class="line">        <span class="keyword">this</span>.members = members;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addPoint</span><span class="params">(Point newPoint)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!members.contains(newPoint)) &#123;</span><br><span class="line">            members.add(newPoint);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;样本数据点 &#123;&quot;</span> + newPoint.toString() + <span class="string">&quot;&#125; 已经存在！&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Point <span class="title">getCenter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> center;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCenter</span><span class="params">(Point center)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.center = center;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Point&gt; <span class="title">getMembers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> members;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String toString = <span class="string">&quot;Cluster \n&quot;</span> + <span class="string">&quot;Cluster_id=&quot;</span> + <span class="keyword">this</span>.id + <span class="string">&quot;, center:&#123;&quot;</span> + <span class="keyword">this</span>.center.toString() + <span class="string">&quot;&#125;&quot;</span>;</span><br><span class="line">        <span class="keyword">for</span> (Point point : members) &#123;</span><br><span class="line">            toString += <span class="string">&quot;\n&quot;</span> + point.toString();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> toString + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="DistanceCompute类-求距离工具"><a href="#DistanceCompute类-求距离工具" class="headerlink" title="DistanceCompute类(求距离工具)"></a>DistanceCompute类(求距离工具)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.algorithm.kmeans;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DistanceCompute</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 求欧式距离</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">getEuclideanDis</span><span class="params">(Point p1, Point p2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> count_dis = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">float</span>[] p1_local_array = p1.getlocalArray();</span><br><span class="line">        <span class="keyword">float</span>[] p2_local_array = p2.getlocalArray();</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> (p1_local_array.length != p2_local_array.length) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;length of array must be equal!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; p1_local_array.length; i++) &#123;</span><br><span class="line">            count_dis += Math.pow(p1_local_array[i] - p2_local_array[i], <span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> Math.sqrt(count_dis);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="KMeansRun类-算法执行类"><a href="#KMeansRun类-算法执行类" class="headerlink" title="KMeansRun类(算法执行类)"></a>KMeansRun类(算法执行类)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.algorithm.kmeans;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KMeansRun</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> kNum;                             <span class="comment">//类中心的个数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> iterNum = <span class="number">10</span>;                     <span class="comment">//迭代次数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> iterMaxTimes = <span class="number">100000</span>;            <span class="comment">//单次迭代最大运行次数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> iterRunTimes = <span class="number">0</span>;                 <span class="comment">//单次迭代实际运行次数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">float</span> disDiff = (<span class="keyword">float</span>) <span class="number">0.01</span>;         <span class="comment">//单次迭代终止条件，两次运行中类中心的距离差</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;<span class="keyword">float</span>[]&gt; original_data = <span class="keyword">null</span>;    <span class="comment">//用于存放，原始数据集</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> List&lt;Point&gt; pointList = <span class="keyword">null</span>;  <span class="comment">//用于存放，原始数据集所构建的点集</span></span><br><span class="line">    <span class="keyword">private</span> DistanceCompute disC = <span class="keyword">new</span> DistanceCompute();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> len = <span class="number">0</span>;                          <span class="comment">//用于记录每个数据点的维度</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KMeansRun</span><span class="params">(<span class="keyword">int</span> k, List&lt;<span class="keyword">float</span>[]&gt; original_data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.kNum = k;</span><br><span class="line">        <span class="keyword">this</span>.original_data = original_data;</span><br><span class="line">        <span class="keyword">this</span>.len = original_data.get(<span class="number">0</span>).length;</span><br><span class="line">        <span class="comment">//检查规范</span></span><br><span class="line">        check();</span><br><span class="line">        <span class="comment">//初始化点集</span></span><br><span class="line">        init();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检查规范</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">check</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (kNum == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;k must be the number &gt; 0&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (original_data == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;program can&#x27;t get real data&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化数据集，把数组转化为Point类型。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        pointList = <span class="keyword">new</span> ArrayList&lt;Point&gt;();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, j = original_data.size(); i &lt; j; i++) &#123;</span><br><span class="line">            pointList.add(<span class="keyword">new</span> Point(i, original_data.get(i)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 随机选取中心点，构建成中心类。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Set&lt;Cluster&gt; <span class="title">chooseCenterCluster</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Set&lt;Cluster&gt; clusterSet = <span class="keyword">new</span> HashSet&lt;Cluster&gt;();</span><br><span class="line">        Random random = <span class="keyword">new</span> Random();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> id = <span class="number">0</span>; id &lt; kNum; ) &#123;</span><br><span class="line">            Point point = pointList.get(random.nextInt(pointList.size()));</span><br><span class="line">            <span class="comment">// 用于标记是否已经选择过该数据</span></span><br><span class="line">            <span class="keyword">boolean</span> flag = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">for</span> (Cluster cluster : clusterSet) &#123;</span><br><span class="line">                <span class="keyword">if</span> (cluster.getCenter().equals(point)) &#123;</span><br><span class="line">                    flag = <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果随机选取的点没有被选中过，则生成一个类中心</span></span><br><span class="line">            <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                Cluster cluster = <span class="keyword">new</span> Cluster(id, point);</span><br><span class="line">                clusterSet.add(cluster);</span><br><span class="line">                id++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> clusterSet;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 为每个点分配一个类！</span></span><br><span class="line"><span class="comment">     * 为每个点选择最近的中心点,以及到中心点的位置</span></span><br><span class="line"><span class="comment">     * 然后为中心点设置成员,成员就是那些选择了该中心点的点</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cluster</span><span class="params">(Set&lt;Cluster&gt; clusterSet)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 计算每个点到K个中心的距离，并且为每个点标记类别号</span></span><br><span class="line">        <span class="keyword">for</span> (Point point : pointList) &#123;</span><br><span class="line">            <span class="keyword">float</span> min_dis = Integer.MAX_VALUE;</span><br><span class="line">            <span class="keyword">for</span> (Cluster cluster : clusterSet) &#123;</span><br><span class="line">                <span class="keyword">float</span> tmp_dis = (<span class="keyword">float</span>) Math.min(disC.getEuclideanDis(point, cluster.getCenter()), min_dis);</span><br><span class="line">                <span class="keyword">if</span> (tmp_dis != min_dis) &#123;</span><br><span class="line">                    min_dis = tmp_dis;</span><br><span class="line">                    point.setClusterId(cluster.getId());</span><br><span class="line">                    point.setDist(min_dis);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 新清除原来所有的类中成员。把所有的点，分别加入每个类别</span></span><br><span class="line">        <span class="keyword">for</span> (Cluster cluster : clusterSet) &#123;</span><br><span class="line">            cluster.getMembers().clear();</span><br><span class="line">            <span class="keyword">for</span> (Point point : pointList) &#123;</span><br><span class="line">                <span class="keyword">if</span> (point.getClusterid() == cluster.getId()) &#123;</span><br><span class="line">                    cluster.addPoint(point);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 计算每个类的中心位置！</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">calculateCenter</span><span class="params">(Set&lt;Cluster&gt; clusterSet)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> ifNeedIter = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (Cluster cluster : clusterSet) &#123;</span><br><span class="line">            List&lt;Point&gt; point_list = cluster.getMembers();</span><br><span class="line">            <span class="keyword">float</span>[] sumAll = <span class="keyword">new</span> <span class="keyword">float</span>[len];</span><br><span class="line">            <span class="comment">// 所有点，对应各个维度进行求和</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; point_list.size(); j++) &#123;</span><br><span class="line">                    sumAll[i] += point_list.get(j).getlocalArray()[i];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 计算平均值</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; sumAll.length; i++) &#123;</span><br><span class="line">                sumAll[i] = (<span class="keyword">float</span>) sumAll[i] / point_list.size();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 计算两个新、旧中心的距离，如果任意一个类中心移动的距离大于dis_diff则继续迭代。</span></span><br><span class="line">            <span class="keyword">if</span> (disC.getEuclideanDis(cluster.getCenter(), <span class="keyword">new</span> Point(sumAll)) &gt; disDiff) &#123;</span><br><span class="line">                ifNeedIter = <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 设置新的类中心位置</span></span><br><span class="line">            cluster.setCenter(<span class="keyword">new</span> Point(sumAll));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ifNeedIter;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 运行 k-means</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Set&lt;Cluster&gt; <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Set&lt;Cluster&gt; clusterSet = chooseCenterCluster();</span><br><span class="line">        <span class="keyword">boolean</span> ifNeedIter = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">while</span> (ifNeedIter) &#123;</span><br><span class="line">            cluster(clusterSet);</span><br><span class="line">            ifNeedIter = calculateCenter(clusterSet);</span><br><span class="line">            iterRunTimes++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> clusterSet;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 返回实际运行次数</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getIterTimes</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> iterRunTimes;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Main类-主函数"><a href="#Main类-主函数" class="headerlink" title="Main类(主函数)"></a>Main类(主函数)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.algorithm.kmeans;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Set;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ArrayList&lt;<span class="keyword">float</span>[]&gt; dataSet = <span class="keyword">new</span> ArrayList&lt;<span class="keyword">float</span>[]&gt;();</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">5</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">8</span>, <span class="number">9</span>, <span class="number">6</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">6</span>, <span class="number">4</span>, <span class="number">2</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">3</span>, <span class="number">9</span>, <span class="number">7</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">5</span>, <span class="number">9</span>, <span class="number">8</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">4</span>, <span class="number">2</span>, <span class="number">10</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">1</span>, <span class="number">9</span>, <span class="number">12</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">7</span>, <span class="number">8</span>, <span class="number">112</span>&#125;);</span><br><span class="line">        dataSet.add(<span class="keyword">new</span> <span class="keyword">float</span>[]&#123;<span class="number">7</span>, <span class="number">8</span>, <span class="number">4</span>&#125;);</span><br><span class="line"></span><br><span class="line">        KMeansRun kRun = <span class="keyword">new</span> KMeansRun(<span class="number">3</span>, dataSet);</span><br><span class="line">        Set&lt;Cluster&gt; clusterSet = kRun.run();</span><br><span class="line">        System.out.println(<span class="string">&quot;单次迭代运行次数：&quot;</span> + kRun.getIterTimes());</span><br><span class="line">        <span class="keyword">for</span> (Cluster cluster : clusterSet) &#123;</span><br><span class="line">            System.out.println(cluster);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>IceBerg如何集成的Flink</title>
    <url>/2021/04/28/IceBerg%E5%A6%82%E4%BD%95%E9%9B%86%E6%88%90%E7%9A%84Flink/</url>
    <content><![CDATA[<blockquote>
<p>再次膜拜SPI机制</p>
</blockquote>
<span id="more"></span>

<h2 id="直入主题"><a href="#直入主题" class="headerlink" title="直入主题"></a>直入主题</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Iceberg主要是在Flink中嵌入了一个新的Catalog</span><br><span class="line"># 怎么嵌入?(和嵌入自定义表连接器一样)</span><br><span class="line">Iceberg的iceberg-flink模块中定义了FlinkCatalogFactory</span><br><span class="line">在使用CREATE CATALOG语法时,FLINK会根据WITH子句的参数去discover对应的工厂类进行操作</span><br><span class="line">针对传递的参数将Catalog建立起来</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="怎么读取-写入数据呢"><a href="#怎么读取-写入数据呢" class="headerlink" title="怎么读取/写入数据呢?"></a>怎么读取/写入数据呢?</h2><h3 id="入口"><a href="#入口" class="headerlink" title="入口"></a>入口</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 各种Catalog&#x2F;Database&#x2F;Table操作对应FlinkCatalog中的各种方法</span><br><span class="line"># 那么表如何被获取到,并进行实例化呢?</span><br><span class="line">FlinkCatalog.getFactory()创建了FlinkDynamicTableFactory</span><br><span class="line"></span><br><span class="line"># 根据表的种类分为Source与Sink(熟悉的Flink自定义表)</span><br><span class="line">@Override</span><br><span class="line">public DynamicTableSource createDynamicTableSource(Context context) &#123;</span><br><span class="line">    # 表路径database.table</span><br><span class="line">    ObjectPath objectPath &#x3D; context.getObjectIdentifier().toObjectPath();</span><br><span class="line">    # 加载器,根据config文件进行构造</span><br><span class="line">    TableLoader tableLoader &#x3D; createTableLoader(objectPath);</span><br><span class="line">    # 获取表结构</span><br><span class="line">    TableSchema tableSchema &#x3D; TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());</span><br><span class="line">    # 重头戏</span><br><span class="line">    return new IcebergTableSource(tableLoader, tableSchema, context.getCatalogTable().getOptions(), context.getConfiguration());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public DynamicTableSink createDynamicTableSink(Context context) &#123;</span><br><span class="line">    ObjectPath objectPath &#x3D; context.getObjectIdentifier().toObjectPath();</span><br><span class="line">    TableLoader tableLoader &#x3D; createTableLoader(objectPath);</span><br><span class="line">    TableSchema tableSchema &#x3D; TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());</span><br><span class="line">    return new IcebergTableSink(tableLoader, tableSchema);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="读取-转化为DataStream"><a href="#读取-转化为DataStream" class="headerlink" title="读取-转化为DataStream"></a>读取-转化为DataStream</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建DataStream(怎么创建?)</span><br><span class="line">IcebergTableSource.createDataStream()</span><br><span class="line"></span><br><span class="line"># 获取配置信息构造DataStream</span><br><span class="line">FlinkSource.forRowData().*.build()</span><br><span class="line"></span><br><span class="line">public DataStream&lt;RowData&gt; build() &#123;</span><br><span class="line">  Preconditions.checkNotNull(env, &quot;StreamExecutionEnvironment should not be null&quot;);</span><br><span class="line">  &#x2F;&#x2F; 构造FlinkInputFormat获取输入ScanPlan</span><br><span class="line">  FlinkInputFormat format &#x3D; buildFormat();</span><br><span class="line">  </span><br><span class="line">  ScanContext context &#x3D; contextBuilder.build();</span><br><span class="line">  &#x2F;&#x2F; 获取上下文</span><br><span class="line">  TypeInformation&lt;RowData&gt; typeInfo &#x3D; FlinkCompatibilityUtil.toTypeInfo(FlinkSchemaUtil.convert(context.project()));</span><br><span class="line"></span><br><span class="line">  if (!context.isStreaming()) &#123;</span><br><span class="line">    &#x2F;&#x2F; 批</span><br><span class="line">    int parallelism &#x3D; inferParallelism(format, context);</span><br><span class="line">    return env.createInput(format, typeInfo).setParallelism(parallelism);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    &#x2F;&#x2F; 流</span><br><span class="line">    &#x2F;&#x2F; 如果是Restore状态会获取之前的SnapshotId</span><br><span class="line">    &#x2F;&#x2F; 如果不是则获取开始的SnapshotId</span><br><span class="line">    StreamingMonitorFunction function &#x3D; new StreamingMonitorFunction(tableLoader, context);</span><br><span class="line"></span><br><span class="line">    String monitorFunctionName &#x3D; String.format(&quot;Iceberg table (%s) monitor&quot;, table);</span><br><span class="line">    String readerOperatorName &#x3D; String.format(&quot;Iceberg table (%s) reader&quot;, table);</span><br><span class="line">    &#x2F;&#x2F; 返回DataStream</span><br><span class="line">    return env.addSource(function, monitorFunctionName)</span><br><span class="line">        .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="写入-addSink"><a href="#写入-addSink" class="headerlink" title="写入-addSink"></a>写入-addSink</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 并行度是个迷,设置了也没有啥用,写死的1</span><br><span class="line">FlinkSink.forRowData().*.build()</span><br><span class="line">public DataStreamSink&lt;RowData&gt; build() &#123;</span><br><span class="line">  Preconditions.checkArgument(rowDataInput !&#x3D; null,</span><br><span class="line">      &quot;Please use forRowData() to initialize the input DataStream.&quot;);</span><br><span class="line">  Preconditions.checkNotNull(tableLoader, &quot;Table loader shouldn&#39;t be null&quot;);</span><br><span class="line"></span><br><span class="line">  if (table &#x3D;&#x3D; null) &#123;</span><br><span class="line">    tableLoader.open();</span><br><span class="line">    try (TableLoader loader &#x3D; tableLoader) &#123;</span><br><span class="line">      this.table &#x3D; loader.loadTable();</span><br><span class="line">    &#125; catch (IOException e) &#123;</span><br><span class="line">      throw new UncheckedIOException(&quot;Failed to load iceberg table from table loader: &quot; + tableLoader, e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Find out the equality field id list based on the user-provided equality field column names.</span><br><span class="line">  List&lt;Integer&gt; equalityFieldIds &#x3D; Lists.newArrayList();</span><br><span class="line">  if (equalityFieldColumns !&#x3D; null &amp;&amp; equalityFieldColumns.size() &gt; 0) &#123;</span><br><span class="line">    for (String column : equalityFieldColumns) &#123;</span><br><span class="line">      org.apache.iceberg.types.Types.NestedField field &#x3D; table.schema().findField(column);</span><br><span class="line">      Preconditions.checkNotNull(field, &quot;Missing required equality field column &#39;%s&#39; in table schema %s&quot;,</span><br><span class="line">          column, table.schema());</span><br><span class="line">      equalityFieldIds.add(field.fieldId());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Convert the requested flink table schema to flink row type.</span><br><span class="line">  RowType flinkRowType &#x3D; toFlinkRowType(table.schema(), tableSchema);</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Distribute the records from input data stream based on the write.distribution-mode.</span><br><span class="line">  rowDataInput &#x3D; distributeDataStream(rowDataInput, table.properties(), table.spec(), table.schema(), flinkRowType);</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; Chain the iceberg stream writer and committer operator.</span><br><span class="line">  IcebergStreamWriter&lt;RowData&gt; streamWriter &#x3D; createStreamWriter(table, flinkRowType, equalityFieldIds);</span><br><span class="line">  IcebergFilesCommitter filesCommitter &#x3D; new IcebergFilesCommitter(tableLoader, overwrite);</span><br><span class="line"></span><br><span class="line">  this.writeParallelism &#x3D; writeParallelism &#x3D;&#x3D; null ? rowDataInput.getParallelism() : writeParallelism;</span><br><span class="line"></span><br><span class="line">  DataStream&lt;Void&gt; returnStream &#x3D; rowDataInput</span><br><span class="line">      .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(WriteResult.class), streamWriter)</span><br><span class="line">      .setParallelism(writeParallelism)</span><br><span class="line">      .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)</span><br><span class="line">      .setParallelism(1)</span><br><span class="line">      .setMaxParallelism(1);</span><br><span class="line"></span><br><span class="line">  return returnStream.addSink(new DiscardingSink())</span><br><span class="line">      .name(String.format(&quot;IcebergSink %s&quot;, table.name()))</span><br><span class="line">      .setParallelism(1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>iceberg</tag>
      </tags>
  </entry>
  <entry>
    <title>KafkaConnector接口使用</title>
    <url>/2021/04/23/KafkaConnector%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>整理之前遗留的使用方式</p>
</blockquote>
<span id="more"></span>


<h2 id="connector接口"><a href="#connector接口" class="headerlink" title="connector接口"></a>connector接口</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 列出所有connector</span><br><span class="line">curl -i -X GET http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors</span><br><span class="line"></span><br><span class="line"># 新建一个connector,请求体是包含name和config的json</span><br><span class="line">curl -i -X POST -H &#39;Content-type&#39;:&#39;application&#x2F;json&#39; -d &#39;config_json&#39; http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors</span><br><span class="line"></span><br><span class="line"># 获取指定connector的信息</span><br><span class="line">curl -i -X GET http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;</span><br><span class="line"></span><br><span class="line"># 获取指定connector的配置信息</span><br><span class="line">curl -i -X GET http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;config</span><br><span class="line"></span><br><span class="line"># 更新指定connector的配置信息</span><br><span class="line">curl -i -X PUT -H &#39;Content-type&#39;:&#39;application&#x2F;json&#39; -d &#39;config_json&#39; http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;config</span><br><span class="line"></span><br><span class="line"># 获取指定connector的状态信息</span><br><span class="line">curl -i -X GET http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;status</span><br><span class="line"></span><br><span class="line"># 列出指定connector的所有任务信息</span><br><span class="line">curl -i -X GET http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;tasks</span><br><span class="line"></span><br><span class="line"># 指定任务的状态信息</span><br><span class="line">curl -i -X GET http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;tasks&#x2F;&#123;taskid&#125;&#x2F;status</span><br><span class="line"></span><br><span class="line"># 暂停connector</span><br><span class="line">curl -i -X PUT http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;pause</span><br><span class="line"></span><br><span class="line"># 恢复connector</span><br><span class="line">curl -i -X PUT http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;resume</span><br><span class="line"></span><br><span class="line"># 重启connector</span><br><span class="line">curl -i -X POST http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;restart</span><br><span class="line"></span><br><span class="line"># 重启task</span><br><span class="line">curl -i -X POST http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;&#x2F;tasks&#x2F;&#123;taskid&#125;&#x2F;restart</span><br><span class="line"></span><br><span class="line"># 删除connector</span><br><span class="line">curl -i -X DELETE http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors&#x2F;&#123;name&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka使用Debezium监听MySQL</title>
    <url>/2019/06/04/Kafka%E4%BD%BF%E7%94%A8Debezium%E7%9B%91%E5%90%ACMySQL/</url>
    <content><![CDATA[<blockquote>
<p>Kafka组件使用</p>
</blockquote>
<span id="more"></span>

<h2 id="理解Kafka-Connect"><a href="#理解Kafka-Connect" class="headerlink" title="理解Kafka Connect"></a>理解Kafka Connect</h2><ul>
<li>先建立一个Connect,这个Connect可以配置一些参数,确定信息的一些格式</li>
<li><a href="http://192.168.142.128:8083/">http://192.168.142.128:8083/</a></li>
<li>然后在这个Connect的基础上建立一系列的Connector,可以确定数据源,以及一系列对数据的变更</li>
<li>PUT <a href="http://192.168.142.128:8083/connectors/c_name/config">http://192.168.142.128:8083/connectors/c_name/config</a></li>
</ul>
<hr>
<h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><blockquote>
<p>使用的是CDH安装的parcels</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Kafka目录</span></span><br><span class="line">/opt/cloudera/parcels/KAFKA/</span><br><span class="line"><span class="comment"># Kafka配置目录</span></span><br><span class="line">/opt/cloudera/parcels/KAFKA/etc/kafka/conf.dist/</span><br><span class="line"><span class="comment"># 将配置目录中的配置文件复制一份</span></span><br><span class="line">cp -r /opt/cloudera/parcels/KAFKA/etc/kafka/conf.dist/* /opt/cloudera/parcels/KAFKA/config </span><br><span class="line"><span class="comment"># 使用CDH的kafka自带的bin目录下并不会包含connect-*的shell脚本</span></span><br><span class="line">cp -r /opt/cloudera/parcels/KAFKA/lib/kafka/bin/connect-* /opt/cloudera/parcels/KAFKA/bin</span><br><span class="line"><span class="comment"># 修改config/connect-distributed.properties</span></span><br><span class="line">plugin.path=/opt/connectors (存放debezium插件的位置)</span><br><span class="line">bootstrap.servers=hadoop01:9092,hadoop02:9092,hadoop03:9092</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Debezium"><a href="#Debezium" class="headerlink" title="Debezium"></a>Debezium</h2><blockquote>
<p>只是一个插件,去官网下载对应的MySQL插件就行,将包解压到/opt/connectors </p>
</blockquote>
<hr>
<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><p>确保MySQL开启了binlog日志功能和query日志</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">VARIABLES</span> <span class="keyword">LIKE</span> <span class="string">&#x27;%log_bin%&#x27;</span>;</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">VARIABLES</span> <span class="keyword">LIKE</span> <span class="string">&#x27;%binlog%&#x27;</span>; </span><br></pre></td></tr></table></figure>
<p>开启binlog</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vi /etc/my.cnf</span><br><span class="line">    [client]</span><br><span class="line">    default-character-set=utf8mb4</span><br><span class="line"></span><br><span class="line">    [mysqld]</span><br><span class="line">    character-set-client-handshake=FALSE</span><br><span class="line">    character-set-server=utf8mb4</span><br><span class="line">    collation-server=utf8mb4_unicode_ci</span><br><span class="line">    init_connect=<span class="string">&#x27;SET NAMES utf8mb4&#x27;</span></span><br><span class="line">    server-id=1</span><br><span class="line">    log-bin=/usr/<span class="built_in">local</span>/mysql/data/my-bin</span><br><span class="line">    binlog_rows_query_log_events=ON</span><br><span class="line"></span><br><span class="line">    [mysql]</span><br><span class="line">    default-character-set=utf8mb4</span><br><span class="line">service mysqld restart</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启动Kafka-Connect"><a href="#启动Kafka-Connect" class="headerlink" title="启动Kafka Connect"></a>启动Kafka Connect</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/connect-distributed.sh etc/kafka/conf.dist/connect-distributed.properties</span><br></pre></td></tr></table></figure>
<p>访问Web: <a href="http://192.168.142.128:8083/">http://192.168.142.128:8083/</a></p>
<hr>
<h2 id="添加connector"><a href="#添加connector" class="headerlink" title="添加connector"></a>添加connector</h2><p>使用Postman的put功能</p>
<p>链接: <a href="http://192.168.142.128:8083/connectors/test2/config">http://192.168.142.128:8083/connectors/test2/config</a></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">body-raw-JSON(application/JSON)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;connector.class&quot;</span>: <span class="string">&quot;io.debezium.connector.mysql.MySqlConnector&quot;</span>,</span><br><span class="line">    <span class="comment">// 设置数据源	                    </span></span><br><span class="line">    <span class="attr">&quot;database.hostname&quot;</span>: <span class="string">&quot;hadoop01&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.port&quot;</span>: <span class="string">&quot;3306&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.user&quot;</span>: <span class="string">&quot;root&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.password&quot;</span>: <span class="string">&quot;123456&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.server.id&quot;</span>: <span class="string">&quot;1&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.server.name&quot;</span>: <span class="string">&quot;demo&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.whitelist&quot;</span>: <span class="string">&quot;test1&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.history.kafka.bootstrap.servers&quot;</span>: <span class="string">&quot;hadoop01:9092&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.history.kafka.topic&quot;</span>: <span class="string">&quot;dbhistory&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.history.store.only.monitored.tables.ddl&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;database.history.skip.unparseable.ddl&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">    <span class="comment">// 去除字段,多个字段用逗号分隔</span></span><br><span class="line">    <span class="attr">&quot;transforms&quot;</span>: <span class="string">&quot;dropField&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;transforms.dropField.type&quot;</span>:<span class="string">&quot;org.apache.kafka.connect.transforms.ReplaceField$Value&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;transforms.dropField.blacklist&quot;</span>:<span class="string">&quot;source&quot;</span>,</span><br><span class="line">    <span class="comment">// 监听sql语句	                    </span></span><br><span class="line">    <span class="attr">&quot;include.query&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;include.schema.events&quot;</span>: <span class="string">&quot;false&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;include.schema.changes&quot;</span>: <span class="string">&quot;false&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;decimal.handling.mode&quot;</span>: <span class="string">&quot;string&quot;</span>,	                    </span><br><span class="line">    <span class="attr">&quot;snapshot.mode&quot;</span>: <span class="string">&quot;schema_only&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="消费Kafka"><a href="#消费Kafka" class="headerlink" title="消费Kafka"></a>消费Kafka</h2><p>topic的组成为:serverName.dbName.tableName</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kafka-console-consumer --bootstrap-server hadoop01:9092 --topic demo.test1.demo</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="遇到的一些问题"><a href="#遇到的一些问题" class="headerlink" title="遇到的一些问题"></a>遇到的一些问题</h2><blockquote>
<p>刚开始我本身并没有对Connect进行修改,所以导致后面的数据格式是一个shcema+payload,这种数据可以说非常完美,因为本身我们是需要对schema的信息进行传递的,但是我想进一步简化数据,监听数据其实我只需要payload内的数据就可以了</p>
</blockquote>
<h3 id="1-如何去除schema"><a href="#1-如何去除schema" class="headerlink" title="1.如何去除schema"></a>1.如何去除schema</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改connect-distributed.properties</span></span><br><span class="line">key.converter=org.apache.kafka.connect.json.JsonConverter</span><br><span class="line">value.converter=org.apache.kafka.connect.json.JsonConverter</span><br><span class="line">key.converter.schemas.enable=<span class="literal">false</span></span><br><span class="line">value.converter.schemas.enable=<span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h3 id="2-如何去除payload的一些不需要的数据"><a href="#2-如何去除payload的一些不需要的数据" class="headerlink" title="2.如何去除payload的一些不需要的数据"></a>2.如何去除payload的一些不需要的数据</h3><p>修改config的PUT请求,上面的PUT请求中有例子</p>
<h3 id="3-日志打印过多的INFO"><a href="#3-日志打印过多的INFO" class="headerlink" title="3.日志打印过多的INFO"></a>3.日志打印过多的INFO</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改connect-log4j.properties</span></span><br><span class="line">log4j.rootLogger=WARN, stdout</span><br></pre></td></tr></table></figure>

<h3 id="4-后台启动connect"><a href="#4-后台启动connect" class="headerlink" title="4.后台启动connect"></a>4.后台启动connect</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/connect-distributed.sh -daemon config/connect-distributed.properties</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka命令系列</title>
    <url>/2018/03/25/Kafka%E5%91%BD%E4%BB%A4%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<blockquote>
<p>介绍常用的命令</p>
</blockquote>
<span id="more"></span>

<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">创建topic</span><br><span class="line">kafka-topics --create --zookeeper hostname:2181 --replication-factor 1 --partitions 1 --topic topicname</span><br><span class="line"></span><br><span class="line">控制台启动生产者</span><br><span class="line">kafka-console-producer --broker-list hostname:9092 --topic topicname</span><br><span class="line"></span><br><span class="line">控制台消費</span><br><span class="line">kafka-console-consumer --bootstrap-server hostname:9092 --topic topicname --from-beginning</span><br><span class="line"></span><br><span class="line">从指定分区的特定offset开始消费topic</span><br><span class="line">kafka-console-consumer --bootstrap-server hostname:9092 --topic topicname --offset 1018277 --partition 0</span><br><span class="line"></span><br><span class="line">删除topic</span><br><span class="line">kafka-topics --delete --zookeeper hostname:2181 --topic <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">查看topic</span><br><span class="line">kafka-topics --zookeeper hostname:2181 --list</span><br><span class="line"></span><br><span class="line">查看特定topic</span><br><span class="line">kafka-topics --zookeeper hostname:2181 --topic <span class="built_in">test</span> --describe</span><br><span class="line">分区数量,备份因子,以及各分区的Leader,Replica信息</span><br><span class="line"></span><br><span class="line">查看消费组列表</span><br><span class="line">kafka-consumer-groups --bootstrap-server hostname:9092 --list</span><br><span class="line"></span><br><span class="line">查看特定消费组</span><br><span class="line">kafka-consumer-groups --bootstrap-server hostname:9092 --group groupName --describe</span><br><span class="line">分区ID,最近一次提交的offset,最拉取的生产消息offset,消费offset与生产offset之间的差值</span><br><span class="line"></span><br><span class="line">重设Kafka消费组的Offset</span><br><span class="line">--to-earliest:重设0</span><br><span class="line">--to-latest:重设为最新</span><br><span class="line">--to-offset:重设到指定offset</span><br><span class="line">--to-current:重设到当前offset</span><br><span class="line">--shift-by:重设为减少指定大小的offset</span><br><span class="line">--to-datetime:重设到指定时间最早offset</span><br><span class="line">--by-duration:重设到30分钟之前最早offset</span><br><span class="line">kafka-consumer-groups.sh --bootstrap-server hostname:9092 --group groupName --reset-offsets --all-topics --to-latest --execute</span><br><span class="line"></span><br><span class="line">修改分区数</span><br><span class="line">kafka-topics --alter --zookeeper hostname:2181 --topic topicname --partitions 6</span><br><span class="line"></span><br><span class="line">修改topic副本数</span><br><span class="line">vi ~/kafka_add_replicas.json</span><br><span class="line">&#123;<span class="string">&quot;topics&quot;</span>:</span><br><span class="line">    [&#123;<span class="string">&quot;topic&quot;</span>:<span class="string">&quot;prod_log_simul&quot;</span>&#125;],</span><br><span class="line">    <span class="string">&quot;version&quot;</span>: 1</span><br><span class="line">&#125;</span><br><span class="line">kafka-reassign-partitions --zookeeper hostname:2181 --topics-to-move-json-file ~/kafka_add_replicas.json --broker-list <span class="string">&quot;0,1,2&quot;</span> --generate</span><br><span class="line">vi ~/topic-reassignment.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;version&quot;</span>:1,</span><br><span class="line">    <span class="string">&quot;partitions&quot;</span>:[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;topic&quot;</span>:<span class="string">&quot;test&quot;</span>,</span><br><span class="line">            <span class="string">&quot;partition&quot;</span>:2,</span><br><span class="line">            <span class="string">&quot;replicas&quot;</span>:[0,1,2]</span><br><span class="line">            &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;topic&quot;</span>:<span class="string">&quot;test&quot;</span>,</span><br><span class="line">            <span class="string">&quot;partition&quot;</span>:1,</span><br><span class="line">            <span class="string">&quot;replicas&quot;</span>:[0,1,2]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;topic&quot;</span>:<span class="string">&quot;test&quot;</span>,</span><br><span class="line">            <span class="string">&quot;partition&quot;</span>:0,</span><br><span class="line">            <span class="string">&quot;replicas&quot;</span>:[0,1,2]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">kafka-reassign-partitions --zookeeper hostname:2181 --reassignment-json-file ~/topic-reassignment.json --execute</span><br><span class="line">查看分配进度</span><br><span class="line">kafka-reassign-partitions --zookeeper hostname:2181 --reassignment-json-file ~/topic-reassignment.json --verify</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka使用Debezium实时同步Oracle数据</title>
    <url>/2021/04/23/Kafka%E4%BD%BF%E7%94%A8Debezium%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5Oracle%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<blockquote>
<p>实时同步Oracle数据到Kafka必要操作,附带connect操作</p>
</blockquote>
<span id="more"></span>
<h1 id="LogMiner支持"><a href="#LogMiner支持" class="headerlink" title="LogMiner支持"></a>LogMiner支持</h1><h2 id="准备数据库"><a href="#准备数据库" class="headerlink" title="准备数据库"></a>准备数据库</h2><h3 id="Oracle-LogMiner配置"><a href="#Oracle-LogMiner配置" class="headerlink" title="Oracle LogMiner配置"></a>Oracle LogMiner配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 默认Oracle数据库已经安装完毕</span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">startup;</span><br><span class="line">CONNECT sys&#x2F;top_secret AS SYSDBA</span><br><span class="line">alter system set db_recovery_file_dest_size &#x3D; 10G;</span><br><span class="line"># 注意目录必须存在</span><br><span class="line">alter system set db_recovery_file_dest &#x3D; &#39;&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;recovery_area&#39; scope&#x3D;spfile;</span><br><span class="line">shutdown immediate</span><br><span class="line">startup mount</span><br><span class="line">alter database archivelog;</span><br><span class="line">alter database open;</span><br><span class="line">archive log list</span><br><span class="line">exit</span><br><span class="line"># 在特定表进行配置,减少Oracle REDO日志信息量</span><br><span class="line">ALTER TABLE dbname.tablename ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;</span><br></pre></td></tr></table></figure>
<h3 id="创建连接器的LogMiner用户"><a href="#创建连接器的LogMiner用户" class="headerlink" title="创建连接器的LogMiner用户"></a>创建连接器的LogMiner用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">CREATE TABLESPACE logminer_tbs DATAFILE &#39;&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;ORCLCDB&#x2F;logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;</span><br><span class="line"># 切换PDB</span><br><span class="line">alter session set container&#x3D;ORCLPDB1;</span><br><span class="line"># 启动PDB</span><br><span class="line">alter pluggable database open;</span><br><span class="line">CREATE TABLESPACE logminer_tbs DATAFILE &#39;&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;ORCLCDB&#x2F;ORCLPDB1&#x2F;logminer_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">CREATE USER c##dbzuser IDENTIFIED BY dbz DEFAULT TABLESPACE logminer_tbs QUOTA UNLIMITED ON logminer_tbs CONTAINER&#x3D;ALL;</span><br><span class="line"></span><br><span class="line">GRANT CREATE SESSION TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SET CONTAINER TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$DATABASE to c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT FLASHBACK ANY TABLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ANY TABLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT_CATALOG_ROLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT EXECUTE_CATALOG_ROLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ANY TRANSACTION TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT LOGMINING TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT CREATE TABLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT LOCK ANY TABLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT ALTER ANY TABLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT CREATE SEQUENCE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT EXECUTE ON DBMS_LOGMNR TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT EXECUTE ON DBMS_LOGMNR_D TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$LOG TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$LOG_HISTORY TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$LOGMNR_LOGS TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$LOGMNR_CONTENTS TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$LOGMNR_PARAMETERS TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$LOGFILE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$ARCHIVED_LOG TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line"></span><br><span class="line"># 开启归档(不开启数据库级别,会报错表定义已修改,读取不到表)</span><br><span class="line">ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="部署连接器"><a href="#部署连接器" class="headerlink" title="部署连接器"></a>部署连接器</h2><h3 id="下载Oracle工具Jar"><a href="#下载Oracle工具Jar" class="headerlink" title="下载Oracle工具Jar"></a>下载Oracle工具Jar</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;download.oracle.com&#x2F;otn_software&#x2F;linux&#x2F;instantclient&#x2F;211000&#x2F;instantclient-basic-linux.x64-21.1.0.0.0.zip</span><br><span class="line"></span><br><span class="line">unzip instantclient-basic-linux.x64-21.1.0.0.0.zip</span><br><span class="line">mv instantclient_21_1 &#x2F;opt&#x2F;instant_client</span><br><span class="line"></span><br><span class="line"># 复制ojdbc.jar和xstreams.jar到Kafka的libs下</span><br><span class="line">cp instant_client&#x2F;ojdbc8.jar &#x2F;opt&#x2F;kafka&#x2F;libs&#x2F;</span><br><span class="line">cp instant_client&#x2F;xstreams.jar &#x2F;opt&#x2F;kafka&#x2F;libs&#x2F;</span><br><span class="line"></span><br><span class="line"># 创建环境变量指向客户端目录</span><br><span class="line">LD_LIBRARY_PATH&#x3D;&#x2F;opt&#x2F;instant_client&#x2F;</span><br></pre></td></tr></table></figure>
<h3 id="启动KafkaConnect"><a href="#启动KafkaConnect" class="headerlink" title="启动KafkaConnect"></a>启动KafkaConnect</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;connect-distributed.sh &#x2F;opt&#x2F;kafka&#x2F;config&#x2F;connect-distributed.properties</span><br><span class="line"></span><br><span class="line"># 访问Web</span><br><span class="line">http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;</span><br></pre></td></tr></table></figure>
<h3 id="添加Connector"><a href="#添加Connector" class="headerlink" title="添加Connector"></a>添加Connector</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看Oracle的service.name</span><br><span class="line">show parameter service_names;</span><br><span class="line"># 查看Oracle的SID</span><br><span class="line">SELECT instance_name FROM v$instance;</span><br><span class="line"></span><br><span class="line"># Post请求(使用CDB)</span><br><span class="line">curl -i -X POST -H &#39;Content-type&#39;:&#39;application&#x2F;json&#39; -d &#39;&#123;&quot;name&quot;:&quot;test&quot;,&quot;config&quot;:&#123;&quot;connector.class&quot;:&quot;io.debezium.connector.oracle.OracleConnector&quot;,&quot;tasks.max&quot;:&quot;1&quot;,&quot;database.server.name&quot;:&quot;ORCLCDB&quot;,&quot;database.hostname&quot;:&quot;192.168.6.128&quot;,&quot;database.port&quot;:&quot;1521&quot;,&quot;database.user&quot;:&quot;c##dbzuser&quot;,&quot;database.password&quot;:&quot;dbz&quot;,&quot;database.dbname&quot;:&quot;ORCLCDB&quot;,&quot;database.history.kafka.bootstrap.servers&quot;:&quot;localhost:9092&quot;,&quot;database.history.kafka.topic&quot;:&quot;test.schema&quot;,&quot;table.include.list&quot;:&quot;C##XS.test&quot;,&quot;decimal.handling.mode&quot;:&quot;string&quot;&#125;&#125;&#39; http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors</span><br></pre></td></tr></table></figure>
<h3 id="查看同步数据"><a href="#查看同步数据" class="headerlink" title="查看同步数据"></a>查看同步数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建表的DDL数据</span><br><span class="line">.&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line"></span><br><span class="line"># 创建table需要指定非系统用户登录</span><br><span class="line">create table test (</span><br><span class="line">    id  varchar2(50) primary key,</span><br><span class="line">    phone number(11) unique</span><br><span class="line">);</span><br><span class="line">ALTER TABLE C##XS.test ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS</span><br><span class="line"></span><br><span class="line">insert into test(id,phone) values(&#39;XS&#39;,13400000002);</span><br><span class="line"></span><br><span class="line"># 同步到数据后Kafka的topic为ORCLCDB.C__XS.test</span><br><span class="line">.&#x2F;kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ORCLCDB.C__XS.test --from-beginning</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="XSteam支持"><a href="#XSteam支持" class="headerlink" title="XSteam支持"></a>XSteam支持</h1><h2 id="准备数据库-1"><a href="#准备数据库-1" class="headerlink" title="准备数据库"></a>准备数据库</h2><h3 id="创建连接器的XStream用户"><a href="#创建连接器的XStream用户" class="headerlink" title="创建连接器的XStream用户"></a>创建连接器的XStream用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Admin用户</span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">CREATE TABLESPACE xstream_adm_tbs DATAFILE &#39;&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;ORCLCDB&#x2F;xstream_adm_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;</span><br><span class="line"># 切换PDB</span><br><span class="line">alter session set container&#x3D;ORCLPDB1;</span><br><span class="line"># 启动PDB</span><br><span class="line">alter pluggable database open;</span><br><span class="line">CREATE TABLESPACE xstream_adm_tbs DATAFILE &#39;&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;ORCLCDB&#x2F;ORCLPDB1&#x2F;xstream_adm_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">CREATE USER c##dbzadmin IDENTIFIED BY dbz DEFAULT TABLESPACE xstream_adm_tbs QUOTA UNLIMITED ON xstream_adm_tbs CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT CREATE SESSION, SET CONTAINER TO c##dbzadmin CONTAINER&#x3D;ALL;</span><br><span class="line"></span><br><span class="line">BEGIN</span><br><span class="line">    DBMS_XSTREAM_AUTH.GRANT_ADMIN_PRIVILEGE(</span><br><span class="line">        grantee                 &#x3D;&gt; &#39;c##dbzadmin&#39;,</span><br><span class="line">        privilege_type          &#x3D;&gt; &#39;CAPTURE&#39;,</span><br><span class="line">        grant_select_privileges &#x3D;&gt; TRUE,</span><br><span class="line">        container               &#x3D;&gt; &#39;ALL&#39;</span><br><span class="line">    );</span><br><span class="line">END;</span><br><span class="line">&#x2F;</span><br><span class="line"></span><br><span class="line"># 连接用户</span><br><span class="line">CREATE TABLESPACE xstream_tbs DATAFILE &#39;&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;ORCLCDB&#x2F;xstream_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;</span><br><span class="line">alter session set container&#x3D;ORCLPDB1;</span><br><span class="line">CREATE TABLESPACE xstream_tbs DATAFILE &#39;&#x2F;opt&#x2F;oracle&#x2F;oradata&#x2F;ORCLCDB&#x2F;ORCLPDB1&#x2F;xstream_tbs.dbf&#39; SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">CREATE USER c##dbzuser IDENTIFIED BY dbz DEFAULT TABLESPACE xstream_tbs QUOTA UNLIMITED ON xstream_tbs CONTAINER&#x3D;ALL;</span><br><span class="line"></span><br><span class="line">GRANT CREATE SESSION TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SET CONTAINER TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT ON V_$DATABASE to c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT FLASHBACK ANY TABLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT SELECT_CATALOG_ROLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">GRANT EXECUTE_CATALOG_ROLE TO c##dbzuser CONTAINER&#x3D;ALL;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure>
<h3 id="创建XStream出站服务器"><a href="#创建XStream出站服务器" class="headerlink" title="创建XStream出站服务器"></a>创建XStream出站服务器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">conn c##dbzadmin&#x2F;dbz</span><br><span class="line">DECLARE</span><br><span class="line">  tables  DBMS_UTILITY.UNCL_ARRAY;</span><br><span class="line">  schemas DBMS_UTILITY.UNCL_ARRAY;</span><br><span class="line">BEGIN</span><br><span class="line">    tables(1)  :&#x3D; NULL;</span><br><span class="line">    schemas(1) :&#x3D; &#39;debezium&#39;;</span><br><span class="line">  DBMS_XSTREAM_ADM.CREATE_OUTBOUND(</span><br><span class="line">    server_name     &#x3D;&gt;  &#39;dbzxout&#39;,</span><br><span class="line">    table_names     &#x3D;&gt;  tables,</span><br><span class="line">    schema_names    &#x3D;&gt;  schemas);</span><br><span class="line">END;</span><br><span class="line">&#x2F;</span><br><span class="line">exit;</span><br><span class="line"></span><br><span class="line"># XStream用户连接到出站服务器(每个连接器需要唯一XStream出站服务器)</span><br><span class="line">sqlplus &#x2F; as sysdba</span><br><span class="line">BEGIN</span><br><span class="line">  DBMS_XSTREAM_ADM.ALTER_OUTBOUND(</span><br><span class="line">    server_name  &#x3D;&gt; &#39;dbzxout&#39;,</span><br><span class="line">    connect_user &#x3D;&gt; &#39;c##dbzuser&#39;);</span><br><span class="line">END;</span><br><span class="line">&#x2F;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="部署连接器-1"><a href="#部署连接器-1" class="headerlink" title="部署连接器"></a>部署连接器</h2><h3 id="添加Connector-1"><a href="#添加Connector-1" class="headerlink" title="添加Connector"></a>添加Connector</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Post请求</span><br><span class="line">curl -i -X POST -H &#39;Content-type&#39;:&#39;application&#x2F;json&#39; -d &#39;&#123;&quot;name&quot;:&quot;test&quot;,&quot;config&quot;:&#123;&quot;connector.class&quot;:&quot;io.debezium.connector.oracle.OracleConnector&quot;,&quot;tasks.max&quot;:&quot;1&quot;,&quot;database.server.name&quot;:&quot;ORCLCDB&quot;,&quot;database.hostname&quot;:&quot;192.168.6.128&quot;,&quot;database.port&quot;:&quot;1521&quot;,&quot;database.user&quot;:&quot;c##dbzuser&quot;,&quot;database.password&quot;:&quot;dbz&quot;,&quot;database.dbname&quot;:&quot;ORCLCDB&quot;,&quot;database.pdb.name&quot;:&quot;ORCLPDB1&quot;,&quot;database.history.kafka.bootstrap.servers&quot;:&quot;localhost:9092&quot;,&quot;database.history.kafka.topic&quot;:&quot;test.schema&quot;,&quot;table.include.list&quot;:&quot;C##XS.test&quot;,&quot;decimal.handling.mode&quot;:&quot;string&quot;,&quot;database.connection.adapter&quot;:&quot;xstream&quot;,&quot;database.out.server.name&quot;:&quot;dbzxout&quot;&#125;&#125;&#39; http:&#x2F;&#x2F;192.168.6.128:8083&#x2F;connectors</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="启动连接器报表定义已修改-读取失败"><a href="#启动连接器报表定义已修改-读取失败" class="headerlink" title="启动连接器报表定义已修改,读取失败"></a>启动连接器报表定义已修改,读取失败</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 需要提前开启归档</span><br><span class="line">ALTER DATABASE ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;</span><br><span class="line"># 引生问题:如果一开始没有开启归档的表怎么实时同步</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Oracle归档日志过多解决-磁盘很大可以忽略"><a href="#Oracle归档日志过多解决-磁盘很大可以忽略" class="headerlink" title="Oracle归档日志过多解决(磁盘很大可以忽略)"></a>Oracle归档日志过多解决(磁盘很大可以忽略)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 由于连接器是依赖于Oracle的归档日志做的实时同步</span><br><span class="line"># 存在归档日志撑爆磁盘的问题</span><br><span class="line">#! &#x2F;bin&#x2F;bash</span><br><span class="line">exec &gt;&gt; &#x2F;home&#x2F;oracle&#x2F;log&#x2F;del_arch&#96;date +%F-%H&#96;.log #记录脚本日志</span><br><span class="line">$ORACLE_HOME&#x2F;bin&#x2F;rman target &#x2F; &lt;&lt;EOF</span><br><span class="line">#检查归档日志</span><br><span class="line">crosscheck archivelog all;</span><br><span class="line">#删除所有过期日志</span><br><span class="line">delete noprompt expired archivelog all;</span><br><span class="line">#删除一个小时前的归档日志</span><br><span class="line">delete noprompt archivelog until time &#39;sysdate-1&#x2F;24&#39;;</span><br><span class="line">exit;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之五ConsumerGroup建立</title>
    <url>/2020/05/07/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E4%BA%94ConsumerGroup%E5%BB%BA%E7%AB%8B/</url>
    <content><![CDATA[<blockquote>
<p>描述consumer实例怎么样加入的group</p>
</blockquote>
<span id="more"></span>

<h2 id="GroupCoordinator角色"><a href="#GroupCoordinator角色" class="headerlink" title="GroupCoordinator角色"></a>GroupCoordinator角色</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GroupCoordinator角色是负责ConsumerGroup Member管理以及Offset管理</span><br><span class="line"></span><br><span class="line">每一个ConsumerGroup都有其对应的GroupCoordinator</span><br><span class="line"></span><br><span class="line">具体由哪个GroupCoordinator负责与groupId的hash值有关</span><br><span class="line">通过abs(GroupId.hashCode()) % NumPartitions来计算出一个值</span><br><span class="line">Numpartitions是__consumer_offsets的partition数,默认50</span><br><span class="line">这个值代表__consumer_offsets的一个partition</span><br><span class="line">这个partition的leader即为这个groupId要交互的GroupCoordinator所在节点</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="KafkaConsumer做了什么"><a href="#KafkaConsumer做了什么" class="headerlink" title="KafkaConsumer做了什么"></a>KafkaConsumer做了什么</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Timer timer, <span class="keyword">final</span> <span class="keyword">boolean</span> includeMetadataInTimeout)</span> </span>&#123;</span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.hasNoSubscriptionOrUserAssignment()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// poll for new data until the timeout expires</span></span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            client.maybeTriggerWakeup();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (includeMetadataInTimeout) &#123;</span><br><span class="line">                <span class="comment">// 判断是否需要更新</span></span><br><span class="line">                <span class="keyword">if</span> (!updateAssignmentMetadataIfNeeded(timer)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 阻塞等待metadata响应</span></span><br><span class="line">                <span class="keyword">while</span> (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) &#123;</span><br><span class="line">                    log.warn(<span class="string">&quot;Still waiting for metadata&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取Fetcher已经拉取到的数据</span></span><br><span class="line">            <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class="line">            <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">                <span class="comment">// 由于消耗的位置已经更新,在返回提取的记录之前,我们不能允许触发唤醒或任何其他错误</span></span><br><span class="line">                <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.hasPendingRequests()) &#123;</span><br><span class="line">                    client.pollNoWakeup();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">while</span> (timer.notExpired());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        release();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) &#123;</span><br><span class="line">    <span class="keyword">long</span> pollTimeout = Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs());</span><br><span class="line">    <span class="comment">// 如果数据已经可用,请立即返回</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class="line">    <span class="keyword">if</span> (!records.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> records;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 向订阅的所有partition发送fetch请求,会从多个partition拉取数据</span></span><br><span class="line">    fetcher.sendFetches();</span><br><span class="line">    <span class="comment">// We do not want to be stuck blocking in poll if we are missing some positions</span></span><br><span class="line">    <span class="comment">// since the offset lookup may be backing off after a failure</span></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> the use of cachedSubscriptionHashAllFetchPositions means we MUST call</span></span><br><span class="line">    <span class="comment">// updateAssignmentMetadataIfNeeded before this method.</span></span><br><span class="line">    <span class="keyword">if</span> (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) &#123;</span><br><span class="line">        pollTimeout = retryBackoffMs;</span><br><span class="line">    &#125;</span><br><span class="line">    Timer pollTimer = time.timer(pollTimeout);</span><br><span class="line">    <span class="comment">// 调用poll方法发送数据</span></span><br><span class="line">    client.poll(pollTimer, () -&gt; &#123;</span><br><span class="line">        <span class="comment">// since a fetch might be completed by the background thread, we need this poll condition</span></span><br><span class="line">        <span class="comment">// to ensure that we do not block unnecessarily in poll()</span></span><br><span class="line">        <span class="keyword">return</span> !fetcher.hasCompletedFetches();</span><br><span class="line">    &#125;);</span><br><span class="line">    timer.update(pollTimer.currentTimeMs());</span><br><span class="line">    <span class="comment">// after the long poll, we should check whether the group needs to rebalance</span></span><br><span class="line">    <span class="comment">// prior to returning data so that the group can stabilize faster</span></span><br><span class="line">    <span class="comment">// 如果group需要rebalance,直接返回空数据,这样能更快的让group进入稳定状态</span></span><br><span class="line">    <span class="keyword">if</span> (coordinator.rejoinNeededOrPending()) &#123;</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fetcher.fetchedRecords();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.updateAssignmentMetadataIfNeeded调用GroupCoordinator的poll方法,获取其分配的tp列表</span></span><br><span class="line"><span class="comment">// 2.更新这些分配的tp列表的the last committed offset</span></span><br><span class="line"><span class="comment">// 3.调用Fetcher获取拉取的数据,如果有数据立即返回</span></span><br><span class="line"><span class="comment">// 4.调用Fetcher发送fetch请求(加入队列,并没有真正发送)</span></span><br><span class="line"><span class="comment">// 5.调用ConsumerNetworkClient.poll发送请求</span></span><br><span class="line"><span class="comment">// 6.如果group需要rebalance,直接返回空集合</span></span><br></pre></td></tr></table></figure>
<p>可以看出,GroupCoordinator.poll才是创建一个Group的真正执行</p>
<hr>
<h2 id="ConsumerCoordinator-poll-具体实现"><a href="#ConsumerCoordinator-poll-具体实现" class="headerlink" title="ConsumerCoordinator.poll()具体实现"></a>ConsumerCoordinator.poll()具体实现</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 确保Group的Coordinator是已知的,并且这个Consumer已经加入到组中,也用于offset的周期性提交</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">poll</span><span class="params">(Timer timer)</span> </span>&#123;</span><br><span class="line">        invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (subscriptions.partitionsAutoAssigned()) &#123;</span><br><span class="line">            <span class="comment">// 更新hearbeat,防因为不活动导致hearbeat线程主动离开group</span></span><br><span class="line">            pollHeartbeat(timer.currentTimeMs());</span><br><span class="line">            <span class="comment">// 如果Coordinator未知且Coordinator没有准备好,直接返回false</span></span><br><span class="line">            <span class="keyword">if</span> (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 判断是否需要重新加入group,如果partition变化或分配的partition变化,需要rejoin</span></span><br><span class="line">            <span class="keyword">if</span> (rejoinNeededOrPending()) &#123;</span><br><span class="line">                <span class="comment">// 重新加入group之前先刷新一下metadata(AUTO_PATTERN)</span></span><br><span class="line">                <span class="keyword">if</span> (subscriptions.hasPatternSubscription()) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (<span class="keyword">this</span>.metadata.timeToAllowUpdate(time.milliseconds()) == <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (!client.ensureFreshMetadata(timer)) &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 确保group是active,加入group,分配订阅的partition</span></span><br><span class="line">                <span class="keyword">if</span> (!ensureActiveGroup(timer)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 发送更新metadata请求</span></span><br><span class="line">            <span class="keyword">if</span> (metadata.updateRequested() &amp;&amp; !client.hasReadyNodes(timer.currentTimeMs())) &#123;</span><br><span class="line">                client.awaitMetadataUpdate(timer);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 自动commit时,当定时达到时,进行自动commit</span></span><br><span class="line">        maybeAutoCommitOffsetsAsync(timer.currentTimeMs());</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="ensureCoordinatorReady"><a href="#ensureCoordinatorReady" class="headerlink" title="ensureCoordinatorReady()"></a>ensureCoordinatorReady()</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">选择一个连接数最小的broker,向其发送GroupCoordinator请求,并建立相应的TCP连接</span><br><span class="line">    lookupCoordinator()-&gt;sendFindCoordinatorRequest()-&gt;FindCoordinatorResponseHandler回调</span><br><span class="line">如果Client获取到Server Response,那么就会与GroupCoordinator建立连接</span><br></pre></td></tr></table></figure>
<h3 id="ensureActiveGroup"><a href="#ensureActiveGroup" class="headerlink" title="ensureActiveGroup()"></a>ensureActiveGroup()</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">向GroupCoordinator发送join-group,sync-group请求,获取assign的TP-list</span><br><span class="line">    ensureCoordinatorReady()-&gt;startHeartbeatThreadIfNeeded()-&gt;joinGroupIfNeeded()</span><br><span class="line">    joinGroupIfNeeded()-&gt;initiateJoinGroup()-&gt;sendJoinGroupRequest()-&gt;JoinGroupResponseHandler.handle()-&gt;onJoinLeader&#x2F;onJoinFollower-&gt;sendSyncGroupRequest()-&gt;SyncGroupResponseHandler</span><br><span class="line">    onJoinComplete</span><br><span class="line">    </span><br><span class="line">1.如果Group是新的GroupId,那么此时group初始化状态为Empty</span><br><span class="line">2.当GroupCoordinator接收到consumer的join-group请求后,group的member列表为空,第一个被加入的member被选为leader</span><br><span class="line">3.如果GroupCoordinator接收到leader发送join-group请求,将会触发rebalance,group状态变为PreparingRebalance</span><br><span class="line">4.此时GroupCoordinator将会等待,在一定时间内,接收到join-group请求的consumer将被认为是存活的,此时group变为AwaitSync状态,并且GroupCoordinator会向这个group的所有member返回其response</span><br><span class="line">5.consumer在接收到GroupCoordinator的response后,如果这个consumer是group的leader,那么这个consumer将会负责为整个group assign partition订阅安排,然后leader将分配后的信息以sendSyncGroupResult()请求的方式发给GroupCoordinator,而作为follower的consumer实例会发送一个空列表</span><br><span class="line">6.GroupCoordinator在接收到leader发来的请求后,将assign的结果返回给所有已经发送sync-group请求的consumer实例,并且group的状态变为Stable,如果后续再收到sync-group请求,将会直接返回其分配结果</span><br><span class="line"></span><br><span class="line">当一个consumer实例加入group成功后,触发onJoinComplete()</span><br><span class="line">更新订阅的tp列表,更新其对应的metadata以及触发注册的listener</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之七Consumer两种模型</title>
    <url>/2020/05/07/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E4%B8%83Consumer%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<blockquote>
<p>这一块主要关注tp分配方式,以及offset的commit方式</p>
</blockquote>
<span id="more"></span>

<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">subscribe(): 动态获取其分配的topic-partition,由group动态管理</span><br><span class="line">    subscribe(Pattern pattern, ConsumerRebalanceListener listener)</span><br><span class="line">    subscribe(Pattern pattern)</span><br><span class="line">    subscribe(Collection&lt;String&gt; topics)</span><br><span class="line">assign()</span><br><span class="line">    subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span><br><span class="line"></span><br><span class="line">assign(): 手动分配topic-partition列表</span><br><span class="line">    assign(Collection&lt;TopicPartition&gt; partitions)</span><br><span class="line">    </span><br><span class="line"># 注意:</span><br><span class="line">如果是来自assign的请求,但这个group的状态不为Empty,意味着这个group已经处在活跃状态了</span><br><span class="line">assign模式下的group是不会处于活跃状态的</span><br><span class="line">意味着assign模式使用的group.id与subscribe模式下使用的group相同</span><br><span class="line">这种情况下,会拒绝assign模式下的这个offset commit请求</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="SubscriptionState"><a href="#SubscriptionState" class="headerlink" title="SubscriptionState"></a>SubscriptionState</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NONE, AUTO_TOPICS, AUTO_PATTERN, USER_ASSIGNED</span><br><span class="line">无</span><br><span class="line">自动发现Topic</span><br><span class="line">正则</span><br><span class="line">用户分配</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Commit机制"><a href="#Commit机制" class="headerlink" title="Commit机制"></a>Commit机制</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">commitSync()</span><br><span class="line">    coordinator.commitOffsetsSync()</span><br><span class="line">        sendOffsetCommitRequest()</span><br><span class="line">            client.poll(同步)</span><br><span class="line">commitAsync()</span><br><span class="line">    coordinator.commitOffsetsAsync()</span><br><span class="line">        doCommitOffsetsAsync()</span><br><span class="line">            sendOffsetCommitRequest()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Partition分配机制"><a href="#Partition分配机制" class="headerlink" title="Partition分配机制"></a>Partition分配机制</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AbstractPartitionAssignor</span><br><span class="line">    RangeAssignor(范围划分)</span><br><span class="line">    RoundRobinAssignor(轮询)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之九日志管理</title>
    <url>/2020/05/07/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E4%B9%9D%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>此日志不是Kafka本身日志,介绍Kafka底层是如何存储日志数据的</p>
</blockquote>
<span id="more"></span>

<h2 id="日志的基本概念"><a href="#日志的基本概念" class="headerlink" title="日志的基本概念"></a>日志的基本概念</h2><p><a href="https://jxeditor.github.io/2018/01/25/Kafka%E7%9A%84%E6%A6%82%E5%BF%B5%E6%80%A7%E7%9F%A5%E8%AF%86%E6%95%B4%E5%90%88/">传送门</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在[Kafka的概念性知识整合]一文有细致的介绍,本文不再赘述</span><br><span class="line"></span><br><span class="line">副本概念:(假设有3个副本)</span><br><span class="line">    每个Partition都会有3个副本,三个副本在不同的Broker上</span><br><span class="line">    三个副本中会选举出来一个Leader,另外俩个就是Follower</span><br><span class="line">    Topic的读写都是在Leader上进行,Follower从Leader同步</span><br><span class="line">Follower不支持读写,为了保证数据一致性</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="日志管理"><a href="#日志管理" class="headerlink" title="日志管理"></a>日志管理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LogManager主要负责日志创建,检索,清理</span><br><span class="line">日志读写操作由日志实例对象Log来处理</span><br></pre></td></tr></table></figure>
<h3 id="初始化LogManager"><a href="#初始化LogManager" class="headerlink" title="初始化LogManager"></a>初始化LogManager</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">logManager = LogManager(config, initialOfflineDirs, zkClient, brokerState, kafkaScheduler, time, brokerTopicStats, logDirFailureChannel)</span><br><span class="line"></span><br><span class="line">LogManager.apply()</span><br><span class="line"></span><br><span class="line">class LogManager(logDirs: Seq[File],</span><br><span class="line">     initialOfflineDirs: Seq[File],</span><br><span class="line">     val topicConfigs: Map[String, LogConfig], </span><br><span class="line">     val initialDefaultConfig: LogConfig,</span><br><span class="line">     val cleanerConfig: CleanerConfig,</span><br><span class="line">     recoveryThreadsPerDataDir: Int,</span><br><span class="line">     val flushCheckMs: Long,</span><br><span class="line">     val flushRecoveryOffsetCheckpointMs: Long,</span><br><span class="line">     val flushStartOffsetCheckpointMs: Long,</span><br><span class="line">     val retentionCheckMs: Long,</span><br><span class="line">     val maxPidExpirationMs: Int,</span><br><span class="line">     scheduler: Scheduler,</span><br><span class="line">     val brokerState: BrokerState,</span><br><span class="line">     brokerTopicStats: BrokerTopicStats,</span><br><span class="line">     logDirFailureChannel: LogDirFailureChannel,</span><br><span class="line">     time: Time) extends Logging with KafkaMetricsGroup &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 检查点表示日志已经刷新到磁盘的位置,用于数据恢复</span></span><br><span class="line">  val RecoveryPointCheckpointFile = <span class="string">&quot;recovery-point-offset-checkpoint&quot;</span> <span class="comment">// 检查点文件</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分区与日志实例的对应关系</span></span><br><span class="line">  <span class="keyword">private</span> val currentLogs = <span class="keyword">new</span> Pool[TopicPartition, Log]()</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 检查日志目录</span></span><br><span class="line">  <span class="keyword">private</span> val _liveLogDirs: ConcurrentLinkedQueue[File] = createAndValidateLogDirs(logDirs, initialOfflineDirs)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 每个数据目录都有一个检查点文件,存储这个数据目录下所有分区的检查点信息</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> recoveryPointCheckpoints = liveLogDirs.map(dir =&gt;</span><br><span class="line">    (dir, <span class="keyword">new</span> OffsetCheckpointFile(<span class="keyword">new</span> File(dir, RecoveryPointCheckpointFile), logDirFailureChannel))).toMap</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 创建指定的数据目录,并做相应的检查</span></span><br><span class="line">  <span class="comment">// 确保数据,目录中没有重复的数据目录</span></span><br><span class="line">  <span class="comment">// 数据不存在的话就创建相应的目录</span></span><br><span class="line">  <span class="comment">// 检查每个目录路径是否是可读的</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">createAndValidateLogDirs</span><span class="params">(dirs: Seq[File], initialOfflineDirs: Seq[File])</span>: ConcurrentLinkedQueue[File] </span>= &#123;</span><br><span class="line">    val liveLogDirs = <span class="keyword">new</span> ConcurrentLinkedQueue[File]()</span><br><span class="line">    val canonicalPaths = mutable.HashSet.empty[String]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (dir &lt;- dirs) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (initialOfflineDirs.contains(dir))</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> IOException(s<span class="string">&quot;Failed to load $&#123;dir.getAbsolutePath&#125; during broker startup&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!dir.exists) &#123;</span><br><span class="line">          info(s<span class="string">&quot;Log directory $&#123;dir.getAbsolutePath&#125; not found, creating it.&quot;</span>)</span><br><span class="line">          val created = dir.mkdirs()</span><br><span class="line">          <span class="keyword">if</span> (!created)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IOException(s<span class="string">&quot;Failed to create data directory $&#123;dir.getAbsolutePath&#125;&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!dir.isDirectory || !dir.canRead)</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> IOException(s<span class="string">&quot;$&#123;dir.getAbsolutePath&#125; is not a readable log directory.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!canonicalPaths.add(dir.getCanonicalPath))</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(s<span class="string">&quot;Duplicate log directory found: $&#123;dirs.mkString(&quot;</span>, <span class="string">&quot;)&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        liveLogDirs.add(dir)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: IOException =&gt;</span><br><span class="line">          logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath, s<span class="string">&quot;Failed to create or validate data directory $&#123;dir.getAbsolutePath&#125;&quot;</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (liveLogDirs.isEmpty) &#123;</span><br><span class="line">      fatal(s<span class="string">&quot;Shutdown broker because none of the specified log dirs from $&#123;dirs.mkString(&quot;</span>, <span class="string">&quot;)&#125; can be created or validated&quot;</span>)</span><br><span class="line">      Exit.halt(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    liveLogDirs</span><br><span class="line">  &#125;  </span><br><span class="line">    </span><br><span class="line">  <span class="comment">// 加载所有的日志,而每个日志也会调用loadSegments()方法加载所有的分段,过程比较慢,所以每个日志都会创建一个单独的线程</span></span><br><span class="line">  <span class="comment">// 日志管理器采用线程池提交任务,表示不用的任务可以同时运行</span></span><br><span class="line">  <span class="function"><span class="keyword">private</span> def <span class="title">loadLogs</span><span class="params">()</span>: Unit </span>= &#123;</span><br><span class="line">    info(<span class="string">&quot;Loading logs.&quot;</span>)</span><br><span class="line">    val startMs = time.milliseconds</span><br><span class="line">    val threadPools = ArrayBuffer.empty[ExecutorService]</span><br><span class="line">    val offlineDirs = mutable.Set.empty[(String, IOException)]</span><br><span class="line">    val jobs = mutable.Map.empty[File, Seq[Future[_]]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (dir &lt;- liveLogDirs) &#123; <span class="comment">// 处理每一个日志目录</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        val pool = Executors.newFixedThreadPool(numRecoveryThreadsPerDataDir) <span class="comment">// 默认为1</span></span><br><span class="line">        threadPools.append(pool) <span class="comment">// 每个对应的数据目录都有一个线程池</span></span><br><span class="line"></span><br><span class="line">        val cleanShutdownFile = <span class="keyword">new</span> File(dir, Log.CleanShutdownFile)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (cleanShutdownFile.exists) &#123;</span><br><span class="line">          debug(s<span class="string">&quot;Found clean shutdown file. Skipping recovery for all logs in data directory: $&#123;dir.getAbsolutePath&#125;&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// log recovery itself is being performed by `Log` class during initialization</span></span><br><span class="line">          brokerState.newState(RecoveringFromUncleanShutdown)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> recoveryPoints = Map[TopicPartition, Long]()</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          recoveryPoints = <span class="keyword">this</span>.recoveryPointCheckpoints(dir).read <span class="comment">// 读取检查点文件</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: Exception =&gt;</span><br><span class="line">            warn(<span class="string">&quot;Error occurred while reading recovery-point-offset-checkpoint file of directory &quot;</span> + dir, e)</span><br><span class="line">            warn(<span class="string">&quot;Resetting the recovery checkpoint to 0&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> logStartOffsets = Map[TopicPartition, Long]()</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          logStartOffsets = <span class="keyword">this</span>.logStartOffsetCheckpoints(dir).read</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: Exception =&gt;</span><br><span class="line">            warn(<span class="string">&quot;Error occurred while reading log-start-offset-checkpoint file of directory &quot;</span> + dir, e)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        val jobsForDir = <span class="keyword">for</span> &#123;</span><br><span class="line">          dirContent &lt;- Option(dir.listFiles).toList <span class="comment">// 数据目录下所有日志目录</span></span><br><span class="line">          logDir &lt;- dirContent <span class="keyword">if</span> logDir.isDirectory <span class="comment">// 日志目录下每个分区目录</span></span><br><span class="line">        &#125; yield &#123;</span><br><span class="line">          CoreUtils.runnable &#123; <span class="comment">// 每个分区的目录都对应了一个线程</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              loadLog(logDir, recoveryPoints, logStartOffsets)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> e: IOException =&gt;</span><br><span class="line">                offlineDirs.add((dir.getAbsolutePath, e)) <span class="comment">// 失效目录</span></span><br><span class="line">                error(<span class="string">&quot;Error while loading log dir &quot;</span> + dir.getAbsolutePath, e)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        jobs(cleanShutdownFile) = jobsForDir.map(pool.submit) <span class="comment">// 提交任务</span></span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: IOException =&gt;</span><br><span class="line">          offlineDirs.add((dir.getAbsolutePath, e))</span><br><span class="line">          error(<span class="string">&quot;Error while loading log dir &quot;</span> + dir.getAbsolutePath, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> ((cleanShutdownFile, dirJobs) &lt;- jobs) &#123;</span><br><span class="line">        dirJobs.foreach(_.get)</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          cleanShutdownFile.delete()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: IOException =&gt;</span><br><span class="line">            offlineDirs.add((cleanShutdownFile.getParent, e))</span><br><span class="line">            error(s<span class="string">&quot;Error while deleting the clean shutdown file $cleanShutdownFile&quot;</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      offlineDirs.foreach &#123; <span class="keyword">case</span> (dir, e) =&gt;</span><br><span class="line">        logDirFailureChannel.maybeAddOfflineLogDir(dir, s<span class="string">&quot;Error while deleting the clean shutdown file in dir $dir&quot;</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: ExecutionException =&gt;</span><br><span class="line">        error(<span class="string">&quot;There was an error in one of the threads during logs loading: &quot;</span> + e.getCause)</span><br><span class="line">        <span class="keyword">throw</span> e.getCause</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      threadPools.foreach(_.shutdown())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    info(s<span class="string">&quot;Logs loading complete in $&#123;time.milliseconds - startMs&#125; ms.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="启动LogManager"><a href="#启动LogManager" class="headerlink" title="启动LogManager"></a>启动LogManager</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// KafkaServer</span></span><br><span class="line">logManager.startup()</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">startup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">/* 安排清理任务以删除旧日志 */</span></span><br><span class="line">    <span class="keyword">if</span> (scheduler != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 定时清理过期的日志segment,并维护日志的大小</span></span><br><span class="line">      info(<span class="string">&quot;Starting log cleanup with a period of %d ms.&quot;</span>.format(retentionCheckMs))</span><br><span class="line">      scheduler.schedule(<span class="string">&quot;kafka-log-retention&quot;</span>,</span><br><span class="line">                         cleanupLogs _,</span><br><span class="line">                         delay = InitialTaskDelayMs,</span><br><span class="line">                         period = retentionCheckMs,</span><br><span class="line">                         TimeUnit.MILLISECONDS)</span><br><span class="line">      <span class="comment">// 定时刷新还没有写到磁盘上的日志</span></span><br><span class="line">      info(<span class="string">&quot;Starting log flusher with a default period of %d ms.&quot;</span>.format(flushCheckMs))</span><br><span class="line">      scheduler.schedule(<span class="string">&quot;kafka-log-flusher&quot;</span>,</span><br><span class="line">                         flushDirtyLogs _,</span><br><span class="line">                         delay = InitialTaskDelayMs,</span><br><span class="line">                         period = flushCheckMs,</span><br><span class="line">                         TimeUnit.MILLISECONDS)</span><br><span class="line">      <span class="comment">// 定时将所有数据目录所有日志的检查点写到检查点文件中</span></span><br><span class="line">      scheduler.schedule(<span class="string">&quot;kafka-recovery-point-checkpoint&quot;</span>,</span><br><span class="line">                         checkpointLogRecoveryOffsets _,</span><br><span class="line">                         delay = InitialTaskDelayMs,</span><br><span class="line">                         period = flushRecoveryOffsetCheckpointMs,</span><br><span class="line">                         TimeUnit.MILLISECONDS)</span><br><span class="line">      scheduler.schedule(<span class="string">&quot;kafka-log-start-offset-checkpoint&quot;</span>,</span><br><span class="line">                         checkpointLogStartOffsets _,</span><br><span class="line">                         delay = InitialTaskDelayMs,</span><br><span class="line">                         period = flushStartOffsetCheckpointMs,</span><br><span class="line">                         TimeUnit.MILLISECONDS)</span><br><span class="line">      <span class="comment">// 定时删除标记为delete的日志文件</span></span><br><span class="line">      scheduler.schedule(<span class="string">&quot;kafka-delete-logs&quot;</span>, <span class="comment">// will be rescheduled after each delete logs with a dynamic period</span></span><br><span class="line">                         deleteLogs _,</span><br><span class="line">                         delay = InitialTaskDelayMs,</span><br><span class="line">                         unit = TimeUnit.MILLISECONDS)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果设置为true,自动清理compaction类型的topic</span></span><br><span class="line">    <span class="keyword">if</span> (cleanerConfig.enableCleaner)</span><br><span class="line">      cleaner.startup()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="检查点文件"><a href="#检查点文件" class="headerlink" title="检查点文件"></a>检查点文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kafka启动时创建LogManager,读取检查点文件,并把每个分区对应的检查点作为日志的恢复点,最后创建分区对应的Log实例</span><br><span class="line">消费追加到分区对应的日志,在刷新日志时,将最新的偏移量作为日志的检查点</span><br><span class="line">    刷新日志时,更新检查点位置</span><br><span class="line">LogManager会启动一个定时任务,读取所有日志的检查点,并写入全局的检查点文件</span><br><span class="line">    定时将检查点的位置更新到检查点文件中</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="日志刷新"><a href="#日志刷新" class="headerlink" title="日志刷新"></a>日志刷新</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">LogManager会定时调度flushDirtyLogs(),定期将缓存中的数据刷新到磁盘中</span><br><span class="line">    如果缓存数据在flush到磁盘之前,Broker宕机,数据就会丢失</span><br><span class="line">Kafka两种策略,将日志刷新到磁盘上</span><br><span class="line">    时间策略(log.flush.interval.ms)默认无限大,即选择大小策略</span><br><span class="line">    大小策略(log.flush.interval.messages)当未刷新的数据超过这个值后,进行刷新</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 周期调度</span><br><span class="line">private def flushDirtyLogs(): Unit &#x3D; &#123;</span><br><span class="line">  debug(&quot;Checking for dirty logs to flush...&quot;)</span><br><span class="line">  for ((topicPartition, log) &lt;- currentLogs.toList ++ futureLogs.toList) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      &#x2F;&#x2F; 每个日志的刷新时间不相同</span><br><span class="line">      val timeSinceLastFlush &#x3D; time.milliseconds - log.lastFlushTime</span><br><span class="line">      debug(&quot;Checking if flush is needed on &quot; + topicPartition.topic + &quot; flush interval  &quot; + log.config.flushMs +</span><br><span class="line">            &quot; last flushed &quot; + log.lastFlushTime + &quot; time since last flush: &quot; + timeSinceLastFlush)</span><br><span class="line">      if(timeSinceLastFlush &gt;&#x3D; log.config.flushMs)</span><br><span class="line">        &#x2F;&#x2F; 最终还是调用Log实例的flush进行刷新操作</span><br><span class="line">        log.flush</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Throwable &#x3D;&gt;</span><br><span class="line">        error(&quot;Error flushing topic &quot; + topicPartition.topic, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def flush(offset: Long) : Unit &#x3D; &#123;</span><br><span class="line">  maybeHandleIOException(s&quot;Error while flushing log for $topicPartition in dir $&#123;dir.getParent&#125; with offset $offset&quot;) &#123;</span><br><span class="line">    if (offset &lt;&#x3D; this.recoveryPoint)</span><br><span class="line">      return</span><br><span class="line">    debug(s&quot;Flushing log up to offset $offset, last flushed: $lastFlushTime,  current time: $&#123;time.milliseconds()&#125;, &quot; +</span><br><span class="line">      s&quot;unflushed: $unflushedMessages&quot;)</span><br><span class="line">    &#x2F;&#x2F; 刷新检查点到最新偏移量之间的所有日志分段</span><br><span class="line">    for (segment &lt;- logSegments(this.recoveryPoint, offset))</span><br><span class="line">      segment.flush() &#x2F;&#x2F; 刷新数据文件以及索引文件(调用操作系统的fsync)</span><br><span class="line">    lock synchronized &#123;</span><br><span class="line">      checkIfMemoryMappedBufferClosed()</span><br><span class="line">      if (offset &gt; this.recoveryPoint) &#123; &#x2F;&#x2F; 如果检查点比最新偏移量小,直接赋值</span><br><span class="line">        this.recoveryPoint &#x3D; offset</span><br><span class="line">        lastFlushedTime.set(time.milliseconds) &#x2F;&#x2F; 更新刷新时间</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">删除: 超过时间或大小阈值的旧segment,直接进行删除</span><br><span class="line">压缩: 不直接删除日志分段,而是采用合并压缩的方式进行</span><br><span class="line"></span><br><span class="line">def cleanupLogs() &#123;</span><br><span class="line">    debug(&quot;Beginning log cleanup...&quot;)</span><br><span class="line">    var total &#x3D; 0</span><br><span class="line">    val startMs &#x3D; time.milliseconds</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; clean current logs.</span><br><span class="line">    val deletableLogs &#x3D; &#123;</span><br><span class="line">      if (cleaner !&#x3D; null) &#123;</span><br><span class="line">        cleaner.pauseCleaningForNonCompactedPartitions()</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        currentLogs.filter &#123;</span><br><span class="line">          case (_, log) &#x3D;&gt; !log.config.compact</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">      deletableLogs.foreach &#123;</span><br><span class="line">        case (topicPartition, log) &#x3D;&gt;</span><br><span class="line">          debug(&quot;Garbage collecting &#39;&quot; + log.name + &quot;&#39;&quot;)</span><br><span class="line">          &#x2F;&#x2F; 清理过期的segment</span><br><span class="line">          total +&#x3D; log.deleteOldSegments()</span><br><span class="line"></span><br><span class="line">          val futureLog &#x3D; futureLogs.get(topicPartition)</span><br><span class="line">          if (futureLog !&#x3D; null) &#123;</span><br><span class="line">            &#x2F;&#x2F; clean future logs</span><br><span class="line">            debug(&quot;Garbage collecting future log &#39;&quot; + futureLog.name + &quot;&#39;&quot;)</span><br><span class="line">            total +&#x3D; futureLog.deleteOldSegments()</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      if (cleaner !&#x3D; null) &#123;</span><br><span class="line">        cleaner.resumeCleaning(deletableLogs.map(_._1))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    debug(&quot;Log cleanup completed. &quot; + total + &quot; files deleted in &quot; +</span><br><span class="line">                  (time.milliseconds - startMs) &#x2F; 1000 + &quot; seconds&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; segment保存时间超过设置的时间,进行删除</span><br><span class="line">&#x2F;&#x2F; 如果当前最新的日志大小加上下一个即将删除的segment分段的大小超过阈值,那么就允许删除该segment</span><br><span class="line">def deleteOldSegments(): Int &#x3D; &#123;</span><br><span class="line">  if (config.delete) &#123;</span><br><span class="line">    deleteRetentionMsBreachedSegments() + deleteRetentionSizeBreachedSegments() + deleteLogStartOffsetBreachedSegments()</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    deleteLogStartOffsetBreachedSegments()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之八GroupCoordinator详解</title>
    <url>/2020/05/07/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%85%ABGroupCoordinator%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>在如何创建一个Group时有过简单使用,这里详细介绍一下</p>
</blockquote>
<span id="more"></span>

<h2 id="ApiKeys"><a href="#ApiKeys" class="headerlink" title="ApiKeys"></a>ApiKeys</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">包含所有Kafka API</span><br><span class="line">常用</span><br><span class="line">OFFSET_COMMIT</span><br><span class="line">OFFSET_FETCH</span><br><span class="line">JOIN_GROUP</span><br><span class="line">SYNC_GROUP</span><br><span class="line">DESCRIBE_GROUPS</span><br><span class="line">LIST_GROUPS</span><br><span class="line">HEARTBEAT</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Broker在启动时,都会启动GroupCoordinator</span></span><br><span class="line">groupCoordinator = GroupCoordinator(config, zkClient, replicaManager, Time.SYSTEM)</span><br><span class="line">groupCoordinator.startup()</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">startup</span><span class="params">(enableMetadataExpiration: Boolean = <span class="keyword">true</span>)</span> </span>&#123;</span><br><span class="line">  info(<span class="string">&quot;Starting up.&quot;</span>)</span><br><span class="line">  <span class="comment">// 启动一个后台线程删除过期的group metadata</span></span><br><span class="line">  groupManager.startup(enableMetadataExpiration)</span><br><span class="line">  <span class="comment">// 标志变量设置为true</span></span><br><span class="line">  isActive.set(<span class="keyword">true</span>)</span><br><span class="line">  info(<span class="string">&quot;Startup complete.&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// GroupMetadata</span></span><br><span class="line">private[group] class GroupMetadata(val groupId: String, initialState: GroupState, time: Time) extends Logging &#123;</span><br><span class="line">    <span class="comment">// group状态</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> state: GroupState = initialState</span><br><span class="line">    <span class="comment">// generation id</span></span><br><span class="line">    <span class="keyword">var</span> generationId = <span class="number">0</span></span><br><span class="line">    <span class="comment">// leader consumer id</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> leaderId: Option[String] = None</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> protocol: Option[String] = None</span><br><span class="line">    <span class="comment">// group的member信息</span></span><br><span class="line">    <span class="keyword">private</span> val members = <span class="keyword">new</span> mutable.HashMap[String, MemberMetadata]</span><br><span class="line">    <span class="comment">// 等待加入的member数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> numMembersAwaitingJoin = <span class="number">0</span></span><br><span class="line">    <span class="keyword">private</span> val supportedProtocols = <span class="keyword">new</span> mutable.HashMap[String, Integer]().withDefaultValue(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">// 对应的commit offset</span></span><br><span class="line">    <span class="keyword">private</span> val offsets = <span class="keyword">new</span> mutable.HashMap[TopicPartition, CommitRecordMetadataAndOffset]</span><br><span class="line">    <span class="comment">// commit offset成功后更新到上面的map中</span></span><br><span class="line">    <span class="keyword">private</span> val pendingOffsetCommits = <span class="keyword">new</span> mutable.HashMap[TopicPartition, OffsetAndMetadata]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MemberMetadata 记录group中每个成员的状态信息</span></span><br><span class="line">private[group] class MemberMetadata(val memberId: String,</span><br><span class="line">        val groupId: String,</span><br><span class="line">        val clientId: String,</span><br><span class="line">        val clientHost: String,</span><br><span class="line">        val rebalanceTimeoutMs: Int,</span><br><span class="line">        val sessionTimeoutMs: Int,</span><br><span class="line">        val protocolType: String,</span><br><span class="line">        <span class="keyword">var</span> supportedProtocols: List[(String, Array[Byte])])</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="GroupCoordinator请求处理"><a href="#GroupCoordinator请求处理" class="headerlink" title="GroupCoordinator请求处理"></a>GroupCoordinator请求处理</h2><h3 id="Offset请求处理"><a href="#Offset请求处理" class="headerlink" title="Offset请求处理"></a>Offset请求处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">OFFSET_FETCH: 查询offset</span><br><span class="line">    handleFetchOffsets()</span><br><span class="line">OFFSET_COMMIT: 提供offset</span><br><span class="line">    handleCommitOffsets()</span><br></pre></td></tr></table></figure>
<h3 id="Group相关处理"><a href="#Group相关处理" class="headerlink" title="Group相关处理"></a>Group相关处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JOIN_GROUP,SYNC_GROUP(前面已经详细说明过)</span><br><span class="line">DESCRIBE_GROUPS: 返回Group中各个member的详细信息</span><br><span class="line">    handleDescribeGroup()</span><br><span class="line">LEAVE_GROUP: 移除失败的member,并进行相应的状态转换</span><br><span class="line">    handleLeaveGroup()</span><br></pre></td></tr></table></figure>
<h3 id="心跳请求处理"><a href="#心跳请求处理" class="headerlink" title="心跳请求处理"></a>心跳请求处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HEARTBEAT</span><br><span class="line">    handleHeartbeat()</span><br><span class="line"></span><br><span class="line">对于Server端,是GroupCoordinator判断consumer member是否存活的重要条件</span><br><span class="line"></span><br><span class="line">对于Client端,是Client感应group状态变化的一个重要中介</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Group的状态机"><a href="#Group的状态机" class="headerlink" title="Group的状态机"></a>Group的状态机</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GroupState</span><br><span class="line">    Dead -&gt; [Stable, PreparingRebalance, CompletingRebalance, Empty, Dead]</span><br><span class="line">    CompletingRebalance -&gt; [PreparingRebalance]</span><br><span class="line">    Stable -&gt; [CompletingRebalance]</span><br><span class="line">    PreparingRebalance -&gt; [Stable, CompletingRebalance, Empty]</span><br><span class="line">    Empty -&gt; [PreparingRebalance]</span><br><span class="line"></span><br><span class="line">状态之间的有效转换关系</span><br><span class="line">    右为前置状态</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之三ProducerNIO网络模型</title>
    <url>/2020/05/06/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E4%B8%89ProducerNIO%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<blockquote>
<p>Producer的网络模型,与JavaNIO模型之间关系,以及整体流程</p>
</blockquote>
<span id="more"></span>

<h2 id="Producer流程概览"><a href="#Producer流程概览" class="headerlink" title="Producer流程概览"></a>Producer流程概览</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">涉及类</span><br><span class="line">KafkaProducer</span><br><span class="line"> -&gt;Sender</span><br><span class="line">  -&gt;NetworkClient</span><br><span class="line">   -&gt;Selector</span><br><span class="line">    -&gt;JavaNIO接口</span><br><span class="line"></span><br><span class="line">涉及方法</span><br><span class="line">KafkaProducer.doSend()</span><br><span class="line"> -&gt;Sender.run()</span><br><span class="line">  -&gt;NetworkClient.poll()</span><br><span class="line">   -&gt;Selector.poll()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="流程详解"><a href="#流程详解" class="headerlink" title="流程详解"></a>流程详解</h2><h3 id="KafkaProducer-doSend"><a href="#KafkaProducer-doSend" class="headerlink" title="KafkaProducer.doSend()"></a>KafkaProducer.doSend()</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">waitOnMetadata(): 请求更新tp(topic-partition) meta,中间调用sender.wakeup()</span><br><span class="line">accumulator.append(): 将信息写入tp对应的deque中,如果tp对应的deque新建了Batch,最后也会调用sender.wakeup()</span><br></pre></td></tr></table></figure>
<h4 id="sender-wakeup"><a href="#sender-wakeup" class="headerlink" title="sender.wakeup()"></a>sender.wakeup()</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// org.apache.kafka.clients.producer.internals.Sender</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.client.wakeup();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaClient</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// org.apache.kafka.clients.NetworkClient</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.selector.wakeup();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Selectable</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// org.apache.kafka.common.network.Selector</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">wakeup</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.nioSelector.wakeup();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Sender-run"><a href="#Sender-run" class="headerlink" title="Sender.run()"></a>Sender.run()</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">此处详细在源码系列一中有详细说明</span><br><span class="line">sendProducerData()</span><br><span class="line"></span><br><span class="line">1.accumulator.ready(): 遍历所有的tp,如果其对应的ProducerBatch可以发送,就将其对应的leader选出来</span><br><span class="line">    最后会返回一个可以发送ProduceRequest的Set&lt;Node&gt;(实际是ReadyCheckResult,Set&lt;Node&gt;是最主要的成员变量)</span><br><span class="line"></span><br><span class="line">2.如果发现tp没有leader,那么将会调用requestUpdate()更新metadata</span><br><span class="line">    实际上还是在第一步对tp的遍历中,遇到没有leader的tp就将其加入到unknownLeaderTopics的set中</span><br><span class="line">    然后会请求这个tp的meta</span><br><span class="line"></span><br><span class="line">3.accumulator.drain(): 遍历每个leader上的所有tp,如果该tp对应的ProducerBatch不在backoff期间</span><br><span class="line">    并且加上这个ProducerBatch大小不超过maxSize(一个request的最大限制,默认1MB)</span><br><span class="line">    那么就把这个ProducerBatch添加到list中,最终返回Map&lt;Integer,List&lt;ProducerBatch&gt;&gt;</span><br><span class="line">    key为leaderId,value为要发送的ProducerBatch列表</span><br><span class="line">    如果ProducerBatch没有达到要求,还是有可能发送</span><br><span class="line">    这样可以减少request的频率,有利于提高发送效率</span><br><span class="line"></span><br><span class="line">4.sendProduceRequests(): 发送Produce请求,这个方法会调用NetworkClient.send()来发送clientRequest</span><br><span class="line"></span><br><span class="line">5.NetworkClient.poll(): 关于socket的IO操作都是在这个方法进行的,调用Selector进行的相应操作</span><br><span class="line">    而Selector底层则封装的JavaNIO的相关接口</span><br></pre></td></tr></table></figure>
<h3 id="NetworkClient-poll"><a href="#NetworkClient-poll" class="headerlink" title="NetworkClient.poll()"></a>NetworkClient.poll()</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果有需要更新的Metadata,就发送metadata请求</span><br><span class="line">调用Selector进行相应的IO操作</span><br><span class="line">处理Server端的response以及其他一些操作</span><br></pre></td></tr></table></figure>
<h3 id="Selector-poll"><a href="#Selector-poll" class="headerlink" title="Selector.poll()"></a>Selector.poll()</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Kafka对JavaNIO相关接口的封装</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (timeout &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;timeout should be &gt;= 0&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> madeReadProgressLastCall = madeReadProgressLastPoll;</span><br><span class="line">    <span class="comment">// 清除相关记录</span></span><br><span class="line">    clear();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> dataInBuffers = !keysWithBufferedRead.isEmpty();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (hasStagedReceives() || !immediatelyConnectedKeys.isEmpty() || (madeReadProgressLastCall &amp;&amp; dataInBuffers))</span><br><span class="line">        timeout = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!memoryPool.isOutOfMemory() &amp;&amp; outOfMemory) &#123;</span><br><span class="line">        <span class="comment">//we have recovered from memory pressure. unmute any channel not explicitly muted for other reasons</span></span><br><span class="line">        log.trace(<span class="string">&quot;Broker no longer low on memory - unmuting incoming sockets&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (KafkaChannel channel : channels.values()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (channel.isInMutableState() &amp;&amp; !explicitlyMutedChannels.contains(channel)) &#123;</span><br><span class="line">                channel.maybeUnmute();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        outOfMemory = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* check ready keys */</span></span><br><span class="line">    <span class="comment">// 获取就绪事件的数量</span></span><br><span class="line">    <span class="keyword">long</span> startSelect = time.nanoseconds();</span><br><span class="line">    <span class="keyword">int</span> numReadyKeys = select(timeout);</span><br><span class="line">    <span class="keyword">long</span> endSelect = time.nanoseconds();</span><br><span class="line">    <span class="keyword">this</span>.sensors.selectTime.record(endSelect - startSelect, time.milliseconds());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理IO操作</span></span><br><span class="line">    <span class="keyword">if</span> (numReadyKeys &gt; <span class="number">0</span> || !immediatelyConnectedKeys.isEmpty() || dataInBuffers) &#123;</span><br><span class="line">        Set&lt;SelectionKey&gt; readyKeys = <span class="keyword">this</span>.nioSelector.selectedKeys();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Poll from channels that have buffered data (but nothing more from the underlying socket)</span></span><br><span class="line">        <span class="keyword">if</span> (dataInBuffers) &#123;</span><br><span class="line">            keysWithBufferedRead.removeAll(readyKeys); <span class="comment">//so no channel gets polled twice</span></span><br><span class="line">            Set&lt;SelectionKey&gt; toPoll = keysWithBufferedRead;</span><br><span class="line">            keysWithBufferedRead = <span class="keyword">new</span> HashSet&lt;&gt;(); <span class="comment">//poll() calls will repopulate if needed</span></span><br><span class="line">            pollSelectionKeys(toPoll, <span class="keyword">false</span>, endSelect);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Poll from channels where the underlying socket has more data</span></span><br><span class="line">        pollSelectionKeys(readyKeys, <span class="keyword">false</span>, endSelect);</span><br><span class="line">        <span class="comment">// Clear all selected keys so that they are included in the ready count for the next select</span></span><br><span class="line">        readyKeys.clear();</span><br><span class="line"></span><br><span class="line">        pollSelectionKeys(immediatelyConnectedKeys, <span class="keyword">true</span>, endSelect);</span><br><span class="line">        immediatelyConnectedKeys.clear();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        madeReadProgressLastPoll = <span class="keyword">true</span>; <span class="comment">//no work is also &quot;progress&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> endIo = time.nanoseconds();</span><br><span class="line">    <span class="keyword">this</span>.sensors.ioTime.record(endIo - endSelect, time.milliseconds());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Close channels that were delayed and are now ready to be closed</span></span><br><span class="line">    completeDelayedChannelClose(endIo);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we use the time at the end of select to ensure that we don&#x27;t close any connections that</span></span><br><span class="line">    <span class="comment">// have just been processed in pollSelectionKeys</span></span><br><span class="line">    <span class="comment">// 每次poll之后会调用一次,连接虽然关闭,但是Client端的缓存依然存在</span></span><br><span class="line">    maybeCloseOldestConnection(endSelect);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add to completedReceives after closing expired connections to avoid removing</span></span><br><span class="line">    <span class="comment">// channels with completed receives until all staged receives are completed.</span></span><br><span class="line">    <span class="comment">// 将处理得到的stagedReceives添加到completedReceives</span></span><br><span class="line">    addToCompletedReceives();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="clear"><a href="#clear" class="headerlink" title="clear()"></a>clear()</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clear方法是在每次poll()执行的第一步</span><br><span class="line">清理上一次poll过程产生的部分缓存</span><br></pre></td></tr></table></figure>
<h4 id="select"><a href="#select" class="headerlink" title="select()"></a>select()</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如果在一次轮询,只要有一个Channel的事件就绪,就立刻返回</span><br></pre></td></tr></table></figure>
<h4 id="pollSelectionKeys"><a href="#pollSelectionKeys" class="headerlink" title="pollSelectionKeys()"></a>pollSelectionKeys()</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一次调用:处理已经就绪的事件,进行相应的IO操作</span><br><span class="line">第二次调用:处理新建立的那些连接,添加缓存以及传输层的握手与认证</span><br></pre></td></tr></table></figure>
<h4 id="addToCompletedReceives"><a href="#addToCompletedReceives" class="headerlink" title="addToCompletedReceives"></a>addToCompletedReceives</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">处理接收到的Receive,在Client和Server端都会调用</span><br><span class="line"></span><br><span class="line">Server端:</span><br><span class="line">    为保证消息的时序性,在Selector中提供了mute(String id)和unmute(String id)</span><br><span class="line">    对该KafkaChannel做标记来保证同时只能处理这个Channel的一个request(排它锁)</span><br><span class="line">    当Server端接收到request后,先将其放入stageReceives集合中</span><br><span class="line">    此时该Channel还未mute,这个Receive会被放入completedReceives集合中</span><br><span class="line">    Server在对completedReceives集合中的request进行处理时,先对该Channel mute</span><br><span class="line">    处理后的response发送完成后再对该Channel unmute</span><br><span class="line">    然后才处理该Channel的其他请求</span><br><span class="line">Client端:</span><br><span class="line">    Client不会调用Selector的mute()和unmute</span><br><span class="line">    Client的时序性通过InFlightRequests和RecordAccumulator的mutePartition来保证</span><br><span class="line">    对于Client端而言,接收到的所有Receives都会放入到completedReceives的集合中等待后续处理</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="另一个流程分支"><a href="#另一个流程分支" class="headerlink" title="另一个流程分支"></a>另一个流程分支</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sender.doSend()-&gt;sendProducerData-&gt;sendProduceRequests-&gt;sendProduceRequest</span><br><span class="line">    -&gt;KafkaClient.send()-&gt;NetworkClient.send()</span><br><span class="line">        -&gt;NetworkClient.doSend()</span><br><span class="line">            -&gt;Selector.send()</span><br><span class="line">                -&gt;KafkaChannel.setSend()</span><br></pre></td></tr></table></figure>
<h3 id="NetworkClient-doSend"><a href="#NetworkClient-doSend" class="headerlink" title="NetworkClient.doSend()"></a>NetworkClient.doSend()</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSend</span><span class="params">(ClientRequest clientRequest, <span class="keyword">boolean</span> isInternalRequest, <span class="keyword">long</span> now, AbstractRequest request)</span> </span>&#123;</span><br><span class="line">    String destination = clientRequest.destination();</span><br><span class="line">    <span class="comment">// 检查版本信息,并根据apiKey构建Request</span></span><br><span class="line">    RequestHeader header = clientRequest.makeHeader(request.version());</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</span><br><span class="line">        <span class="keyword">int</span> latestClientVersion = clientRequest.apiKey().latestVersion();</span><br><span class="line">        <span class="keyword">if</span> (header.apiVersion() == latestClientVersion) &#123;</span><br><span class="line">            log.trace(<span class="string">&quot;Sending &#123;&#125; &#123;&#125; with correlation id &#123;&#125; to node &#123;&#125;&quot;</span>, clientRequest.apiKey(), request,</span><br><span class="line">                    clientRequest.correlationId(), destination);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;Using older server API v&#123;&#125; to send &#123;&#125; &#123;&#125; with correlation id &#123;&#125; to node &#123;&#125;&quot;</span>,</span><br><span class="line">                    header.apiVersion(), clientRequest.apiKey(), request, clientRequest.correlationId(), destination);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 创建NetworkSend实例</span></span><br><span class="line">    Send send = request.toSend(destination, header);</span><br><span class="line">    InFlightRequest inFlightRequest = <span class="keyword">new</span> InFlightRequest(</span><br><span class="line">            clientRequest,</span><br><span class="line">            header,</span><br><span class="line">            isInternalRequest,</span><br><span class="line">            request,</span><br><span class="line">            send,</span><br><span class="line">            now);</span><br><span class="line">    <span class="keyword">this</span>.inFlightRequests.add(inFlightRequest);</span><br><span class="line">    <span class="comment">// 调用Selector.send发送该Send</span></span><br><span class="line">    selector.send(send);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Selector-send"><a href="#Selector-send" class="headerlink" title="Selector.send()"></a>Selector.send()</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(Send send)</span> </span>&#123;</span><br><span class="line">    String connectionId = send.destination();</span><br><span class="line">    <span class="comment">// 获取Send对应的KafkaChannel</span></span><br><span class="line">    KafkaChannel channel = openOrClosingChannelOrFail(connectionId);</span><br><span class="line">    <span class="keyword">if</span> (closingChannels.containsKey(connectionId)) &#123;</span><br><span class="line">        <span class="comment">// ensure notification via `disconnected`, leave channel in the state in which closing was triggered</span></span><br><span class="line">        <span class="keyword">this</span>.failedSends.add(connectionId);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 调用setSend()注册Write事件</span></span><br><span class="line">            channel.setSend(send);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// update the state for consistency, the channel will be discarded after `close`</span></span><br><span class="line">            channel.state(ChannelState.FAILED_SEND);</span><br><span class="line">            <span class="comment">// ensure notification via `disconnected` when `failedSends` are processed in the next poll</span></span><br><span class="line">            <span class="keyword">this</span>.failedSends.add(connectionId);</span><br><span class="line">            close(channel, CloseMode.DISCARD_NO_NOTIFY);</span><br><span class="line">            <span class="keyword">if</span> (!(e <span class="keyword">instanceof</span> CancelledKeyException)) &#123;</span><br><span class="line">                log.error(<span class="string">&quot;Unexpected exception during send, closing connection &#123;&#125; and rethrowing exception &#123;&#125;&quot;</span>,</span><br><span class="line">                        connectionId, e);</span><br><span class="line">                <span class="keyword">throw</span> e;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="KafkaChannel-setSend"><a href="#KafkaChannel-setSend" class="headerlink" title="KafkaChannel.setSend()"></a>KafkaChannel.setSend()</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">setSend()方法需要配置write()(在Selector.poll中调用pollSelectionKeys时使用)方法一起分析</span><br><span class="line"></span><br><span class="line"><span class="comment">// 每次调用都会注册一个OP_WRITE事件</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSend</span><span class="params">(Send send)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.send != <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Attempt to begin a send operation with prior send operation still in progress, connection id is &quot;</span> + id);</span><br><span class="line">    <span class="keyword">this</span>.send = send;</span><br><span class="line">    <span class="keyword">this</span>.transportLayer.addInterestOps(SelectionKey.OP_WRITE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用send()发送Send</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Send <span class="title">write</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Send result = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (send != <span class="keyword">null</span> &amp;&amp; send(send)) &#123;</span><br><span class="line">        result = send;</span><br><span class="line">        send = <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送完成后,删除这个OP_WRITE事件</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">send</span><span class="params">(Send send)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    send.writeTo(transportLayer);</span><br><span class="line">    <span class="keyword">if</span> (send.completed())</span><br><span class="line">        transportLayer.removeInterestOps(SelectionKey.OP_WRITE);</span><br><span class="line">    <span class="keyword">return</span> send.completed();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之六ConsumerPoll调用</title>
    <url>/2020/05/07/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%85%ADConsumerPoll%E8%B0%83%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>在上一篇有大概的涉及到一点,但方向更多的是Coordinator上,所以这里补充一下</p>
</blockquote>
<span id="more"></span>

<h2 id="调用流程"><a href="#调用流程" class="headerlink" title="调用流程"></a>调用流程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KafkaConsumer.poll()</span><br><span class="line">    -&gt;KafkaConsumer.pollForFetches()</span><br><span class="line">        -&gt;Fetcher.fetchedRecords()</span><br><span class="line">            -&gt;Fetcher.fetchRecords(PartitionRecords partitionRecords, int maxRecords)</span><br><span class="line">                -&gt;Fetcher.fetchRecords(int maxRecords)</span><br><span class="line">                    -&gt;Fetcher.parseRecord()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KafkaConsumer.poll()</span><br><span class="line">    检查这个consumer是否订阅相应的tp</span><br><span class="line">    调用pollForFetches()方法获取相应的records</span><br><span class="line">    如果在给定时间内获取不到可用的records返回空数据</span><br><span class="line"></span><br><span class="line">pollForFetches()</span><br><span class="line">    coordinator.timeToNextPoll(): 下一次调用poll的时间</span><br><span class="line">    fetchedRecords(): 实际获取数据方法</span><br><span class="line">    sendFetches(): 发送fetch请求</span><br><span class="line">    client.poll(): 调用底层NetworkClient发送相应的请求</span><br><span class="line">    coordinator.rejoinNeededOrPending(): 如果实例分配的tp列表发生变化,consumergroup需要rebalance</span><br><span class="line">    </span><br><span class="line">fetchRecords(PartitionRecords partitionRecords, int maxRecords)</span><br><span class="line">    处理PartitionRecords对象,在这个里面会去验证fetchOffset是否能对得上</span><br><span class="line">    只有fetchOffset是一致的情况下才会去处理相应的数据,并更新the fetch offset信息</span><br><span class="line">    如果不一致,不会处理,the fetch offset就不会更新</span><br><span class="line">    下次fetch请求时会接着这个位置去请求相应的数据</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="更新offset的position"><a href="#更新offset的position" class="headerlink" title="更新offset的position"></a>更新offset的position</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主要实现在KafkaConsumer.poll中</span><br><span class="line">updateAssignmentMetadataIfNeeded()方法</span><br><span class="line">    updateFetchPositions()</span><br><span class="line">        resetOffsetsIfNeeded()</span><br><span class="line">            resetOffsetsAsync()</span><br><span class="line">                resetOffsetIfNeeded()</span><br><span class="line">                    seek()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十七Server网络处理</title>
    <url>/2020/05/09/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E4%B8%83Server%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>Server端对于不同类型的请求处理实现</p>
</blockquote>
<span id="more"></span>

<h2 id="Server网络模型整体流程"><a href="#Server网络模型整体流程" class="headerlink" title="Server网络模型整体流程"></a>Server网络模型整体流程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KafkaServer在启动时会初始化SocketServer,KafkaApis,KafkaRequestHandlerPool对象</span><br><span class="line">这些是Server网络处理模型的主要组成部分</span><br><span class="line"></span><br><span class="line">KafkaServer的网络处理模型也是基于JavaNIO机制实现</span><br></pre></td></tr></table></figure>
<h3 id="初始化以及启动"><a href="#初始化以及启动" class="headerlink" title="初始化以及启动"></a>初始化以及启动</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 初始化SocketServer</span></span><br><span class="line">socketServer = <span class="keyword">new</span> <span class="type">SocketServer</span>(config, metrics, time, credentialProvider)</span><br><span class="line"><span class="comment">// 延时启动处理器,直到初始化序列结束</span></span><br><span class="line">socketServer.startup(startupProcessors = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaApis</span></span><br><span class="line">apis = <span class="keyword">new</span> <span class="type">KafkaApis</span>(socketServer.requestChannel, replicaManager, adminManager, groupCoordinator, transactionCoordinator,</span><br><span class="line">          kafkaController, zkClient, config.brokerId, config, metadataCache, metrics, authorizer, quotaManagers,</span><br><span class="line">          fetchManager, brokerTopicStats, clusterId, time, tokenManager)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 请求处理响应池</span></span><br><span class="line">requestHandlerPool = <span class="keyword">new</span> <span class="type">KafkaRequestHandlerPool</span>(config.brokerId, socketServer.requestChannel, apis, time,</span><br><span class="line">          config.numIoThreads)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 启动处理器</span></span><br><span class="line">socketServer.startProcessors()</span><br></pre></td></tr></table></figure>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">组成部分:</span><br><span class="line">    1个Acceptor线程,负责监听Socket新的连接请求,注册了OP_ACCEPT事件</span><br><span class="line">        将新的连接按照RoundRobin方式交给对应的Processor线程处理</span><br><span class="line">    N个Processor线程,其中每个Processor都有自己的Selector</span><br><span class="line">        它会向Acceptor分配的SocketChannel注册相应的OP_READ事件</span><br><span class="line">        N的大小由num.networker.threads决定</span><br><span class="line">    M个KafkaRequestHandler线程处理请求,并将处理的结果返回给Processor线程对应的ResponseQueue中</span><br><span class="line">        由Processor将处理的结果返回给相应的请求发送者</span><br><span class="line">        M的大小由num.io.threads决定</span><br><span class="line"></span><br><span class="line">处理流程:</span><br><span class="line">    1.Acceptor监听到来自请求者(client或者server)的新连接,Acceptor将这个请求者按照RoundRobin的方式交给对应的Processor进行处理</span><br><span class="line">    2.Processor注册这个SocketChannel的OP_READ事件,如果有请求发送过来就可以被Processor的Selector选中</span><br><span class="line">    3.Processor将请求者发送的请求放入一个RequestQueue中,这是所有Processor共有的一个队列</span><br><span class="line">    4.KafkaResultHandler从RequestQueue中取出请求</span><br><span class="line">    5.调用KafkaApis进行相应的处理</span><br><span class="line">    6.处理的结果放入到该Processor对应的ResponseQueue中(每个request都标识它们来自哪个Processor),ResponseQueue与Processor数量保持一致</span><br><span class="line">    7.Processor从对应的ResponseQueue中取出response</span><br><span class="line">    8.Processor将处理的结果返回给相应的请求者</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><h3 id="SocketServer"><a href="#SocketServer" class="headerlink" title="SocketServer"></a>SocketServer</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SocketServer</span>(<span class="params">val config: <span class="type">KafkaConfig</span>, val metrics: <span class="type">Metrics</span>, val time: <span class="type">Time</span>, val credentialProvider: <span class="type">CredentialProvider</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// broker开放的端口</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">endpoints</span> </span>= config.listeners.map(l =&gt; l.listenerName -&gt; l).toMap</span><br><span class="line">  <span class="comment">// 队列中允许的最多请求数,默认是500</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxQueuedRequests = config.queuedMaxRequests</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 请求队列</span></span><br><span class="line">  <span class="keyword">val</span> requestChannel = <span class="keyword">new</span> <span class="type">RequestChannel</span>(maxQueuedRequests)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> processors = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">Int</span>, <span class="type">Processor</span>]()</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> nextProcessorId = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[network] <span class="keyword">val</span> acceptors = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">EndPoint</span>, <span class="type">Acceptor</span>]()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RequestChannel</span>(<span class="params">val queueSize: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line">  <span class="keyword">import</span> <span class="type">RequestChannel</span>._</span><br><span class="line">  <span class="comment">// 1个requestQueue,N个response队列(有N个processor就有N个responseQueue)</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> requestQueue = <span class="keyword">new</span> <span class="type">ArrayBlockingQueue</span>[<span class="type">BaseRequest</span>](queueSize)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> processors = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">Int</span>, <span class="type">Processor</span>]()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">startup()方法中,初始一个<span class="type">Acceptor</span>和<span class="type">N</span>个<span class="type">Processor</span>线程(每个<span class="type">EndPoint</span>都会初始化这么多)</span><br><span class="line">connectionQuotas = <span class="keyword">new</span> <span class="type">ConnectionQuotas</span>(config.maxConnectionsPerIp, config.maxConnectionsPerIpOverrides)</span><br><span class="line"><span class="comment">// numNetworkThreads决定有多少个Processor</span></span><br><span class="line">createAcceptorAndProcessors(config.numNetworkThreads, config.listeners)</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAcceptorAndProcessors</span></span>(processorsPerListener: <span class="type">Int</span>,</span><br><span class="line">                                        endpoints: <span class="type">Seq</span>[<span class="type">EndPoint</span>]): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">val</span> sendBufferSize = config.socketSendBufferBytes</span><br><span class="line">  <span class="keyword">val</span> recvBufferSize = config.socketReceiveBufferBytes</span><br><span class="line">  <span class="keyword">val</span> brokerId = config.brokerId</span><br><span class="line">  endpoints.foreach &#123; endpoint =&gt;</span><br><span class="line">    <span class="keyword">val</span> listenerName = endpoint.listenerName</span><br><span class="line">    <span class="keyword">val</span> securityProtocol = endpoint.securityProtocol</span><br><span class="line">    <span class="keyword">val</span> acceptor = <span class="keyword">new</span> <span class="type">Acceptor</span>(endpoint, sendBufferSize, recvBufferSize, brokerId, connectionQuotas)</span><br><span class="line">    addProcessors(acceptor, endpoint, processorsPerListener)</span><br><span class="line">    <span class="comment">// 守护线程启动Acceptor</span></span><br><span class="line">    <span class="type">KafkaThread</span>.nonDaemon(<span class="string">s&quot;kafka-socket-acceptor-<span class="subst">$listenerName</span>-<span class="subst">$securityProtocol</span>-<span class="subst">$&#123;endpoint.port&#125;</span>&quot;</span>, acceptor).start()</span><br><span class="line">    <span class="comment">// Acceptor等待启动完成</span></span><br><span class="line">    acceptor.awaitStartup()</span><br><span class="line">    acceptors.put(endpoint, acceptor)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// N个Processor</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">addProcessors</span></span>(acceptor: <span class="type">Acceptor</span>, endpoint: <span class="type">EndPoint</span>, newProcessorsPerListener: <span class="type">Int</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">val</span> listenerName = endpoint.listenerName</span><br><span class="line">  <span class="keyword">val</span> securityProtocol = endpoint.securityProtocol</span><br><span class="line">  <span class="keyword">val</span> listenerProcessors = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Processor</span>]()</span><br><span class="line">  <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until newProcessorsPerListener) &#123;</span><br><span class="line">    <span class="keyword">val</span> processor = newProcessor(nextProcessorId, connectionQuotas, listenerName, securityProtocol, memoryPool)</span><br><span class="line">    listenerProcessors += processor</span><br><span class="line">    requestChannel.addProcessor(processor)</span><br><span class="line">    nextProcessorId += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  listenerProcessors.foreach(p =&gt; processors.put(p.id, p))</span><br><span class="line">  acceptor.addProcessors(listenerProcessors)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Acceptor处理"><a href="#Acceptor处理" class="headerlink" title="Acceptor处理"></a>Acceptor处理</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">    <span class="comment">// 注册accept事件</span></span><br><span class="line">    serverChannel.register(nioSelector, <span class="type">SelectionKey</span>.<span class="type">OP_ACCEPT</span>)</span><br><span class="line">    startupComplete()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">var</span> currentProcessor = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> ready = nioSelector.select(<span class="number">500</span>)</span><br><span class="line">          <span class="keyword">if</span> (ready &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">val</span> keys = nioSelector.selectedKeys()</span><br><span class="line">            <span class="keyword">val</span> iter = keys.iterator()</span><br><span class="line">            <span class="keyword">while</span> (iter.hasNext &amp;&amp; isRunning) &#123;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">val</span> key = iter.next</span><br><span class="line">                iter.remove()</span><br><span class="line">                <span class="keyword">if</span> (key.isAcceptable) &#123;</span><br><span class="line">                  <span class="keyword">val</span> processor = synchronized &#123;</span><br><span class="line">                    currentProcessor = currentProcessor % processors.size</span><br><span class="line">                    processors(currentProcessor)</span><br><span class="line">                  &#125;</span><br><span class="line">                  <span class="comment">// 拿到一个socket连接,轮询选一个processor进行处理</span></span><br><span class="line">                  accept(key, processor)</span><br><span class="line">                &#125; <span class="keyword">else</span></span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;Unrecognized key state for acceptor thread.&quot;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">// round robin to the next processor thread, mod(numProcessors) will be done later</span></span><br><span class="line">                <span class="comment">// 轮询算法</span></span><br><span class="line">                currentProcessor = currentProcessor + <span class="number">1</span></span><br><span class="line">              &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">&quot;Error while accepting connection&quot;</span>, e)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="comment">// We catch all the throwables to prevent the acceptor thread from exiting on exceptions due</span></span><br><span class="line">          <span class="comment">// to a select operation on a specific channel or a bad request. We don&#x27;t want</span></span><br><span class="line">          <span class="comment">// the broker to stop responding to requests from other clients in these scenarios.</span></span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">ControlThrowable</span> =&gt; <span class="keyword">throw</span> e</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">&quot;Error occurred&quot;</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      debug(<span class="string">&quot;Closing server socket and selector.&quot;</span>)</span><br><span class="line">      <span class="type">CoreUtils</span>.swallow(serverChannel.close(), <span class="keyword">this</span>, <span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">      <span class="type">CoreUtils</span>.swallow(nioSelector.close(), <span class="keyword">this</span>, <span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">      shutdownComplete()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将新连接交给对应的Processor</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(key: <span class="type">SelectionKey</span>, processor: <span class="type">Processor</span>) &#123;</span><br><span class="line">    <span class="comment">// accept事件发生时,获取注册到Selector上的ServerSocketChannel</span></span><br><span class="line">    <span class="keyword">val</span> serverSocketChannel = key.channel().asInstanceOf[<span class="type">ServerSocketChannel</span>]</span><br><span class="line">    <span class="keyword">val</span> socketChannel = serverSocketChannel.accept()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      connectionQuotas.inc(socketChannel.socket().getInetAddress)</span><br><span class="line">      socketChannel.configureBlocking(<span class="literal">false</span>)</span><br><span class="line">      socketChannel.socket().setTcpNoDelay(<span class="literal">true</span>)</span><br><span class="line">      socketChannel.socket().setKeepAlive(<span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">if</span> (sendBufferSize != <span class="type">Selectable</span>.<span class="type">USE_DEFAULT_BUFFER_SIZE</span>)</span><br><span class="line">        socketChannel.socket().setSendBufferSize(sendBufferSize)</span><br><span class="line"></span><br><span class="line">      debug(<span class="string">&quot;Accepted connection from %s on %s and assigned it to processor %d, sendBufferSize [actual|requested]: [%d|%d] recvBufferSize [actual|requested]: [%d|%d]&quot;</span></span><br><span class="line">            .format(socketChannel.socket.getRemoteSocketAddress, socketChannel.socket.getLocalSocketAddress, processor.id,</span><br><span class="line">                  socketChannel.socket.getSendBufferSize, sendBufferSize,</span><br><span class="line">                  socketChannel.socket.getReceiveBufferSize, recvBufferSize))</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// 轮询选择不同的Processor进行处理</span></span><br><span class="line">      processor.accept(socketChannel)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">TooManyConnectionsException</span> =&gt;</span><br><span class="line">        info(<span class="string">&quot;Rejected connection from %s, address already has the configured maximum of %d connections.&quot;</span>.format(e.ip, e.count))</span><br><span class="line">        close(socketChannel)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Processor处理"><a href="#Processor处理" class="headerlink" title="Processor处理"></a>Processor处理</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">实际是将该<span class="type">SocketChannel</span>添加到该<span class="type">Processor</span>的newConnections队列中</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept</span></span>(socketChannel: <span class="type">SocketChannel</span>) &#123;</span><br><span class="line">  newConnections.add(socketChannel)</span><br><span class="line">  wakeup()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Processo线程做了什么</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">    startupComplete()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// setup any new connections that have been queued up</span></span><br><span class="line">          <span class="comment">// 对新的socket连接进行配置,并注册READ事件</span></span><br><span class="line">          configureNewConnections()</span><br><span class="line">          <span class="comment">// register any new responses for writing</span></span><br><span class="line">          <span class="comment">// 处理response队列中response</span></span><br><span class="line">          processNewResponses()</span><br><span class="line">          poll() <span class="comment">// 监听所有的socketchannel,是否有新的请求发送过来</span></span><br><span class="line">          processCompletedReceives() <span class="comment">// 处理接收到的请求,将其加入到requestqueue中</span></span><br><span class="line">          processCompletedSends() <span class="comment">// 处理已经完成的发送</span></span><br><span class="line">          processDisconnected() <span class="comment">// 处理断开的连接</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; processException(<span class="string">&quot;Processor got uncaught exception.&quot;</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      debug(<span class="string">&quot;Closing selector - processor &quot;</span> + id)</span><br><span class="line">      <span class="type">CoreUtils</span>.swallow(closeAll(), <span class="keyword">this</span>, <span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">      shutdownComplete()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="KafkaRequestHandlerPool"><a href="#KafkaRequestHandlerPool" class="headerlink" title="KafkaRequestHandlerPool"></a>KafkaRequestHandlerPool</h3><h4 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaRequestHandlerPool</span>(<span class="params">val brokerId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              val requestChannel: <span class="type">RequestChannel</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              val apis: <span class="type">KafkaApis</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              time: <span class="type">Time</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                              numThreads: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 建立M个KafkaRequestHandler</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> threadPoolSize: <span class="type">AtomicInteger</span> = <span class="keyword">new</span> <span class="type">AtomicInteger</span>(numThreads)</span><br><span class="line">  <span class="comment">/* a meter to track the average free capacity of the request handlers */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> aggregateIdleMeter = newMeter(<span class="string">&quot;RequestHandlerAvgIdlePercent&quot;</span>, <span class="string">&quot;percent&quot;</span>, <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">this</span>.logIdent = <span class="string">&quot;[Kafka Request Handler on Broker &quot;</span> + brokerId + <span class="string">&quot;], &quot;</span></span><br><span class="line">  <span class="keyword">val</span> runnables = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">KafkaRequestHandler</span>](numThreads)</span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until numThreads) &#123;</span><br><span class="line">    <span class="comment">// requestChannel是Processor存放request请求的地方</span></span><br><span class="line">    <span class="comment">// 也是Handler处理完请求存放response的地方</span></span><br><span class="line">    createHandler(i)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shutdown</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">    info(<span class="string">&quot;shutting down&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> (handler &lt;- runnables)</span><br><span class="line">      handler.initiateShutdown()</span><br><span class="line">    <span class="keyword">for</span> (handler &lt;- runnables)</span><br><span class="line">      handler.awaitShutdown()</span><br><span class="line">    info(<span class="string">&quot;shut down completely&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">    <span class="keyword">while</span> (!stopped) &#123;</span><br><span class="line">      <span class="comment">// We use a single meter for aggregate idle percentage for the thread pool.</span></span><br><span class="line">      <span class="comment">// Since meter is calculated as total_recorded_value / time_window and</span></span><br><span class="line">      <span class="comment">// time_window is independent of the number of threads, each recorded idle</span></span><br><span class="line">      <span class="comment">// time should be discounted by # threads.</span></span><br><span class="line">      <span class="keyword">val</span> startSelectTime = time.nanoseconds</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 从requestQueue中拿取request</span></span><br><span class="line">      <span class="keyword">val</span> req = requestChannel.receiveRequest(<span class="number">300</span>)</span><br><span class="line">      <span class="keyword">val</span> endTime = time.nanoseconds</span><br><span class="line">      <span class="keyword">val</span> idleTime = endTime - startSelectTime</span><br><span class="line">      aggregateIdleMeter.mark(idleTime / totalHandlerThreads.get)</span><br><span class="line"></span><br><span class="line">      req <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">RequestChannel</span>.<span class="type">ShutdownRequest</span> =&gt;</span><br><span class="line">          debug(<span class="string">s&quot;Kafka request handler <span class="subst">$id</span> on broker <span class="subst">$brokerId</span> received shut down command&quot;</span>)</span><br><span class="line">          shutdownComplete.countDown()</span><br><span class="line">          <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> request: <span class="type">RequestChannel</span>.<span class="type">Request</span> =&gt;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            request.requestDequeueTimeNanos = endTime</span><br><span class="line">            trace(<span class="string">s&quot;Kafka request handler <span class="subst">$id</span> on broker <span class="subst">$brokerId</span> handling request <span class="subst">$request</span>&quot;</span>)</span><br><span class="line">            <span class="comment">// 处理请求,并将处理的结果通过sendResponse放入responseQueue中</span></span><br><span class="line">            apis.handle(request)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">FatalExitError</span> =&gt;</span><br><span class="line">              shutdownComplete.countDown()</span><br><span class="line">              <span class="type">Exit</span>.exit(e.statusCode)</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">&quot;Exception when handling request&quot;</span>, e)</span><br><span class="line">          &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            request.releaseBuffer()</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> <span class="literal">null</span> =&gt; <span class="comment">// continue</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    shutdownComplete.countDown()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaApis</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sendResponse</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>,</span><br><span class="line">                         responseOpt: <span class="type">Option</span>[<span class="type">AbstractResponse</span>],</span><br><span class="line">                         onComplete: <span class="type">Option</span>[<span class="type">Send</span> =&gt; <span class="type">Unit</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// Update error metrics for each error code in the response including Errors.NONE</span></span><br><span class="line">  responseOpt.foreach(response =&gt; requestChannel.updateErrorMetrics(request.header.apiKey, response.errorCounts.asScala))</span><br><span class="line">  <span class="keyword">val</span> response = responseOpt <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(response) =&gt;</span><br><span class="line">      <span class="keyword">val</span> responseSend = request.context.buildResponse(response)</span><br><span class="line">      <span class="keyword">val</span> responseString =</span><br><span class="line">        <span class="keyword">if</span> (<span class="type">RequestChannel</span>.isRequestLoggingEnabled) <span class="type">Some</span>(response.toString(request.context.apiVersion))</span><br><span class="line">        <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">SendResponse</span>(request, responseSend, responseString, onComplete)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">RequestChannel</span>.<span class="type">NoOpResponse</span>(request)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 调用对应的Processor的enqueueResponse方法添加response到responseQueue,wakeup唤醒</span></span><br><span class="line">  sendResponse(response)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaApis.handle***Request()</span></span><br><span class="line"><span class="comment">// KafkaApis.sendResponse()-&gt;sendResponse()</span></span><br><span class="line"><span class="comment">// ResultChannel.sendResponse()-&gt;获取对应的Processor</span></span><br><span class="line"><span class="comment">// Processor.enqueueResponse()-&gt;添加Response到队列</span></span><br><span class="line"><span class="comment">// wakeup()唤醒Processor线程</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十五状态机</title>
    <url>/2020/05/09/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E4%BA%94%E7%8A%B6%E6%80%81%E6%9C%BA/</url>
    <content><![CDATA[<blockquote>
<p>介绍KafkaController中的两种状态机</p>
</blockquote>
<span id="more"></span>

<h2 id="ReplicaStateMachine"><a href="#ReplicaStateMachine" class="headerlink" title="ReplicaStateMachine"></a>ReplicaStateMachine</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">ReplicaStateMachine</span>记录集群所有<span class="type">Replica</span>的状态信息</span><br><span class="line">决定着<span class="type">Replica</span>处在什么状态,能够转变为什么状态</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaController.onControllerFailover().replicaStateMachine.startup()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">  info(<span class="string">&quot;Initializing replica state&quot;</span>)</span><br><span class="line">  <span class="comment">// 初始化</span></span><br><span class="line">  initializeReplicaState()</span><br><span class="line">  info(<span class="string">&quot;Triggering online replica state changes&quot;</span>)</span><br><span class="line">  <span class="comment">// 将存活的副本状态转变为OnlineReplica</span></span><br><span class="line">  handleStateChanges(controllerContext.allLiveReplicas().toSeq, <span class="type">OnlineReplica</span>)</span><br><span class="line">  info(<span class="string">s&quot;Started replica state machine with initial state -&gt; <span class="subst">$replicaState</span>&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里只是将Replica状态信息更新副本状态机缓存中replicaState,并没有真正进行状态转移操作</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">initializeReplicaState</span></span>() &#123;</span><br><span class="line">  controllerContext.allPartitions.foreach &#123; partition =&gt;</span><br><span class="line">    <span class="keyword">val</span> replicas = controllerContext.partitionReplicaAssignment(partition)</span><br><span class="line">    replicas.foreach &#123; replicaId =&gt;</span><br><span class="line">      <span class="keyword">val</span> partitionAndReplica = <span class="type">PartitionAndReplica</span>(partition, replicaId)</span><br><span class="line">      <span class="comment">// 如果副本是存活的,将状态置为OnlineReplica</span></span><br><span class="line">      <span class="keyword">if</span> (controllerContext.isReplicaOnline(replicaId, partition))</span><br><span class="line">        replicaState.put(partitionAndReplica, <span class="type">OnlineReplica</span>)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        <span class="comment">// 将不存活的副本状态置为ReplicaDeletionIneligible</span></span><br><span class="line">        replicaState.put(partitionAndReplica, <span class="type">ReplicaDeletionIneligible</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// handleStateChanges()才是真正进行状态转移的地方</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleStateChanges</span></span>(replicas: <span class="type">Seq</span>[<span class="type">PartitionAndReplica</span>], targetState: <span class="type">ReplicaState</span>,</span><br><span class="line">                       callbacks: <span class="type">Callbacks</span> = <span class="keyword">new</span> <span class="type">Callbacks</span>()): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (replicas.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      controllerBrokerRequestBatch.newBatch()</span><br><span class="line">      replicas.groupBy(_.replica).map &#123; <span class="keyword">case</span> (replicaId, replicas) =&gt;</span><br><span class="line">        <span class="keyword">val</span> partitions = replicas.map(_.topicPartition)</span><br><span class="line">        <span class="comment">// 状态转移</span></span><br><span class="line">        doHandleStateChanges(replicaId, partitions, targetState, callbacks)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 向Broker发送相应请求</span></span><br><span class="line">      controllerBrokerRequestBatch.sendRequestsToBrokers(controllerContext.epoch)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; error(<span class="string">s&quot;Error while moving some replicas to <span class="subst">$targetState</span> state&quot;</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="种类"><a href="#种类" class="headerlink" title="种类"></a>种类</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NewReplica: </span><br><span class="line">    此状态下,Controller可以创建这个Replica,该Replica只能为Follower,可以是Replica删除后的一个临时状态</span><br><span class="line">    有效前置状态: </span><br><span class="line">        NonExistentReplica</span><br><span class="line"></span><br><span class="line">OnlineReplica:</span><br><span class="line">    一旦这个Replica被分配到指定的Partition上,并且Replica创建完成,那么它会被置为这个状态,这个状态下,既可以为Leader,也可以为Follower</span><br><span class="line">    有效前置状态:</span><br><span class="line">        NewReplica</span><br><span class="line">        OnlineReplica</span><br><span class="line">        OfflineReplica</span><br><span class="line"></span><br><span class="line">OfflineReplica:</span><br><span class="line">    如果一个Replica挂掉,该Replica转换到这个状态</span><br><span class="line">    有效前置状态:</span><br><span class="line">        NewReplica</span><br><span class="line">        OnlineReplica</span><br><span class="line">        OfflineReplica</span><br><span class="line"></span><br><span class="line">ReplicaDeletionStarted:</span><br><span class="line">    Replica开始删除时被置为的状态</span><br><span class="line">    有效前置状态:</span><br><span class="line">        OfflineReplica</span><br><span class="line"></span><br><span class="line">ReplicaDeletionSuccessful:</span><br><span class="line">    Replica在删除时没有任何问题,将被置为这个状态,代表Replica的数据已经从节点上清除了</span><br><span class="line">    有效前置状态:</span><br><span class="line">        ReplicaDeletionStarted</span><br><span class="line"></span><br><span class="line">ReplicaDeletionIneligible:</span><br><span class="line">    Replica删除失败,转换为这个状态</span><br><span class="line">    有效前置状态:</span><br><span class="line">        ReplicaDeletionStarted</span><br><span class="line"></span><br><span class="line">NonExistentReplica:</span><br><span class="line">    Replica删除成功,转换为这个状态</span><br><span class="line">    有效前置状态:</span><br><span class="line">        ReplicaDeletionSuccessful</span><br></pre></td></tr></table></figure>
<h3 id="副本状态转移"><a href="#副本状态转移" class="headerlink" title="副本状态转移"></a>副本状态转移</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">参考ReplicaStateMachine中各状态的调用情况</span><br><span class="line">doHandleStateChanges()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="PartitionStateMachine"><a href="#PartitionStateMachine" class="headerlink" title="PartitionStateMachine"></a>PartitionStateMachine</h2><h3 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PartitionStateMachine记录着集群所有Partition的状态信息</span><br><span class="line">决定一个Partition处在什么状态以及可以转变为什么状态</span><br><span class="line"></span><br><span class="line">def startup() &#123;</span><br><span class="line">  info(&quot;Initializing partition state&quot;)</span><br><span class="line">  &#x2F;&#x2F; 初始化</span><br><span class="line">  initializePartitionState()</span><br><span class="line">  info(&quot;Triggering online partition state changes&quot;)</span><br><span class="line">  &#x2F;&#x2F; 为所有处理NewPartition,OnlinePartition状态的Partition选举Leader</span><br><span class="line">  triggerOnlinePartitionStateChange()</span><br><span class="line">  info(s&quot;Started partition state machine with initial state -&gt; $partitionState&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 如果该Partition有LeaderAndIsr信息,PartitionLeader所在的机器是alive的,那么将其状态设置OnlinePartition,否则设置为OfflinePartition</span><br><span class="line">&#x2F;&#x2F; 如果该Partition没有LeaderAndIsr信息,状态设置为NewPartition</span><br><span class="line">&#x2F;&#x2F; 同样也是缓存partitionState</span><br><span class="line">private def initializePartitionState() &#123;</span><br><span class="line">  for (topicPartition &lt;- controllerContext.allPartitions) &#123;</span><br><span class="line">    &#x2F;&#x2F; check if leader and isr path exists for partition. If not, then it is in NEW state</span><br><span class="line">    controllerContext.partitionLeadershipInfo.get(topicPartition) match &#123;</span><br><span class="line">      case Some(currentLeaderIsrAndEpoch) &#x3D;&gt;</span><br><span class="line">        &#x2F;&#x2F; else, check if the leader for partition is alive. If yes, it is in Online state, else it is in Offline state</span><br><span class="line">        if (controllerContext.isReplicaOnline(currentLeaderIsrAndEpoch.leaderAndIsr.leader, topicPartition))</span><br><span class="line">        &#x2F;&#x2F; leader is alive</span><br><span class="line">          changeStateTo(topicPartition, NonExistentPartition, OnlinePartition)</span><br><span class="line">        else</span><br><span class="line">          changeStateTo(topicPartition, NonExistentPartition, OfflinePartition)</span><br><span class="line">      case None &#x3D;&gt;</span><br><span class="line">        changeStateTo(topicPartition, NonExistentPartition, NewPartition)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 修改状态,在controller选举后或者broker上下线的时候触发</span><br><span class="line">def triggerOnlinePartitionStateChange(partitionState: Map[TopicPartition, PartitionState]) &#123;</span><br><span class="line">  &#x2F;&#x2F; try to move all partitions in NewPartition or OfflinePartition state to OnlinePartition state except partitions</span><br><span class="line">  &#x2F;&#x2F; that belong to topics to be deleted</span><br><span class="line">  val partitionsToTrigger &#x3D; partitionState.filter &#123; case (partition, partitionState) &#x3D;&gt;</span><br><span class="line">    !topicDeletionManager.isTopicQueuedUpForDeletion(partition.topic) &amp;&amp;</span><br><span class="line">      (partitionState.equals(OfflinePartition) || partitionState.equals(NewPartition))</span><br><span class="line">  &#125;.keys.toSeq</span><br><span class="line">  &#x2F;&#x2F; 更改状态</span><br><span class="line">  handleStateChanges(partitionsToTrigger, OnlinePartition, Option(OfflinePartitionLeaderElectionStrategy))</span><br><span class="line">  &#x2F;&#x2F; TODO: If handleStateChanges catches an exception, it is not enough to bail out and log an error.</span><br><span class="line">  &#x2F;&#x2F; It is important to trigger leader election for those partitions.</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">def handleStateChanges(partitions: Seq[TopicPartition], targetState: PartitionState,</span><br><span class="line">                       partitionLeaderElectionStrategyOpt: Option[PartitionLeaderElectionStrategy] &#x3D; None): Unit &#x3D; &#123;</span><br><span class="line">  if (partitions.nonEmpty) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      controllerBrokerRequestBatch.newBatch()</span><br><span class="line">      &#x2F;&#x2F; 尝试为处在OfflinePartition或NewPartition状态的Partition选主</span><br><span class="line">      &#x2F;&#x2F; 成功后转换为OnlinePartition</span><br><span class="line">      doHandleStateChanges(partitions, targetState, partitionLeaderElectionStrategyOpt)</span><br><span class="line">      &#x2F;&#x2F; 发送请求给所有broker,包括LeaderAndIsr请求和UpdateMetadata请求,添加到队列中</span><br><span class="line">      controllerBrokerRequestBatch.sendRequestsToBrokers(controllerContext.epoch)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Throwable &#x3D;&gt; error(s&quot;Error while moving some partitions to $targetState state&quot;, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="种类-1"><a href="#种类-1" class="headerlink" title="种类"></a>种类</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NonExistentPartition:</span><br><span class="line">    代表这个Partition之前没有被创建过或者之前创建了现在又被删除了</span><br><span class="line">    有效前置状态:</span><br><span class="line">        OfflinePartition</span><br><span class="line"></span><br><span class="line">NewPartition:</span><br><span class="line">    Partition创建之后,处于这个状态,这个状态下Partition还没有Leader和ISR</span><br><span class="line">    有效前置状态:</span><br><span class="line">        NonExistentPartition</span><br><span class="line"></span><br><span class="line">OnlinePartition:</span><br><span class="line">    一旦这个Partition的Leader被选举出来了,将处于这个状态</span><br><span class="line">    有效前置状态:</span><br><span class="line">        NewPartition</span><br><span class="line">        OnlinePartition</span><br><span class="line">        OfflinePartition</span><br><span class="line"></span><br><span class="line">OfflinePartition:</span><br><span class="line">    如果这个Partition的Leader掉线,这个Partition将被转移到这个状态</span><br><span class="line">    有效前置状态:</span><br><span class="line">        NewPartition</span><br><span class="line">        OnlinePartition</span><br><span class="line">        OfflinePartition</span><br></pre></td></tr></table></figure>
<h3 id="分区状态转移"><a href="#分区状态转移" class="headerlink" title="分区状态转移"></a>分区状态转移</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">参考PartitionStateMachine中各状态的调用情况</span><br><span class="line">doHandleStateChanges()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十三ReplicaManager详解</title>
    <url>/2020/05/08/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E4%B8%89ReplicaManager%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>详细介绍ReplicaManager中有什么成员变量,又做了些什么</p>
</blockquote>
<span id="more"></span>

<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">处理的请求</span><br><span class="line">    LeaderAndIsr</span><br><span class="line">    StopReplica</span><br><span class="line">    UpdateMetadata</span><br><span class="line">    Produce</span><br><span class="line">    Fetch</span><br><span class="line">    ListOffset</span><br><span class="line"></span><br><span class="line">LogManager作为ReplicaManager的变量传入了ReplicaManager中</span><br><span class="line">ReplicaManager中的allPartitions负责管理本节点所有的Partition实例</span><br><span class="line">创建Partition实例时,ReplicaManager会作为变量传入Partition中</span><br><span class="line">Partition会为它的每一个副本创建一个Replica实例,但只会为那个在本地副本创建Log对象实例</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启动时做了什么"><a href="#启动时做了什么" class="headerlink" title="启动时做了什么"></a>启动时做了什么</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">KafkaServer</span>在启动时,就初始化了<span class="type">ReplicaManager</span>实例</span><br><span class="line"><span class="type">KafkaServer</span>在初始化<span class="type">LogManager</span>之后,就传给了<span class="type">ReplicaManager</span></span><br><span class="line"></span><br><span class="line">replicaManager = createReplicaManager(isShuttingDown)</span><br><span class="line">replicaManager.startup()</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">createReplicaManager</span></span>(isShuttingDown: <span class="type">AtomicBoolean</span>): <span class="type">ReplicaManager</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ReplicaManager</span>(config, metrics, time, zkClient, kafkaScheduler, logManager, isShuttingDown, quotaManagers,</span><br><span class="line">      brokerTopicStats, metadataCache, logDirFailureChannel)</span><br><span class="line"></span><br><span class="line">周期性任务:</span><br><span class="line">    maybeShrinkIsr: 判断tp的isr是否有replica因为延迟或hang主需要从isr中移除</span><br><span class="line">    maybePropagateIsrChanges: 判断是不是需要对isr进行更新</span><br><span class="line">    shutdownIdleReplicaAlterLogDirsThread: 定时关闭空闲的<span class="type">ReplicaAlterDirThread</span>线程</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() &#123;</span><br><span class="line">    <span class="comment">// start ISR expiration thread</span></span><br><span class="line">    <span class="comment">// A follower can lag behind leader for up to config.replicaLagTimeMaxMs x 1.5 before it is removed from ISR</span></span><br><span class="line">    scheduler.schedule(<span class="string">&quot;isr-expiration&quot;</span>, maybeShrinkIsr _, period = config.replicaLagTimeMaxMs / <span class="number">2</span>, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    scheduler.schedule(<span class="string">&quot;isr-change-propagation&quot;</span>, maybePropagateIsrChanges _, period = <span class="number">2500</span>L, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    scheduler.schedule(<span class="string">&quot;shutdown-idle-replica-alter-log-dirs-thread&quot;</span>, shutdownIdleReplicaAlterLogDirsThread _, period = <span class="number">10000</span>L, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If inter-broker protocol (IBP) &lt; 1.0, the controller will send LeaderAndIsrRequest V0 which does not include isNew field.</span></span><br><span class="line">    <span class="comment">// In this case, the broker receiving the request cannot determine whether it is safe to create a partition if a log directory has failed.</span></span><br><span class="line">    <span class="comment">// Thus, we choose to halt the broker on any log diretory failure if IBP &lt; 1.0</span></span><br><span class="line">    <span class="keyword">val</span> haltBrokerOnFailure = config.interBrokerProtocolVersion &lt; <span class="type">KAFKA_1_0_IV0</span></span><br><span class="line">    logDirFailureHandler = <span class="keyword">new</span> <span class="type">LogDirFailureHandler</span>(<span class="string">&quot;LogDirFailureHandler&quot;</span>, haltBrokerOnFailure)</span><br><span class="line">    logDirFailureHandler.start()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ISR变化"><a href="#ISR变化" class="headerlink" title="ISR变化"></a>ISR变化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">maybeShrinkIsr()是检查isr是否有replica需要被移除</span><br><span class="line">增加操作为maybeExpandIsr</span><br><span class="line"></span><br><span class="line">ReplicaManager在FetchMessages()方法对来自副本的Fetch请求进行处理</span><br><span class="line">实际上会更新相应replica的LEO信息</span><br><span class="line">这时候leader会根据副本LEO信息的变动来判断这个副本是否满足加入isr的条件</span><br></pre></td></tr></table></figure>
<h3 id="updateFollowerLogReadResults"><a href="#updateFollowerLogReadResults" class="headerlink" title="updateFollowerLogReadResults"></a>updateFollowerLogReadResults</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">更新远程副本的信息</span><br><span class="line">找到本节点的<span class="type">Partition</span>对象,然后调用其updateReplicaLogReadResult()</span><br><span class="line">更新副本的<span class="type">LEO</span>信息和拉取时间信息</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>,</span><br><span class="line">                                           readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)] = &#123;</span><br><span class="line">    debug(<span class="string">s&quot;Recording follower broker <span class="subst">$replicaId</span> log end offsets: <span class="subst">$readResults</span>&quot;</span>)</span><br><span class="line">    readResults.map &#123; <span class="keyword">case</span> (topicPartition, readResult) =&gt;</span><br><span class="line">      <span class="keyword">var</span> updatedReadResult = readResult</span><br><span class="line">      nonOfflinePartition(topicPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</span><br><span class="line">          partition.getReplica(replicaId) <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(replica) =&gt;</span><br><span class="line">              <span class="comment">// 更新副本的相关信息</span></span><br><span class="line">              partition.updateReplicaLogReadResult(replica, readResult)</span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">              warn(<span class="string">s&quot;Leader <span class="subst">$localBrokerId</span> failed to record follower <span class="subst">$replicaId</span>&#x27;s position &quot;</span> +</span><br><span class="line">                <span class="string">s&quot;<span class="subst">$&#123;readResult.info.fetchOffsetMetadata.messageOffset&#125;</span> since the replica is not recognized to be &quot;</span> +</span><br><span class="line">                <span class="string">s&quot;one of the assigned replicas <span class="subst">$&#123;partition.assignedReplicas.map(_.brokerId).mkString(&quot;,&quot;)&#125;</span> &quot;</span> +</span><br><span class="line">                <span class="string">s&quot;for partition <span class="subst">$topicPartition</span>. Empty records will be returned for this partition.&quot;</span>)</span><br><span class="line">              updatedReadResult = readResult.withEmptyFetchInfo</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          warn(<span class="string">s&quot;While recording the replica LEO, the partition <span class="subst">$topicPartition</span> hasn&#x27;t been created.&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      topicPartition -&gt; updatedReadResult</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="updateReplicaLogReadResult"><a href="#updateReplicaLogReadResult" class="headerlink" title="updateReplicaLogReadResult"></a>updateReplicaLogReadResult</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>updateLogReadResult(): 更新副本的相关信息,这里是更新该副本的<span class="type">LEO</span>,<span class="type">LastFetchLeaderLogEndOffset</span>和<span class="type">LastFetchTimeMs</span></span><br><span class="line"><span class="number">2.</span>maybeExpandIsr(): 判断isr是否需要扩充,即是否有不在isr内的副本满足进入isr的条件</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateReplicaLogReadResult</span></span>(replica: <span class="type">Replica</span>, logReadResult: <span class="type">LogReadResult</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> replicaId = replica.brokerId</span><br><span class="line">    <span class="comment">// No need to calculate low watermark if there is no delayed DeleteRecordsRequest</span></span><br><span class="line">    <span class="keyword">val</span> oldLeaderLW = <span class="keyword">if</span> (replicaManager.delayedDeleteRecordsPurgatory.delayed &gt; <span class="number">0</span>) lowWatermarkIfLeader <span class="keyword">else</span> <span class="number">-1</span>L</span><br><span class="line">    <span class="comment">// 更新副本的信息</span></span><br><span class="line">    replica.updateLogReadResult(logReadResult)</span><br><span class="line">    <span class="keyword">val</span> newLeaderLW = <span class="keyword">if</span> (replicaManager.delayedDeleteRecordsPurgatory.delayed &gt; <span class="number">0</span>) lowWatermarkIfLeader <span class="keyword">else</span> <span class="number">-1</span>L</span><br><span class="line">    <span class="comment">// check if the LW of the partition has incremented</span></span><br><span class="line">    <span class="comment">// since the replica&#x27;s logStartOffset may have incremented</span></span><br><span class="line">    <span class="keyword">val</span> leaderLWIncremented = newLeaderLW &gt; oldLeaderLW</span><br><span class="line">    <span class="comment">// check if we need to expand ISR to include this replica</span></span><br><span class="line">    <span class="comment">// if it is not in the ISR yet</span></span><br><span class="line">    <span class="comment">// 如果副本不在ISR中,检查是否需要进行扩充</span></span><br><span class="line">    <span class="keyword">val</span> leaderHWIncremented = maybeExpandIsr(replicaId, logReadResult)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = leaderLWIncremented || leaderHWIncremented</span><br><span class="line">    <span class="comment">// some delayed operations may be unblocked after HW or LW changed</span></span><br><span class="line">    <span class="keyword">if</span> (result)</span><br><span class="line">      tryCompleteDelayedRequests()</span><br><span class="line"></span><br><span class="line">    debug(<span class="string">s&quot;Recorded replica <span class="subst">$replicaId</span> log end offset (LEO) position <span class="subst">$&#123;logReadResult.info.fetchOffsetMetadata.messageOffset&#125;</span>.&quot;</span>)</span><br><span class="line">    result</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="maybeExpandIsr"><a href="#maybeExpandIsr" class="headerlink" title="maybeExpandIsr"></a>maybeExpandIsr</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">根据replicaId的<span class="type">LEO</span>来判断是否满足进入<span class="type">ISR</span>的条件</span><br><span class="line">如果满足,添加到<span class="type">ISR</span>中</span><br><span class="line">之后调用updateIsr更新这个tp的isr信息和更新<span class="type">HW</span>信息</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeExpandIsr</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    inWriteLock(leaderIsrUpdateLock) &#123;</span><br><span class="line">      <span class="comment">// check if this replica needs to be added to the ISR</span></span><br><span class="line">      leaderReplicaIfLocal <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</span><br><span class="line">          <span class="keyword">val</span> replica = getReplica(replicaId).get</span><br><span class="line">          <span class="keyword">val</span> leaderHW = leaderReplica.highWatermark</span><br><span class="line">          <span class="keyword">val</span> fetchOffset = logReadResult.info.fetchOffsetMetadata.messageOffset</span><br><span class="line">          <span class="comment">// replica LEO大于HW情况下,加入ISR列表</span></span><br><span class="line">          <span class="keyword">if</span> (!inSyncReplicas.contains(replica) &amp;&amp;</span><br><span class="line">             assignedReplicas.map(_.brokerId).contains(replicaId) &amp;&amp;</span><br><span class="line">             replica.logEndOffset.offsetDiff(leaderHW) &gt;= <span class="number">0</span> &amp;&amp;</span><br><span class="line">             leaderEpochStartOffsetOpt.exists(fetchOffset &gt;= _)) &#123;</span><br><span class="line">            <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas + replica</span><br><span class="line">            info(<span class="string">s&quot;Expanding ISR from <span class="subst">$&#123;inSyncReplicas.map(_.brokerId).mkString(&quot;,&quot;)&#125;</span> &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;to <span class="subst">$&#123;newInSyncReplicas.map(_.brokerId).mkString(&quot;,&quot;)&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment">// update ISR in ZK and cache</span></span><br><span class="line">            <span class="comment">// 更新到ZK</span></span><br><span class="line">            updateIsr(newInSyncReplicas)</span><br><span class="line">            replicaManager.isrExpandRate.mark()</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// check if the HW of the partition can now be incremented</span></span><br><span class="line">          <span class="comment">// since the replica may already be in the ISR and its LEO has just incremented</span></span><br><span class="line">          <span class="comment">// 检查HW是否需要更新</span></span><br><span class="line">          maybeIncrementLeaderHW(leaderReplica, logReadResult.fetchTimeMs)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span> <span class="comment">// nothing to do if no longer leader</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="UpdateMetadata请求"><a href="#UpdateMetadata请求" class="headerlink" title="UpdateMetadata请求"></a>UpdateMetadata请求</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleUpdateMetadataRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> correlationId = request.header.correlationId</span><br><span class="line">    <span class="keyword">val</span> updateMetadataRequest = request.body[<span class="type">UpdateMetadataRequest</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isAuthorizedClusterAction(request)) &#123;</span><br><span class="line">      <span class="comment">// 更新metadata,返回需要删除的partition</span></span><br><span class="line">      <span class="keyword">val</span> deletedPartitions = replicaManager.maybeUpdateMetadataCache(correlationId, updateMetadataRequest)</span><br><span class="line">      <span class="keyword">if</span> (deletedPartitions.nonEmpty)</span><br><span class="line">        <span class="comment">// GroupCoordinator会清除相关partition的信息</span></span><br><span class="line">        groupCoordinator.handleDeletedPartitions(deletedPartitions)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (adminManager.hasDelayedTopicOperations) &#123;</span><br><span class="line">        updateMetadataRequest.partitionStates.keySet.asScala.map(_.topic).foreach &#123; topic =&gt;</span><br><span class="line">          adminManager.tryCompleteDelayedTopicOperations(topic)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      quotas.clientQuotaCallback.foreach &#123; callback =&gt;</span><br><span class="line">        <span class="keyword">if</span> (callback.updateClusterMetadata(metadataCache.getClusterMetadata(clusterId, request.context.listenerName))) &#123;</span><br><span class="line">          quotas.fetch.updateQuotaMetricConfigs()</span><br><span class="line">          quotas.produce.updateQuotaMetricConfigs()</span><br><span class="line">          quotas.request.updateQuotaMetricConfigs()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      sendResponseExemptThrottle(request, <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">NONE</span>))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      sendResponseMaybeThrottle(request, _ =&gt; <span class="keyword">new</span> <span class="type">UpdateMetadataResponse</span>(<span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="maybeUpdateMetadataCache"><a href="#maybeUpdateMetadataCache" class="headerlink" title="maybeUpdateMetadataCache"></a>maybeUpdateMetadataCache</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeUpdateMetadataCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>) : <span class="type">Seq</span>[<span class="type">TopicPartition</span>] =  &#123;</span><br><span class="line">    replicaStateChangeLock synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span>(updateMetadataRequest.controllerEpoch &lt; controllerEpoch) &#123;</span><br><span class="line">        <span class="keyword">val</span> stateControllerEpochErrorMessage = <span class="string">s&quot;Received update metadata request with correlation id <span class="subst">$correlationId</span> &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;from an old controller <span class="subst">$&#123;updateMetadataRequest.controllerId&#125;</span> with epoch <span class="subst">$&#123;updateMetadataRequest.controllerEpoch&#125;</span>. &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;Latest known controller epoch is <span class="subst">$controllerEpoch</span>&quot;</span></span><br><span class="line">        stateChangeLogger.warn(stateControllerEpochErrorMessage)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(stateChangeLogger.messageWithPrefix(stateControllerEpochErrorMessage))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 更新metadata信息,并返回要删除的partition信息</span></span><br><span class="line">        <span class="keyword">val</span> deletedPartitions = metadataCache.updateMetadata(correlationId, updateMetadataRequest)</span><br><span class="line">        controllerEpoch = updateMetadataRequest.controllerEpoch</span><br><span class="line">        deletedPartitions</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="updateMetadata"><a href="#updateMetadata" class="headerlink" title="updateMetadata"></a>updateMetadata</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>本节点的aliveNodes和aliveBrokers记录,更新为最新的记录</span><br><span class="line"><span class="number">2.</span>对于要删除的tp,从缓存中删除,并记录下来作为这个方法的返回</span><br><span class="line"><span class="number">3.</span>对于其他的tp,addOrUpdatePartitionInfo</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateMetadata</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>): <span class="type">Seq</span>[<span class="type">TopicPartition</span>] = &#123;</span><br><span class="line">    inWriteLock(partitionMetadataLock) &#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//since kafka may do partial metadata updates, we start by copying the previous state</span></span><br><span class="line">      <span class="keyword">val</span> partitionStates = <span class="keyword">new</span> mutable.<span class="type">AnyRefMap</span>[<span class="type">String</span>, mutable.<span class="type">LongMap</span>[<span class="type">UpdateMetadataRequest</span>.<span class="type">PartitionState</span>]](metadataSnapshot.partitionStates.size)</span><br><span class="line">      metadataSnapshot.partitionStates.foreach &#123; <span class="keyword">case</span> (topic, oldPartitionStates) =&gt;</span><br><span class="line">        <span class="keyword">val</span> copy = <span class="keyword">new</span> mutable.<span class="type">LongMap</span>[<span class="type">UpdateMetadataRequest</span>.<span class="type">PartitionState</span>](oldPartitionStates.size)</span><br><span class="line">        copy ++= oldPartitionStates</span><br><span class="line">        partitionStates += (topic -&gt; copy)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 更新</span></span><br><span class="line">      <span class="keyword">val</span> aliveBrokers = <span class="keyword">new</span> mutable.<span class="type">LongMap</span>[<span class="type">Broker</span>](metadataSnapshot.aliveBrokers.size)</span><br><span class="line">      <span class="keyword">val</span> aliveNodes = <span class="keyword">new</span> mutable.<span class="type">LongMap</span>[collection.<span class="type">Map</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]](metadataSnapshot.aliveNodes.size)</span><br><span class="line">      <span class="keyword">val</span> controllerId = updateMetadataRequest.controllerId <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> id <span class="keyword">if</span> id &lt; <span class="number">0</span> =&gt; <span class="type">None</span></span><br><span class="line">          <span class="keyword">case</span> id =&gt; <span class="type">Some</span>(id)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      updateMetadataRequest.liveBrokers.asScala.foreach &#123; broker =&gt;</span><br><span class="line">        <span class="comment">// `aliveNodes` is a hot path for metadata requests for large clusters, so we use java.util.HashMap which</span></span><br><span class="line">        <span class="comment">// is a bit faster than scala.collection.mutable.HashMap. When we drop support for Scala 2.10, we could</span></span><br><span class="line">        <span class="comment">// move to `AnyRefMap`, which has comparable performance.</span></span><br><span class="line">        <span class="keyword">val</span> nodes = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]</span><br><span class="line">        <span class="keyword">val</span> endPoints = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">EndPoint</span>]</span><br><span class="line">        broker.endPoints.asScala.foreach &#123; ep =&gt;</span><br><span class="line">          endPoints += <span class="type">EndPoint</span>(ep.host, ep.port, ep.listenerName, ep.securityProtocol)</span><br><span class="line">          nodes.put(ep.listenerName, <span class="keyword">new</span> <span class="type">Node</span>(broker.id, ep.host, ep.port))</span><br><span class="line">        &#125;</span><br><span class="line">        aliveBrokers(broker.id) = <span class="type">Broker</span>(broker.id, endPoints, <span class="type">Option</span>(broker.rack))</span><br><span class="line">        aliveNodes(broker.id) = nodes.asScala</span><br><span class="line">      &#125;</span><br><span class="line">      aliveNodes.get(brokerId).foreach &#123; listenerMap =&gt;</span><br><span class="line">        <span class="keyword">val</span> listeners = listenerMap.keySet</span><br><span class="line">        <span class="keyword">if</span> (!aliveNodes.values.forall(_.keySet == listeners))</span><br><span class="line">          error(<span class="string">s&quot;Listeners are not identical across brokers: <span class="subst">$aliveNodes</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> deletedPartitions = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">TopicPartition</span>]</span><br><span class="line">      updateMetadataRequest.partitionStates.asScala.foreach &#123; <span class="keyword">case</span> (tp, info) =&gt;</span><br><span class="line">        <span class="keyword">val</span> controllerId = updateMetadataRequest.controllerId</span><br><span class="line">        <span class="keyword">val</span> controllerEpoch = updateMetadataRequest.controllerEpoch</span><br><span class="line">        <span class="keyword">if</span> (info.basePartitionState.leader == <span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>) &#123;</span><br><span class="line">          <span class="comment">// 删除</span></span><br><span class="line">          removePartitionInfo(partitionStates, tp.topic, tp.partition)</span><br><span class="line">          stateChangeLogger.trace(<span class="string">s&quot;Deleted partition <span class="subst">$tp</span> from metadata cache in response to UpdateMetadata &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>&quot;</span>)</span><br><span class="line">          deletedPartitions += tp</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 更新</span></span><br><span class="line">          addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, info)</span><br><span class="line">          stateChangeLogger.trace(<span class="string">s&quot;Cached leader info <span class="subst">$info</span> for partition <span class="subst">$tp</span> in response to &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;UpdateMetadata request sent by controller <span class="subst">$controllerId</span> epoch <span class="subst">$controllerEpoch</span> with correlation id <span class="subst">$correlationId</span>&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      metadataSnapshot = <span class="type">MetadataSnapshot</span>(partitionStates, controllerId, aliveBrokers, aliveNodes)</span><br><span class="line">      deletedPartitions</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十六各Handler详解</title>
    <url>/2020/05/09/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E5%85%AD%E5%90%84Handler%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<blockquote>
<p>主要介绍Controller在启动时注册的这么多监听,各自的用处</p>
</blockquote>
<span id="more"></span>

<h2 id="监听概览"><a href="#监听概览" class="headerlink" title="监听概览"></a>监听概览</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// KafkaController.Startup</span></span><br><span class="line">zkClient.registerZNodeChangeHandlerAndCheckExistence(controllerChangeHandler)</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaController.onControllerFailover()</span></span><br><span class="line"><span class="keyword">val</span> childChangeHandlers = <span class="type">Seq</span>(brokerChangeHandler, topicChangeHandler, topicDeletionHandler, logDirEventNotificationHandler, isrChangeNotificationHandler)</span><br><span class="line">childChangeHandlers.foreach(zkClient.registerZNodeChildChangeHandler)</span><br><span class="line"><span class="keyword">val</span> nodeChangeHandlers = <span class="type">Seq</span>(preferredReplicaElectionHandler, partitionReassignmentHandler)</span><br><span class="line">nodeChangeHandlers.foreach(zkClient.registerZNodeChangeHandlerAndCheckExistence)</span><br><span class="line"></span><br><span class="line">controllerChangeHandler</span><br><span class="line">brokerChangeHandler</span><br><span class="line">topicChangeHandler</span><br><span class="line">topicDeletionHandler</span><br><span class="line">logDirEventNotificationHandler</span><br><span class="line">isrChangeNotificationHandler</span><br><span class="line">preferredReplicaElectionHandler</span><br><span class="line">partitionReassignmentHandler</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ZNodeChangeHandler"><a href="#ZNodeChangeHandler" class="headerlink" title="ZNodeChangeHandler"></a>ZNodeChangeHandler</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ZK节点变化监听</span><br></pre></td></tr></table></figure>
<h3 id="ControllerChangeHandler"><a href="#ControllerChangeHandler" class="headerlink" title="ControllerChangeHandler"></a>ControllerChangeHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">&quot;/controller&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 主要监听controller节点的变化</span></span><br><span class="line"><span class="comment">// 对应的操作有ControllerChange,Reelect</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControllerChangeHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">ControllerZNode</span>.path</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleCreation</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">ControllerChange</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleDeletion</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">Reelect</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleDataChange</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">ControllerChange</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="PreferredReplicaElectionHandler"><a href="#PreferredReplicaElectionHandler" class="headerlink" title="PreferredReplicaElectionHandler"></a>PreferredReplicaElectionHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">s&quot;<span class="subst">$&#123;AdminZNode.path&#125;</span>/preferred_replica_election&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 监听Partition最优Leader选举</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PreferredReplicaElectionHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">PreferredReplicaElectionZNode</span>.path</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleCreation</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">PreferredReplicaLeaderElection</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="PartitionReassignmentHandler"><a href="#PartitionReassignmentHandler" class="headerlink" title="PartitionReassignmentHandler"></a>PartitionReassignmentHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">s&quot;<span class="subst">$&#123;AdminZNode.path&#125;</span>/reassign_partitions&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 监听分区副本迁移</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartitionReassignmentHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">ReassignPartitionsZNode</span>.path</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Note that the event is also enqueued when the znode is deleted, but we do it explicitly instead of relying on</span></span><br><span class="line">  <span class="comment">// handleDeletion(). This approach is more robust as it doesn&#x27;t depend on the watcher being re-registered after</span></span><br><span class="line">  <span class="comment">// it&#x27;s consumed during data changes (we ensure re-registration when the znode is deleted).</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleCreation</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">PartitionReassignment</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ZNodeChildChangeHandler"><a href="#ZNodeChildChangeHandler" class="headerlink" title="ZNodeChildChangeHandler"></a>ZNodeChildChangeHandler</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">监听ZK子节点信息变化</span><br></pre></td></tr></table></figure>
<h3 id="BrokerChangeHandler"><a href="#BrokerChangeHandler" class="headerlink" title="BrokerChangeHandler"></a>BrokerChangeHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">Broker</span>上下线变化</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">s&quot;<span class="subst">$&#123;BrokersZNode.path&#125;</span>/ids&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 监听Broker变化,对应操作BrokerChange</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrokerChangeHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChildChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">BrokerIdsZNode</span>.path</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleChildChange</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    eventManager.put(controller.<span class="type">BrokerChange</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="TopicChangeHandler"><a href="#TopicChangeHandler" class="headerlink" title="TopicChangeHandler"></a>TopicChangeHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">s&quot;<span class="subst">$&#123;BrokersZNode.path&#125;</span>/topics&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 监听topic变化,对应操作TopicChange</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicChangeHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChildChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">TopicsZNode</span>.path</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleChildChange</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">TopicChange</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="TopicDeletionHandler"><a href="#TopicDeletionHandler" class="headerlink" title="TopicDeletionHandler"></a>TopicDeletionHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">s&quot;<span class="subst">$&#123;AdminZNode.path&#125;</span>/delete_topics&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 监听删除topic的变化,TopicDeletion</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicDeletionHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChildChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">DeleteTopicsZNode</span>.path</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleChildChange</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">TopicDeletion</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="LogDirEventNotificationHandler"><a href="#LogDirEventNotificationHandler" class="headerlink" title="LogDirEventNotificationHandler"></a>LogDirEventNotificationHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">&quot;/log_dir_event_notification&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 监听日志目录事件通知,LogDirEventNotification</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogDirEventNotificationHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChildChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">LogDirEventNotificationZNode</span>.path</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleChildChange</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">LogDirEventNotification</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="IsrChangeNotificationHandler"><a href="#IsrChangeNotificationHandler" class="headerlink" title="IsrChangeNotificationHandler"></a>IsrChangeNotificationHandler</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path</span> </span>= <span class="string">&quot;/isr_change_notification&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 监听Partition ISR变化,IsrChangeNotification</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IsrChangeNotificationHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChildChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">IsrChangeNotificationZNode</span>.path</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleChildChange</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">IsrChangeNotification</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之四单Partition顺序性保证</title>
    <url>/2020/05/07/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%9B%9B%E5%8D%95Partition%E9%A1%BA%E5%BA%8F%E6%80%A7%E4%BF%9D%E8%AF%81/</url>
    <content><![CDATA[<blockquote>
<p>主要介绍Kafka怎么做到单Partition保证顺序性的原理,在业务上其实有时候也会有响应的需要,例如行为日志的一个顺序保证</p>
</blockquote>
<span id="more"></span>

<h2 id="RecordAccumulator"><a href="#RecordAccumulator" class="headerlink" title="RecordAccumulator"></a>RecordAccumulator</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">每个topic-partion都有对应的deque</span><br><span class="line">deque存储的是ProducerBatch,是发送的基本单位</span><br><span class="line">只有当这个topic-partition的ProducerBatch达到大小或时间要求才会触发发送操作(不一定非要满足这两个条件才能发送)</span><br><span class="line"></span><br><span class="line">append(): 向Accumulator添加一条Record,并返回添加后的结果,用于各种条件的判断,分配新的ProducerBatch</span><br><span class="line">tryAppend(): 添加Record的实际方法</span><br><span class="line"></span><br><span class="line">mutePartition(): 对partition mute,保证只有一个Batch正在发送,保证顺序性</span><br><span class="line">unmutePartition(): 发送完成unmute,这样才能进行下一次发送</span><br><span class="line"></span><br><span class="line">ready(): 获得可发送的node列表</span><br><span class="line">drain(): 遍历可发送node列表,然后在leader在当前的所有tp,直到发送的batch达到max.request.size,就将这些batch作为一个request发送出去</span><br><span class="line"></span><br><span class="line">deallocate(): 释放ProducerBatch占用的内存</span><br><span class="line">reenqueue(): 将发送失败并且可以再次发送batch重新添加到deque中,添加在deque的头部(避免乱序)</span><br><span class="line">abortBatches(): 由方法abortIncompleteBatches调用,在Sender强制退出时,移除未完成的batch</span><br><span class="line">awitFlushCompletion(): 由Producer的flush()方法调用,block直到所有未完成的batch发送完成</span><br><span class="line">abortExpireBatches(): 移除那些由于分区不可用而无法发送的batch</span><br><span class="line">abortIncompleteBatches()</span><br><span class="line"></span><br><span class="line">RecordAppendResult: batch的meta信息,在append()方法返回中调用</span><br><span class="line">IncompleteBatches: 保存发送未完成的batch,线程安全类</span><br><span class="line">ReadyCheckResult: ready()方法返回的对象类型,记录可以发送的node列表</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="如何保证顺序性"><a href="#如何保证顺序性" class="headerlink" title="如何保证顺序性"></a>如何保证顺序性</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">可以了解KafkaProducer是怎么初始化一个Sender对象的</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaProducer</span></span><br><span class="line"><span class="keyword">int</span> maxInflightRequests = configureInflightRequests(producerConfig, transactionManager != <span class="keyword">null</span>);</span><br><span class="line">config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Sender(logContext,</span><br><span class="line">    client,</span><br><span class="line">    metadata,</span><br><span class="line">    <span class="keyword">this</span>.accumulator,</span><br><span class="line">    maxInflightRequests == <span class="number">1</span>, <span class="comment">// 设置为1时保证顺序性 </span></span><br><span class="line">    producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),</span><br><span class="line">    acks,</span><br><span class="line">    retries,</span><br><span class="line">    metricsRegistry.senderMetrics,</span><br><span class="line">    time,</span><br><span class="line">    requestTimeoutMs,</span><br><span class="line">    producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),</span><br><span class="line">    <span class="keyword">this</span>.transactionManager,</span><br><span class="line">    apiVersions);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Sender</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Sender</span><span class="params">(LogContext logContext,</span></span></span><br><span class="line"><span class="function"><span class="params">              KafkaClient client,</span></span></span><br><span class="line"><span class="function"><span class="params">              Metadata metadata,</span></span></span><br><span class="line"><span class="function"><span class="params">              RecordAccumulator accumulator,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">boolean</span> guaranteeMessageOrder,// 为<span class="keyword">true</span>保证顺序性</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">int</span> maxRequestSize,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">short</span> acks,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">int</span> retries,</span></span></span><br><span class="line"><span class="function"><span class="params">              SenderMetricsRegistry metricsRegistry,</span></span></span><br><span class="line"><span class="function"><span class="params">              Time time,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">int</span> requestTimeoutMs,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">long</span> retryBackoffMs,</span></span></span><br><span class="line"><span class="function"><span class="params">              TransactionManager transactionManager,</span></span></span><br><span class="line"><span class="function"><span class="params">              ApiVersions apiVersions)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.log = logContext.logger(Sender.class);</span><br><span class="line">    <span class="keyword">this</span>.client = client;</span><br><span class="line">    <span class="keyword">this</span>.accumulator = accumulator;</span><br><span class="line">    <span class="keyword">this</span>.metadata = metadata;</span><br><span class="line">    <span class="keyword">this</span>.guaranteeMessageOrder = guaranteeMessageOrder;</span><br><span class="line">    <span class="keyword">this</span>.maxRequestSize = maxRequestSize;</span><br><span class="line">    <span class="keyword">this</span>.running = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">this</span>.acks = acks;</span><br><span class="line">    <span class="keyword">this</span>.retries = retries;</span><br><span class="line">    <span class="keyword">this</span>.time = time;</span><br><span class="line">    <span class="keyword">this</span>.sensors = <span class="keyword">new</span> SenderMetrics(metricsRegistry, metadata, client, time);</span><br><span class="line">    <span class="keyword">this</span>.requestTimeoutMs = requestTimeoutMs;</span><br><span class="line">    <span class="keyword">this</span>.retryBackoffMs = retryBackoffMs;</span><br><span class="line">    <span class="keyword">this</span>.apiVersions = apiVersions;</span><br><span class="line">    <span class="keyword">this</span>.transactionManager = transactionManager;</span><br><span class="line">    <span class="keyword">this</span>.inFlightBatches = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ProducerConfig"><a href="#ProducerConfig" class="headerlink" title="ProducerConfig"></a>ProducerConfig</h2><p><a href="http://kafka.apache.org/0102/documentation.html#producerconfigs">传送门</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">max.in.flight.requests.per.connection: 对一个connection,同时发送最大请求数,不为1时不能保证顺序性,默认值5</span><br><span class="line">详细配置信息,可以参考Kafka的官方文档</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka的高吞吐量</title>
    <url>/2019/12/20/Kafka%E7%9A%84%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F/</url>
    <content><![CDATA[<blockquote>
<p>Kafka为什么速度快,吞吐量大</p>
</blockquote>
<span id="more"></span>

<h2 id="顺序读写"><a href="#顺序读写" class="headerlink" title="顺序读写"></a>顺序读写</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.Kafka将消息记录持久化到本地磁盘,并且顺序读写</span><br><span class="line">b.磁盘&#x2F;内存的快慢取决于寻址的方式,磁盘随机读写慢,但是磁盘顺序读写性能高于内存随机读写</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Page-Cache-页高缓"><a href="#Page-Cache-页高缓" class="headerlink" title="Page Cache(页高缓)"></a>Page Cache(页高缓)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.使用了操作系统本身的Page Cache,而不是JVM空间内存,避免了Object消耗以及GC问题</span><br><span class="line">b.Kafka的读写操作基本上是基于内存的,读写速度得到了极大的提升</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Zero-Copy-零拷贝"><a href="#Zero-Copy-零拷贝" class="headerlink" title="Zero Copy(零拷贝)"></a>Zero Copy(零拷贝)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.同样是操作系统本身机制</span><br><span class="line">b.允许操作系统使用sendFile方法直接将数据Page Cache发送到网络(常规socket网络需要使用用户空间缓存区)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="分区分段"><a href="#分区分段" class="headerlink" title="分区分段"></a>分区分段</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.Kafka的信息记录按topic分类存储</span><br><span class="line">b.topic中的数据按partition存储在不同的broker节点</span><br><span class="line">c.每个partition对应操作系统的一个文件夹</span><br><span class="line">d.partition按segment分段存储</span><br><span class="line">e.每次文件操作也是直接操作segment</span><br><span class="line">f.segment又有索引文件.index</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka集成Prometheus与Grafana</title>
    <url>/2020/01/08/Kafka%E9%9B%86%E6%88%90Prometheus%E4%B8%8EGrafana/</url>
    <content><![CDATA[<blockquote>
<p>监控Kafka</p>
</blockquote>
<span id="more"></span>

<h2 id="依赖情况"><a href="#依赖情况" class="headerlink" title="依赖情况"></a>依赖情况</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 只需部署一台服务器上</span><br><span class="line">grafana</span><br><span class="line">prometheus</span><br><span class="line"># 部署在各kafka服务器上</span><br><span class="line">kafka_exporter</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="prometheus-yml"><a href="#prometheus-yml" class="headerlink" title="prometheus.yml"></a>prometheus.yml</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#39;kafka&#39;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;hadoop01:9308&#39;,&#39;hadoop02:9308&#39;,&#39;hadoop03:9308&#39;]</span><br><span class="line">        labels:</span><br><span class="line">          instance: &#39;kafka&#39;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kafka_exporter(多台)</span><br><span class="line">.&#x2F;kafka_exporter --kafka.server&#x3D;KafkaIP或者域名:9092 &amp;</span><br><span class="line"># prometheus</span><br><span class="line">.&#x2F;prometheus --config.file&#x3D;.&#x2F;conf&#x2F;prometheus.yml &amp;</span><br><span class="line"># grafana</span><br><span class="line">.&#x2F;bin&#x2F;grafana-server web &amp;</span><br><span class="line">username&#x2F;password: admin&#x2F;admin</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Grafana-Dashboard"><a href="#Grafana-Dashboard" class="headerlink" title="Grafana Dashboard"></a>Grafana Dashboard</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ID: 7589</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>prometheus</tag>
        <tag>grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux下软件版本控制的使用</title>
    <url>/2020/01/17/Linux%E4%B8%8B%E8%BD%AF%E4%BB%B6%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>方便多个版本之间的来回切换</p>
</blockquote>
<span id="more"></span>

<h2 id="创建脚本的软连接"><a href="#创建脚本的软连接" class="headerlink" title="创建脚本的软连接"></a>创建脚本的软连接</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ln  -s &#x2F;usr&#x2F;local&#x2F;pushgateway-0.9.0&#x2F;pushgateway &#x2F;usr&#x2F;bin&#x2F;pushgateway09</span><br><span class="line">ln  -s &#x2F;usr&#x2F;local&#x2F;pushgateway-1.0.1&#x2F;pushgateway &#x2F;usr&#x2F;bin&#x2F;pushgateway10</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用alternatives"><a href="#使用alternatives" class="headerlink" title="使用alternatives"></a>使用alternatives</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置多个版本</span><br><span class="line">alternatives --install &#x2F;usr&#x2F;bin&#x2F;pushgateway pushgateway &#x2F;usr&#x2F;local&#x2F;pushgateway-0.9.0&#x2F;pushgateway 1400</span><br><span class="line">alternatives --install &#x2F;usr&#x2F;bin&#x2F;pushgateway pushgateway &#x2F;usr&#x2F;local&#x2F;pushgateway-1.0.1&#x2F;pushgateway 1500</span><br><span class="line"></span><br><span class="line"># 切换版本</span><br><span class="line">alternatives --config pushgateway</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux防火墙限制</title>
    <url>/2016/05/31/Linux%E9%98%B2%E7%81%AB%E5%A2%99%E9%99%90%E5%88%B6/</url>
    <content><![CDATA[<blockquote>
<p>防火墙</p>
</blockquote>
<span id="more"></span>

<h2 id="防火墙的限制"><a href="#防火墙的限制" class="headerlink" title="防火墙的限制"></a>防火墙的限制</h2><h3 id="1-限制只允许指定IP访问指定端口"><a href="#1-限制只允许指定IP访问指定端口" class="headerlink" title="1. 限制只允许指定IP访问指定端口"></a>1. 限制只允许指定IP访问指定端口</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-A INPUT -m state --state NEW -m tcp -p tcp -s 127.0.0.1 --dport 22,3306,8080 -j ACCEPT</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="2-白名单设置"><a href="#2-白名单设置" class="headerlink" title="2. 白名单设置"></a>2. 白名单设置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#定义白名单变量名</span><br><span class="line">-N whitelist</span><br><span class="line">#设置白名单ip段</span><br><span class="line">-A whitelist -s 120.25.122.0 -j ACCEPT</span><br><span class="line">-A whitelist -s 120.25.122.1 -j ACCEPT</span><br><span class="line"></span><br><span class="line">#系统远程连接及数据库端口规定白名单ip才可访问</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j whitelist</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 1521 -j whitelist</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 8080 -j whitelist</span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 3306 -j whitelist</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="3-保存防火墙设置"><a href="#3-保存防火墙设置" class="headerlink" title="3. 保存防火墙设置"></a>3. 保存防火墙设置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">service iptables save</span><br><span class="line">service iptables restart</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>Kylin初体验</title>
    <url>/2019/06/12/Kylin%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
    <content><![CDATA[<blockquote>
<p>Kylin初入门</p>
</blockquote>
<span id="more"></span>

<h2 id="一、部署"><a href="#一、部署" class="headerlink" title="一、部署"></a>一、部署</h2><hr>
<h2 id="二、Restful-API"><a href="#二、Restful-API" class="headerlink" title="二、Restful API"></a>二、Restful API</h2><h3 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h3><blockquote>
<p><strong>Authentication</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># POST /kylin/api/user/authentication</span></span><br><span class="line">curl -c ./cookiefile.txt -X POST -H <span class="string">&quot;Authorization:Basic QURNSU46S1lMSU4=&quot;</span> -H <span class="string">&quot;Content-Type:application/json&quot;</span> <span class="string">&quot;http://192.168.142.128:7070/kylin/api/user/authentication&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用上面的cookiefile来build cube</span></span><br><span class="line">curl -b ./cookiefile.txt -X PUT -H <span class="string">&quot;Authorization:Basic QURNSU46S1lMSU4=&quot;</span> -H <span class="string">&quot;Content-Type:application/json&quot;</span> -d <span class="string">&#x27;&#123;&quot;startTime&quot;:1423526400000,&quot;endTime&quot;:1423612800000,&quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27;</span> <span class="string">&quot;http://192.168.142.128:7070/kylin/api/cubes/your_cube/build&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用user/password来build cube </span></span><br><span class="line">curl -X PUT --user ADMIN:KYLIN -H <span class="string">&quot;Content-Type:application/json;charset=utf-8&quot;</span> -d <span class="string">&#x27;&#123;&quot;startTime&quot;:820454400000,&quot;endTime&quot;:821318400000,&quot;buildType&quot;:&quot;BUILD&quot;&#125;&#x27;</span> <span class="string">&quot;http://192.168.142.128:7070/kylin/api/cubes/your_cube/build&quot;</span> </span><br></pre></td></tr></table></figure>
<p><strong>Query</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># POST /kylin/api/query</span></span><br><span class="line">curl -X POST -H <span class="string">&quot;Authorization:Basic QURNSU46S1lMSU4=&quot;</span> -H <span class="string">&quot;Content-Type:application/json&quot;</span> -d <span class="string">&#x27;&#123;&quot;sql&quot;:&quot;select part_dt,sum(price) as total_sold,count(distinct seller_id) as sellers from kylin_sales group by part_dt order by part_dt&quot;,&quot;project&quot;:&quot;learn_kylin&quot;&#125;&#x27;</span>  <span class="string">&quot;http://192.168.142.128:7070/kylin/api/query&quot;</span></span><br><span class="line">curl -X POST --user ADMIN:KYLIN -H <span class="string">&quot;Content-Type:application/json&quot;</span> -d <span class="string">&#x27;&#123;&quot;sql&quot;:&quot;select part_dt,sum(price) as total_sold,count(distinct seller_id) as sellers from kylin_sales group by part_dt order by part_dt&quot;,&quot;project&quot;:&quot;learn_kylin&quot;&#125;&#x27;</span>  <span class="string">&quot;http://192.168.142.128:7070/kylin/api/query&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>List queryable tables</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GET /kylin/api/tables_and_columns</span></span><br><span class="line"><span class="comment"># 必要的参数project</span></span><br><span class="line">curl -X GET --user ADMIN:KYLIN -H <span class="string">&quot;Content-Type:application/json&quot;</span> <span class="string">&quot;http://192.168.142.128:7070/kylin/api/tables_and_columns?project=learn_kylin&quot;</span></span><br></pre></td></tr></table></figure>
<p><strong>List cubes</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GET /kylin/api/cubes</span></span><br><span class="line"><span class="comment"># 必要的参数offset,limit</span></span><br><span class="line">curl -X GET --user ADMIN:KYLIN -H <span class="string">&quot;Content-Type:application/json&quot;</span> <span class="string">&quot;http://192.168.142.128:7070/kylin/api/cubes&quot;</span></span><br></pre></td></tr></table></figure></blockquote>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kylin</tag>
      </tags>
  </entry>
  <entry>
    <title>Logstash与Flume监控之后做了什么操作</title>
    <url>/2017/10/29/Logstash%E4%B8%8EFlume%E7%9B%91%E6%8E%A7%E4%B9%8B%E5%90%8E%E5%81%9A%E4%BA%86%E4%BB%80%E4%B9%88%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<blockquote>
<p>对于Flume与Logstash重启后的操作</p>
</blockquote>
<span id="more"></span>

<h2 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h2><h3 id="监控变化"><a href="#监控变化" class="headerlink" title="监控变化"></a>监控变化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Logstash主要是通过input插件来进行监控</span><br><span class="line"># File插件中的sincedb_path</span><br><span class="line">第一次读取新文件不会有.sincedb,所以默认会根据start_position去读,如果start_position是end,则从最后读,如果start_position是begin,则从开始读</span><br><span class="line">不是第一次读取文件,则会有.sincedb文件,无论start_position是什么,都会根据.sincedb文件去读.</span><br></pre></td></tr></table></figure>
<h3 id="Logstash持久化数据"><a href="#Logstash持久化数据" class="headerlink" title="Logstash持久化数据"></a>Logstash持久化数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在config&#x2F;logstash.yml中进行配置以下内容</span><br><span class="line">queue.type: persisted</span><br><span class="line">path.queue: &#x2F;usr&#x2F;share&#x2F;logstash&#x2F;data #队列存储路径；如果队列类型为persisted，则生效</span><br><span class="line">queue.page_capacity: 250mb #队列为持久化，单个队列大小</span><br><span class="line">queue.max_events: 0 #当启用持久化队列时，队列中未读事件的最大数量，0为不限制</span><br><span class="line">queue.max_bytes: 1024mb #队列最大容量</span><br><span class="line">queue.checkpoint.acks: 1024 #在启用持久队列时强制执行检查点的最大数量,0为不限制</span><br><span class="line">queue.checkpoint.writes: 1024 #在启用持久队列时强制执行检查点之前的最大数量的写入事件，0为不限制</span><br><span class="line">queue.checkpoint.interval: 1000 #当启用持久队列时，在头页面上强制一个检查点的时间间隔</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><h3 id="监控变化-1"><a href="#监控变化-1" class="headerlink" title="监控变化"></a>监控变化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 监控文件的情况下</span><br><span class="line">会将监控文件重命名以.COMPLETE结尾,然后将文件中的数据上传到目标位置</span><br><span class="line">如果是HDFS,则会在HDFS上创建个临时文件filename.tmp,上传成功后重命名HDFS上的临时文件,将后缀.tmp去掉就可以了.</span><br></pre></td></tr></table></figure>
<h3 id="Flume持久化数据"><a href="#Flume持久化数据" class="headerlink" title="Flume持久化数据"></a>Flume持久化数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 根据Flume的架构原理,Flume采用FileChannel是不可能丢失数据的</span><br><span class="line"># 唯一有可能丢失数据则是Channel采用了MemoryChannel</span><br><span class="line">    在agent宕机时导致数据在内存中丢失</span><br><span class="line">    Channel存储数据已满,导致Source不再写入数据,造成未写入的数据丢失</span><br><span class="line"></span><br><span class="line"># 根据FileChannel的配置确定持久化数据存放位置以及每个Log文件的大小</span><br><span class="line">dataDirs</span><br><span class="line">maxFileSize</span><br><span class="line"></span><br><span class="line"># 但是请注意数据重复的问题</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>elk</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>MVN命令</title>
    <url>/2017/05/31/MVN%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<blockquote>
<p>MVN常用命令</p>
</blockquote>
<span id="more"></span>

<h2 id="1-安装包"><a href="#1-安装包" class="headerlink" title="1.安装包"></a>1.安装包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 能够下载到jar包的情况</span><br><span class="line">1. 首先下载Maven无法加载的jar包</span><br><span class="line">2. 执行mvn命令</span><br><span class="line">mvn install:install-file</span><br><span class="line">-DgroupId&#x3D;com.oracle    # groupId</span><br><span class="line">-DartifactId&#x3D;ojdbc14 #artifactId</span><br><span class="line">-Dversion&#x3D;10.2.0.4.0    # 版本号</span><br><span class="line">-Dpackaging&#x3D;jar # 打包方式</span><br><span class="line">-Dfile&#x3D;已下载的jar位置</span><br><span class="line"></span><br><span class="line"># 只能下载到工程文件的情况</span><br><span class="line">1. 首先下载源代码</span><br><span class="line">2. 进入工程目录&lt;与src同级目录&gt;</span><br><span class="line">3. 执行mvn命令</span><br><span class="line">mvn install -Dmaven.test.skip&#x3D;true</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-项目打包"><a href="#2-项目打包" class="headerlink" title="2.项目打包"></a>2.项目打包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clear package</span><br></pre></td></tr></table></figure>

<h2 id="3-SpringBoot项目运行"><a href="#3-SpringBoot项目运行" class="headerlink" title="3.SpringBoot项目运行"></a>3.SpringBoot项目运行</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn springboot:run</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-总体"><a href="#4-总体" class="headerlink" title="4.总体"></a>4.总体</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Maven常用命令： </span><br><span class="line">    <span class="comment"># 创建Maven的普通java项目： </span></span><br><span class="line">       mvn archetype:create </span><br><span class="line">       -DgroupId=packageName </span><br><span class="line">       -DartifactId=projectName  </span><br><span class="line">    <span class="comment"># 创建Maven的Web项目：   </span></span><br><span class="line">        mvn archetype:create </span><br><span class="line">        -DgroupId=packageName    </span><br><span class="line">        -DartifactId=webappName </span><br><span class="line">        -DarchetypeArtifactId=maven-archetype-webapp    </span><br><span class="line">    <span class="comment"># 编译源代码： </span></span><br><span class="line">        mvn compile </span><br><span class="line">    <span class="comment"># 编译测试代码：</span></span><br><span class="line">        mvn test-compile    </span><br><span class="line">    <span class="comment"># 运行测试：</span></span><br><span class="line">        mvn <span class="built_in">test</span>   </span><br><span class="line">    <span class="comment"># 产生site：</span></span><br><span class="line">        mvn site   </span><br><span class="line">    <span class="comment"># 打包：</span></span><br><span class="line">        mvn package   </span><br><span class="line">    <span class="comment"># 在本地Repository中安装jar：</span></span><br><span class="line">        mvn install </span><br><span class="line">    <span class="comment"># 清除产生的项目：</span></span><br><span class="line">        mvn clean   </span><br><span class="line">    <span class="comment"># 生成eclipse项目：</span></span><br><span class="line">        mvn eclipse:eclipse  </span><br><span class="line">    <span class="comment"># 生成idea项目：</span></span><br><span class="line">        mvn idea:idea  </span><br><span class="line">    <span class="comment"># 组合使用goal命令，如只打包不测试：</span></span><br><span class="line">        mvn -Dtest package   </span><br><span class="line">    <span class="comment"># 编译测试的内容：</span></span><br><span class="line">        mvn test-compile  </span><br><span class="line">    <span class="comment"># 只打jar包: </span></span><br><span class="line">        mvn jar:jar  </span><br><span class="line">    <span class="comment"># 只测试而不编译，也不测试编译：</span></span><br><span class="line">        mvn <span class="built_in">test</span> -skipping compile -skipping test-compile (-skipping 的灵活运用，当然也可以用于其他组合命令)  </span><br><span class="line">    <span class="comment"># 清除eclipse的一些系统设置:</span></span><br><span class="line">        mvn eclipse:clean </span><br><span class="line"></span><br><span class="line">ps：</span><br><span class="line">    一般使用情况是这样，首先通过cvs或svn下载代码到本机，然后执行mvn eclipse:eclipse生成ecllipse项目文件，然后导入到eclipse就行了；修改代码后执行mvn compile或mvn <span class="built_in">test</span>检验，也可以下载eclipse的maven插件。</span><br><span class="line"></span><br><span class="line">mvn -version/-v               显示版本信息 </span><br><span class="line">mvn archetype:generate        创建mvn项目 </span><br><span class="line">mvn archetype:create -DgroupId=com.oreilly -DartifactId=my-app   创建mvn项目</span><br><span class="line">mvn package              生成target目录，编译、测试代码，生成测试报告，生成jar/war文件 </span><br><span class="line">mvn jetty:run            运行项目于jetty上, </span><br><span class="line">mvn compile              编译 </span><br><span class="line">mvn <span class="built_in">test</span>                 编译并测试 </span><br><span class="line">mvn clean                清空生成的文件 </span><br><span class="line">mvn site                 生成项目相关信息的网站 </span><br><span class="line">mvn -Dwtpversion=1.0 eclipse:eclipse        生成Wtp插件的Web项目 </span><br><span class="line">mvn -Dwtpversion=1.0 eclipse:clean          清除Eclipse项目的配置信息(Web项目) </span><br><span class="line">mvn eclipse:eclipse                         将项目转化为Eclipse项目</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在应用程序用使用多个存储库 </span></span><br><span class="line">&lt;repositories&gt;    </span><br><span class="line">    &lt;repository&gt;      </span><br><span class="line">        &lt;id&gt;Ibiblio&lt;/id&gt;      </span><br><span class="line">        &lt;name&gt;Ibiblio&lt;/name&gt;      </span><br><span class="line">        &lt;url&gt;http://www.ibiblio.org/maven/&lt;/url&gt;    </span><br><span class="line">    &lt;/repository&gt;    </span><br><span class="line">    &lt;repository&gt;      </span><br><span class="line">        &lt;id&gt;PlanetMirror&lt;/id&gt;      </span><br><span class="line">        &lt;name&gt;Planet Mirror&lt;/name&gt;      </span><br><span class="line">        &lt;url&gt;http://public.planetmirror.com/pub/maven/&lt;/url&gt;    </span><br><span class="line">    &lt;/repository&gt;  </span><br><span class="line">&lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发布第三方Jar到本地库中：</span></span><br><span class="line">mvn deploy:deploy-file -DgroupId=com -DartifactId=client -Dversion=0.1.0 -Dpackaging=jar -Dfile=d:\client-0.1.0.jar -DrepositoryId=maven-repository-inner -Durl=ftp://xxxxxxx/opt/maven/repository/</span><br><span class="line"></span><br><span class="line">mvn install:install-file -DgroupId=com -DartifactId=client -Dversion=0.1.0 -Dpackaging=jar -Dfile=d:\client-0.1.0.jar</span><br><span class="line"></span><br><span class="line">-DdownloadSources=<span class="literal">true</span></span><br><span class="line">-DdownloadJavadocs=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">mvn -e              显示详细错误 信息. </span><br><span class="line">mvn validate        验证工程是否正确，所有需要的资源是否可用。 </span><br><span class="line">mvn test-compile    编译项目测试代码。 。 </span><br><span class="line">mvn integration-test     在集成测试可以运行的环境中处理和发布包。 </span><br><span class="line">mvn verify               运行任何检查，验证包是否有效且达到质量标准。     </span><br><span class="line">mvn generate-sources     产生应用需要的任何额外的源代码，如xdoclet。</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常用命令： </span></span><br><span class="line">mvn -v 显示版本 </span><br><span class="line">mvn <span class="built_in">help</span>:describe -Dplugin=<span class="built_in">help</span> 使用 <span class="built_in">help</span> 插件的  describe 目标来输出 Maven Help 插件的信息。 </span><br><span class="line">mvn <span class="built_in">help</span>:describe -Dplugin=<span class="built_in">help</span> -Dfull 使用Help 插件输出完整的带有参数的目标列 </span><br><span class="line">mvn <span class="built_in">help</span>:describe -Dplugin=compiler -Dmojo=compile -Dfull 获取单个目标的信息,设置  mojo 参数和  plugin 参数。此命令列出了Compiler 插件的compile 目标的所有信息 </span><br><span class="line">mvn <span class="built_in">help</span>:describe -Dplugin=<span class="built_in">exec</span> -Dfull 列出所有 Maven Exec 插件可用的目标 </span><br><span class="line">mvn <span class="built_in">help</span>:effective-pom 看这个“有效的 (effective)”POM，它暴露了 Maven的默认设置</span><br><span class="line"></span><br><span class="line">mvn archetype:create -DgroupId=org.sonatype.mavenbook.ch03 -DartifactId=simple -DpackageName=org.sonatype.mavenbook 创建Maven的普通java项目，在命令行使用Maven Archetype 插件 </span><br><span class="line">mvn <span class="built_in">exec</span>:java -Dexec.mainClass=org.sonatype.mavenbook.weather.Main Exec 插件让我们能够在不往 classpath 载入适当的依赖的情况下，运行这个程序 </span><br><span class="line">mvn dependency:resolve 打印出已解决依赖的列表 </span><br><span class="line">mvn dependency:tree 打印整个依赖树</span><br><span class="line"></span><br><span class="line">mvn install -X 想要查看完整的依赖踪迹，包含那些因为冲突或者其它原因而被拒绝引入的构件，打开 Maven 的调试标记运行 </span><br><span class="line">mvn install -Dmaven.test.skip=<span class="literal">true</span> 给任何目标添加maven.test.skip 属性就能跳过测试 </span><br><span class="line">mvn install assembly:assembly 构建装配Maven Assembly 插件是一个用来创建你应用程序特有分发包的插件</span><br><span class="line"></span><br><span class="line">mvn jetty:run     调用 Jetty 插件的 Run 目标在 Jetty Servlet 容器中启动 web 应用 </span><br><span class="line">mvn compile       编译你的项目 </span><br><span class="line">mvn clean install 删除再编译</span><br><span class="line"></span><br><span class="line">mvn hibernate3:hbm2ddl 使用 Hibernate3 插件构造数据库</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>编译</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>MR的测试小例子</title>
    <url>/2017/09/24/MR%E7%9A%84%E6%B5%8B%E8%AF%95%E5%B0%8F%E4%BE%8B%E5%AD%90/</url>
    <content><![CDATA[<blockquote>
<p>入门测试,测试数据可能得自己看代码捋一捋,挺有意思的</p>
</blockquote>
<span id="more"></span>

<h2 id="递归关系物品分类"><a href="#递归关系物品分类" class="headerlink" title="递归关系物品分类"></a>递归关系物品分类</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.LogManager;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.PropertyConfigurator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.TreeMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RelationDemo</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RelationMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">        Text k = <span class="keyword">new</span> Text();</span><br><span class="line">        Text v = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            String line = value.toString();</span><br><span class="line">            String[] cp = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            <span class="comment">// 1    0   家电</span></span><br><span class="line">            <span class="comment">// 0    1_家电</span></span><br><span class="line">            k.set(cp[<span class="number">1</span>]);</span><br><span class="line">            v.set(cp[<span class="number">0</span>] + <span class="string">&quot;_&quot;</span> + cp[<span class="number">2</span>]);</span><br><span class="line">            context.write(k, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RelationReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">        Text k = <span class="keyword">new</span> Text();</span><br><span class="line">        Text v = <span class="keyword">new</span> Text();</span><br><span class="line">        Map&lt;String, List&lt;String&gt;&gt; relaMap = <span class="keyword">new</span> TreeMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">// 0    &lt;1_家电,2_服装,3_食品&gt;</span></span><br><span class="line">            <span class="comment">// 1    &lt;4_洗衣机,5_冰箱&gt;</span></span><br><span class="line">            <span class="comment">// 2    &lt;6_男装,7_女装&gt;</span></span><br><span class="line">            <span class="comment">// 3    &lt;8_零食,9_水果&gt;</span></span><br><span class="line">            <span class="comment">// 4    &lt;10_美的&gt;</span></span><br><span class="line">            <span class="comment">// 5    &lt;11_海尔&gt;</span></span><br><span class="line">            <span class="comment">// 6    &lt;12_衬衫&gt;</span></span><br><span class="line">            <span class="comment">// 7    &lt;13_蓬蓬裙&gt;</span></span><br><span class="line">            <span class="comment">// 8    &lt;14_牛奶&gt;</span></span><br><span class="line">            <span class="comment">// 14   &lt;15_特仑苏&gt;</span></span><br><span class="line">            List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">                list.add(value.toString());</span><br><span class="line">            &#125;</span><br><span class="line">            relaMap.put(key.toString(), list);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            List&lt;String&gt; list = relaMap.get(<span class="string">&quot;0&quot;</span>);</span><br><span class="line">            <span class="keyword">for</span> (String c : list) &#123;</span><br><span class="line">                String name = c.split(<span class="string">&quot;_&quot;</span>)[<span class="number">1</span>];</span><br><span class="line">                k.set(name);</span><br><span class="line">                String str = relation(c);</span><br><span class="line">                String[] split = str.split(name);</span><br><span class="line">                <span class="keyword">for</span> (String s : split) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (s.length() != <span class="number">0</span>) &#123;</span><br><span class="line">                        v.set(s.trim());</span><br><span class="line">                        context.write(k, v);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 传进来的是1_家电</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">relation</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 考虑物品名称含有key值,不好进行切割</span></span><br><span class="line">            <span class="comment">// 所以将带编号的key一起传进,有待优化</span></span><br><span class="line">            String result = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            String[] info = str.split(<span class="string">&quot;_&quot;</span>);</span><br><span class="line">            String index = info[<span class="number">0</span>];</span><br><span class="line">            String name = info[<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">if</span> (relaMap.containsKey(index)) &#123;</span><br><span class="line">                List&lt;String&gt; child = relaMap.get(index);</span><br><span class="line">                <span class="keyword">for</span> (String s : child) &#123;</span><br><span class="line">                    result += name + <span class="string">&quot; &quot;</span> + relation(s) + <span class="string">&quot; &quot;</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> name;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = getConf();</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(RelationDemo.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入输出参数</span></span><br><span class="line">        setInputAndOutput(job, conf, args);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入</span></span><br><span class="line">        job.setMapperClass(RelationMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输出</span></span><br><span class="line">        job.setReducerClass(RelationReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交并退出</span></span><br><span class="line">        <span class="keyword">boolean</span> res = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> res ? <span class="number">0</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setConf</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Configuration <span class="title">getConf</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Configuration();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">setInputAndOutput</span><span class="params">(Job job, Configuration conf, String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;usage:yarn jar /*.jar package.className inputPath outputPath &quot;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取文件系统</span></span><br><span class="line">            FileSystem fs = FileSystem.get(conf);</span><br><span class="line">            Path outputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">if</span> (fs.exists(outputPath)) &#123;</span><br><span class="line">                fs.delete(outputPath, <span class="keyword">true</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 主方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        PropertyConfigurator.configure(<span class="string">&quot;D:\\log4j.properties&quot;</span>);<span class="comment">// 加载.properties文件</span></span><br><span class="line">        Logger logger = LogManager.getLogger(<span class="string">&quot;MyTestLog&quot;</span>);</span><br><span class="line">        <span class="keyword">int</span> isOk = ToolRunner.run(<span class="keyword">new</span> Configuration(), <span class="keyword">new</span> RelationDemo(), args);</span><br><span class="line">        System.exit(isOk);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="求解共同朋友"><a href="#求解共同朋友" class="headerlink" title="求解共同朋友"></a>求解共同朋友</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 求解共同好友</span><br><span class="line">public class FridensDemo &#123;</span><br><span class="line">    public static class StepOneMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123;</span><br><span class="line">        Text k &#x3D; new Text();</span><br><span class="line">        Text v &#x3D; new Text();</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            &#x2F;&#x2F; &lt;A B,C,D,F,E,O&gt;</span><br><span class="line">            String[] fields &#x3D; value.toString().split(&quot;:&quot;);</span><br><span class="line">            String person &#x3D; fields[0];</span><br><span class="line">            String[] friends &#x3D; fields[1].split(&quot;,&quot;);</span><br><span class="line">            for (String friend : friends) &#123;</span><br><span class="line">                k.set(friend);</span><br><span class="line">                v.set(person);</span><br><span class="line">                context.write(k, v);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 输出多个&lt;firend,person&gt;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class StepOneReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123;</span><br><span class="line">        Text v &#x3D; new Text();</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            &#x2F;&#x2F; 接收,就是一个friend,多个person</span><br><span class="line">            StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">            for (Text value : values) &#123;</span><br><span class="line">                if (sb.length() !&#x3D; 0) &#123;</span><br><span class="line">                    sb.append(&quot;,&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">                sb.append(value.toString());</span><br><span class="line">            &#125;</span><br><span class="line">            v.set(sb.toString());</span><br><span class="line">            &#x2F;&#x2F; 人,人,人..</span><br><span class="line">            context.write(key, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class StepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123;</span><br><span class="line">        Text k &#x3D; new Text();</span><br><span class="line">        Text v &#x3D; new Text();</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            &#x2F;&#x2F; &lt;好友 人,人,人,人...&gt;</span><br><span class="line">            String[] fields &#x3D; value.toString().split(&quot;\t&quot;);</span><br><span class="line">            String friend &#x3D; fields[0];</span><br><span class="line">            String[] persons &#x3D; fields[1].split(&quot;,&quot;);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 排序</span><br><span class="line">            Arrays.sort(persons);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 输出&lt;人-人,好友&gt;</span><br><span class="line">            for (int i &#x3D; 0; i &lt; persons.length - 2; i++) &#123;</span><br><span class="line">                for (int j &#x3D; i + 1; j &lt; persons.length - 1; j++) &#123;</span><br><span class="line">                    k.set(persons[i] + &quot;-&quot; + persons[j]);</span><br><span class="line">                    v.set(friend);</span><br><span class="line">                    context.write(k, v);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class StepTwoReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123;</span><br><span class="line">        Text v &#x3D; new Text();</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            &#x2F;&#x2F; 接收多个&lt;人-人,好友&gt;对</span><br><span class="line">            StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">            for (Text value : values) &#123;</span><br><span class="line">                if (sb.length() !&#x3D; 0) &#123;</span><br><span class="line">                    sb.append(&quot;,&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">                sb.append(value.toString());</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F; 人,人,人..</span><br><span class="line">            v.set(sb.toString());</span><br><span class="line">            context.write(key, v);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws IOException, InterruptedException &#123;</span><br><span class="line">        Configuration conf &#x3D; new Configuration();</span><br><span class="line">        Job oneJob &#x3D; Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        oneJob.setJarByClass(FridensDemo.class);</span><br><span class="line"></span><br><span class="line">        oneJob.setMapperClass(StepOneMapper.class);</span><br><span class="line">        oneJob.setMapOutputKeyClass(Text.class);</span><br><span class="line">        oneJob.setMapOutputValueClass(Text.class);</span><br><span class="line">        FileInputFormat.addInputPath(oneJob, new Path(args[0]));</span><br><span class="line"></span><br><span class="line">        oneJob.setReducerClass(StepOneReducer.class);</span><br><span class="line">        oneJob.setOutputKeyClass(Text.class);</span><br><span class="line">        oneJob.setOutputValueClass(Text.class);</span><br><span class="line">        FileOutputFormat.setOutputPath(oneJob, new Path(args[1]));</span><br><span class="line"></span><br><span class="line">        Job twoJob &#x3D; Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        twoJob.setJarByClass(FridensDemo.class);</span><br><span class="line"></span><br><span class="line">        twoJob.setMapperClass(StepTwoMapper.class);</span><br><span class="line">        twoJob.setMapOutputKeyClass(Text.class);</span><br><span class="line">        twoJob.setMapOutputValueClass(Text.class);</span><br><span class="line">        FileInputFormat.addInputPath(twoJob, new Path(args[1]));</span><br><span class="line"></span><br><span class="line">        twoJob.setReducerClass(StepTwoReducer.class);</span><br><span class="line">        twoJob.setOutputKeyClass(Text.class);</span><br><span class="line">        twoJob.setOutputValueClass(Text.class);</span><br><span class="line">        FileOutputFormat.setOutputPath(twoJob, new Path(args[2]));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 我觉得最最主要的技术点为,用到了job链</span><br><span class="line">        ControlledJob one &#x3D; new ControlledJob(oneJob.getConfiguration());</span><br><span class="line">        ControlledJob two &#x3D; new ControlledJob(twoJob.getConfiguration());</span><br><span class="line"></span><br><span class="line">        two.addDependingJob(one);</span><br><span class="line"></span><br><span class="line">        JobControl jc &#x3D; new JobControl(&quot;friend&quot;);</span><br><span class="line">        jc.addJob(one);</span><br><span class="line">        jc.addJob(two);</span><br><span class="line"></span><br><span class="line">        Thread th &#x3D; new Thread(jc);</span><br><span class="line">        th.start();</span><br><span class="line"></span><br><span class="line">        &#x2F;*if (jc.allFinished()) &#123;</span><br><span class="line">            Thread.sleep(2000);</span><br><span class="line">            jc.stop();</span><br><span class="line">            th.stop();</span><br><span class="line">            System.exit(0);</span><br><span class="line">        &#125;*&#x2F;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>mr</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac编译GitHub上项目出现CRLF问题</title>
    <url>/2021/04/27/Mac%E7%BC%96%E8%AF%91GitHub%E4%B8%8A%E9%A1%B9%E7%9B%AE%E5%87%BA%E7%8E%B0CRLF%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>参数配置下</p>
</blockquote>
<span id="more"></span>

<h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置拉取全局配置</span><br><span class="line">git config --global core.autocrlf input</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>编译</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac通过Brew安装组件出现环境问题</title>
    <url>/2021/05/26/Mac%E9%80%9A%E8%BF%87Brew%E5%AE%89%E8%A3%85%E7%BB%84%E4%BB%B6%E5%87%BA%E7%8E%B0%E7%8E%AF%E5%A2%83%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>Brew使用过程中的问题,自己修改下Ruby脚本就好</p>
</blockquote>
<span id="more"></span>

<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Java环境的问题,因为Mac中使用了Brew安装了openjdk版本的Java</span><br><span class="line">又安装了Oracle版本的Java,最终导致Java版本依赖错误</span><br><span class="line">#</span><br><span class="line"># A fatal error has been detected by the Java Runtime Environment:</span><br><span class="line">#</span><br><span class="line">#  Internal Error (sharedRuntime.cpp:531), pid&#x3D;20268, tid&#x3D;7171</span><br><span class="line">#  Error: ShouldNotReachHere()</span><br><span class="line">#</span><br><span class="line"># JRE version: OpenJDK Runtime Environment (16.0+14) (build 16+14)</span><br><span class="line"># Java VM: OpenJDK 64-Bit Server VM (16+14, mixed mode, tiered, compressed oops, g1 gc, bsd-aarch64)</span><br><span class="line"># No core dump will be written. Core dumps have been disabled. To enable core dumping, try &quot;ulimit -c unlimited&quot; before starting Java again</span><br><span class="line">#</span><br><span class="line"># An error report file with more information is saved as:</span><br><span class="line"># &#x2F;opt&#x2F;homebrew&#x2F;Cellar&#x2F;kafka&#x2F;2.7.0&#x2F;bin&#x2F;hs_err_pid20268.log</span><br><span class="line">#</span><br><span class="line"># If you would like to submit a bug report, please visit:</span><br><span class="line">#   https:&#x2F;&#x2F;bugreport.java.com&#x2F;bugreport&#x2F;crash.jsp</span><br><span class="line">#</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以kafka为例</span></span><br><span class="line">brew edit kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># depends_on &quot;openjdk&quot;</span></span><br><span class="line"><span class="comment"># 替换成Oracle版本Java</span></span><br><span class="line">bin.env_script_all_files(libexec/<span class="string">&quot;bin&quot;</span>, Hash[<span class="string">&quot;JAVA_HOME&quot;</span> =&gt; <span class="string">&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_281.jdk/Contents/Home&quot;</span>])</span><br><span class="line"></span><br><span class="line">brew uninstall kafka</span><br><span class="line">brew install kafka</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这里可以尝试学习下Ruby.^-^</span><br><span class="line">没有太搞懂Language::Java.java_home_env是调用了哪里</span><br><span class="line">百度也没有找到相关资料</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven使用CDH仓库</title>
    <url>/2018/03/29/Maven%E4%BD%BF%E7%94%A8CDH%E4%BB%93%E5%BA%93/</url>
    <content><![CDATA[<blockquote>
<p>解决maven默认地址CDH依赖不足,或下载速度较慢,解决方式选择一种即可</p>
</blockquote>
<span id="more"></span>

<h2 id="修改setting文件"><a href="#修改setting文件" class="headerlink" title="修改setting文件"></a>修改setting文件</h2><p><strong>注意</strong> id不要乱写哟</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;profile&gt;</span><br><span class="line">    &lt;id&gt;remote-repo&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;nexus-aliyun&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;name&gt;Nexus aliyun&lt;&#x2F;name&gt;  </span><br><span class="line">            &lt;url&gt;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&lt;&#x2F;url&gt;</span><br><span class="line">            &lt;release&gt;&lt;enable&gt;true&lt;&#x2F;enable&gt;&lt;&#x2F;release&gt;</span><br><span class="line">            &lt;snapshot&gt;&lt;enable&gt;true&lt;&#x2F;enable&gt;&lt;&#x2F;snapshot&gt;</span><br><span class="line">        &lt;&#x2F;repository&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;name&gt;Cloudera Repo&lt;&#x2F;name&gt;</span><br><span class="line">            &lt;!--&lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&lt;&#x2F;url&gt;--&gt;</span><br><span class="line">            &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;content&#x2F;repositories&#x2F;releases&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">            &lt;releases&gt;</span><br><span class="line">            &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">            &lt;&#x2F;releases&gt;</span><br><span class="line">            &lt;snapshots&gt;</span><br><span class="line">            &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">            &lt;&#x2F;snapshots&gt;</span><br><span class="line">        &lt;&#x2F;repository&gt;</span><br><span class="line">    &lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;&#x2F;profile&gt;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="修改pom文件"><a href="#修改pom文件" class="headerlink" title="修改pom文件"></a>修改pom文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&lt;&#x2F;url&gt;</span><br><span class="line">        &lt;releases&gt;</span><br><span class="line">            &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">        &lt;&#x2F;releases&gt;</span><br><span class="line">        &lt;snapshots&gt;</span><br><span class="line">            &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">        &lt;&#x2F;snapshots&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>编译</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac基于M1芯片安装Tensorflow</title>
    <url>/2021/04/21/Mac%E5%9F%BA%E4%BA%8EM1%E8%8A%AF%E7%89%87%E5%AE%89%E8%A3%85Tensorflow/</url>
    <content><![CDATA[<blockquote>
<p>机器学习Tensorflow环境搭建教程</p>
</blockquote>
<span id="more"></span>

<h2 id="安装工作"><a href="#安装工作" class="headerlink" title="安装工作"></a>安装工作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.下载tensorflow</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;apple&#x2F;tensorflow_macos&#x2F;releases</span><br><span class="line">选择tensorflow_macos-0.1alpha3.tar.gz</span><br><span class="line"></span><br><span class="line">2.下载anaconda_arm</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;conda-forge&#x2F;miniforge&#x2F;releases&#x2F;latest&#x2F;download&#x2F;Miniforge3-MacOSX-arm64.sh</span><br><span class="line"></span><br><span class="line">3.安装anaconda(安装后重启终端)</span><br><span class="line">&#x2F;bin&#x2F;bash .&#x2F;Miniforge3-MacOSX-arm64.sh</span><br><span class="line"></span><br><span class="line">4.创建虚拟环境</span><br><span class="line">conda create -n tensorflow_env python&#x3D;3.8</span><br><span class="line"># conda activate tensorflow_env 激活虚拟环境</span><br><span class="line"># conda deactivate 退出虚拟环境</span><br><span class="line"></span><br><span class="line">5.查看Python路径</span><br><span class="line">which python</span><br><span class="line"></span><br><span class="line">6.安装tensorflow</span><br><span class="line">tar -zxvf tensorflow_macos-0.1alpha3.tar.gz -C &#x2F;Users&#x2F;xz&#x2F;Local&#x2F;Envs</span><br><span class="line">libs&#x3D;&quot;&#x2F;Users&#x2F;xz&#x2F;Local&#x2F;Envs&#x2F;tensorflow_macos&#x2F;arm64&quot;</span><br><span class="line">env&#x3D;&quot;&#x2F;Users&#x2F;xz&#x2F;miniforge3&#x2F;envs&#x2F;tensorflow_env&quot;</span><br><span class="line">pip install --upgrade pip wheel setuptools cached-property six</span><br><span class="line">pip install --upgrade -t &quot;$env&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;&quot; --no-dependencies --force &quot;$libs&#x2F;grpcio-1.33.2-cp38-cp38-macosx_11_0_arm64.whl&quot;</span><br><span class="line">pip install --upgrade -t &quot;$env&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;&quot; --no-dependencies --force &quot;$libs&#x2F;h5py-2.10.0-cp38-cp38-macosx_11_0_arm64.whl&quot;</span><br><span class="line">pip install --upgrade -t &quot;$env&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;&quot; --no-dependencies --force &quot;$libs&#x2F;numpy-1.18.5-cp38-cp38-macosx_11_0_arm64.whl&quot;</span><br><span class="line">pip install --upgrade -t &quot;$env&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;&quot; --no-dependencies --force &quot;$libs&#x2F;tensorflow_addons_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl&quot;</span><br><span class="line">pip install absl-py astunparse flatbuffers gast google_pasta keras_preprocessing opt_einsum protobuf tensorflow_estimator termcolor typing_extensions wrapt wheel tensorboard typeguard</span><br><span class="line">pip install --upgrade -t &quot;$env&#x2F;lib&#x2F;python3.8&#x2F;site-packages&#x2F;&quot; --no-dependencies --force &quot;$libs&#x2F;tensorflow_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl&quot;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="试验"><a href="#试验" class="headerlink" title="试验"></a>试验</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 注意是在虚拟环境下执行</span><br><span class="line">python</span><br><span class="line">import tensorflow</span><br><span class="line"># 成功即证明安装成功</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>MaxCompute常用命令整理</title>
    <url>/2020/08/03/MaxCompute%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>整理日常开发中会使用到的一些命令</p>
</blockquote>
<span id="more"></span>

<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载表数据</span></span><br><span class="line">tunnel download tableName/partitionKey=partitionValue result.csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载查询结果</span></span><br><span class="line">tunnel download instance://instanceID result.csv</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL大数据迁移</title>
    <url>/2018/10/06/MySQL%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/</url>
    <content><![CDATA[<blockquote>
<p>记录一下MySQL在面临短时间进行跨多版本升级,数据如何进行迁移的问题</p>
</blockquote>
<span id="more"></span>

<h2 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用的是MySQL5.1的版本</span><br><span class="line">升级至5.7版本</span><br><span class="line">数据上T级别</span><br><span class="line">5.1分区仅支持1024个,分区即将用尽,线上目前正在使用</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="已使用的方案"><a href="#已使用的方案" class="headerlink" title="已使用的方案"></a>已使用的方案</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">因为比较急,需要赶在国庆假期之前弄好</span><br><span class="line">采取了mysqldump的方式,在当天数据计算完毕之后,进行dump成sql文件,耗时4-5小时</span><br><span class="line">然后进行load进新版本的MySQL中,耗时7-8小时&lt;其中包括出错时间&gt;</span><br><span class="line"></span><br><span class="line">其实这也是一种方式,但其实并不适用于那种时时刻刻都有数据写入MySQL的业务情景</span><br><span class="line">只是恰好我们只是存当天报表任务的指标数据</span><br><span class="line">所以可以使用直接dump</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="合理方案Canal"><a href="#合理方案Canal" class="headerlink" title="合理方案Canal"></a>合理方案<a href="https://github.com/alibaba/canal">Canal</a></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># canal原理</span><br><span class="line">使用canal进行对mysql的binlog进行解析</span><br><span class="line">模拟MySQL slave的交互协议，伪装自己为MySQL slave，向MySQL master发送dump协议</span><br><span class="line">MySQL master收到dump请求，开始推送binary log给slave(即canal)</span><br><span class="line">canal解析binary log对象(原始为byte流)</span><br><span class="line"></span><br><span class="line"># 过程</span><br><span class="line">只需要旧MySQL执行flush logs命令,生成新的binary log文件</span><br><span class="line">同时canal开始同步数据到新MySQL</span><br><span class="line">这时新MySQL的数据应该与旧数据库的一致</span><br><span class="line">将流量切到新MySQL</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">使用flush logs相当于确定一个时间点,这个时间点之前的数据同步好,那么新数据库的数据一定与旧数据库一致</span><br><span class="line">接着就是切流量的数据同步了</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Protobuf以及Protostuff的使用</title>
    <url>/2020/07/07/Protobuf%E4%BB%A5%E5%8F%8AProtostuff%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>继Kryo序列化操作之后，另外两种序列化方法</p>
</blockquote>
<span id="more"></span>

<h2 id="Protobuf"><a href="#Protobuf" class="headerlink" title="Protobuf"></a>Protobuf</h2><h3 id="获取Protoc工具"><a href="#获取Protoc工具" class="headerlink" title="获取Protoc工具"></a>获取Protoc工具</h3><p><a href="https://github.com/protocolbuffers/protobuf/releases">下载地址</a></p>
<h3 id="自定义Proto文件"><a href="#自定义Proto文件" class="headerlink" title="自定义Proto文件"></a>自定义Proto文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; proto3版本协议</span><br><span class="line">syntax &#x3D; &quot;proto3&quot;;  </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 编译生成的包名</span><br><span class="line">option java_package &#x3D; &quot;com.example.protobuf&quot;;  </span><br><span class="line">&#x2F;&#x2F; 编译生成的类名</span><br><span class="line">option java_outer_classname &#x3D; &quot;PersonFactory&quot;;  </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 类</span><br><span class="line">message Person&#123;  </span><br><span class="line">     int32 id &#x3D; 1;  </span><br><span class="line">     string name &#x3D; 2;  </span><br><span class="line">     int32 age &#x3D; 3;  </span><br><span class="line">     Addr addr &#x3D; 4;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 类</span><br><span class="line">message Addr&#123;   </span><br><span class="line">     string contry &#x3D; 1;  </span><br><span class="line">     string city &#x3D; 2;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>
<h3 id="生成Java文件"><a href="#生成Java文件" class="headerlink" title="生成Java文件"></a>生成Java文件</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">protoc.exe --proto_path=./ --java_out=./ ./PersonFactory.proto</span><br></pre></td></tr></table></figure>
<h3 id="Pom文件"><a href="#Pom文件" class="headerlink" title="Pom文件"></a>Pom文件</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.google.protobuf<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>protobuf-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="代码测试"><a href="#代码测试" class="headerlink" title="代码测试"></a>代码测试</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.protobuf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.protobuf.ByteString;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.ByteArrayInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ByteArrayOutputStream;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/7/7.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProtobufDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// Person构造器</span></span><br><span class="line">        PersonFactory.Person.Builder personBuilder = PersonFactory.Person.newBuilder();</span><br><span class="line">        personBuilder.setAge(<span class="number">10</span>);</span><br><span class="line">        personBuilder.setId(<span class="number">1</span>);</span><br><span class="line">        personBuilder.setName(<span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        PersonFactory.Addr.Builder addrBuilder = PersonFactory.Addr.newBuilder();</span><br><span class="line">        addrBuilder.setCity(<span class="string">&quot;beijing&quot;</span>);</span><br><span class="line">        addrBuilder.setContry(<span class="string">&quot;china&quot;</span>);</span><br><span class="line"></span><br><span class="line">        personBuilder.setAddr(addrBuilder);</span><br><span class="line"></span><br><span class="line">        PersonFactory.Person person = personBuilder.build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 序列化/反序列化,方式1,byte[]</span></span><br><span class="line">        <span class="keyword">byte</span>[] bytes = person.toByteArray(); <span class="comment">// 序列化</span></span><br><span class="line">        PersonFactory.Person dePerson01 = PersonFactory.Person.parseFrom(bytes);<span class="comment">// 反序列化</span></span><br><span class="line">        System.out.println(dePerson01.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 序列化/反序列化,方式2,ByteString</span></span><br><span class="line">        ByteString byteString = person.toByteString();<span class="comment">// 序列化</span></span><br><span class="line">        PersonFactory.Person dePerson02 = PersonFactory.Person.parseFrom(byteString);<span class="comment">// 反序列化</span></span><br><span class="line">        System.out.println(dePerson02.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 序列化/反序列化,方式3,InputStream</span></span><br><span class="line">        ByteArrayOutputStream byteArrayOutputStream = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">        person.writeDelimitedTo(byteArrayOutputStream);<span class="comment">// 序列化</span></span><br><span class="line">        ByteArrayInputStream byteArrayInputStream = <span class="keyword">new</span> ByteArrayInputStream(byteArrayOutputStream.toByteArray());</span><br><span class="line">        PersonFactory.Person dePerson03 = PersonFactory.Person.parseDelimitedFrom(byteArrayInputStream);<span class="comment">// 反序列化</span></span><br><span class="line">        System.out.println(dePerson03.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Protostuff"><a href="#Protostuff" class="headerlink" title="Protostuff"></a>Protostuff</h2><h3 id="Pom文件-1"><a href="#Pom文件-1" class="headerlink" title="Pom文件"></a>Pom文件</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.protostuff<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>protostuff-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.protostuff<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>protostuff-runtime<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>io.protostuff<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>protostuff-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.5.9<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.protostuff;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io.protostuff.LinkedBuffer;</span><br><span class="line"><span class="keyword">import</span> io.protostuff.ProtostuffIOUtil;</span><br><span class="line"><span class="keyword">import</span> io.protostuff.runtime.RuntimeSchema;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.ByteArrayInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ByteArrayOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/7/7.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProtostuffDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">        list.add(<span class="string">&quot;a&quot;</span>);</span><br><span class="line">        list.add(<span class="string">&quot;b&quot;</span>);</span><br><span class="line">        Person person = <span class="keyword">new</span> Person(<span class="number">1</span>, <span class="string">&quot;111&quot;</span>, list);</span><br><span class="line">        RuntimeSchema&lt;Person&gt; schema = RuntimeSchema.createFrom(Person.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 序列化/反序列化,方式1,byte[]</span></span><br><span class="line">        <span class="keyword">byte</span>[] bytes = ProtostuffIOUtil.toByteArray(person, schema, LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE)); <span class="comment">// 序列化</span></span><br><span class="line">        System.out.println(bytes.length);</span><br><span class="line">        Person dePerson1 = schema.newMessage();</span><br><span class="line">        ProtostuffIOUtil.mergeFrom(bytes, dePerson1, schema); <span class="comment">// 反序列化</span></span><br><span class="line">        System.out.println(dePerson1.toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 序列化/反序列化,方式2,InputStream</span></span><br><span class="line">        ByteArrayOutputStream byteArrayOutputStream = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">        ProtostuffIOUtil.writeDelimitedTo(byteArrayOutputStream, person, schema, LinkedBuffer.allocate(LinkedBuffer.DEFAULT_BUFFER_SIZE)); <span class="comment">// 序列化</span></span><br><span class="line">        ByteArrayInputStream byteArrayInputStream = <span class="keyword">new</span> ByteArrayInputStream(byteArrayOutputStream.toByteArray());</span><br><span class="line">        Person dePerson2 = schema.newMessage();</span><br><span class="line">        ProtostuffIOUtil.mergeDelimitedFrom(byteArrayInputStream, dePerson2, schema); <span class="comment">// 反序列化</span></span><br><span class="line">        System.out.println(dePerson2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> age;</span><br><span class="line">    List&lt;String&gt; more;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Person</span><span class="params">(<span class="keyword">int</span> age, String name, List&lt;String&gt; more)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">        <span class="keyword">this</span>.more = more;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(<span class="keyword">int</span> age)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.age = age;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">getMore</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> more;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setMore</span><span class="params">(List&lt;String&gt; more)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.more = more;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;Person&#123;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;age=&quot;</span> + age +</span><br><span class="line">                <span class="string">&quot;, name=&#x27;&quot;</span> + name + <span class="string">&#x27;\&#x27;&#x27;</span> +</span><br><span class="line">                <span class="string">&quot;, more=&quot;</span> + more +</span><br><span class="line">                <span class="string">&#x27;&#125;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>Python搜索工程的输入输出表</title>
    <url>/2020/07/31/Python%E6%90%9C%E7%B4%A2%E5%B7%A5%E7%A8%8B%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E8%A1%A8/</url>
    <content><![CDATA[<blockquote>
<p>编写python相关脚本全自动搜索的方式来取代人工查找每个作业的输入输出表<br>主要用于作业汇总,作业流查询等用处<br>对错误数据以及可忽略的部分进行过滤处理</p>
</blockquote>
<span id="more"></span>

<h4 id="指定具体某个作业-半自动"><a href="#指定具体某个作业-半自动" class="headerlink" title="指定具体某个作业(半自动)"></a>指定具体某个作业(半自动)</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># 解析程序的输出输出表关系</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">with open(&#39;E:&#x2F;relationship-analysis&#x2F;file&#x2F;DoQuesAndTeachingStat.scala&#39;,encoding&#x3D;&#39;UTF-8&#39;)as file:</span><br><span class="line">    encoding &#x3D; &#39;UTF-8&#39;</span><br><span class="line">    line &#x3D; file.read()</span><br><span class="line"></span><br><span class="line">    appName &#x3D; re.findall(r&quot;appName\(\&quot;(.+)\&quot;&quot;, line)[0]</span><br><span class="line">    # 查找以from开头,以&quot;结尾的内容</span><br><span class="line">    inputTable &#x3D; re.findall(r&quot;[fF][rR][oO][mM]\s+(.+)&quot;, line)</span><br><span class="line">    input &#x3D; []</span><br><span class="line">    inputTable &#x3D; list(set(inputTable))</span><br><span class="line">    for i in range(len(inputTable)):</span><br><span class="line">        #rfind返回字符串最后一次出现的位置,如果没有匹配则返回-1</span><br><span class="line">        inputTable[i] &#x3D; inputTable[i][0:inputTable[i].find(&#39;&quot;&#39;, 1)]</span><br><span class="line">        # find_ &#x3D; inputTable[i].find(&#39; &#39;) + 1</span><br><span class="line">        find_ &#x3D; inputTable[i].find(&#39; &#39;, 1) + 1</span><br><span class="line">        if find_ &#x3D;&#x3D; 0:</span><br><span class="line">            if inputTable[i].find(&quot;$&quot;) &#x3D;&#x3D; -1 and inputTable[i].find(&quot;tmp&quot;) &#x3D;&#x3D; -1 and inputTable[i].find(&quot;temp&quot;) &#x3D;&#x3D; -1 :</span><br><span class="line">                input.append(inputTable[i] + &#39;,&#39; + appName + &quot;,&quot; + &quot;INPUT&quot;)</span><br><span class="line">        else:</span><br><span class="line">            input.append(inputTable[i][:inputTable[i].find(&#39; &#39;)] + &#39;,&#39; + appName + &quot;,&quot; + &quot;INPUT&quot;)</span><br><span class="line">    print(set(input))</span><br><span class="line"></span><br><span class="line">    #outputTable &#x3D; re.findall(r&quot;(saveToEs\s*\(\s*|valueOf\s*\(|OUTPUT_TABLE\s*,\s*|saveAsTable\s*\(\s*|insertInto\s*\(\s*)(.+)\)&quot;,line)</span><br><span class="line">    outputTable &#x3D; re.findall(</span><br><span class="line">        r&quot;(saveToEs\s*\(\s*|valueOf\s*\(|OUTPUT_TABLE\s*,\s*|saveAsTable\s*\(\s*|insertInto\s*\(\s*)(.+)\)&quot;, line)</span><br><span class="line">    out &#x3D; []</span><br><span class="line">    if(line.__contains__(&quot;syncEs&quot;)):</span><br><span class="line">        tmp &#x3D; re.findall(r&quot;syncEs\s*?[\s\S]*?\)&quot;, line)</span><br><span class="line">        outTmp&#x3D; list(set(tmp))</span><br><span class="line">        for j in range(len(outTmp)):</span><br><span class="line">            out_&#x3D;outTmp[j].find(&#39;&quot;&#39;)</span><br><span class="line">            if out_ !&#x3D; -1:</span><br><span class="line">                cin &#x3D; outTmp[j].split(&#39;&quot;&#39;)[1]</span><br><span class="line">                outputTable.append([&quot;syncEs&quot;, &#39;&quot;&#39;+cin+&#39;&quot;&#39;])</span><br><span class="line">    for i in range(len(outputTable)):</span><br><span class="line">        find_&#x3D;outputTable[i][1].find(&#39;&quot;&#39;)+1</span><br><span class="line">        if find_ &#x3D;&#x3D; 0:</span><br><span class="line">            regex &#x3D; outputTable[i][1] + r&quot;\s*&#x3D;\s*\&quot;(.+)\&quot;&quot;</span><br><span class="line">            if len(re.findall(regex, line)) !&#x3D; 0:</span><br><span class="line">                out.append(appName[0] + &quot;,&quot; + re.findall(regex, line)[0] + &quot;,OUTPUT&quot;)</span><br><span class="line">        else:</span><br><span class="line">            if (outputTable[i][1] !&#x3D; &quot;snapTable&quot;) and (outputTable[i][1].find(&quot;index&quot;)&#x3D;&#x3D;-1):</span><br><span class="line">                out.append(appName + &quot;,&quot; + outputTable[i][1][find_:outputTable[i][1].find(&#39;&quot;&#39;, 2)] + &quot;,OUTPUT&quot;)</span><br><span class="line">    print(set(out))</span><br><span class="line"></span><br><span class="line">    with open(&#39;E:&#x2F;relationship-analysis&#x2F;file&#x2F;relation.csv&#39;,mode&#x3D;&#39;w&#39;) as relation:</span><br><span class="line">        inputter &#x3D; list(set(input))</span><br><span class="line">        output &#x3D; list(set(out))</span><br><span class="line">        for i in range(len(inputter)):</span><br><span class="line">            relation.write(inputter[i])</span><br><span class="line">            relation.write(&quot;\n&quot;)</span><br><span class="line">        for i in range(len(output)):</span><br><span class="line">            relation.write(output[i])</span><br><span class="line">            relation.write(&quot;\n&quot;)</span><br></pre></td></tr></table></figure>


<h4 id="全自动-只要指定工程的目录-就能搜索相关作业的所有输入输出内容"><a href="#全自动-只要指定工程的目录-就能搜索相关作业的所有输入输出内容" class="headerlink" title="全自动,只要指定工程的目录,就能搜索相关作业的所有输入输出内容"></a>全自动,只要指定工程的目录,就能搜索相关作业的所有输入输出内容</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import re</span><br><span class="line">import os</span><br><span class="line">import uuid</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def writeTable(path, fileName):</span><br><span class="line">    arr &#x3D; []</span><br><span class="line">    f1 &#x3D; open(path, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    f2 &#x3D; open(path, encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line">    line &#x3D; f1.read()</span><br><span class="line">    for l in f2:</span><br><span class="line">        arr.append(l)</span><br><span class="line"></span><br><span class="line">    # appName</span><br><span class="line">    appName &#x3D; re.findall(r&quot;\.appName\(\&quot;(.+)\&quot;\)&quot;, line)</span><br><span class="line">    if(len(appName) &#x3D;&#x3D; 0):</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    # 输入表</span><br><span class="line">    inputTable &#x3D; re.findall(r&quot;[fF][rR][oO][mM]\s+(.+)&quot;, line)</span><br><span class="line">    input &#x3D; []</span><br><span class="line">    for i in range(len(inputTable)):</span><br><span class="line">        # print(inputTable[i])</span><br><span class="line">        inputTable[i] &#x3D; inputTable[i][0:inputTable[i].find(&#39;&quot;&#39;, 1)]</span><br><span class="line">        find_ &#x3D; inputTable[i].find(&#39; &#39;, 1) + 1</span><br><span class="line">        if find_ &#x3D;&#x3D; 0:</span><br><span class="line">            if inputTable[i].find(&quot;$&quot;) &#x3D;&#x3D; -1 and inputTable[i].find(&quot;tmp&quot;) &#x3D;&#x3D; -1 and inputTable[i].find(</span><br><span class="line">                    &quot;temp&quot;) &#x3D;&#x3D; -1:</span><br><span class="line">                input.append(inputTable[i] + &quot;,&quot; + appName[0] + &quot;,INPUT&quot;)</span><br><span class="line">        else:</span><br><span class="line">            if not str(inputTable[i][0:inputTable[i].find(&#39; &#39;, 1)]).__eq__(&#39;&quot;&#39;):</span><br><span class="line">                if inputTable[i].find(&quot;$&quot;) &#x3D;&#x3D; -1 and inputTable[i].find(&quot;tmp&quot;) &#x3D;&#x3D; -1 and inputTable[i].find(</span><br><span class="line">                        &quot;temp&quot;) &#x3D;&#x3D; -1:</span><br><span class="line">                    input.append(inputTable[i][0:inputTable[i].find(&#39; &#39;, 1)] + &quot;,&quot; + appName[0] + &quot;,INPUT&quot;)</span><br><span class="line">    input &#x3D; list(set(input))</span><br><span class="line"></span><br><span class="line">    # 输出表</span><br><span class="line">    out &#x3D; []</span><br><span class="line">    outputTable &#x3D; re.findall(</span><br><span class="line">        r&quot;(saveToEs\s*\(\s*|valueOf\s*\(|OUTPUT_TABLE\s*,\s*|saveAsTable\s*\(\s*|insertInto\s*\(\s*)(.+)\)&quot;,</span><br><span class="line">        line)</span><br><span class="line">    if (line.__contains__(&quot;syncEs&quot;)):</span><br><span class="line">        tmp &#x3D; re.findall(r&quot;syncEs\s*?[\s\S]*?\)&quot;, line)</span><br><span class="line">        outTmp &#x3D; list(set(tmp))</span><br><span class="line">        for j in range(len(outTmp)):</span><br><span class="line">            out_ &#x3D; outTmp[j].find(&#39;&quot;&#39;)</span><br><span class="line">            if out_ !&#x3D; -1:</span><br><span class="line">                cin &#x3D; outTmp[j].split(&#39;&quot;&#39;)[1]</span><br><span class="line">                outputTable.append([&quot;syncEs&quot;, &#39;&quot;&#39; + cin + &#39;&quot;&#39;])</span><br><span class="line">    for i in range(len(outputTable)):</span><br><span class="line">        find_ &#x3D; outputTable[i][1].find(&#39;&quot;&#39;) + 1</span><br><span class="line">        if find_ &#x3D;&#x3D; 0:</span><br><span class="line">            regex &#x3D; outputTable[i][1] + r&quot;\s*&#x3D;\s*\&quot;(.+)\&quot;&quot;</span><br><span class="line">            if len(re.findall(regex, line)) !&#x3D; 0:</span><br><span class="line">                out.append(appName[0] + &quot;,&quot; + re.findall(regex, line)[0] + &quot;,OUTPUT&quot;)</span><br><span class="line">        else:</span><br><span class="line">            if (outputTable[i][1] !&#x3D; &quot;snapTable&quot;) and (outputTable[i][1].find(&quot;index&quot;) &#x3D;&#x3D; -1):</span><br><span class="line">                out.append(appName[0] + &quot;,&quot; + outputTable[i][1][find_:outputTable[i][1].find(&#39;&quot;&#39;, 2)] + &quot;,OUTPUT&quot;)</span><br><span class="line">    out &#x3D; list(set(out))</span><br><span class="line"></span><br><span class="line">    with open(&#39;E:\\relationship-analysis\\file\\&#39; + fileName + &#39;.csv&#39;, mode&#x3D;&#39;a&#39;) as relation:</span><br><span class="line">        for i in range(len(input)):</span><br><span class="line">            relation.write(input[i])</span><br><span class="line">            relation.write(&quot;\n&quot;)</span><br><span class="line">        for i in range(len(out)):</span><br><span class="line">            relation.write(out[i])</span><br><span class="line">            relation.write(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    target_dir &#x3D; &quot;E:\\spark2\\&quot;</span><br><span class="line">    fileName &#x3D; str(&quot;relation-&quot; + time.strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, time.localtime()))</span><br><span class="line">    for root, dirs, files in os.walk(target_dir):</span><br><span class="line">        for name in files:</span><br><span class="line">            if name.endswith(&quot;.scala&quot;):</span><br><span class="line">                print(os.path.join(root, name))</span><br><span class="line">                writeTable(os.path.join(root, name), fileName)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Neo4j图形数据库实操</title>
    <url>/2019/10/08/Neo4j%E5%9B%BE%E5%BD%A2%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E6%93%8D/</url>
    <content><![CDATA[<blockquote>
<p>针对于知识图谱项目进行neo4j实际操作</p>
</blockquote>
<span id="more"></span>

<hr>
<h2 id="推荐网站"><a href="#推荐网站" class="headerlink" title="推荐网站"></a>推荐网站</h2><p><a href="http://www.actkg.com/">北航知行中文图谱管理系统</a><br><a href="http://kw.fudan.edu.cn/cndbpedia/search/">复旦CN-DBpedia</a><br><a href="http://kw.fudan.edu.cn/cnprobase/search/">复旦CN-Probase</a><br><a href="http://openkg.cn/home">中文开放知识图谱OpenKG</a><br><a href="http://www.bigcilin.com/WSDTest/?q=">大词林</a></p>
<hr>
<h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h2><p>谈到图形数据库,最直接的就是知识图谱,一个数据大屏,上面是各个实体和各个实体的连接.我个人理解,建立一个知识图谱需要的数据有三个,一是实体,实体有属性与属性值;二是关系,表示实体与实体间的关系;三是三元组,用来将实体连接成知识图谱.</p>
<p>Nodes[node01,node02,node03]<br>Links[link01,link02]</p>
<p>node01[id,neoId,category,name]<br>link01[id,source,target,name]</p>
<p>通过这些数据,我们就可以得到一张知识图谱</p>
<hr>
<h2 id="Neo4j使用"><a href="#Neo4j使用" class="headerlink" title="Neo4j使用"></a>Neo4j使用</h2><h3 id="CREATE命令"><a href="#CREATE命令" class="headerlink" title="CREATE命令"></a>CREATE命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># CREATE命令语法</span><br><span class="line">CREATE (&lt;node-name&gt;:&lt;label-name&gt;)</span><br><span class="line"></span><br><span class="line">CREATE (emp:Employee)</span><br><span class="line">CREATE (dept:Dept)</span><br><span class="line"></span><br><span class="line"># 创建具有属性的节点</span><br><span class="line">CREATE (</span><br><span class="line">   &lt;node-name&gt;:&lt;label-name&gt;</span><br><span class="line">   &#123; 	</span><br><span class="line">      &lt;Property1-name&gt;:&lt;Property1-Value&gt;</span><br><span class="line">      ........</span><br><span class="line">      &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt;</span><br><span class="line">   &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">CREATE (emp:Employee&#123;id:123,name:&quot;Lokesh&quot;,sal:35000,deptno:10&#125;)</span><br><span class="line">CREATE (dept:Dept &#123; deptno:10,dname:&quot;Accounting&quot;,location:&quot;Hyderabad&quot; &#125;)</span><br></pre></td></tr></table></figure>

<h3 id="MATCH命令-lt-不单独使用-gt"><a href="#MATCH命令-lt-不单独使用-gt" class="headerlink" title="MATCH命令&lt;不单独使用&gt;"></a>MATCH命令&lt;不单独使用&gt;</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># MATCH命令语法</span><br><span class="line">MATCH </span><br><span class="line">(</span><br><span class="line">   &lt;node-name&gt;:&lt;label-name&gt;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 检索数据</span><br><span class="line">MATCH (dept:Dept)</span><br></pre></td></tr></table></figure>

<h3 id="RETURN命令-lt-不单独使用-gt"><a href="#RETURN命令-lt-不单独使用-gt" class="headerlink" title="RETURN命令&lt;不单独使用&gt;"></a>RETURN命令&lt;不单独使用&gt;</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># RETURN命令语法</span><br><span class="line">RETURN </span><br><span class="line">   &lt;node-name&gt;.&lt;property1-name&gt;,</span><br><span class="line">   ........</span><br><span class="line">   &lt;node-name&gt;.&lt;propertyn-name&gt;</span><br><span class="line"></span><br><span class="line"># 检索节点和关联关系的所有属性</span><br><span class="line">RETURN dept.deptno</span><br></pre></td></tr></table></figure>

<h3 id="MATCH-amp-RETURN匹配和返回"><a href="#MATCH-amp-RETURN匹配和返回" class="headerlink" title="MATCH &amp; RETURN匹配和返回"></a>MATCH &amp; RETURN匹配和返回</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># MATCH RETURN命令语法</span><br><span class="line">MATCH Command</span><br><span class="line">RETURN Command</span><br><span class="line"></span><br><span class="line"># 匹配和返回</span><br><span class="line">MATCH (dept: Dept)</span><br><span class="line">RETURN dept</span><br><span class="line"></span><br><span class="line">MATCH (dept: Dept)</span><br><span class="line">RETURN dept.deptno,dept.dname</span><br></pre></td></tr></table></figure>

<h3 id="CREATE-MATCH-RETURN命令"><a href="#CREATE-MATCH-RETURN命令" class="headerlink" title="CREATE+MATCH+RETURN命令"></a>CREATE+MATCH+RETURN命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建</span><br><span class="line">CREATE (e:Customer&#123;id:&quot;1001&quot;,name:&quot;Abc&quot;,dob:&quot;01&#x2F;10&#x2F;1982&quot;&#125;)</span><br><span class="line">CREATE (cc:CreditCard&#123;id:&quot;5001&quot;,number:&quot;1234567890&quot;,cvv:&quot;888&quot;,expiredate:&quot;20&#x2F;17&quot;&#125;)</span><br><span class="line"></span><br><span class="line"># 查看</span><br><span class="line">MATCH (e:Customer)</span><br><span class="line">RETURN e.id,e.name,e.dob</span><br><span class="line"></span><br><span class="line">MATCH (cc:CreditCard)</span><br><span class="line">RETURN cc.id,cc.number,cc.cvv,cc.expiredate</span><br></pre></td></tr></table></figure>

<h3 id="关系基础"><a href="#关系基础" class="headerlink" title="关系基础"></a>关系基础</h3><h4 id="没有属性的关系"><a href="#没有属性的关系" class="headerlink" title="没有属性的关系"></a>没有属性的关系</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">MATCH (&lt;node1-label-name&gt;:&lt;node1-name&gt;),(&lt;node2-label-name&gt;:&lt;node2-name&gt;)</span><br><span class="line">CREATE  </span><br><span class="line">	(&lt;node1-label-name&gt;)-[&lt;relationship-label-name&gt;:&lt;relationship-name&gt;]-&gt;(&lt;node2-label-name&gt;)</span><br><span class="line">RETURN &lt;relationship-label-name&gt;</span><br><span class="line"></span><br><span class="line"># 验证节点</span><br><span class="line">MATCH (e:Customer) </span><br><span class="line">RETURN e</span><br><span class="line"></span><br><span class="line">MATCH (cc:CreditCard) </span><br><span class="line">RETURN cc</span><br><span class="line"></span><br><span class="line"># 创建关系</span><br><span class="line">MATCH (e:Customer),(cc:CreditCard) </span><br><span class="line">CREATE (e)-[r:DO_SHOPPING_WITH ]-&gt;(cc) </span><br><span class="line"></span><br><span class="line"># 查看关系</span><br><span class="line">MATCH (e)-[r:DO_SHOPPING_WITH ]-&gt;(cc) </span><br><span class="line">RETURN r</span><br><span class="line"></span><br><span class="line"># 使用新节点创建</span><br><span class="line">CREATE (fb1:FaceBookProfile1)-[like:LIKES]-&gt;(fb2:FaceBookProfile2) </span><br><span class="line">RETURN like</span><br><span class="line"></span><br><span class="line"># 如果再进行相反的创建操作,则会得到一个双向关系</span><br></pre></td></tr></table></figure>
<h4 id="有属性的关系"><a href="#有属性的关系" class="headerlink" title="有属性的关系"></a>有属性的关系</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">MATCH (&lt;node1-label-name&gt;:&lt;node1-name&gt;),(&lt;node2-label-name&gt;:&lt;node2-name&gt;)</span><br><span class="line">CREATE  </span><br><span class="line">	(&lt;node1-label-name&gt;)-[&lt;relationship-label-name&gt;:&lt;relationship-name&gt;</span><br><span class="line">	&#123;&lt;define-properties-list&gt;&#125;]-&gt;(&lt;node2-label-name&gt;)</span><br><span class="line">RETURN &lt;relationship-label-name&gt;</span><br><span class="line"></span><br><span class="line"># 验证</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"># 创建关系</span><br><span class="line">MATCH (e:Customer),(cc:CreditCard) </span><br><span class="line">CREATE (e)-[r:DO_SHOPPING_WITH&#123;shopdate:&quot;12&#x2F;12&#x2F;2014&quot;,price:55000&#125;]-&gt;(cc) </span><br><span class="line">RETURN r</span><br><span class="line"></span><br><span class="line"># 使用新节点创建</span><br><span class="line">CREATE (video1:YoutubeVideo1&#123;title:&quot;Action Movie1&quot;,updated_by:&quot;Abc&quot;,uploaded_date:&quot;10&#x2F;10&#x2F;2010&quot;&#125;)</span><br><span class="line">-[movie:ACTION_MOVIES&#123;rating:1&#125;]-&gt;</span><br><span class="line">(video2:YoutubeVideo2&#123;title:&quot;Action Movie2&quot;,updated_by:&quot;Xyz&quot;,uploaded_date:&quot;12&#x2F;12&#x2F;2012&quot;&#125;) </span><br><span class="line">RETURN movie</span><br></pre></td></tr></table></figure>

<h3 id="CREATE创建标签"><a href="#CREATE创建标签" class="headerlink" title="CREATE创建标签"></a>CREATE创建标签</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 单个标签到节点</span><br><span class="line">CREATE (&lt;node-name&gt;:&lt;label-name&gt;)</span><br><span class="line">CREATE (google1:GooglePlusProfile)</span><br><span class="line"></span><br><span class="line"># 多个标签到节点</span><br><span class="line">CREATE (&lt;node-name&gt;:&lt;label-name1&gt;:&lt;label-name2&gt;.....:&lt;label-namen&gt;)</span><br><span class="line">CREATE (m:Movie:Cinema:Film:Picture)</span><br><span class="line"></span><br><span class="line"># 单个标签到关系</span><br><span class="line">CREATE (&lt;node1-name&gt;:&lt;label1-name&gt;)-</span><br><span class="line">	[(&lt;relationship-name&gt;:&lt;relationship-label-name&gt;)]</span><br><span class="line">	-&gt;(&lt;node2-name&gt;:&lt;label2-name&gt;)</span><br><span class="line">CREATE (p1:Profile1)-[r1:LIKES]-&gt;(p2:Profile2)</span><br></pre></td></tr></table></figure>

<h3 id="WHERE子句"><a href="#WHERE子句" class="headerlink" title="WHERE子句"></a>WHERE子句</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">WHERE &lt;condition&gt;</span><br><span class="line">WHERE &lt;condition&gt; &lt;boolean-operator&gt; &lt;condition&gt;</span><br><span class="line"># &lt;condition&gt;语法</span><br><span class="line">&lt;property-name&gt; &lt;comparison-operator&gt; &lt;value&gt;</span><br><span class="line"></span><br><span class="line"># 布尔运算符</span><br><span class="line">AND OR NOT XOR</span><br><span class="line"></span><br><span class="line"># 比较运算符</span><br><span class="line">&#x3D; &lt;&gt; &lt; &gt; &lt;&#x3D; &gt;&#x3D;</span><br><span class="line"></span><br><span class="line"># 匹配</span><br><span class="line">MATCH (emp:Employee) </span><br><span class="line">WHERE emp.name &#x3D; &#39;Abc&#39;</span><br><span class="line">RETURN emp</span><br><span class="line"></span><br><span class="line"># 使用WHERE子句创建关系</span><br><span class="line">MATCH (&lt;node1-label-name&gt;:&lt;node1-name&gt;),(&lt;node2-label-name&gt;:&lt;node2-name&gt;) </span><br><span class="line">WHERE &lt;condition&gt;</span><br><span class="line">CREATE (&lt;node1-label-name&gt;)-[&lt;relationship-label-name&gt;:&lt;relationship-name&gt;</span><br><span class="line">       &#123;&lt;relationship-properties&gt;&#125;]-&gt;(&lt;node2-label-name&gt;) </span><br><span class="line">       </span><br><span class="line">MATCH (cust:Customer),(cc:CreditCard) </span><br><span class="line">WHERE cust.id &#x3D; &quot;1001&quot; AND cc.id&#x3D; &quot;5001&quot; </span><br><span class="line">CREATE (cust)-[r:DO_SHOPPING_WITH&#123;shopdate:&quot;12&#x2F;12&#x2F;2014&quot;,price:55000&#125;]-&gt;(cc) </span><br><span class="line">RETURN r</span><br></pre></td></tr></table></figure>

<h3 id="DELETE删除-lt-节点和关系-gt"><a href="#DELETE删除-lt-节点和关系-gt" class="headerlink" title="DELETE删除&lt;节点和关系&gt;"></a>DELETE删除&lt;节点和关系&gt;</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># DELETE节点子句语法</span><br><span class="line">DELETE &lt;node-name-list&gt;</span><br><span class="line">MATCH (e: Employee) DELETE e</span><br><span class="line"></span><br><span class="line"># DELETE节点和关系子句语法</span><br><span class="line">DELETE &lt;node1-name&gt;,&lt;node2-name&gt;,&lt;relationship-name&gt;</span><br><span class="line">MATCH (cc: CreditCard)-[rel]-(c:Customer) </span><br><span class="line">DELETE cc,c,rel</span><br></pre></td></tr></table></figure>

<h3 id="REMOVE删除-lt-标签和属性-gt"><a href="#REMOVE删除-lt-标签和属性-gt" class="headerlink" title="REMOVE删除&lt;标签和属性&gt;"></a>REMOVE删除&lt;标签和属性&gt;</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># REMOVE属性子句语法</span><br><span class="line">REMOVE &lt;property-name-list&gt;</span><br><span class="line"></span><br><span class="line"># 创建</span><br><span class="line">CREATE (book:Book &#123;id:122,title:&quot;Neo4j Tutorial&quot;,pages:340,price:250&#125;) </span><br><span class="line"></span><br><span class="line"># 删除</span><br><span class="line">MATCH (book &#123; id:122 &#125;)</span><br><span class="line">REMOVE book.price</span><br><span class="line">RETURN book</span><br><span class="line"></span><br><span class="line"># REMOVE一个Label子句语法</span><br><span class="line">REMOVE &lt;label-name-list&gt; </span><br><span class="line"></span><br><span class="line"># 删除</span><br><span class="line">MATCH (m:Movie) </span><br><span class="line">REMOVE m:Picture</span><br></pre></td></tr></table></figure>

<h3 id="SET子句"><a href="#SET子句" class="headerlink" title="SET子句"></a>SET子句</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 语法</span><br><span class="line">SET  &lt;property-name-list&gt;</span><br><span class="line"></span><br><span class="line">MATCH (dc:DebitCard)</span><br><span class="line">SET dc.atm_pin &#x3D; 3456</span><br><span class="line">RETURN dc</span><br></pre></td></tr></table></figure>

<h3 id="Sorting排序"><a href="#Sorting排序" class="headerlink" title="Sorting排序"></a>Sorting排序</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ORDER BY子句语法</span><br><span class="line">ORDER BY  &lt;property-name-list&gt;  [DESC]	 </span><br><span class="line"></span><br><span class="line">MATCH (emp:Employee)</span><br><span class="line">RETURN emp.empid,emp.name,emp.salary,emp.deptno</span><br><span class="line">ORDER BY emp.name</span><br><span class="line"></span><br><span class="line">MATCH (emp:Employee)</span><br><span class="line">RETURN emp.empid,emp.name,emp.salary,emp.deptno</span><br><span class="line">ORDER BY emp.name DESC</span><br></pre></td></tr></table></figure>

<h3 id="UNION合并"><a href="#UNION合并" class="headerlink" title="UNION合并"></a>UNION合并</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># UNION语法</span><br><span class="line">&lt;MATCH Command1&gt;</span><br><span class="line">UNION</span><br><span class="line">&lt;MATCH Command2&gt;</span><br><span class="line"></span><br><span class="line">MATCH (cc:CreditCard)</span><br><span class="line">RETURN cc.id as id,cc.number as number,cc.name as name,</span><br><span class="line">   cc.valid_from as valid_from,cc.valid_to as valid_to</span><br><span class="line">UNION</span><br><span class="line">MATCH (dc:DebitCard)</span><br><span class="line">RETURN dc.id as id,dc.number as number,dc.name as name,</span><br><span class="line">   dc.valid_from as valid_from,dc.valid_to as valid_to</span><br><span class="line">   </span><br><span class="line"># UNION ALL语法</span><br><span class="line">&lt;MATCH Command1&gt;</span><br><span class="line">UNION ALL</span><br><span class="line">&lt;MATCH Command2&gt;</span><br><span class="line"></span><br><span class="line">MATCH (cc:CreditCard)</span><br><span class="line">RETURN cc.id as id,cc.number as number,cc.name as name,</span><br><span class="line">   cc.valid_from as valid_from,cc.valid_to as valid_to</span><br><span class="line">UNION ALL</span><br><span class="line">MATCH (dc:DebitCard)</span><br><span class="line">RETURN dc.id as id,dc.number as number,dc.name as name,</span><br><span class="line">   dc.valid_from as valid_from,dc.valid_to as valid_to</span><br></pre></td></tr></table></figure>

<h3 id="LIMIT和SKIP子句"><a href="#LIMIT和SKIP子句" class="headerlink" title="LIMIT和SKIP子句"></a>LIMIT和SKIP子句</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 底部</span><br><span class="line">MATCH (emp:Employee) </span><br><span class="line">RETURN emp</span><br><span class="line">LIMIT 2</span><br><span class="line"></span><br><span class="line"># 顶部</span><br><span class="line">MATCH (emp:Employee) </span><br><span class="line">RETURN emp</span><br><span class="line">SKIP 2</span><br></pre></td></tr></table></figure>

<h3 id="MERGE命令"><a href="#MERGE命令" class="headerlink" title="MERGE命令"></a>MERGE命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MERGE &#x3D; CREATE + MATCH</span><br><span class="line"></span><br><span class="line"># MERGE语法</span><br><span class="line">MERGE (&lt;node-name&gt;:&lt;label-name&gt;</span><br><span class="line">&#123;</span><br><span class="line">   &lt;Property1-name&gt;:&lt;Pro&lt;rty1-Value&gt;</span><br><span class="line">   .....</span><br><span class="line">   &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt;</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"># 只有不存在时才会添加</span><br><span class="line">MERGE (gp2:GoogleProfile2&#123; Id: 201402,Name:&quot;Nokia&quot;&#125;)</span><br></pre></td></tr></table></figure>

<h3 id="NULL值"><a href="#NULL值" class="headerlink" title="NULL值"></a>NULL值</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MATCH (e:Employee) </span><br><span class="line">WHERE e.id IS NOT NULL</span><br><span class="line">RETURN e.id,e.name,e.sal,e.deptno</span><br><span class="line"></span><br><span class="line">MATCH (e:Employee) </span><br><span class="line">WHERE e.id IS NULL</span><br><span class="line">RETURN e.id,e.name,e.sal,e.deptno</span><br></pre></td></tr></table></figure>

<h3 id="IN操作符"><a href="#IN操作符" class="headerlink" title="IN操作符"></a>IN操作符</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MATCH (e:Employee) </span><br><span class="line">WHERE e.id IN [123,124]</span><br><span class="line">RETURN e.id,e.name,e.sal,e.deptno</span><br></pre></td></tr></table></figure>

<h3 id="ID属性"><a href="#ID属性" class="headerlink" title="ID属性"></a>ID属性</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ID是节点和关系的默认内部属性</span><br><span class="line"># 节点的ID属性的最大值约为35亿</span><br><span class="line"># ID的最大值关系的属性的大约35亿</span><br></pre></td></tr></table></figure>

<h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 大写</span><br><span class="line">MATCH (e:Employee) </span><br><span class="line">RETURN e.id,UPPER(e.name),e.sal,e.deptno</span><br><span class="line"># 小写</span><br><span class="line">MATCH (e:Employee) </span><br><span class="line">RETURN e.id,LOWER(e.name),e.sal,e.deptno</span><br><span class="line"># 切割&lt;右闭&gt;</span><br><span class="line">MATCH (e:Employee) </span><br><span class="line">RETURN e.id,SUBSTRING(e.name,0,2),e.sal,e.deptno</span><br></pre></td></tr></table></figure>

<h3 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># COUNT</span><br><span class="line">MATCH (e:Employee) RETURN COUNT(*)</span><br><span class="line"># MAX &amp; MIN</span><br><span class="line">MATCH (e:Employee) </span><br><span class="line">RETURN MAX(e.sal),MIN(e.sal)</span><br><span class="line"># AVG &amp; SUM</span><br><span class="line">MATCH (e:Employee) </span><br><span class="line">RETURN SUM(e.sal),AVG(e.sal)</span><br></pre></td></tr></table></figure>

<h3 id="关系函数"><a href="#关系函数" class="headerlink" title="关系函数"></a>关系函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 检索开始节点</span><br><span class="line">MATCH (a)-[movie:ACTION_MOVIES]-&gt;(b) </span><br><span class="line">RETURN STARTNODE(movie)</span><br><span class="line"># 检索结束节点</span><br><span class="line">MATCH (a)-[movie:ACTION_MOVIES]-&gt;(b) </span><br><span class="line">RETURN ENDNODE(movie)</span><br><span class="line"># 检索关系的ID和类型详细信息</span><br><span class="line">MATCH (a)-[movie:ACTION_MOVIES]-&gt;(b) </span><br><span class="line">RETURN ID(movie),TYPE(movie)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title>NodeJS实现发送信息到Kafka</title>
    <url>/2019/12/17/NodeJS%E5%AE%9E%E7%8E%B0%E5%8F%91%E9%80%81%E4%BF%A1%E6%81%AF%E5%88%B0Kafka/</url>
    <content><![CDATA[<blockquote>
<p>利用nodejs实现模拟发送数据到kafka</p>
</blockquote>
<span id="more"></span>

<h2 id="需要依赖"><a href="#需要依赖" class="headerlink" title="需要依赖"></a>需要依赖</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nodejs的kafka-node以及mockjs</span><br><span class="line">npm install -g kafka-node</span><br><span class="line">npm install -g mockjs</span><br><span class="line"></span><br><span class="line">第一次启动时,如果kafka中没有topic会报错,再启动一次就可以了</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="直接上代码"><a href="#直接上代码" class="headerlink" title="直接上代码"></a>直接上代码</h2><figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// vi kafka.js</span></span><br><span class="line"><span class="keyword">var</span> kafka = <span class="built_in">require</span>(<span class="string">&#x27;kafka-node&#x27;</span>);</span><br><span class="line"><span class="keyword">var</span> Mock = <span class="built_in">require</span>(<span class="string">&#x27;mockjs&#x27;</span>);</span><br><span class="line"><span class="keyword">const</span> Random = Mock.Random;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> conn = &#123; <span class="string">&#x27;kafkaHost&#x27;</span>: <span class="string">&#x27;hadoop01:9092&#x27;</span> &#125;;</span><br><span class="line"><span class="keyword">var</span> MQ = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="built_in">this</span>.mq_producers = &#123;&#125;;</span><br><span class="line">    <span class="built_in">this</span>.client = &#123;&#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MQ.prototype.AddProducer = <span class="function"><span class="keyword">function</span> (<span class="params">conn, handler</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">&#x27;增加生产者&#x27;</span>, conn, <span class="built_in">this</span>);</span><br><span class="line">    <span class="built_in">this</span>.client = <span class="keyword">new</span> kafka.KafkaClient(conn);</span><br><span class="line">    <span class="keyword">let</span> producer = <span class="keyword">new</span> kafka.Producer(<span class="built_in">this</span>.client);</span><br><span class="line"></span><br><span class="line">    producer.on(<span class="string">&#x27;ready&#x27;</span>, <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!!handler) &#123;</span><br><span class="line">            handler(producer);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    producer.on(<span class="string">&#x27;error&#x27;</span>, <span class="function"><span class="keyword">function</span> (<span class="params">err</span>) </span>&#123;</span><br><span class="line">        <span class="built_in">console</span>.error(<span class="string">&#x27;producer error &#x27;</span>, err.stack);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">this</span>.mq_producers[<span class="string">&#x27;common&#x27;</span>] = producer;</span><br><span class="line">    <span class="keyword">return</span> producer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(MQ);</span><br><span class="line"><span class="keyword">var</span> mq = <span class="keyword">new</span> MQ();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//topic 名称写入时候，会先创建topic，如果不存在的话</span></span><br><span class="line"><span class="keyword">var</span> topicName = <span class="string">&quot;test01&quot;</span></span><br><span class="line"><span class="keyword">var</span> datajson =  &#123;</span><br><span class="line">    <span class="string">&quot;business&quot;</span>: <span class="string">&quot;sdasf&quot;</span>,</span><br><span class="line">    <span class="string">&quot;database&quot;</span>: <span class="string">&quot;sqweqr&quot;</span>,</span><br><span class="line">    <span class="string">&quot;es&quot;</span>: <span class="number">2314</span>,</span><br><span class="line">    <span class="string">&quot;sql&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">    <span class="string">&quot;table&quot;</span>: <span class="string">&quot;t_cash_loan&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ts&quot;</span>: <span class="number">1576050001925</span>,</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;UPDATE&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mq.AddProducer(conn, <span class="function"><span class="keyword">function</span> (<span class="params">producer</span>) </span>&#123;</span><br><span class="line">    producer.createTopics([topicName], <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">        <span class="built_in">setInterval</span>(<span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">            <span class="comment">//只需要改这开就可以了，了解mockjs的数据用法</span></span><br><span class="line">            <span class="keyword">let</span> data = Mock.mock(datajson)</span><br><span class="line">            <span class="keyword">let</span> msg = <span class="built_in">JSON</span>.stringify(data)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">var</span> _msg = &#123;</span><br><span class="line">                topic: [topicName],</span><br><span class="line">                messages: msg</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// console.log(&#x27;clientId : &#x27;,mq.client.clientId);</span></span><br><span class="line">            <span class="comment">// console.log(&#x27;topicMetadata &#x27;,mq.client.topicMetadata);</span></span><br><span class="line">            <span class="comment">// console.log(&#x27;brokerMetadata &#x27;,mq.client.brokerMetadata);</span></span><br><span class="line">            <span class="comment">// console.log(&#x27;clusterMetadata &#x27;,mq.client.clusterMetadata);</span></span><br><span class="line">            <span class="comment">// console.log(&#x27;brokerMetadataLastUpdate &#x27;,mq.client.brokerMetadataLastUpdate);</span></span><br><span class="line">            mq.mq_producers[<span class="string">&#x27;common&#x27;</span>].send([_msg], <span class="function"><span class="keyword">function</span> (<span class="params">err, data</span>) </span>&#123;</span><br><span class="line">                <span class="built_in">console</span>.log(<span class="string">&quot;send you can check \n kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic &quot;</span>+topicName+<span class="string">&quot; --from-beginning \n&quot;</span>, data);</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;, <span class="number">2000</span>);</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">node kafka.js</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell小知识记录</title>
    <url>/2018/11/28/Shell%E5%B0%8F%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>记录一下日常脚本的使用</p>
</blockquote>
<span id="more"></span>

<h2 id="的使用"><a href="#的使用" class="headerlink" title="=~的使用"></a>=~的使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 正则匹配,用来判断左侧的参数是否符合右边的规则</span><br><span class="line">$: 以什么结尾</span><br><span class="line">^: 以什么开头</span><br><span class="line">例子: 输出&#x2F;root&#x2F;目录下以.jar结尾的文件</span><br><span class="line">for row in &#96;ls -l &#x2F;root&#x2F; | awk &#39;&#123;print $9&#125;&#39;&#96;; do</span><br><span class="line">	if [[ &quot;$row&quot; &#x3D;~ \.jar$ ]]; then</span><br><span class="line">		echo &quot;$row&quot;</span><br><span class="line">	fi</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="readlink的使用"><a href="#readlink的使用" class="headerlink" title="readlink的使用"></a>readlink的使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 直接输出java脚本的真正位置</span><br><span class="line">readlink -f &#x2F;usr&#x2F;bin&#x2F;java</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="判断进程是否存在-不在则启动"><a href="#判断进程是否存在-不在则启动" class="headerlink" title="判断进程是否存在,不在则启动"></a>判断进程是否存在,不在则启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PIDS&#x3D;&#96;ps -ef |grep command |grep -v grep | awk &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">if [ &quot;$PIDS&quot; !&#x3D; &quot;&quot; ]; then</span><br><span class="line">    echo &quot;command is runing!&quot;</span><br><span class="line">else</span><br><span class="line">    cd &#x2F;root&#x2F;</span><br><span class="line">    .&#x2F;script</span><br><span class="line">    #运行进程</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>SBT的使用</title>
    <url>/2019/08/19/SBT%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>IDEA与Scala的安装不进行细说，主要记录下Sbt的安装过程</p>
</blockquote>
<span id="more"></span>

<h1 id="项目开发环境"><a href="#项目开发环境" class="headerlink" title="项目开发环境"></a>项目开发环境</h1><h2 id="IDEA-Scala-Sbt"><a href="#IDEA-Scala-Sbt" class="headerlink" title="IDEA+Scala+Sbt"></a>IDEA+Scala+Sbt</h2><ul>
<li><p>去官网下载Sbt.msi，我的是1.2.8，傻瓜式安装，安装目录路径不能有空格</p>
</li>
<li><p>安装成功后，进入安装目录的 conf/ 文件夹</p>
</li>
<li><p>编辑sbtconfig.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-Dsbt.global.base&#x3D;安装目录&#x2F;.sbt</span><br><span class="line">-Dsbt.repository.config&#x3D;安装目录&#x2F;repositories</span><br><span class="line">-Dsbt.boot.directory&#x3D;安装目录&#x2F;.sbt&#x2F;boot</span><br><span class="line">-Dsbt.ivy.home&#x3D;安装目录&#x2F;.ivy2</span><br></pre></td></tr></table></figure></li>
<li><p>创建repositories文件</p>
</li>
<li><p>编辑repositories</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[repositories]</span><br><span class="line">local</span><br><span class="line">maven-local: file:&#x2F;&#x2F;&#x2F;&#x2F;D:&#x2F;.m2&#x2F;repository&#x2F;</span><br><span class="line">maven-repo1: http:&#x2F;&#x2F;nexus.dev.com&#x2F;repository&#x2F;maven-public&#x2F;</span><br></pre></td></tr></table></figure>
</li>
<li><p>IDEA设置sbt的Launcher为Custom，不使用IDEA自带的sbt</p>
</li>
<li><p>IDEA设置sbt的VM parameters,目录路径有空格则会报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-Dsbt.global.base&#x3D;安装目录&#x2F;.sbt</span><br><span class="line">-Dsbt.repository.config&#x3D;安装目录&#x2F;repositories</span><br><span class="line">-Dsbt.boot.directory&#x3D;安装目录&#x2F;.sbt&#x2F;boot</span><br><span class="line">-Dsbt.ivy.home&#x3D;安装目录&#x2F;.ivy2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>注意</strong> 因为第一次执行Sbt时是需要联网下载依赖的，如果本地仓库比较全面当然没有问题；<br>如果本地仓库没有，需要准备一台能联网的机器，repositories文件配置国内镜像，可以提高速度；<br>然后拷贝.ivy2和.sbt文件夹。</p>
<p><strong>镜像</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">my-maven-repo01: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;apache-snapshots</span><br><span class="line">my-maven-repo02: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;central</span><br><span class="line">my-maven-repo03: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;google</span><br><span class="line">my-maven-repo04: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;gradle-plugin</span><br><span class="line">my-maven-repo05: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;jcenter</span><br><span class="line">my-maven-repo06: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;spring</span><br><span class="line">my-maven-repo07: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;spring-plugin</span><br><span class="line">my-maven-repo08: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;public</span><br><span class="line">my-maven-repo09: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;releases</span><br><span class="line">my-maven-repo10: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;snapshots</span><br><span class="line">my-maven-repo11: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;grails-core</span><br><span class="line">my-maven-repo12: https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;mapr-public</span><br><span class="line">my-maven-repo13: https:&#x2F;&#x2F;repo.typesafe.com&#x2F;typesafe&#x2F;ivy-releases&#x2F;</span><br><span class="line">my-maven-repo14: https:&#x2F;&#x2F;repo.scala-sbt.org&#x2F;scalasbt&#x2F;sbt-plugin-releases&#x2F;</span><br><span class="line">my-maven-repo15: https:&#x2F;&#x2F;oss.sonatype.org&#x2F;content&#x2F;repositories&#x2F;releases&#x2F;</span><br><span class="line">my-maven-repo16: https:&#x2F;&#x2F;oss.sonatype.org&#x2F;content&#x2F;repositories&#x2F;snapshots&#x2F;</span><br></pre></td></tr></table></figure>

<h2 id="Sbt的基本使用"><a href="#Sbt的基本使用" class="headerlink" title="Sbt的基本使用"></a>Sbt的基本使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动项目</span><br><span class="line">sbt projectID&#x2F;run</span><br><span class="line"></span><br><span class="line"># 打包项目</span><br><span class="line">sbt projectID&#x2F;assembly</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong> 使用IDEA的Run进行作业,需要先对项目进行编译,也就是先执行sbt projectID/run</p>
]]></content>
      <categories>
        <category>编译</category>
      </categories>
      <tags>
        <tag>sbt</tag>
      </tags>
  </entry>
  <entry>
    <title>ROOT没有操作HDFS的权限问题</title>
    <url>/2017/01/13/ROOT%E6%B2%A1%E6%9C%89%E6%93%8D%E4%BD%9CHDFS%E7%9A%84%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>没有权限操作hdfs</p>
</blockquote>
<span id="more"></span>

<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 不能su hdfs的话,修改&#x2F;etc&#x2F;passwd中hdfs对应的&#x2F;sbin&#x2F;nologin,改为&#x2F;bin&#x2F;bash</span><br><span class="line">su - hdfs</span><br><span class="line">hdfs dfs -chown -R root:hdfs &#x2F;</span><br><span class="line"># 就可以了</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>cdh</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell脚本监控Flink作业状态指标</title>
    <url>/2020/08/10/Shell%E8%84%9A%E6%9C%AC%E7%9B%91%E6%8E%A7Flink%E4%BD%9C%E4%B8%9A%E7%8A%B6%E6%80%81%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<blockquote>
<p>最基础的指标为作业状态相关的,如作业是否出故障,作业是否存活,作业是否稳定运行</p>
</blockquote>
<span id="more"></span>
<h4 id="监控flink作业的需求"><a href="#监控flink作业的需求" class="headerlink" title="监控flink作业的需求"></a>监控flink作业的需求</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原因: flink作业运行至yarn上,可以直接在yarn的Resource Manager上进行查看该作业是否因为某些原因强制退出以及containers是否大于等于2</span><br><span class="line">(发现只有一个container的作业都有问题,需要报警进行查看原因)</span><br><span class="line"></span><br><span class="line">解决方案:使用shell监控yarn上的flink相关脚本</span><br></pre></td></tr></table></figure>

<h4 id="shell脚本相关内容"><a href="#shell脚本相关内容" class="headerlink" title="shell脚本相关内容"></a>shell脚本相关内容</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">#获取flink作业在yarn上的名称</span><br><span class="line">nameIds&#x3D;(&#96;yarn application -list -appStates RUNNING|sed -n &quot;3~1p&quot; |awk -F &quot;\t&quot; &#39;&#123;print $1&quot;|&quot; $2&#125;&#39;|sed -e &#39;s&#x2F;[[:space:]]&#x2F;&#x2F;g&#39;&#96;)</span><br><span class="line"></span><br><span class="line">#获取需要监控的flink名称</span><br><span class="line">arr&#x3D;(&#96;cat .&#x2F;name|sed -e &#39;s&#x2F;[[:space:]]&#x2F;&#x2F;g&#39;&#96;)</span><br><span class="line">#cd &#x2F;home&#x2F;etiantian&#x2F;common-jars</span><br><span class="line">jarName&#x3D;&#39;send-mail.jar&#39;</span><br><span class="line"></span><br><span class="line">base&#x3D;&#96;pwd&#96;</span><br><span class="line">dt&#x3D;&#96;date &quot;+%Y-%m-%d_%H:%M:%S&quot;&#96;</span><br><span class="line">echo $dt</span><br><span class="line">echo $base</span><br><span class="line">log&#x3D;&quot;.&#x2F;&quot;$dt&quot;.stop&quot;</span><br><span class="line">for i in $&#123;arr[@]&#125;</span><br><span class="line">do</span><br><span class="line">        isExist&#x3D;&quot;false&quot;</span><br><span class="line">        flag&#x3D;&quot;true&quot;</span><br><span class="line">        for j in $&#123;nameIds[@]&#125;</span><br><span class="line">        do</span><br><span class="line">                nameId&#x3D;($&#123;j&#x2F;&#x2F;\|&#x2F; &#125;)</span><br><span class="line">                if [ &quot;$i&quot; &#x3D; &quot;$&#123;nameId[1]&#125;&quot; ]</span><br><span class="line">                then</span><br><span class="line">                        isExist&#x3D;&quot;true&quot;</span><br><span class="line">                        attempt&#x3D;(&#96;yarn applicationattempt -list $&#123;nameId[0]&#125;|sed -n &quot;3~1p&quot; |awk -F &quot;\t&quot; &#39;&#123;print $1&#125;&#39;&#96;)</span><br><span class="line">                        #判断该作业对应containers是否大于2处于正常状态</span><br><span class="line">                        container&#x3D;&#96;yarn container -list $&#123;attempt[0]&#125;|grep &quot;Total number of containers&quot;|awk -F &quot;:&quot; &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">                        if [ $container -lt 2  ]</span><br><span class="line">                        then</span><br><span class="line">                                flag&#x3D;&quot;false&quot;</span><br><span class="line">                        fi</span><br><span class="line">                fi</span><br><span class="line">        done</span><br><span class="line">        if [ &quot;$flag&quot; !&#x3D; &quot;true&quot; ] || [ &quot;$isExist&quot; !&#x3D; &quot;true&quot; ]</span><br><span class="line">        #if [ &quot;$flag&quot; &#x3D;&#x3D; &quot;true&quot; ] &amp;&amp; [ &quot;$isExist&quot; &#x3D;&#x3D; &quot;true&quot; ]</span><br><span class="line">        then</span><br><span class="line">                echo $i &gt;&gt; $log</span><br><span class="line">        fi</span><br><span class="line">done</span><br><span class="line">#对有问题的flink作业报警告处理</span><br><span class="line">if [ -e $log ]</span><br><span class="line">then</span><br><span class="line">   z&#x3D;&#96;cat $log|xargs echo|sed -e &#39;s&#x2F; &#x2F;,&#x2F;g&#39;&#96;</span><br><span class="line">   #z&#x3D;&#96;cat $log&#96;</span><br><span class="line">   echo $z</span><br><span class="line">   java -jar $BASE&#x2F;$jarName $BASE&#x2F;$config $z</span><br><span class="line">   #cd $base &amp;&amp; java -jar $BASE&#x2F;$jarName $BASE&#x2F;$config $z</span><br><span class="line">   rm -rf $log</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell脚本监控Hive日志表每日数据</title>
    <url>/2020/08/10/Shell%E8%84%9A%E6%9C%AC%E7%9B%91%E6%8E%A7Hive%E6%97%A5%E5%BF%97%E8%A1%A8%E6%AF%8F%E6%97%A5%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<blockquote>
<p>日志数据由研发存储至kafka,偶尔发现相关表数据量为0的情况</p>
</blockquote>
<span id="more"></span>

<h4 id="监控hive日志表数据的需求"><a href="#监控hive日志表数据的需求" class="headerlink" title="监控hive日志表数据的需求"></a>监控hive日志表数据的需求</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原因:研发将相关日志数据存储至kafka,通过flink存储至hbase,映射至hive表,进行清洗处理</span><br><span class="line">发现研发有丢失的情况</span><br><span class="line"></span><br><span class="line">解决方案: 每日查询13张日志表数据,通过html的方式发送至每人的邮箱</span><br></pre></td></tr></table></figure>

<h4 id="脚本内容"><a href="#脚本内容" class="headerlink" title="脚本内容"></a>脚本内容</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line"></span><br><span class="line">#获取需要监控的表名称</span><br><span class="line">arr&#x3D;(&#96;cat .&#x2F;name|sed -e &#39;s&#x2F;[[:space:]]&#x2F;&#x2F;g&#39;&#96;)</span><br><span class="line"></span><br><span class="line">dt&#x3D;&#96;date &quot;+%Y-%m-%d %H:%M:%S&quot;&#96;</span><br><span class="line">echo $dt</span><br><span class="line">yesDate&#x3D;&#96;date +%Y-%m-%d -d &#39;-1 day&#39;&#96;</span><br><span class="line"></span><br><span class="line">base&#x3D;&#96;pwd&#96;</span><br><span class="line">echo $BASE</span><br><span class="line"></span><br><span class="line">log&#x3D;&quot;.&#x2F;&quot;$yesDate&quot;.html&quot;</span><br><span class="line">#如果存储对应目录文件,进行删除</span><br><span class="line">$(&gt; $log)</span><br><span class="line">jarName&#x3D;&#39;send-mail.jar&#39;</span><br><span class="line"></span><br><span class="line">echo -n &quot;&lt;html&gt;&quot; &gt;&gt; $log</span><br><span class="line">echo -n &quot;&lt;body&gt;&lt;h2 lign&#x3D;center&gt;13张日志表数据每日统计&lt;&#x2F;h2&gt;&quot; &gt;&gt; $log</span><br><span class="line">echo -n &quot;&lt;table&gt;&quot; &gt;&gt;$log</span><br><span class="line">echo -n &quot;&lt;tr&gt;&lt;th&gt;Table Name&lt;&#x2F;th&gt;&lt;th width&#x3D;200px&gt;Yes Count&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&quot; &gt;&gt; $log</span><br><span class="line"></span><br><span class="line">for i in $&#123;arr[@]&#125;</span><br><span class="line">do</span><br><span class="line">  count&#x3D;&#96;hive -e &quot;select count(*) from $i where c_date &#x3D;&#39;$yesDate&#39;&quot;&#96;</span><br><span class="line">  #echo $i $count &gt;&gt; $log</span><br><span class="line">  echo -n &quot;&lt;tr&gt;&lt;td align&#x3D;center&gt;$i&lt;&#x2F;td&gt;&lt;td align&#x3D;center&gt;$count&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;&quot; &gt;&gt; $log</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">echo -n &quot;&lt;&#x2F;table&gt;&lt;&#x2F;body&gt;&quot; &gt;&gt;$log</span><br><span class="line">echo -n &quot;&lt;&#x2F;html&gt;&quot; &gt;&gt; $log</span><br><span class="line"></span><br><span class="line">if [ -e $log ]</span><br><span class="line">then</span><br><span class="line">  z&#x3D;&#96;cat $log|xargs&#96;</span><br><span class="line">  java -jar $BASE&#x2F;$jarName $BASE&#x2F;$config &quot;$z&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell远程执行命令</title>
    <url>/2017/01/07/Shell%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<blockquote>
<p>解决经常远程其他节点执行脚本的问题,需要ssh免密登陆</p>
</blockquote>
<span id="more"></span>

<h2 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">ssh root@node01 <span class="string">&quot;cd /home ; touch abc.txt&quot;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ssh user@remoteNode &gt; /dev/null 2&gt;&amp;1 &lt;&lt; <span class="string">eeooff</span></span><br><span class="line"><span class="string">cd /home</span></span><br><span class="line"><span class="string">touch abcdefg.txt</span></span><br><span class="line"><span class="string">exit</span></span><br><span class="line"><span class="string">eeooff</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># eeooff之间是目标服务器执行的命令,eeooff可以改成其他的</span></span><br><span class="line"><span class="comment"># 使用&gt; /dev/null 2&gt;&amp;1 重定向是为了不打印目标服务器的日志</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="带密码远程执行"><a href="#带密码远程执行" class="headerlink" title="带密码远程执行"></a>带密码远程执行</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 有时候服务器之间并没有配置免密,需要输入密码才能访问</span></span><br><span class="line">yum install -y sshpass</span><br><span class="line">sshpass -p <span class="string">&quot;password&quot;</span> ssh root@ip <span class="string">&quot;df -h&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Shell异常Job in state DEFINE instead of RUNNING</title>
    <url>/2019/11/24/Spark-Shell%E5%BC%82%E5%B8%B8Job-in-state-DEFINE-instead-of-RUNNING/</url>
    <content><![CDATA[<blockquote>
<p>Spark-Shell测试把数据写入Hbase遇到异常</p>
</blockquote>
<span id="more"></span>

<h2 id="异常分析"><a href="#异常分析" class="headerlink" title="异常分析"></a>异常分析</h2><p><strong>现象</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从日志报错分析,Job状态不对</span><br></pre></td></tr></table></figure>
<p><strong>原因</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在Spark shell模式下</span><br><span class="line">每运行一行代码其都会输出这个对象</span><br><span class="line">所以在初始化job的时候会调用其toString方法来打印出这个对象</span><br><span class="line">但是在toString方法的实现里面会对其状态进行检查</span><br><span class="line">确保job实例是JobState.RUNNING状态</span><br><span class="line">但是这个时候job的状态是JobState.DEFINE</span><br><span class="line">所以会导致异常</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h3 id="解决方法一："><a href="#解决方法一：" class="headerlink" title="解决方法一："></a>解决方法一：</h3><p>不要再spark-shell中执行上面代码，使用spark-submit来提交执行代码，这样就不会检查状态</p>
<h3 id="解决方法二："><a href="#解决方法二：" class="headerlink" title="解决方法二："></a>解决方法二：</h3><p>使用<code>lazy</code>来初始化定义对象，这样会只有job对象被真正使用的时候才会初始化</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lazy val job &#x3D; Job.getInstance(sc.hadoopConfiguration)lazy val job &#x3D; Job.getInstance(jobConf)</span><br></pre></td></tr></table></figure>

<h3 id="解决方法三："><a href="#解决方法三：" class="headerlink" title="解决方法三："></a>解决方法三：</h3><p>将Job对象封装到类里面，这样就不会调用Job的toString方法，这样就可以避免出现异常</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class JobWrapper(sc:SparkContext)&#123; val job &#x3D; Job.getInstance(sc.hadoopConfiguration); &#125;</span><br><span class="line">val jobWrapper &#x3D; new JobWrapper(sc)</span><br><span class="line">FileInputFormat.setInputPaths(jobWrapper.job, paths)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL内核解析总结一</title>
    <url>/2020/07/23/SparkSQL%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90%E6%80%BB%E7%BB%93%E4%B8%80/</url>
    <content><![CDATA[<blockquote>
<p>主要来自spark SQL内核解析书本内容</p>
</blockquote>
<span id="more"></span>



<h4 id="简单的案例分析"><a href="#简单的案例分析" class="headerlink" title="简单的案例分析"></a>简单的案例分析</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val spark &#x3D; SparkSession.builder().appName(&quot;test&quot;).config(&quot;es.index.auto.create&quot;,&quot;true&quot;).config(&quot;hive.exec.dynamic.partition&quot;,&quot;true&quot;).config(&quot;hive.exec.dynamic.partition.mode&quot;:&quot;nonstrict&quot;).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line">用户表：</span><br><span class="line">val userInfo &#x3D; spark.sql(&quot;select user_id,ref,ett_user_id,dc_shool_id from user_info_mysql&quot;)</span><br><span class="line"></span><br><span class="line">val schoolName &#x3D; spark.sql(&quot;select school_id,name,belong_name,province from school_info_mysql&quot;)</span><br><span class="line"></span><br><span class="line">spark SQL可以使用dataFrame接口进行调用，但是sql到rdd的执行需要经过复杂的流程；一般分为逻辑计划和物理计划</span><br><span class="line"></span><br><span class="line">1.逻辑计划会将用户所提交的SQL语句转换成树形数据结构，SQL语句中蕴含的逻辑映射到逻辑算子数的不同节点；</span><br><span class="line">一般分为未解析的逻辑算子数，解析后的逻辑算子数和优化后的逻辑算子数三个子阶段</span><br><span class="line">主要对sql中所包含的各种处理逻辑（过滤、裁剪等）和数据信息都被整合在逻辑算子的不同节点中</span><br><span class="line">filter 减少全表扫描的可能</span><br><span class="line">select 指定字段 减少全表字段的扫描</span><br><span class="line"></span><br><span class="line">2.物理计划将上一步生产的逻辑算子数进一步转换，生产物理算子树。</span><br><span class="line">物理算子树会直接生成RDD或对RDD进行transfromation转换操作：</span><br><span class="line">物理算子数也分为三个阶段：</span><br><span class="line">1.生成物理算子数列表（同样的逻辑算子数可能对应多个物理算子数）</span><br><span class="line">2.从列表中按照策略选择最优的物理算子数</span><br><span class="line">3.对选取的物理算子数进行提交前的准备工作</span><br><span class="line">执行action操作</span><br></pre></td></tr></table></figure>

<h4 id="物理计划执行策略-strategy体系"><a href="#物理计划执行策略-strategy体系" class="headerlink" title="物理计划执行策略(strategy体系)"></a>物理计划执行策略(strategy体系)</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">所有的策略都继承自GenericStrategy类，其中定义了planLater和apply方法；sparkStrategy继承GenericStrategy，strategy是生成物理算子树的基础</span><br><span class="line">SparkPlanner中默认添加了8中Strategy来生成物理计划</span><br><span class="line">1.fileSourceStrategy与dataSourceStrategy主要针对数据源</span><br><span class="line">2.Aggregation和JoinSelection分别针对聚合与关联操作；</span><br><span class="line">3.BasicOperatiors涉及范围广，包含了过滤、投影等各种操作</span><br><span class="line"></span><br><span class="line">1) fileSourceStategy: 数据文件扫描计划</span><br><span class="line">2) DataSourceStategy: 各种数据源相关的计划</span><br><span class="line">3) DDLStrategy： DDL操作执行计划</span><br><span class="line">4) specialLimits：特殊limit操作的执行计划</span><br><span class="line">5）Aggregation ： 集合算子相关的执行计划</span><br><span class="line">6）JoinSelection： Join操作相关的执行计划</span><br><span class="line">7）InMemoryScans： 内存数据表扫描计划</span><br><span class="line">8）BasicOperators：对基本算子生成的执行计划</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="transformation算子"><a href="#transformation算子" class="headerlink" title="transformation算子"></a>transformation算子</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">map ： 原来rdd的每个数据项通过map中的用户自定义函数映射转变成一个新的元素</span><br><span class="line">flatMap： 原来rdd中的每个元素通过函数转换成新的元素，并将生成的rdd的每个集合中的元素合并为一个集合</span><br><span class="line">mapPartition：获取到每个分区的迭代器，在函数中通过这个分区整体的迭代器对整个分区的元素进行操作（每个分区对filter后数据进行保留）</span><br><span class="line">union ：保证两个rdd元素的数据类型相同，返回的rdd数据类型和被合并的rdd元素数据类型相同，并不进行去重操作</span><br><span class="line">distinct ：返回一个包含源数据集中所有不重复元素</span><br><span class="line">groupByKey：在一个kv对组成的数据集上调用，输出结果的并行度依赖于父RDD的分区数目</span><br><span class="line">reduceByKey：在kv对的数据集上调用，相同key的，由reduce的task个数的方式进行聚合</span><br><span class="line">join: 宽依赖，每个key中的所有元素都在一起的数据集</span><br><span class="line">repartition或coalesce：减少分区数，coalesce还可以用于left join后获取非空字段的数据</span><br></pre></td></tr></table></figure>

<h4 id="action算子"><a href="#action算子" class="headerlink" title="action算子"></a>action算子</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reduce: 通过函数聚集数据集中的所有元素，确保可以被正确的并发执行</span><br><span class="line">collect：以数组的形式，返回数据集的所有元素，通常会在使用filter或者其他操作后，返回一个足够小的数据子集使用</span><br><span class="line">count：返回数据集的元素个数</span><br><span class="line">first，take，limit：返回一个数组，由前面的n个元素组成</span><br><span class="line">foreach：每个元素遍历执行一次函数</span><br><span class="line">foreachPartition：每个分区执行一次函数</span><br></pre></td></tr></table></figure>

<h4 id="spark开发调优，对多次使用的rdd进行持久"><a href="#spark开发调优，对多次使用的rdd进行持久" class="headerlink" title="spark开发调优，对多次使用的rdd进行持久"></a>spark开发调优，对多次使用的rdd进行持久</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一般使用cache和persist</span><br><span class="line">1.cache：使用非序列化的方式将rdd的数据全部尝试持久化到内存中，cache只是一个transformation，是lazy，必须通过一个action触发，才能真正的将该rdd cache到内存中</span><br><span class="line">2.persist：手动选择持久化级别，并使用指定的方式进行持久化</span><br><span class="line"></span><br><span class="line">缓存类型：</span><br><span class="line">内存，磁盘，内存+磁盘以及相对应的反序列化和序列化以及双副本</span><br><span class="line">反序列化：把RDD作为反序列化的方式存储，假如RDD的内存存不下，剩余的分区在以后需要时会重新计算，不会刷到磁盘上</span><br><span class="line">序列化：序列化方式，每个partition以字节数据存储，好处是能带来更好的空间存储，但CPU耗费高</span><br><span class="line">双副本：RDD以反序列化的方式存内存，假如rdd的内容存储不下，会存储至磁盘</span><br><span class="line"></span><br><span class="line">手动移除缓存数据：unpersist</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Shell总体归纳</title>
    <url>/2016/05/31/Shell%E6%80%BB%E4%BD%93%E5%BD%92%E7%BA%B3/</url>
    <content><![CDATA[<blockquote>
<p>Shell指南</p>
</blockquote>
<span id="more"></span>

<h3 id="一-约定标记"><a href="#一-约定标记" class="headerlink" title="一. 约定标记"></a>一. 约定标记</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="二-变量"><a href="#二-变量" class="headerlink" title="二. 变量"></a>二. 变量</h3><h4 id="1-定义变量"><a href="#1-定义变量" class="headerlink" title="1. 定义变量"></a>1. 定义变量</h4><ul>
<li>变量名和等号之间<strong>不能有空格</strong></li>
<li>首个字符必须为字母(a-z,A-Z)</li>
<li>中间不能有空格,可以使用下划线</li>
<li>不能使用标点符号</li>
<li>不能使用bash里的关键字<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">country&#x3D;&quot;china&quot;</span><br></pre></td></tr></table></figure>
<h4 id="2-使用变量"><a href="#2-使用变量" class="headerlink" title="2. 使用变量"></a>2. 使用变量</h4><blockquote>
<p>只需要在一个定义过的变量前面加上美元符号$就可以了,另外,对于变量的{}是可以选择的,它的目的为帮助解释器识别变量的边界</p>
</blockquote>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo $country</span><br><span class="line">echo $&#123;country&#125;</span><br><span class="line">echo &quot;I love my $&#123;country&#125;abcd!&quot;</span><br></pre></td></tr></table></figure>

<h4 id="3-重定义变量"><a href="#3-重定义变量" class="headerlink" title="3. 重定义变量"></a>3. 重定义变量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">country&#x3D;&quot;China&quot;</span><br><span class="line">country&#x3D;&quot;USA&quot;</span><br></pre></td></tr></table></figure>

<h4 id="4-只读变量"><a href="#4-只读变量" class="headerlink" title="4. 只读变量"></a>4. 只读变量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">readonly country&#x3D;&quot;China&quot;</span><br><span class="line">或</span><br><span class="line">country&#x3D;&quot;China&quot;</span><br><span class="line">readonly country</span><br></pre></td></tr></table></figure>

<h4 id="5-删除变量"><a href="#5-删除变量" class="headerlink" title="5. 删除变量"></a>5. 删除变量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unset country</span><br></pre></td></tr></table></figure>

<h4 id="6-特殊变量"><a href="#6-特殊变量" class="headerlink" title="6. 特殊变量"></a>6. 特殊变量</h4><table>
<thead>
<tr>
<th><strong>变量</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td>$0</td>
<td>当前脚本的文件名</td>
</tr>
<tr>
<td>$n</td>
<td>传递给脚本或函数的参数.n是一个数字,表示第几个参数</td>
</tr>
<tr>
<td>$#</td>
<td>传递给脚本或函数的参数个数</td>
</tr>
<tr>
<td>$*</td>
<td>传递给脚本或函数的所有参数</td>
</tr>
<tr>
<td>$@</td>
<td>传递给脚本或函数的所有参数</td>
</tr>
<tr>
<td>$?</td>
<td>上个命令的退出状态,或函数的返回值</td>
</tr>
<tr>
<td>$$</td>
<td>当前Shell进程ID</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$* 和 $@ 的区别为: </span><br><span class="line">    1. $* 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(&quot; &quot;)包含时，都以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数。</span><br><span class="line">    2. 当它们被双引号(&quot; &quot;)包含时，&quot;$*&quot; 会将所有的参数作为一个整体，以&quot;$1 $2 … $n&quot;的形式输出所有参数；&quot;$@&quot; 会将各个参数分开，以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数。</span><br><span class="line">一般直接使用$@</span><br><span class="line"></span><br><span class="line">$? 可以获取上一个命令的退出状态。所谓退出状态，就是上一个命令执行后的返回结果。退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1。</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="三-Shell的替换"><a href="#三-Shell的替换" class="headerlink" title="三. Shell的替换"></a>三. Shell的替换</h3><h4 id="1-转义符"><a href="#1-转义符" class="headerlink" title="1. 转义符"></a>1. 转义符</h4><table>
<thead>
<tr>
<th>header 1</th>
<th>header 2</th>
</tr>
</thead>
<tbody><tr>
<td>\a</td>
<td>发出警告声；</td>
</tr>
<tr>
<td>\b</td>
<td>删除前一个字符；</td>
</tr>
<tr>
<td>\c</td>
<td>最后不加上换行符号；</td>
</tr>
<tr>
<td>\f</td>
<td>换行但光标仍旧停留在原来的位置；</td>
</tr>
<tr>
<td>\n</td>
<td>换行且光标移至行首；</td>
</tr>
<tr>
<td>\r</td>
<td>光标移至行首，但不换行；</td>
</tr>
<tr>
<td>\t</td>
<td>插入tab；</td>
</tr>
<tr>
<td>\v</td>
<td>与\f相同；</td>
</tr>
<tr>
<td>\</td>
<td>插入\字符；</td>
</tr>
<tr>
<td>\nnn</td>
<td>插入nnn（八进制）所代表的ASCII字符；</td>
</tr>
</tbody></table>
<blockquote>
<p>使用 echo 命令的 –E 选项禁止转义，默认也是不转义的；</p>
</blockquote>
<blockquote>
<p>使用 –n 选项可以禁止插入换行符；</p>
</blockquote>
<blockquote>
<p>使用 echo 命令的 –e 选项可以对转义字符进行替换.</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;\\&quot;           #得到 \</span><br><span class="line">echo -e &quot;\\&quot;        #得到 \</span><br><span class="line"></span><br><span class="line">echo &quot;\\\\&quot;         #得到 \\</span><br><span class="line">echo -e &quot;\\&quot;        #得到 \</span><br><span class="line"></span><br><span class="line">使用-e 如果存在其他转义字符,那么\\\\等于\</span><br><span class="line">echo -e &quot;aa\\\\aa&quot;  #得到 aa\aa</span><br><span class="line">echo -e &quot;aa\\aa&quot;    #得到 aaa</span><br></pre></td></tr></table></figure>

<h4 id="2-命令替换"><a href="#2-命令替换" class="headerlink" title="2. 命令替换"></a>2. 命令替换</h4><blockquote>
<p>把一个命令的输出赋值给一个变量,方法为把命令用<strong>反引号</strong>(在Esc下方)引起来</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">directory&#x3D;&#96;pwd&#96;</span><br><span class="line">echo $directory</span><br></pre></td></tr></table></figure>

<h4 id="3-变量替换"><a href="#3-变量替换" class="headerlink" title="3. 变量替换"></a>3. 变量替换</h4><blockquote>
<p>可以根据变量的状态（是否为空、是否定义等）来改变它的值</p>
</blockquote>
<table>
<thead>
<tr>
<th>形式</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>${var}</td>
<td>变量本来的值</td>
</tr>
<tr>
<td>${var:-word}</td>
<td>如果变量var为空或已被删除(unset),那么返回word,但不改变var的值</td>
</tr>
<tr>
<td>${var:=word}</td>
<td>如果变量var为空或已被删除(unset),那么返回word,并将var的值设置为word</td>
</tr>
<tr>
<td>${var:?message}</td>
<td>如果变量var为空或已被删除(unset),那么将消息message送到标准错误输出,可以用来检测变量var是否可以被正常赋值.<strong>若此替换出现在Shell脚本中,那么脚本将停止运行</strong></td>
</tr>
<tr>
<td>${var:+word}</td>
<td>如果变量var被定义,那么返回word,但不改变var的值</td>
</tr>
</tbody></table>
<hr>
<h3 id="四-Shell的运算符"><a href="#四-Shell的运算符" class="headerlink" title="四. Shell的运算符"></a>四. Shell的运算符</h3><h4 id="1-算数运算符"><a href="#1-算数运算符" class="headerlink" title="1. 算数运算符"></a>1. 算数运算符</h4><table>
<thead>
<tr>
<th>运算符</th>
<th>说明</th>
<th>举例</th>
</tr>
</thead>
</table>
<ul>
<li>| 加法 | <code>expr $a + $b</code></li>
</ul>
<ul>
<li>| 减法 | <code>expr $a - $b</code></li>
</ul>
<ul>
<li>| 乘法 | <code>expr $a \* $b</code><br>/ | 除法 | <code>expr $a / $b</code><br>% | 取余 | <code>expr $a % $b</code><br>= | 赋值 | a=$b<br>== | 相等 | [ $a == $b ]<br>!= | 不相等 | [ $a != $b ]</li>
</ul>
<blockquote>
<p>在expr中的乖号为：*</p>
</blockquote>
<blockquote>
<p>在expr中的表达式与运算符之间要有空格，否则错误；</p>
</blockquote>
<blockquote>
<p>在[ $a == $b ]与[ $a != $b ]中，要需要在方括号与变量以及变量与运算符之间也需要有括号， 否则为错误的。</p>
</blockquote>
<h4 id="2-关系运算符"><a href="#2-关系运算符" class="headerlink" title="2. 关系运算符"></a>2. 关系运算符</h4><blockquote>
<p>只支持数字，不支持字符串，除非字符串的值是数字。</p>
</blockquote>
<table>
<thead>
<tr>
<th>运算符</th>
<th>说明</th>
<th>举例</th>
</tr>
</thead>
<tbody><tr>
<td>-eq</td>
<td>相等返回true</td>
<td>[ $a -eq $b ]</td>
</tr>
<tr>
<td>-ne</td>
<td>不相等返回true</td>
<td>[ $a -ne $b ]</td>
</tr>
<tr>
<td>-gt</td>
<td>大于返回true</td>
<td>[ $a -gt $b ]</td>
</tr>
<tr>
<td>-lt</td>
<td>小于返回true</td>
<td>[ $a -lt $b ]</td>
</tr>
<tr>
<td>-ge</td>
<td>大于等于返回true</td>
<td>[ $a -ge $b ]</td>
</tr>
<tr>
<td>-le</td>
<td>小于等于返回true</td>
<td>[ $a -le $b ]</td>
</tr>
</tbody></table>
<h4 id="3-布尔运算符"><a href="#3-布尔运算符" class="headerlink" title="3. 布尔运算符"></a>3. 布尔运算符</h4><table>
<thead>
<tr>
<th>运算符</th>
<th>说明</th>
<th>举例</th>
</tr>
</thead>
<tbody><tr>
<td>!</td>
<td>非运算,表达式为true则返回false,否则返回true</td>
<td>[ !false ]返回true</td>
</tr>
<tr>
<td>-o</td>
<td>或运算,有true返回true</td>
<td>[ $a -lt 20 -o $b -gt 100 ]</td>
</tr>
<tr>
<td>-a</td>
<td>与运算,有false返回false</td>
<td>[ $a -lt 20 -a $b -gt 100 ]</td>
</tr>
</tbody></table>
<h4 id="4-字符串运算符"><a href="#4-字符串运算符" class="headerlink" title="4. 字符串运算符"></a>4. 字符串运算符</h4><table>
<thead>
<tr>
<th>运算符</th>
<th>说明</th>
<th>举例</th>
</tr>
</thead>
<tbody><tr>
<td>=</td>
<td>相等返回true</td>
<td>[ $a = $b ]</td>
</tr>
<tr>
<td>!=</td>
<td>不相等返回true</td>
<td>[ $a != $b ]</td>
</tr>
<tr>
<td>-z</td>
<td>检测字符串长度是否为0,为0返回true</td>
<td>[ -z $a ]</td>
</tr>
<tr>
<td>-n</td>
<td>检测字符串长度是否为0,不为0返回true</td>
<td>[ -n $a ]</td>
</tr>
<tr>
<td>str</td>
<td>检测字符串是否为空,不为空返回true</td>
<td>[ $a ]</td>
</tr>
</tbody></table>
<h4 id="5-文件测试运算符"><a href="#5-文件测试运算符" class="headerlink" title="5. 文件测试运算符"></a>5. 文件测试运算符</h4><table>
<thead>
<tr>
<th>运算符</th>
<th>说明</th>
<th>举例</th>
</tr>
</thead>
<tbody><tr>
<td>-b file</td>
<td>检测文件是否是块设备文件</td>
<td>[ -b $file ]</td>
</tr>
<tr>
<td>-c file</td>
<td>检测文件是否是字符设备文件</td>
<td>[ -c $file ]</td>
</tr>
<tr>
<td>-d file</td>
<td>检测文件是否是目录</td>
<td>[ -d $file ]</td>
</tr>
<tr>
<td>-f file</td>
<td>检测文件是否是普通文件</td>
<td>[ -f $file ]</td>
</tr>
<tr>
<td>-g file</td>
<td>检测文件是否设置了SGID位</td>
<td>[ -g $file ]</td>
</tr>
<tr>
<td>-k file</td>
<td>检测文件是否设置了黏着位</td>
<td>[ -k $file ]</td>
</tr>
<tr>
<td>-p file</td>
<td>检测文件是否是具名管道</td>
<td>[ -p $file ]</td>
</tr>
<tr>
<td>-u file</td>
<td>检测文件是否设置了SUID位</td>
<td>[ -u $file ]</td>
</tr>
<tr>
<td>-r file</td>
<td>检测文件是否可读</td>
<td>[ -r $file ]</td>
</tr>
<tr>
<td>-w file</td>
<td>检测文件是否可写</td>
<td>[ -w $file ]</td>
</tr>
<tr>
<td>-x file</td>
<td>检测文件是否可执行</td>
<td>[ -x $file ]</td>
</tr>
<tr>
<td>-s file</td>
<td>检测文件是否为空(文件大小是否大于0)</td>
<td>[ -s $file ]</td>
</tr>
<tr>
<td>-e file</td>
<td>检测文件(目录)是否存在</td>
<td>[ -e $file ]</td>
</tr>
</tbody></table>
<hr>
<h3 id="五-Shell的字符串"><a href="#五-Shell的字符串" class="headerlink" title="五. Shell的字符串"></a>五. Shell的字符串</h3><h4 id="1-单引号"><a href="#1-单引号" class="headerlink" title="1. 单引号"></a>1. 单引号</h4><blockquote>
<ul>
<li>单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的；</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>单引号字串中不能出现单引号（对单引号使用转义符后也不行）。</li>
</ul>
</blockquote>
<h4 id="2-双引号"><a href="#2-双引号" class="headerlink" title="2. 双引号"></a>2. 双引号</h4><blockquote>
<ul>
<li>双引号里可以有变量</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>双引号里可以出现转义字符</li>
</ul>
</blockquote>
<h4 id="3-拼接字符串"><a href="#3-拼接字符串" class="headerlink" title="3. 拼接字符串"></a>3. 拼接字符串</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">country&#x3D;&quot;China&quot;</span><br><span class="line">echo &quot;hello, $country&quot;</span><br><span class="line">或</span><br><span class="line">echo &quot;hello, &quot;$country&quot; &quot;</span><br></pre></td></tr></table></figure>

<h4 id="4-获取字符串长度"><a href="#4-获取字符串长度" class="headerlink" title="4. 获取字符串长度"></a>4. 获取字符串长度</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">string&#x3D;&quot;abcd&quot;</span><br><span class="line">echo $&#123;#string&#125; #输出 4</span><br><span class="line">或</span><br><span class="line">echo &#96;expr length abcde&#96; #输出 5</span><br></pre></td></tr></table></figure>

<h4 id="5-提取子字符串"><a href="#5-提取子字符串" class="headerlink" title="5. 提取子字符串"></a>5. 提取子字符串</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">string&#x3D;&quot;alibaba is a great company&quot;</span><br><span class="line">echo $&#123;string:1:4&#125; #输出 liba</span><br><span class="line">或</span><br><span class="line">echo &#96;expr substr abcde 2 3&#96; #输出 bcd</span><br></pre></td></tr></table></figure>

<h4 id="6-查找字符下标"><a href="#6-查找字符下标" class="headerlink" title="6. 查找字符下标:"></a>6. 查找字符下标:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">string&#x3D;&quot;alibaba is a great company&quot;</span><br><span class="line">echo &#96;expr index &quot;$string&quot; i&#96; #输出 3</span><br></pre></td></tr></table></figure>

<h4 id="7-处理路径的字符串"><a href="#7-处理路径的字符串" class="headerlink" title="7. 处理路径的字符串"></a>7. 处理路径的字符串</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">basename</span><br><span class="line">basename &#x2F;home&#x2F;yin&#x2F;1.txt #输出 1.txt</span><br><span class="line">basename &#x2F;home&#x2F;yin&#x2F;1.txt .txt #输出 1</span><br><span class="line"></span><br><span class="line">dirname</span><br><span class="line">dirname &#x2F;usr&#x2F;bin&#x2F; #输出 &#x2F;usr</span><br><span class="line">dirname &#x2F;usr&#x2F;bin&#x2F;sort #输出 &#x2F;usr&#x2F;bin</span><br><span class="line">dirname 1.txt #输出 .</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="六-Shell的数组"><a href="#六-Shell的数组" class="headerlink" title="六. Shell的数组"></a>六. Shell的数组</h3><blockquote>
<p>bash支持一维数组,不支持多维数组,它的下标从0开始编号.用下标[n]获取数组元素；</p>
</blockquote>
<h4 id="1-定义数组"><a href="#1-定义数组" class="headerlink" title="1. 定义数组"></a>1. 定义数组</h4><blockquote>
<p>用括号来表示数组，数组元素用”空格”符号分割开。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">declare -a array_name</span><br><span class="line">或</span><br><span class="line">array_name&#x3D;(value0 value1 value2 value3)</span><br><span class="line">或</span><br><span class="line">array_name[0]&#x3D;value0</span><br><span class="line">array_name[1]&#x3D;value1</span><br><span class="line">array_name[2]&#x3D;value2</span><br></pre></td></tr></table></figure>

<h4 id="2-读取数组"><a href="#2-读取数组" class="headerlink" title="2. 读取数组"></a>2. 读取数组</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">读取某个下标的元素</span><br><span class="line">$&#123;array_name[index]&#125;</span><br><span class="line"></span><br><span class="line">读取数组的全部元素</span><br><span class="line">$&#123;array_name[*]&#125;</span><br><span class="line">$&#123;array_name[@]&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-获取数组信息"><a href="#3-获取数组信息" class="headerlink" title="3. 获取数组信息"></a>3. 获取数组信息</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">arr_name&#x3D;(1 2 3 4)</span><br><span class="line"></span><br><span class="line">取得数组元素的个数</span><br><span class="line">length&#x3D;$&#123;#array_name[@]&#125;</span><br><span class="line">或</span><br><span class="line">length&#x3D;$&#123;#array_name[*]&#125;</span><br><span class="line">#输出 4</span><br><span class="line"></span><br><span class="line">获取数组的下标</span><br><span class="line">length&#x3D;$&#123;!array_name[@]&#125;</span><br><span class="line">或</span><br><span class="line">length&#x3D;$&#123;!array_name[*]&#125;</span><br><span class="line">#输出 0 1 2 3</span><br><span class="line"></span><br><span class="line">取得数组单个元素的长度</span><br><span class="line">lengthn&#x3D;$&#123;#array_name[n]&#125;</span><br><span class="line">#输出 1</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="七-printf函数"><a href="#七-printf函数" class="headerlink" title="七. printf函数"></a>七. printf函数</h3><blockquote>
<ul>
<li>printf 命令不用加括号</li>
<li>format-string 可以没有引号，但最好加上，单引号双引号均可。</li>
<li>参数多于格式控制符(%)时，format-string 可以重用，可以将所有参数都转换。</li>
<li>arguments 使用空格分隔，不用逗号。</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># format-string为双引号</span><br><span class="line">printf &quot;%d %s\n&quot; 1 &quot;abc&quot;</span><br><span class="line">#输出 1 abc</span><br><span class="line"></span><br><span class="line"># 单引号与双引号效果一样 </span><br><span class="line">printf &#39;%d %s\n&#39; 1 &quot;abc&quot; </span><br><span class="line">#输出 1 abc</span><br><span class="line"></span><br><span class="line"># 没有引号也可以输出</span><br><span class="line">printf %s abcdef</span><br><span class="line">#输出 abcdef</span><br><span class="line"></span><br><span class="line"># 格式只指定了一个参数，但多出的参数仍然会按照该格式输出，format-string 被重用</span><br><span class="line">printf %s abc def</span><br><span class="line">#输出 abcdef</span><br><span class="line">printf &quot;%s\n&quot; abc def</span><br><span class="line">#输出</span><br><span class="line"># abc</span><br><span class="line"># def</span><br><span class="line">printf &quot;%s %s %s\n&quot; a b c d e f g h i j</span><br><span class="line">#输出</span><br><span class="line"># a b c</span><br><span class="line"># d e f</span><br><span class="line"># g h i</span><br><span class="line"># j</span><br><span class="line"></span><br><span class="line"># 如果没有 arguments，那么 %s 用NULL代替，%d 用 0 代替</span><br><span class="line">printf &quot;%s and %d \n&quot; </span><br><span class="line">#输出 and 0</span><br><span class="line"></span><br><span class="line"># 如果以 %d 的格式来显示字符串，那么会有警告，提示无效的数字，此时默认置为 0</span><br><span class="line">printf &quot;The first program always prints&#39;%s,%d\n&#39;&quot; Hello Shell</span><br><span class="line">#输出</span><br><span class="line"># -bash: printf: Shell: invalid number</span><br><span class="line"># The first program always prints &#39;Hello,0&#39;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="八-Shell中条件语句"><a href="#八-Shell中条件语句" class="headerlink" title="八. Shell中条件语句"></a>八. Shell中条件语句</h3><h4 id="1-if语句"><a href="#1-if语句" class="headerlink" title="1. if语句"></a>1. if语句</h4><blockquote>
<ul>
<li>if [ 表达式 ] then  语句  fi</li>
<li>if [ 表达式 ] then 语句 else 语句 fi</li>
<li>if [ 表达式] then 语句  elif[ 表达式 ] then 语句 elif[ 表达式 ] then 语句   …… fi</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;10</span><br><span class="line">b&#x3D;20</span><br><span class="line">if [ $a &#x3D;&#x3D; $b ]</span><br><span class="line">then</span><br><span class="line">   echo &quot;a is equal to b&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;a is not equal to b&quot;</span><br><span class="line">fi</span><br><span class="line">或</span><br><span class="line">if test $[2*3] -eq $[1+5]; then echo &#39;The two numbers are equal!&#39;; fi;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>test 命令用于检查某个条件是否成立，与方括号([ ])类似</p>
</blockquote>
<h4 id="2-case语句"><a href="#2-case语句" class="headerlink" title="2. case语句"></a>2. case语句</h4><blockquote>
<ul>
<li>取值后面必须为关键字<strong>in</strong>，每一模式必须以<strong>右括号</strong>结束。</li>
<li>取值可以为<strong>变量</strong>或<strong>常数</strong>。</li>
<li>匹配发现取值符合某一模式后，其间所有命令开始<strong>执行至;;处</strong>。</li>
<li><strong>;;与其他语言中的break类似</strong>，意思是跳到整个<strong>case</strong>语句的最后。</li>
<li>如果无一匹配模式，使用<strong>星号*捕获</strong>该值，再执行后面的命令。</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">case 值 in</span><br><span class="line">模式1)</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    command3</span><br><span class="line">    ;;</span><br><span class="line">模式2）</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    command3</span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    command3</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="九-Shell中循环语句"><a href="#九-Shell中循环语句" class="headerlink" title="九. Shell中循环语句"></a>九. Shell中循环语句</h3><h4 id="1-for循环"><a href="#1-for循环" class="headerlink" title="1. for循环"></a>1. for循环</h4><blockquote>
<ul>
<li>列表是一组值（数字、字符串等）组成的序列，每个值通过空格分隔。</li>
<li>每循环一次，就将列表中的下一个值赋给变量。</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for 变量 in 列表</span><br><span class="line">do</span><br><span class="line">    command1</span><br><span class="line">    command2</span><br><span class="line">    ...</span><br><span class="line">    commandN</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for loop in 1 2 3 4 5</span><br><span class="line">do</span><br><span class="line">    echo &quot;The value is: $loop&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">for FILE in $HOME&#x2F;.bash*</span><br><span class="line">do</span><br><span class="line">   echo $FILE</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h4 id="2-while循环"><a href="#2-while循环" class="headerlink" title="2. while循环"></a>2. while循环</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">while command</span><br><span class="line">do</span><br><span class="line">   Statement(s) to be executed if command is true</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">COUNTER&#x3D;0</span><br><span class="line">while [ $COUNTER -lt 5 ]</span><br><span class="line">do</span><br><span class="line">    COUNTER&#x3D;&#39;expr $COUNTER+1&#39;</span><br><span class="line">    echo $COUNTER</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h4 id="3-until循环"><a href="#3-until循环" class="headerlink" title="3. until循环"></a>3. until循环</h4><blockquote>
<ul>
<li>until 循环执行一系列命令直至条件为 true 时停止。</li>
<li>until 循环与 while 循环在处理方式上刚好相反。</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">until command</span><br><span class="line">do</span><br><span class="line">   Statement(s) to be executed until command is true</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">command 一般为条件表达式，如果返回值为false，则继续执行循环体内的语句，否则跳出循环。</span><br><span class="line">类似地，在循环中使用 break 与 continue 跳出循环。</span><br><span class="line">另外，break 命令后面还可以跟一个整数，表示跳出第几层循环。</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="十-Shell的函数"><a href="#十-Shell的函数" class="headerlink" title="十. Shell的函数"></a>十. Shell的函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Shell函数必须先定义后使用</span><br><span class="line">function_name () &#123;</span><br><span class="line">    list of commands</span><br><span class="line">    [ return value ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">也可以加上function关键字：</span><br><span class="line">function function_name () &#123;</span><br><span class="line">    list of commands</span><br><span class="line">    [ return value ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>调用函数只需要给出函数名，不需要加括号。</li>
<li>函数返回值，可以显式增加return语句；如果不加，会将最后一条命令运行结果作为返回值。</li>
<li>Shell 函数返回值只能是整数，一般用来表示函数执行成功与否，0表示成功，其他值表示失败。</li>
<li>函数的参数可以通过 $n</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">funWithParam()&#123;</span><br><span class="line">    echo &quot;The value of the first parameter is $1 !&quot;</span><br><span class="line">    echo &quot;The value of the second parameter is $2 !&quot;</span><br><span class="line">    echo &quot;The value of the tenth parameter is $&#123;10&#125; !&quot;</span><br><span class="line">    echo &quot;The value of the eleventh parameter is $&#123;11&#125; !&quot;</span><br><span class="line">    echo &quot;The amount of the parameters is $# !&quot;  # 参数个数</span><br><span class="line">    echo &quot;The string of the parameters is $* !&quot;  # 传递给函数的所有参数</span><br><span class="line">&#125;</span><br><span class="line">funWithParam 1 2 3 4 5 6 7 8 9 34 73</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>像删除变量一样，删除函数也可以使用 unset 命令，不过要加上 .f 选项</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">unset .f function_name</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="十一-Shell的文件包含"><a href="#十一-Shell的文件包含" class="headerlink" title="十一. Shell的文件包含"></a>十一. Shell的文件包含</h3><blockquote>
<ul>
<li>两种方式的效果相同，简单起见，一般使用点号(.)，但是注意点号(.)和文件名中间有一空格。</li>
<li>被包含脚本不需要有执行权限.</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#脚本1 s1.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">echo &quot;脚本1&quot;</span><br><span class="line"></span><br><span class="line">#脚本2 s2.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">echo &quot;脚本2&quot;</span><br><span class="line">echo &quot;调用脚本1&quot;</span><br><span class="line">. s1.sh</span><br><span class="line">source s1.sh</span><br><span class="line"></span><br><span class="line"># 调用脚本2</span><br><span class="line">#输出</span><br><span class="line"># 脚本2</span><br><span class="line"># 调用脚本1</span><br><span class="line"># 脚本1</span><br><span class="line"># 脚本1</span><br></pre></td></tr></table></figure>

<h3 id="补充-内置命令-一些常用的命令"><a href="#补充-内置命令-一些常用的命令" class="headerlink" title="补充,内置命令,一些常用的命令"></a>补充,内置命令,一些常用的命令</h3><h4 id="1-awk-amp-amp-sed"><a href="#1-awk-amp-amp-sed" class="headerlink" title="1.awk&amp;&amp;sed"></a>1.awk&amp;&amp;sed</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将你找的字符串下一行,替换成指定字符串</span><br><span class="line">sed -i &#39;&#x2F;你找的字符串&#x2F; &#123; N; s&#x2F;\n.*$&#x2F;\n你要写的字符串&#x2F;&#125;&#39; 你的文件</span><br><span class="line"></span><br><span class="line"># 将文件大小大于0的打印出来</span><br><span class="line">ll .&#x2F;*.log| awk -F &quot; &quot; &#39;&#123;if($5&gt;0)&#123;print $9&#125;&#125;&#39;</span><br><span class="line"></span><br><span class="line"># 可以去除文本中多打的分割符</span><br><span class="line">awk -v OFS&#x3D;&quot;\t&quot; &#39;&#123;print $1,$2,$3,$4,$5,$6&#125;&#39; test.txt &gt; test.bak</span><br><span class="line"></span><br><span class="line"># 输出第一行</span><br><span class="line">awk &#39;NR&#x3D;&#x3D;1&#39; test.txt</span><br><span class="line"></span><br><span class="line"># 输出奇数行</span><br><span class="line">awk &#39;NR%2&#39; test.txt</span><br><span class="line">awk &#39;++i%2&#39; test.txt</span><br><span class="line">sed -n &#39;1~2p&#39; test1.txt</span><br><span class="line"></span><br><span class="line"># 输出偶数行</span><br><span class="line">awk &#39;!(NR%2)&#39; test.txt</span><br><span class="line">awk &#39;i++%2&#39; test.txt</span><br><span class="line">sed -n &#39;2~2p&#39; test1.txt</span><br><span class="line"></span><br><span class="line"># 行数对3取余,余数为1的输出</span><br><span class="line">awk &#39;NR%3&#x3D;&#x3D;1&#39; test1.txt</span><br></pre></td></tr></table></figure>

<h4 id="2-date"><a href="#2-date" class="headerlink" title="2. date"></a>2. date</h4><blockquote>
<p>如果带时分秒,指定时间应放在日期操作后面</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将输入日期转换成时间戳</span><br><span class="line">START_DAY&#x3D;$(date -d &quot;2018-10-27&quot; +%s)</span><br><span class="line"></span><br><span class="line"># 指定日期两个月前的日期</span><br><span class="line">date -d &quot;2018-08-17 -2 month&quot; +&quot;%Y-%m-%d&quot;</span><br><span class="line"></span><br><span class="line"># 获取当前时间</span><br><span class="line">date &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line"></span><br><span class="line"># 将时间戳转成字符串</span><br><span class="line">date -d @1287331200  &quot;+%Y-%m-%d&quot;</span><br><span class="line"></span><br><span class="line"># 如果要得到指定日期的前后的日期</span><br><span class="line"># 1.得到时间戳</span><br><span class="line">seconds&#x3D;&#96;date -d &quot;2010-10-18 00:00:00&quot; +%s&#96;       </span><br><span class="line"># 2.加上一天的秒数86400</span><br><span class="line">seconds_new&#x3D;&#96;expr $seconds + 86400&#96;</span><br><span class="line"># 3.获得指定日前加上一天的日期</span><br><span class="line">date_new&#x3D;&#96;date -d @$seconds_new &quot;+%Y-%m-%d&quot;&#96;   </span><br><span class="line"></span><br><span class="line"># 获取前一秒时间</span><br><span class="line">date -d &quot;1 seconds ago&quot; &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line">date -d &quot;1 seconds ago 2010-01-11 13:24:59&quot; &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line"></span><br><span class="line"># 获取前一分钟时间</span><br><span class="line">date -d &quot;1 minutes ago&quot; &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line">date -d &quot;1 minutes ago 2010-01-11 13:24:59&quot; &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line"></span><br><span class="line"># 获取前一小时时间</span><br><span class="line">date -d &quot;1 hours ago&quot; &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line">date -d &quot;1 hours ago 2010-01-11 13:24:59&quot; &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line"></span><br><span class="line"># 获取前一天的日期</span><br><span class="line">date -d &quot;1 days ago&quot; +%Y-%m-%d</span><br><span class="line">date -d &quot;yesterday&quot; +%Y-%m-%d</span><br><span class="line">date -d &quot;2018-12-17 -1 day&quot; +%Y-%m-%d</span><br><span class="line">date -d &quot;-1 day 2018-12-17 13:24:59&quot; &quot;+%Y-%m-%d %H:%M:%S&quot;</span><br><span class="line"></span><br><span class="line"># 获取前一周的日期</span><br><span class="line">date -d &quot;1 weeks ago&quot; +%Y-%m-%d</span><br><span class="line">date -d &quot;2018-12-18 -1 week&quot; +%Y-%m-%d</span><br></pre></td></tr></table></figure>

<h4 id="3-截取字符串"><a href="#3-截取字符串" class="headerlink" title="3. 截取字符串"></a>3. 截取字符串</h4><blockquote>
<p>从0开始计算下标<br>左边的第一个字符是用<strong>0</strong>表示，右边的第一个字符用<strong>0-1</strong>表示</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># #号截取,删除左边字符,保留右边字符</span><br><span class="line"># var 是变量名,# 号是运算符,*&#x2F;&#x2F; 表示从左边开始删除第一个 &#x2F;&#x2F; 号及左边的所有字符</span><br><span class="line">var&#x3D;&quot;http:&#x2F;&#x2F;www.aaa.com&#x2F;123.html&quot;</span><br><span class="line">echo $&#123;var#*&#x2F;&#x2F;&#125;</span><br><span class="line">#输出 www.aaa.com&#x2F;123.html</span><br><span class="line"></span><br><span class="line"># ##号截取,删除左边字符,保留右边字符</span><br><span class="line"># ##*&#x2F; 表示从左边开始删除最后（最右边）一个 &#x2F; 号及左边的所有字符</span><br><span class="line">echo $&#123;var##*&#x2F;&#125;</span><br><span class="line">#输出 123.html</span><br><span class="line"></span><br><span class="line"># %号截取,删除右边字符,保留左边字符</span><br><span class="line"># %&#x2F;* 表示从右边开始,删除第一个 &#x2F; 号及右边的字符</span><br><span class="line">echo $&#123;var%&#x2F;*&#125;</span><br><span class="line">#输出 http:&#x2F;&#x2F;www.aaa.com</span><br><span class="line"></span><br><span class="line"># %%号截取,删除右边字符,保留左边字符</span><br><span class="line"># %%&#x2F;* 表示从右边开始,删除最后（最左边）一个 &#x2F; 号及右边的字符</span><br><span class="line">echo $&#123;var%%&#x2F;*&#125;</span><br><span class="line">#输出 http:</span><br><span class="line"></span><br><span class="line"># 截取从左边第几个字符开始，及字符的个数</span><br><span class="line"># 其中的0表示左边第一个字符开始,5表示字符的总个数</span><br><span class="line">echo $&#123;var:0:5&#125;</span><br><span class="line">#输出 http:</span><br><span class="line"></span><br><span class="line"># 从左边第几个字符开始,一直到结束</span><br><span class="line">echo $&#123;var:7&#125;</span><br><span class="line">#输出 www.aaa.com&#x2F;123.html</span><br><span class="line"></span><br><span class="line"># 从右边第几个字符开始,及字符的个数</span><br><span class="line"># 其中的 0-8 表示右边算起第八个字符开始,3 表示字符的个数</span><br><span class="line">echo $&#123;var:0-8:3&#125;</span><br><span class="line">#输出 123</span><br><span class="line"></span><br><span class="line"># 从右边第几个字符开始,一直到结束</span><br><span class="line"># 表示从右边第八个字符开始,一直到结束</span><br><span class="line">echo $&#123;var:0-8&#125;</span><br><span class="line">#输出 123.html</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark[RDD-DataFrame-DataSet]</title>
    <url>/2019/05/31/Spark%5BRDD-DataFrame-DataSet%5D/</url>
    <content><![CDATA[<blockquote>
<p>Spark三类简述</p>
</blockquote>
<span id="more"></span>

<h3 id="一-Spark2-x创建Spark对象"><a href="#一-Spark2-x创建Spark对象" class="headerlink" title="一. Spark2.x创建Spark对象"></a>一. Spark2.x创建Spark对象</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val spark &#x3D; SparkSession</span><br><span class="line">            .builder</span><br><span class="line">            .appName(&quot;&quot;)</span><br><span class="line">            .enableHiveSupport()</span><br><span class="line">            .getOrCreate()</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="二-RDD-DataFrame和DataSet"><a href="#二-RDD-DataFrame和DataSet" class="headerlink" title="二. RDD,DataFrame和DataSet"></a>二. RDD,DataFrame和DataSet</h3><h4 id="1-RDD"><a href="#1-RDD" class="headerlink" title="1. RDD"></a>1. RDD</h4><p><strong>优缺点</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">优点:</span><br><span class="line">    编译时类型安全,编译时能检查出类型错误</span><br><span class="line">    面向对象的编程风格,直接通过类名点的方式来操作数据</span><br><span class="line">缺点:</span><br><span class="line">    序列化和反序列化的性能开销,无论是集群间的通信还是IO操作都需要对对象的结构和数据进行序列化和反序列化</span><br><span class="line">    GC的性能开销,频繁的创建和销毁对象,势必会增加GC</span><br></pre></td></tr></table></figure>

<h4 id="2-DataFrame"><a href="#2-DataFrame" class="headerlink" title="2. DataFrame"></a>2. DataFrame</h4><p><strong>核心特征</strong></p>
<blockquote>
<p><strong>Schema:</strong><br>    包含了以ROW为单位的每行数据的列的信息;Spark通过Schema就能够读懂数据,因此在通信和IO时只需要序列化和反序列化数据,不需要考虑结构部分</p>
</blockquote>
<blockquote>
<p><strong>off_heap:</strong><br>    Spark能够以二进制的形式序列化数据(不包含结构)到off-heap中,当要操作数据时,就直接操作off-heap内存</p>
</blockquote>
<blockquote>
<p><strong>Tungsten:</strong><br>    新的执行引擎</p>
</blockquote>
<blockquote>
<p><strong>Catalyst:</strong><br>    新的语法解析框架</p>
</blockquote>
<p><strong>优缺点</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">优点:</span><br><span class="line">    对外off-heap就像地盘,schema就像地图,spark有地图又有自己的地盘,就可以自己说了算,不再受jvm的限制,也就不再受GC的困扰,通过schema和off-heap,DataFrame解决了RDD的缺点</span><br><span class="line">    对比RDD提升了计算效率,减少数据读取,底层计算优化</span><br><span class="line">缺点:</span><br><span class="line">    DataFrame解决了RDD的缺点,但是丢失了RDD的优点</span><br><span class="line">    DataFrame不是类型安全的</span><br><span class="line">    API也不是面向对象风格</span><br><span class="line">总结:</span><br><span class="line">    在效率上得到了优化,但是在代码编写上需要仔细</span><br></pre></td></tr></table></figure>

<h4 id="3-DataSet"><a href="#3-DataSet" class="headerlink" title="3. DataSet"></a>3. DataSet</h4><p><strong>核心特征(Encoder)</strong></p>
<blockquote>
<p>编译时的类型安全检查,性能极大的提升,内存使用极大降低,减少GC,极大的减少了网络数据的传输,极大的减少采用scala和java编程代码的差异性</p>
</blockquote>
<blockquote>
<p>DataFrame每一行对应一个Row,而DataSet的定义更加宽松,每一个record对应了一个任意的类型,DataFrame只是DataSet的一种特例</p>
</blockquote>
<blockquote>
<p>不同于Row是一个泛化的无类型JVM object,DataSet是由一系列的强类型JVM object组成的,Scala的case class或者java class定义.因此DataSet可以在变异时进行类型检查</p>
</blockquote>
<blockquote>
<p>DataSet以Catalyst逻辑执行计划表示,并且数据以编码的二进制形式被存储,不需要反序列化就可以执行sorting,shuffle等操作</p>
</blockquote>
<blockquote>
<p>DataSet创立需要一个显式的Encoder,把对象序列化为二进制</p>
</blockquote>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务生成和提交过程之OnStandAlone</title>
    <url>/2017/11/21/Spark%E4%BB%BB%E5%8A%A1%E7%94%9F%E6%88%90%E5%92%8C%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BOnStandAlone/</url>
    <content><![CDATA[<blockquote>
<p>整理Spark任务生成和提交过程,最好能在脑海形成一个图</p>
</blockquote>
<span id="more"></span>

<h2 id="Client-OR-Cluster"><a href="#Client-OR-Cluster" class="headerlink" title="Client OR Cluster"></a>Client OR Cluster</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Client</span><br><span class="line">Client模式提交任务,直接启动Driver,会在客户端看到task的执行情况和结果.用于测试环境</span><br><span class="line"># Cluster</span><br><span class="line">Cluster模式提交任务,会在Worker节点随机选择一个节点启动Driver,用于生产环境</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="1-Application"><a href="#1-Application" class="headerlink" title="1.Application"></a>1.Application</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 提交命令&lt;standalone&gt;</span><br><span class="line">spark-submit \</span><br><span class="line">--class MainClass \</span><br><span class="line">--master spark:&#x2F;&#x2F;hadoop01:7077 \</span><br><span class="line">JAR</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-Driver端"><a href="#2-Driver端" class="headerlink" title="2.Driver端"></a>2.Driver端</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.调用SparkSubmit类,内部执行submit--&gt;doRunMain--&gt;通过反射获取应用程序的主类对象--&gt;执行主类的main方法</span><br><span class="line">b.构建SparkConf和SparkContext对象,在SparkContext入口类做了三件事:</span><br><span class="line">    SparkEnv对象(创建ActorSystem对象)</span><br><span class="line">    TaskScheduler(用来生成并发送task给Executor)</span><br><span class="line">    DAGScheduler(用来划分Stage)</span><br><span class="line">c.ClientActor将任务信息封装到ApplicationDescription对象里并且提交给Master</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-Master端"><a href="#3-Master端" class="headerlink" title="3.Master端"></a>3.Master端</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.Master收到ClientActor提交的任务信息后,把任务信息存到内存中,然后又将任务信息放到队列中(waitingApps)</span><br><span class="line">b.当开始执行这个任务信息时,调用scheduler方法,进行资源调度</span><br><span class="line">c.将调度好的资源封装到LaunchExecutor并发送给对应的Worker</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="4-Worker端"><a href="#4-Worker端" class="headerlink" title="4.Worker端"></a>4.Worker端</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.Worker接收到Master发送过来的调度信息(LaunchExecutor)后,将信息封装成一个ExecutorRunner对象</span><br><span class="line">b.封装成ExecutorRunner后,调用ExecutorRunner的start方法,开始启动CoarseGrainedExecutorBackend对象</span><br><span class="line">c.Executor启动后向DriverActor反向注册</span><br><span class="line">d.与DriverActor注册成功后,创建一个线程池(TreadPool),用来执行任务</span><br></pre></td></tr></table></figure>

<h2 id="5-Driver端"><a href="#5-Driver端" class="headerlink" title="5.Driver端"></a>5.Driver端</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.当所有Executor注册完成后,意味着作业环境准备好了,Driver端会结束与SparkContext对象的初始化</span><br><span class="line">b.当Driver初始化完成后(sc实例创建完毕),会继续执行我们自己提交的App代码,当触发了Action的RDD算子时,就触发了一个Job,这时就会调用DAGScheduler对象进行Stage划分</span><br><span class="line">c.DAGScheduler开始进行Stage划分</span><br><span class="line">d.将划分好的Stage按照分区生成一个一个的task,并且封装到TaskSet对象,然后TaskSet提交到TaskScheduler</span><br><span class="line">e.TaskScheduler接收到提交过来的TaskSet,拿到一个序列化器,对TaskSet序列化,将序列化好的TaskSet封装到LaunchExecutor并提交到DriverActor</span><br><span class="line">f.把LaunchExecutor发送到Executor上</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="6-Worker端"><a href="#6-Worker端" class="headerlink" title="6.Worker端"></a>6.Worker端</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.Executor接收到DriverActor发送过来的任务(LaunchExecutor),会将其封装成TaskRunner,然后从线程池中获取线程来执行TaskRunner</span><br><span class="line">b.TaskRunner拿到反序列化器,反序列化TaskSet,然后执行App代码,也就是对RDD分区上执行的算子和自定义函数</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark读写HBase</title>
    <url>/2018/07/03/Spark%E8%AF%BB%E5%86%99HBase/</url>
    <content><![CDATA[<blockquote>
<p>不考虑HbaseUtil这种方式</p>
</blockquote>
<span id="more"></span>
<h2 id="读取HBase"><a href="#读取HBase" class="headerlink" title="读取HBase"></a>读取HBase</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获得HBase的配置</span></span><br><span class="line"><span class="keyword">val</span> hbaseConf = <span class="type">HbaseUtil</span>.getConf()</span><br><span class="line">hbaseConf.set(org.apache.hadoop.hbase.mapreduce.<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, tableName)</span><br><span class="line"><span class="keyword">val</span> admin = <span class="keyword">new</span> <span class="type">HBaseAdmin</span>(hbaseConf)</span><br><span class="line"><span class="keyword">if</span> (!admin.isTableAvailable(tableName))&#123;</span><br><span class="line">    <span class="keyword">val</span> tableDesc = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">    admin.createTable(tableDesc)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hBaseRdd = sc.newAPIHadoopRDD(hbaseConf,classOf[<span class="type">TableInputFormat</span>],classOf[org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span>],classOf[org.apache.hadoop.hbase.client.<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> count = hBaseRdd.count()</span><br><span class="line">hBaseRdd.foreach(x=&gt;&#123;</span><br><span class="line">    <span class="keyword">val</span> result = x._2</span><br><span class="line">    <span class="comment">// 获取行键</span></span><br><span class="line">    <span class="keyword">val</span> key = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">    <span class="comment">// 通过列簇和列名获取列</span></span><br><span class="line">    <span class="keyword">val</span> name = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">&quot;cf&quot;</span>.getBytes,<span class="string">&quot;name&quot;</span>.getBytes))</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="写入HBase"><a href="#写入HBase" class="headerlink" title="写入HBase"></a>写入HBase</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获得HBase的配置</span></span><br><span class="line"><span class="keyword">val</span> hbaseConf = <span class="type">HbaseUtil</span>.getConf()</span><br><span class="line">messages.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">  <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">    <span class="type">HbaseUtil</span>.getTable(hbaseConn, <span class="type">TABLE_NAME</span>)</span><br><span class="line">    <span class="keyword">val</span> jobConf = <span class="type">HbaseUtil</span>.getNewJobConf(hbaseConf, <span class="type">TABLE_NAME</span>)</span><br><span class="line">    <span class="comment">// 先处理消息</span></span><br><span class="line">    rdd.map(data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> rowKey = data._2.toString</span><br><span class="line">      <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(rowKey.getBytes())</span><br><span class="line">      put.addColumn(<span class="type">TABLE_CF</span>.getBytes(), <span class="string">&quot;count&quot;</span>.getBytes(), <span class="string">&quot;1&quot;</span>.getBytes())</span><br><span class="line">      <span class="comment">// 转成(Writable,Put)形式,调用spark函数写入HBase</span></span><br><span class="line">      <span class="comment">// saveAsHadoopDataset也是同理</span></span><br><span class="line">      (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>(), put)</span><br><span class="line">    &#125;).saveAsNewAPIHadoopDataset(jobConf)</span><br><span class="line">    <span class="comment">// 再更新offsets</span></span><br><span class="line">    km.updateZKOffsets(rdd)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HBaseUtil"><a href="#HBaseUtil" class="headerlink" title="HBaseUtil"></a>HBaseUtil</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">object HbaseUtil &#123;</span><br><span class="line">    var conn: Connection &#x3D; null</span><br><span class="line">    var tables: HashMap[String, Table] &#x3D; new HashMap[String, Table]</span><br><span class="line">    def initConn() &#123;</span><br><span class="line">        if (conn &#x3D;&#x3D; null || conn.isClosed) &#123;</span><br><span class="line">            println(&quot;----  Init Conn  -----&quot;)</span><br><span class="line">            val conf &#x3D; getConf()</span><br><span class="line">            conn &#x3D; ConnectionFactory.createConnection(conf)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    def getConn() &#x3D; &#123;</span><br><span class="line">        initConn</span><br><span class="line">        conn</span><br><span class="line">    &#125;</span><br><span class="line">    def getConf() &#x3D; &#123;</span><br><span class="line">        val conf &#x3D; HBaseConfiguration.create()</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;, HBASE_ZOOKEEPER)</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)</span><br><span class="line">        conf</span><br><span class="line">    &#125;</span><br><span class="line">    def getTable(tablename: String) &#x3D; &#123;</span><br><span class="line">        if (!getConn().getAdmin.tableExists(TableName.valueOf(tablename))) &#123;</span><br><span class="line">            conn.getAdmin.createTable(new HTableDescriptor(TableName.valueOf(tablename)).addFamily(new HColumnDescriptor(&quot;info&quot;)))</span><br><span class="line">        &#125;</span><br><span class="line">        tables.getOrElse(tablename, &#123;</span><br><span class="line">            initConn</span><br><span class="line">            conn.getTable(TableName.valueOf(tablename))</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">    def put(tableName: String, p: Put) &#123;</span><br><span class="line">        getTable(tableName)</span><br><span class="line">        .put(p)</span><br><span class="line">    &#125;</span><br><span class="line">    def get(tableName: String, get: Get, cf: String, column: String) &#x3D; &#123;</span><br><span class="line">        val r &#x3D; getTable(tableName)</span><br><span class="line">        .get(get)</span><br><span class="line">        if (r !&#x3D; null &amp;&amp; !r.isEmpty()) &#123;</span><br><span class="line">            new String(r.getValue(cf.getBytes, column.getBytes))</span><br><span class="line">        &#125; else null</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;  接受配置文件</span><br><span class="line">    &#x2F;**</span><br><span class="line">    * 用于直接建立HBase连接</span><br><span class="line">    *</span><br><span class="line">    * @param properties</span><br><span class="line">    * @return</span><br><span class="line">    *&#x2F;</span><br><span class="line">    def getConf(properties: Properties) &#x3D; &#123;</span><br><span class="line">        val conf &#x3D; HBaseConfiguration.create()</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;, properties.getProperty(&quot;hbase.zookeeper.quorum&quot;))</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, properties.getProperty(&quot;hbase.zookeeper.property.clientPort&quot;))</span><br><span class="line">        conf.set(&quot;hbase.master&quot;, properties.getProperty(&quot;hbase.master&quot;))</span><br><span class="line">        conf</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;**</span><br><span class="line">    * 获取连接</span><br><span class="line">    *</span><br><span class="line">    * @param conf</span><br><span class="line">    * @return</span><br><span class="line">    *&#x2F;</span><br><span class="line">    def getConn(conf: Configuration) &#x3D; &#123;</span><br><span class="line">        if (conn &#x3D;&#x3D; null || conn.isClosed) &#123;</span><br><span class="line">            conn &#x3D; ConnectionFactory.createConnection(conf)</span><br><span class="line">        &#125;</span><br><span class="line">        conn</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;**</span><br><span class="line">    * 获取表,没有表则创建</span><br><span class="line">    *</span><br><span class="line">    * @param conn</span><br><span class="line">    * @param tableName</span><br><span class="line">    * @return</span><br><span class="line">    *&#x2F;</span><br><span class="line">    def getTable(conn: Connection, tableName: String) &#x3D; &#123;</span><br><span class="line">        createTable(conn, tableName)</span><br><span class="line">        conn.getTable(TableName.valueOf(tableName))</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;**</span><br><span class="line">    * 创建表</span><br><span class="line">    *</span><br><span class="line">    * @param conn</span><br><span class="line">    * @param tableName</span><br><span class="line">    *&#x2F;</span><br><span class="line">    def createTable(conn: Connection, tableName: String) &#x3D; &#123;</span><br><span class="line">        if (!conn.getAdmin.tableExists(TableName.valueOf(tableName))) &#123;</span><br><span class="line">            val tableDescriptor &#x3D; new HTableDescriptor(TableName.valueOf(tableName))</span><br><span class="line">            tableDescriptor.addFamily(new HColumnDescriptor(&quot;info&quot;.getBytes()))</span><br><span class="line">            conn.getAdmin.createTable(tableDescriptor)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;**</span><br><span class="line">    * 提交数据</span><br><span class="line">    *</span><br><span class="line">    * @param conn</span><br><span class="line">    * @param tableName</span><br><span class="line">    * @param data</span><br><span class="line">    *&#x2F;</span><br><span class="line">    def putData(conn: Connection, tableName: String, data: Put) &#x3D; &#123;</span><br><span class="line">        getTable(conn, tableName).put(data)</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;**</span><br><span class="line">    * 对表直接进行批量写入时使用</span><br><span class="line">    *</span><br><span class="line">    * @param conf</span><br><span class="line">    * @param tableName</span><br><span class="line">    * @return</span><br><span class="line">    *&#x2F;</span><br><span class="line">    def getNewJobConf(conf: Configuration, tableName: String) &#x3D; &#123;</span><br><span class="line">        conf.set(&quot;hbase.defaults.for.version.skip&quot;, &quot;true&quot;)</span><br><span class="line">        conf.set(org.apache.hadoop.hbase.mapreduce.TableOutputFormat.OUTPUT_TABLE, tableName)</span><br><span class="line">        conf.setClass(&quot;mapreduce.job.outputformat.class&quot;, classOf[org.apache.hadoop.hbase.mapreduce.TableOutputFormat[String]], classOf[org.apache.hadoop.mapreduce.OutputFormat[String, Mutation]])</span><br><span class="line">        new JobConf(conf)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark任务生成和提交过程之OnYarn</title>
    <url>/2017/11/21/Spark%E4%BB%BB%E5%8A%A1%E7%94%9F%E6%88%90%E5%92%8C%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BOnYarn/</url>
    <content><![CDATA[<blockquote>
<p>整理Spark任务生成和提交过程,最好能在脑海形成一个图</p>
</blockquote>
<span id="more"></span>

<h2 id="Client-OR-Cluster"><a href="#Client-OR-Cluster" class="headerlink" title="Client OR Cluster"></a>Client OR Cluster</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Client</span><br><span class="line">Client模式提交任务,直接启动Driver,会在客户端看到task的执行情况和结果,用于测试环境</span><br><span class="line"># Cluster</span><br><span class="line">Cluster模式提交任务,会在某个NodeManager节点随机选择一个节点启动ApplicationMaster,AM启动Driver,用于生产环境</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Client模式"><a href="#Client模式" class="headerlink" title="Client模式"></a>Client模式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.客户端启动后直接运行应用程序，直接启动Driver</span><br><span class="line">b.Driver初始化并生成一系列Task</span><br><span class="line">c.客户端将job发布到yarn上</span><br><span class="line">d.RM为该job在某个NM分配一个AM</span><br><span class="line">e.AM向RM申请资源，RM返回Executor信息</span><br><span class="line">f.AM通过RPC启动相应的SparkExecutor</span><br><span class="line">g.Driver向Executor分配task</span><br><span class="line">h.Executor执行task并将结果写入第三方存储系统或者Driver端</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Cluster模式"><a href="#Cluster模式" class="headerlink" title="Cluster模式"></a>Cluster模式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.客户端向yarn提交一个job</span><br><span class="line">b.RM为该job在某个NM上分配一个AM，NM启动AM，AM启动Driver</span><br><span class="line">c.AM启动后完成初始化作业，Driver生成一系列task</span><br><span class="line">d.AM向RM申请资源，RM返回Executor信息</span><br><span class="line">e.AM通过RPC启动相应的SparkExecutor</span><br><span class="line">f.Driver向Executor分配task</span><br><span class="line">g.Executor执行结果写入文件或返回Driver端</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark操作ES出现InvalidFormat:Date</title>
    <url>/2018/12/12/Spark%E6%93%8D%E4%BD%9CES%E5%87%BA%E7%8E%B0InvalidFormat-Date/</url>
    <content><![CDATA[<blockquote>
<p>读取ES出现时间解析错误</p>
</blockquote>
<span id="more"></span>

<h1 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sparkconf中设置es.mapping.date.rich为false</span><br><span class="line">conf.set(&quot;es.mapping.date.rich&quot;, &quot;false&quot;);</span><br><span class="line"></span><br><span class="line"># 在命令行提交时设置spark.es.mapping.date.rich为false</span><br><span class="line">--conf spark.es.mapping.date.rich&#x3D;false</span><br><span class="line"></span><br><span class="line"># 可以不解析为date，直接返回string。</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>elk</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkSQL源码概览</title>
    <url>/2020/05/06/SparkSQL%E6%BA%90%E7%A0%81%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<blockquote>
<p>SparkSQL内核剖析读后总结</p>
</blockquote>
<span id="more"></span>

<h2 id="SparkSQL引擎-Catalyst"><a href="#SparkSQL引擎-Catalyst" class="headerlink" title="SparkSQL引擎:Catalyst"></a>SparkSQL引擎:Catalyst</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">InternalRow(数据行)</span><br><span class="line">TreeNode(执行树)</span><br><span class="line">AbstractDataType(类型)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="编译器"><a href="#编译器" class="headerlink" title="编译器"></a>编译器</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ANTLR4文法解析,通过调用SQLBase.g4文件对SQL进行解析</span><br><span class="line">生成AST语法树</span><br><span class="line"></span><br><span class="line">涉及到的类</span><br><span class="line">ParserInterface</span><br><span class="line">    AbstractSqlParser-&gt;AstBuilder-&gt;SqlBaseBaseVisitor</span><br><span class="line">        CatalystSqlParser</span><br><span class="line">        SparkSqlParser-&gt;SparkSqlAstBuilder</span><br><span class="line">        </span><br><span class="line">整体语法树生成使用访问者模式</span><br><span class="line">SQL各种语法对应以Context结尾</span><br><span class="line">SingleStatementContext</span><br><span class="line">    QuerySpecificationContext</span><br><span class="line">        NamedExpressionSeqContext(列名)</span><br><span class="line">        FromClauseContext(表名)</span><br><span class="line">        BooleanDefaultContext(where条件)</span><br><span class="line">        AggregationContext(groupby分组)</span><br><span class="line">    QueryOrganizationContext</span><br><span class="line">        SortItemContext(排序)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="逻辑计划-物理计划"><a href="#逻辑计划-物理计划" class="headerlink" title="逻辑计划/物理计划"></a>逻辑计划/物理计划</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">都继承QueryPlan父类</span><br><span class="line">    输入输出</span><br><span class="line">    基本属性</span><br><span class="line">    字符串</span><br><span class="line">    规范化</span><br><span class="line">    表达式操作</span><br><span class="line">    约束</span><br><span class="line"></span><br><span class="line">LogicalPlan</span><br><span class="line">    UnresolvedLogicalPlan(未解析的逻辑计划)</span><br><span class="line">    AnalyzedLogicalPlan(解析的逻辑计划)</span><br><span class="line">    OptimizerLogicalPlan(优化的逻辑计划)</span><br><span class="line">        LeafNode(关系,内存&#x2F;Hive&#x2F;RDD)</span><br><span class="line">        UnaryNode(排序&#x2F;重分区&#x2F;数据转换&#x2F;过滤等)</span><br><span class="line">        BinaryNode(连接&#x2F;集合&#x2F;CoGroup)</span><br><span class="line"></span><br><span class="line">Analyzed和Optimizer都是去获取对应的规则</span><br><span class="line">    进行生成&#x2F;解析与优化</span><br><span class="line">    </span><br><span class="line">经过QueryPlanner实现逻辑计划到物理计划的转变</span><br><span class="line">    SparkStreategies</span><br><span class="line">        SparkPlanner</span><br><span class="line"></span><br><span class="line">PhysicalPlan</span><br><span class="line">    LeafExecNode</span><br><span class="line">    UnaryExecNode</span><br><span class="line">    BinaryExecNode</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark连接Neo4J操作</title>
    <url>/2019/11/01/Spark%E8%BF%9E%E6%8E%A5Neo4J%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<blockquote>
<p>Spark对于Neo4J进行整合</p>
</blockquote>
<span id="more"></span>

<blockquote>
<p><a href="https://github.com/neo4j-contrib/neo4j-spark-connector/tree/master">Github</a></p>
</blockquote>
<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果是Spark1的话使用1.0.0-RC1</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">  &lt;!-- list of dependencies --&gt;</span><br><span class="line">  &lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;neo4j-contrib&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;neo4j-spark-connector&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.4.0-M6&lt;&#x2F;version&gt;</span><br><span class="line">  &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br><span class="line">&lt;repositories&gt;</span><br><span class="line">  &lt;!-- list of other repositories --&gt;</span><br><span class="line">  &lt;repository&gt;</span><br><span class="line">    &lt;id&gt;SparkPackagesRepo&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;url&gt;http:&#x2F;&#x2F;dl.bintray.com&#x2F;spark-packages&#x2F;maven&lt;&#x2F;url&gt;</span><br><span class="line">  &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;repositories&gt;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Spark1连接Neo4J"><a href="#Spark1连接Neo4J" class="headerlink" title="Spark1连接Neo4J"></a>Spark1连接Neo4J</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val conf &#x3D; new SparkConf().setAppName(&quot;InitSpark&quot;)</span><br><span class="line">conf.set(&quot;spark.neo4j.bolt.url&quot;,properties.getProperty(&quot;bolt.url&quot;))</span><br><span class="line">conf.set(&quot;spark.neo4j.bolt.user&quot;,properties.getProperty(&quot;bolt.user&quot;))</span><br><span class="line">conf.set(&quot;spark.neo4j.bolt.password&quot;,properties.getProperty(&quot;bolt.password&quot;))</span><br><span class="line"></span><br><span class="line"># Neo4jDataFrame.withDataType(sqlContext: SQLContext, query: String, parameters: Seq[(String, Any)], schema: (String, DataType)*)</span><br><span class="line">Neo4jDataFrame.withDataType(spark.sqlContext, query, Seq.empty, &quot;id&quot; -&gt; LongType, &quot;nodes&quot; -&gt; ArrayType.apply(LongType))</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Spark2连接Neo4J"><a href="#Spark2连接Neo4J" class="headerlink" title="Spark2连接Neo4J"></a>Spark2连接Neo4J</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val spark &#x3D; SparkSession.builder()</span><br><span class="line">    .appName(&quot;InitSpark&quot;)</span><br><span class="line">    .config(&quot;spark.neo4j.bolt.url&quot;, properties.getProperty(&quot;bolt.url&quot;))</span><br><span class="line">    .config(&quot;spark.neo4j.bolt.user&quot;, properties.getProperty(&quot;bolt.user&quot;))</span><br><span class="line">    .config(&quot;spark.neo4j.bolt.password&quot;, properties.getProperty(&quot;bolt.password&quot;))</span><br><span class="line">    .config(&quot;spark.es.write.operation&quot;, &quot;upsert&quot;)</span><br><span class="line">    .config(&quot;spark.es.index.auto.create&quot;, &quot;true&quot;)</span><br><span class="line">    .config(&quot;spark.es.nodes&quot;, properties.getProperty(&quot;es.nodes&quot;))</span><br><span class="line">    .config(&quot;spark.es.port&quot;, properties.getProperty(&quot;es.port&quot;))</span><br><span class="line">    .config(&quot;spark.es.nodes.wan.only&quot;, &quot;true&quot;)</span><br><span class="line">    .config(&quot;spark.port.maxRetries&quot;, &quot;100&quot;)</span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .getOrCreate()</span><br><span class="line">    </span><br><span class="line"># Neo4jDataFrame.withDataType(sqlContext: SQLContext, query: String, parameters: Seq[(String, Any)], schema: (String, DataType)*)</span><br><span class="line">Neo4jDataFrame.withDataType(spark.sqlContext, query, Seq.empty, &quot;id&quot; -&gt; LongType, &quot;nodes&quot; -&gt; StringType)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在spark1中,Neo4J的语句是可以返回数组的</span><br><span class="line"># 在Spark2中,Neo4J的语句返回数组后,schema不接受数组类型</span><br><span class="line"></span><br><span class="line"># 解决方案:</span><br><span class="line">Neo4J不返回数组,而是拼接字符串;</span><br><span class="line">成功返回DataFrame之后再进行分割成List;</span><br><span class="line">最后通过explode方法进行行转列.</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat的一些小应用</title>
    <url>/2018/06/18/Tomcat%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>Tomcat的修改</p>
</blockquote>
<span id="more"></span>

<h2 id="CMD启动乱码"><a href="#CMD启动乱码" class="headerlink" title="CMD启动乱码"></a>CMD启动乱码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改tomcat的config目录下的logging.properties文件</span><br><span class="line">java.util.logging.ConsoleHandler.encoding &#x3D; UTF-8</span><br><span class="line">改为</span><br><span class="line">java.util.logging.ConsoleHandler.encoding &#x3D; GBK</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="自动重新加载"><a href="#自动重新加载" class="headerlink" title="自动重新加载"></a>自动重新加载</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改tomcat&#x2F;conf&#x2F;context.xml文件</span><br><span class="line"># 加上reloadable&#x3D;&quot;true&quot;</span><br><span class="line">&lt;Context reloadable&#x3D;&quot;true&quot;&gt;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="开启manager"><a href="#开启manager" class="headerlink" title="开启manager"></a>开启manager</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改tomcat&#x2F;conf&#x2F;tomcat-users.xml文件</span><br><span class="line">&lt;role rolename&#x3D;&quot;manager-gui&quot;&#x2F;&gt;</span><br><span class="line">&lt;role rolename&#x3D;&quot;admin-gui&quot;&#x2F;&gt;</span><br><span class="line">&lt;role rolename&#x3D;&quot;manager-script&quot;&#x2F;&gt;</span><br><span class="line">&lt;user username&#x3D;&quot;admin&quot; password&#x3D;&quot;admin&quot; roles&#x3D;&quot;admin-gui,manager-gui,manager-script&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>

<h2 id="设置Tomcat展开目录"><a href="#设置Tomcat展开目录" class="headerlink" title="设置Tomcat展开目录"></a>设置Tomcat展开目录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 打开Tomcat所在安装目录,打开到conf配置文件下,打开web.xml文件</span><br><span class="line"># 修改listings属性,将其设定为true</span><br><span class="line">&lt;init-param&gt;</span><br><span class="line">    &lt;param-name&gt;listings&lt;&#x2F;param-name&gt;</span><br><span class="line">    &lt;param-value&gt;false&lt;&#x2F;param-value&gt;</span><br><span class="line">&lt;&#x2F;init-param&gt;</span><br><span class="line"># 重启Tomcat服务</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark面试整理</title>
    <url>/2020/03/06/Spark%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>记录面试中有关Spark的概念知识</p>
</blockquote>
<span id="more"></span>

<h1 id="Spark面试整理"><a href="#Spark面试整理" class="headerlink" title="Spark面试整理"></a>Spark面试整理</h1><h1 id="Spark概念"><a href="#Spark概念" class="headerlink" title="Spark概念"></a>Spark概念</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">三大核心数据结构</span><br><span class="line">    RDD</span><br><span class="line">    广播变量(分布式只读变量)</span><br><span class="line">    累加器(分布式只写变量)</span><br><span class="line">四大组件</span><br><span class="line">    Spark SQL</span><br><span class="line">    Spark Streaming</span><br><span class="line">    MLlib</span><br><span class="line">    GraphX</span><br></pre></td></tr></table></figure>

<h1 id="Spark的工作机制"><a href="#Spark的工作机制" class="headerlink" title="Spark的工作机制"></a>Spark的工作机制</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">用户在客户端提交作业后,会由Driver运行main方法并创建SparkContext上下文</span><br><span class="line">SparkContext向资源管理器申请资源,启动Execotor进程</span><br><span class="line">并通过执行rdd算子,形成DAG有向无环图,输入DAGscheduler</span><br><span class="line">然后通过DAGscheduler调度器,将DAG有向无环图按照rdd之间的依赖关系划分为几个阶段,也就是stage</span><br><span class="line">输入TaskScheduler,然后通过任务调度器TaskScheduler将stage划分为Task Set分发到各个节点的Executor中执行。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Spark的stage划分"><a href="#Spark的stage划分" class="headerlink" title="Spark的stage划分"></a>Spark的stage划分</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在DAG调度的过程中,Stage阶段的划分是根据是否有shuffle过程</span><br><span class="line">也就是存在ShuffleDependency宽依赖的时候</span><br><span class="line">需要进行shuffle,这时候会将作业job划分成多个Stage</span><br><span class="line"></span><br><span class="line">整体思路:</span><br><span class="line">从后往前推,遇到宽依赖就断开,划分为一个stage</span><br><span class="line">遇到窄依赖就将这个RDD加入该stage中</span><br></pre></td></tr></table></figure>

<h1 id="简单介绍一下RDD"><a href="#简单介绍一下RDD" class="headerlink" title="简单介绍一下RDD?"></a>简单介绍一下RDD?</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个抽象弹性分布式数据集</span><br><span class="line">可以将RDD理解为一个分布式对象集合,本质上是一个只读的分区记录集合</span><br><span class="line">每个RDD可以分成多个分区,每个分区就是一个数据集片段</span><br><span class="line">一个RDD的不同分区可以保存到集群中的不同结点上,从而可以在集群中的不同节点上进行并行计算</span><br><span class="line"></span><br><span class="line"># 五大特性</span><br><span class="line">1.分区列表</span><br><span class="line">    RDD是一个由多个partition(某个节点里的某一片连续的数据)组成的的List</span><br><span class="line">    将数据加载为RDD时,一般一个hdfs里的block会加载为一个partition</span><br><span class="line">2.每一个分区都有一个计算函数</span><br><span class="line">    RDD的每个partition上面都会有function,也就是函数应用,其作用是实现RDD之间partition的转换</span><br><span class="line">3.依赖于其他RDD列表</span><br><span class="line">    RDD会记录它的依赖,为了容错,也就是说在内存中的RDD操作时出错或丢失会进行重算</span><br><span class="line">4.Key-Value数据类型的RDD分区器</span><br><span class="line">    可选项,如果RDD里面存的数据是key-value形式,则可以传递一个自定义的Partitioner进行重新分区</span><br><span class="line">    例如这里自定义的Partitioner是基于key进行分区,那则会将不同RDD里面的相同key的数据放到同一个partition里面</span><br><span class="line">5.每个分区都有一个优先位置列表</span><br><span class="line">    可选项,最优的位置去计算,也就是数据的本地性</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Transformations-amp-Action"><a href="#Transformations-amp-Action" class="headerlink" title="Transformations &amp; Action"></a>Transformations &amp; Action</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个Application中有几个Action类算子执行,就会有几个Job运行</span><br><span class="line"></span><br><span class="line">转换</span><br><span class="line">    map</span><br><span class="line">    filter</span><br><span class="line">    flatMap</span><br><span class="line">    mapPartitions</span><br><span class="line">    mapPartitionsWithIndex</span><br><span class="line">    sample</span><br><span class="line">    union</span><br><span class="line">    intersection</span><br><span class="line">    distinct</span><br><span class="line">    groupByKey</span><br><span class="line">    reduceByKey</span><br><span class="line">    aggregateByKey</span><br><span class="line">    sortByKey</span><br><span class="line">    join</span><br><span class="line">    cogroup</span><br><span class="line">    cartesian</span><br><span class="line">    pipe</span><br><span class="line">    coalesce</span><br><span class="line">    repartition</span><br><span class="line">    repartitionAndSortWithinPartitions</span><br><span class="line"></span><br><span class="line">操作</span><br><span class="line">    reduce</span><br><span class="line">    collect</span><br><span class="line">    count</span><br><span class="line">    first</span><br><span class="line">    take</span><br><span class="line">    takeSample</span><br><span class="line">    takeOrdered</span><br><span class="line">    saveAsTextFile</span><br><span class="line">    saveAsSequenceFile</span><br><span class="line">    saveAsObjectFile</span><br><span class="line">    countByKey</span><br><span class="line">    foreach</span><br><span class="line">    </span><br><span class="line">注意:</span><br><span class="line">    区别宽依赖与窄依赖的算子</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="宽依赖-amp-窄依赖"><a href="#宽依赖-amp-窄依赖" class="headerlink" title="宽依赖 &amp; 窄依赖"></a>宽依赖 &amp; 窄依赖</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">宽依赖用于划分Stage,也就是一个Shuffle操作,子RDD的每个分区依赖于所有父分区</span><br><span class="line">窄依赖子RDD的每个分区依赖于常数个父分区</span><br><span class="line"></span><br><span class="line">宽依赖算子</span><br><span class="line">    groupByKey</span><br><span class="line">    reduceByKey</span><br><span class="line">    join</span><br><span class="line">    等</span><br><span class="line"></span><br><span class="line">窄依赖算子</span><br><span class="line">    map</span><br><span class="line">    flatMap</span><br><span class="line">    union</span><br><span class="line">    filter</span><br><span class="line">    distinct</span><br><span class="line">    subtract</span><br><span class="line">    sample</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="repartition-amp-coalesce"><a href="#repartition-amp-coalesce" class="headerlink" title="repartition &amp; coalesce"></a>repartition &amp; coalesce</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">注意:</span><br><span class="line">    repartition &#x3D; coalesce(分区数,true)</span><br><span class="line"></span><br><span class="line">原分区数大于目标分区数,不会发生shuffle的情况使用coalesce</span><br><span class="line"></span><br><span class="line">原分区小于目标分区数,需要发生shuffle的情况下使用repartition</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Structured-Streaming"><a href="#Structured-Streaming" class="headerlink" title="Structured Streaming"></a>Structured Streaming</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StructuredStreaming周期性或连续不断的生成微小DataSet,然后交给SparkSQL</span><br><span class="line">跟SparkSQL原有引擎相比,增加了增量处理的功能</span><br><span class="line">增量就是为了状态和流标功能实现</span><br><span class="line">不过也是微批处理,底层执行也是依赖SparkSQL的</span><br><span class="line">不支持异步维表join</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之三上手使用</title>
    <url>/2019/03/22/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B8%89%E4%B8%8A%E6%89%8B%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Flink入门之三"><a href="#Flink入门之三" class="headerlink" title="Flink入门之三"></a>Flink入门之三</h1><h1 id="编译Flink代码"><a href="#编译Flink代码" class="headerlink" title="编译Flink代码"></a>编译Flink代码</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean install -DskipTests</span><br><span class="line"># 或者</span><br><span class="line">mvn clean package -DskipTests</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DAG图中不能被Chain在一起的Operator会被分隔到不同的Task中</span><br><span class="line">Task是Flink中资源调度的最小单元</span><br><span class="line"></span><br><span class="line">两类进程</span><br><span class="line">JobManager:协调Task的分布式执行,包括调度Task,协调创建CK以及当Job Failover时协调各个Task从CK恢复等</span><br><span class="line">TaskManager:执行DataFlow中的Tasks,包括内存Buffer的分配,DataStream的传递等</span><br><span class="line"></span><br><span class="line">Task Slot是TM中最小资源分配单位,一个TM中有多少个Task Slot就意味着支持多少并发Task处理</span><br><span class="line">一个Task Slot可以执行多个Operator</span><br><span class="line">Operator是能够被Chain在一起处理的,任务链</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="日志配置"><a href="#日志配置" class="headerlink" title="日志配置"></a>日志配置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">默认使用log4j配置,如果需要使用其他的日志方式,可以删除文件</span><br><span class="line">log4j-cli.properties:用Flink命令行时用的log配置,比如执行flink run</span><br><span class="line">log4j-yarn-session.properties:用yarn-session.sh启动时命令行执行时用的日志配置</span><br><span class="line">log4j.properties:无论Standalone还是Yarn模式,JM和TM使用的日志配置</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Yarn跑Flink的好处"><a href="#Yarn跑Flink的好处" class="headerlink" title="Yarn跑Flink的好处"></a>Yarn跑Flink的好处</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">资源按需使用，提高集群的资源利用率</span><br><span class="line">任务有优先级，根据优先级运行作业</span><br><span class="line">基于 Yarn 调度系统，能够自动化地处理各个角色的 Failover</span><br><span class="line">    JobManager进程和TaskManager进程都由Yarn NodeManager监控</span><br><span class="line">    如果JobManager进程异常退出，则Yarn ResourceManager会重新调度JobManager到其他机器</span><br><span class="line">    如果TaskManager进程异常退出，JobManager会收到消息并重新向Yarn ResourceManager申请资源，重新启动TaskManager</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="参数关系"><a href="#参数关系" class="headerlink" title="参数关系"></a>参数关系</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-n(yarn-session) 与 -p 的关系</span><br><span class="line">    -n和-yn在社区版本中没有实际的控制作用,实际资源由-p申请</span><br><span class="line">    在blink的开源版本的yarn-session中,-n用于启动指定的TM,即使Job需要更多的slot也不会申请新的TM</span><br><span class="line">-yn(yarn-cluster) 与 -p 的关系</span><br><span class="line">    在blink的开源版本的yarn-cluster中,-yn表示初始TM数量,不设置上限,实际还是由-p和-ys决定</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之七状态管理容错机制</title>
    <url>/2019/04/19/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B8%83%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="状态类型"><a href="#状态类型" class="headerlink" title="状态类型"></a>状态类型</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Managed State &amp; Raw State</span><br><span class="line">MS: Flink Runtime管理,已知数据结构,大多数情况均可使用</span><br><span class="line">RS: 用户自定义,自己管理,字节数组,需要自定义时使用</span><br><span class="line"></span><br><span class="line">MS分为两种,KeyedState,OperatorState</span><br><span class="line">KeyState</span><br><span class="line">    Datastream经过keyBy的操作可以变为KeyedStream</span><br><span class="line">    每个Key对应一个State,即一个Operator实例处理多个Key,访问相应的多个State,并由此就衍生了KeyedState</span><br><span class="line">    KeyedState只能用在KeyedStream的算子中,即在整个程序中没有keyBy的过程就没有办法使用KeyedState</span><br><span class="line">OperatorState</span><br><span class="line">     可以用于所有算子,相对于数据源有一个更好的匹配方式</span><br><span class="line">     常用于Source,例如FlinkKafkaConsumer</span><br><span class="line">     相比KeyedState,一个Operator实例对应一个State</span><br><span class="line">     随着并发的改变,KeyedState中,State随着Key在实例间迁移</span><br><span class="line">     OperatorState没有Key,并发改变时需要选择状态如何重新分配</span><br><span class="line">         均匀分配</span><br><span class="line">         所有State合并为全量State再分发给每个实例</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="KeyedState使用"><a href="#KeyedState使用" class="headerlink" title="KeyedState使用"></a>KeyedState使用</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">State</span><br><span class="line">    ValueState 存储单个值,Get&#x2F;Set,Update</span><br><span class="line">    MapState Put&#x2F;Remove</span><br><span class="line">    AppendlingState</span><br><span class="line">        MergingState</span><br><span class="line">            ListState Add&#x2F;Update</span><br><span class="line">            ReducingState 单个值,可以将数据相加</span><br><span class="line">            AggregatingState 单个值,输入输出类型可以不一致</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">env.enableCheckpointing(1000) # 每1秒做CK</span><br><span class="line">env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) # EXACTLY_ONCE代表Barries对齐</span><br><span class="line">env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500) # 2个CK之间至少等待500ms</span><br><span class="line">env.getCheckpointConfig().setCheckpointTimeout(60000) # CK超时,一分钟没有做完就超时</span><br><span class="line">env.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION) # 是否Cancel时是否保留当前的CK</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="状态存储方式"><a href="#状态存储方式" class="headerlink" title="状态存储方式"></a>状态存储方式</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MemoryStateBackend</span><br><span class="line">    存储方式</span><br><span class="line">        State: TaskManager内存</span><br><span class="line">        CK: JobManager内存</span><br><span class="line">    容量限制</span><br><span class="line">        单个State MaxStateSize默认5M</span><br><span class="line">        maxStateSize &lt;&#x3D; akka.framesize默认10M</span><br><span class="line">        CK总大小不超过JobManager的内存</span><br><span class="line"></span><br><span class="line">FsStateBackend</span><br><span class="line">    存储方式</span><br><span class="line">        State: TaskManager内存</span><br><span class="line">        CK: 外部文件系统(本地或HDFS)</span><br><span class="line">    容量限制</span><br><span class="line">        单TM上State总量不超过它的内存</span><br><span class="line">        CK总大小不超过配置的文件系统容量</span><br><span class="line">        </span><br><span class="line">RocksDBStateBackend</span><br><span class="line">    存储方式</span><br><span class="line">        State: TM的KV数据库(内存+磁盘)</span><br><span class="line">        CK: 外部文件系统(本地或HDFS)</span><br><span class="line">    容量限制</span><br><span class="line">        单TM上State总量不超过它的内存+磁盘</span><br><span class="line">        单Key最大2G</span><br><span class="line">        CK总大小不超过配置的文件系统容量</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之一大纲介绍</title>
    <url>/2019/03/07/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B8%80%E5%A4%A7%E7%BA%B2%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Flink入门之一"><a href="#Flink入门之一" class="headerlink" title="Flink入门之一"></a>Flink入门之一</h1><h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink Application</span><br><span class="line">    基础处理语义</span><br><span class="line">    多层次API</span><br><span class="line">Flink Architecture</span><br><span class="line">    有界和无界数据流</span><br><span class="line">    部署灵活</span><br><span class="line">    极高可伸缩性</span><br><span class="line">    极致流式处理性能</span><br><span class="line">FLink Operation</span><br><span class="line">    7*24小时高可用</span><br><span class="line">    业务应用监控运维</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink-Application-Streams"><a href="#Flink-Application-Streams" class="headerlink" title="Flink Application - Streams"></a>Flink Application - Streams</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">有边界的流</span><br><span class="line">    Batch可以看做一个有边界的流</span><br><span class="line">无边界的流</span><br><span class="line">    Stream</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink-Application-State"><a href="#Flink-Application-State" class="headerlink" title="Flink Application - State"></a>Flink Application - State</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">带状态的计算</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink-Application-Time"><a href="#Flink-Application-Time" class="headerlink" title="Flink Application - Time"></a>Flink Application - Time</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Event Time</span><br><span class="line">Ingestion Time</span><br><span class="line">Processing Time</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink-Application-API"><a href="#Flink-Application-API" class="headerlink" title="Flink Application - API"></a>Flink Application - API</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SQL&#x2F;Table API</span><br><span class="line">DataStream API</span><br><span class="line">ProcessFunction</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink-Architecture-Stateful"><a href="#Flink-Architecture-Stateful" class="headerlink" title="Flink Architecture - Stateful"></a>Flink Architecture - Stateful</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tasks会保存自身的状态，本地内存或硬盘</span><br><span class="line">Flink会定期远程对State归档</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Scenario-Data-Pipeline"><a href="#Flink-Scenario-Data-Pipeline" class="headerlink" title="Flink Scenario - Data Pipeline"></a>Flink Scenario - Data Pipeline</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">数据搬运，在搬运过程中进行清洗转换，流式的ETL</span><br><span class="line">实时数仓</span><br><span class="line">实时搜索引擎</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Scenario-Data-Analytics"><a href="#Flink-Scenario-Data-Analytics" class="headerlink" title="Flink Scenario - Data Analytics"></a>Flink Scenario - Data Analytics</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">实时报表</span><br><span class="line">实时大屏</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Scenario-Data-Driven"><a href="#Flink-Scenario-Data-Driven" class="headerlink" title="Flink Scenario - Data Driven"></a>Flink Scenario - Data Driven</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">规则触发</span><br><span class="line">风控系统</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之九SQL编程</title>
    <url>/2019/04/30/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B9%9DSQL%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Window-Aggregation"><a href="#Window-Aggregation" class="headerlink" title="Window Aggregation"></a>Window Aggregation</h1><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 三种内置函数</span></span><br><span class="line">TUMBLE(t, INTERVAL &#x27;2&#x27; HOUR)滚动,2小时一个窗口</span><br><span class="line">HOP(t, INTERVAL &#x27;2&#x27; HOUR, INTERVAL &#x27;1&#x27; HOUR)滑动,每1小时聚合前2小时窗口</span><br><span class="line">SESSION(t,INTERVAL &#x27;30&#x27; MINUTE)会话,30分钟无响应化为一个窗口</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例:每小时每个用户点击的次数</span></span><br><span class="line">clicks:user, cTime, url</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">user</span>,</span><br><span class="line">    TUMBLE_END(cTime, <span class="built_in">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOURS</span>) <span class="keyword">AS</span> endT,</span><br><span class="line">    <span class="keyword">COUNT</span>(<span class="keyword">url</span>) <span class="keyword">as</span> cnt</span><br><span class="line"><span class="keyword">FROM</span> clicks</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">    <span class="keyword">user</span>,</span><br><span class="line">    TUMBLE(cTime, <span class="built_in">INTERVAL</span> <span class="string">&#x27;1&#x27;</span> <span class="keyword">HOURS</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># WindowAgg &amp; GroupAgg区别</span></span><br><span class="line">输出模式</span><br><span class="line">    <span class="keyword">Window</span>按时输出</span><br><span class="line">    <span class="keyword">Group</span>提前输出</span><br><span class="line">输出量</span><br><span class="line">    <span class="keyword">Window</span>只输出一次结果</span><br><span class="line">    <span class="keyword">Group</span> Per <span class="keyword">Key</span>输出N个结果(Sink压力)</span><br><span class="line">输出流</span><br><span class="line">    <span class="keyword">Window</span> AppendStream</span><br><span class="line">    <span class="keyword">Group</span> UpdateStream</span><br><span class="line">状态清理</span><br><span class="line">    <span class="keyword">Window</span>及时清理过期数据</span><br><span class="line">    <span class="keyword">Group</span>状态无限增长</span><br><span class="line">Sink</span><br><span class="line">    <span class="keyword">Window</span>均可</span><br><span class="line">    <span class="keyword">Group</span>可更新的结果表</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 实例: 纽约每个区块每5分钟的进入的车辆数,只关心至少有5辆车子进入的区块</span></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">  toAreaId(lon, lat) <span class="keyword">AS</span> area, </span><br><span class="line">  TUMBLE_END(rideTime, <span class="built_in">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>) <span class="keyword">AS</span> window_end, </span><br><span class="line">  <span class="keyword">COUNT</span>(*) <span class="keyword">AS</span> cnt </span><br><span class="line"><span class="keyword">FROM</span> Rides </span><br><span class="line"><span class="keyword">WHERE</span> isInNYC(lon, lat) <span class="keyword">and</span> isStart</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> </span><br><span class="line">  toAreaId(lon, lat), </span><br><span class="line">  TUMBLE(rideTime, <span class="built_in">INTERVAL</span> <span class="string">&#x27;5&#x27;</span> <span class="keyword">MINUTE</span>) </span><br><span class="line"><span class="keyword">HAVING</span> <span class="keyword">COUNT</span>(*) &gt;= <span class="number">5</span>;</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Window的Start和End的时间怎么恰好划分为整分的</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之八TableAPI</title>
    <url>/2019/04/26/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E5%85%ABTableAPI/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Table-API-amp-SQL"><a href="#Table-API-amp-SQL" class="headerlink" title="Table API &amp; SQL"></a>Table API &amp; SQL</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">声明式,用户只关心做什么,不用关心怎么做</span><br><span class="line">高性能,支持查询优化,可以获取更好的执行性能</span><br><span class="line">流批统一,相同的逻辑,既可以流模式运行,也可以批模式运行</span><br><span class="line">标准稳定,语义遵循SQL标准,不易变动</span><br><span class="line">易理解,语义明确,所见即所得</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Table使用"><a href="#Table使用" class="headerlink" title="Table使用"></a>Table使用</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 获取一个Table</span><br><span class="line">    注册对应的TableSource</span><br><span class="line">        通过TableDescription来注册</span><br><span class="line">        自定义Source</span><br><span class="line">        DataStream</span><br><span class="line">    调用TableEnv的scan方法获取Table对象</span><br><span class="line"></span><br><span class="line"># 输出一个Table</span><br><span class="line">    TableDescription</span><br><span class="line">    自定义TableSink</span><br><span class="line">    DataStream</span><br><span class="line">    </span><br><span class="line"># 操作Table</span><br><span class="line">    select,filter,where</span><br><span class="line">    groupBy,flatAggrgate</span><br><span class="line">    join</span><br><span class="line">    addColumns添加列</span><br><span class="line">    addOrReplaceColumns替换列</span><br><span class="line">    dropColumns删除列</span><br><span class="line">    renameColumns重命名列</span><br><span class="line">    withColumns选择范围列</span><br><span class="line">    withoutColumns反选范围列</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="易用性"><a href="#易用性" class="headerlink" title="易用性"></a>易用性</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Map</span><br><span class="line">UDF,继承ScalarFunction</span><br><span class="line">table.map(udf())</span><br><span class="line">table.select(udf1(),udf2(),udf3())</span><br><span class="line"></span><br><span class="line"># FlatMap</span><br><span class="line">UDTF,继承TableFunction</span><br><span class="line">table.flatMap(udtf())</span><br><span class="line">table.joinLateral(udtf())</span><br><span class="line"></span><br><span class="line"># Aggregate</span><br><span class="line">UDAF,继承AggregateFunction</span><br><span class="line">table.aggregate(agg())</span><br><span class="line">table.select(agg1(),agg2()...)</span><br><span class="line"></span><br><span class="line"># FlatAggregate</span><br><span class="line">UD(TA)F,继承TableAggregateFunction</span><br><span class="line">table.groupBy(&#39;a&#39;)</span><br><span class="line">    .flatAggregate(flatAggFunc(&#39;e,&#39;f) as (&#39;a,&#39;b,&#39;c))</span><br><span class="line">    .select(&#39;a,&#39;c)</span><br><span class="line">新增Agg,能输出多行</span><br><span class="line"></span><br><span class="line"># AggregateFunction &amp; TableAggregateFunction</span><br><span class="line">AggregateFunction适合做最大值</span><br><span class="line">TableAggregateFunction可以做TopN</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之五客户端操作</title>
    <url>/2019/04/12/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E4%BA%94%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="交互方式"><a href="#交互方式" class="headerlink" title="交互方式"></a>交互方式</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Scala Shell</span><br><span class="line">SQL Client</span><br><span class="line">Command Line</span><br><span class="line">Restful</span><br><span class="line">Web</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 完整说明</span><br><span class="line">bin&#x2F;flink -h</span><br><span class="line"></span><br><span class="line"># 某命令的参数,如run</span><br><span class="line">bin&#x2F;flink run -h</span><br><span class="line"></span><br><span class="line"># 查看任务列表(Standalone)</span><br><span class="line">bin&#x2F;flink list -m 127.0.0.1:8081</span><br><span class="line"></span><br><span class="line"># 停止与取消</span><br><span class="line">bin&#x2F;flink stop -m 127.0.0.1:8081 d67420e52bd051fae2fddbaa79e046bb</span><br><span class="line">cancel调用,立即调用作业算子的cancel方法,以尽快取消它们,如果算子在接到cancel调用后没有停止,Flink将开始定期中断算子线程的执行,直到所有算子停止为止</span><br><span class="line">stop调用,是更优雅的停止正在运行流作业的方式,stop仅适用于Source实现了StoppableFunction接口的作业,当用户请求停止作业时,作业的所有Source都将接收stop方法调用,直到所有Source正常关闭时,作业才会正常结束,这种方式,使作业正常处理完所有作业</span><br><span class="line"></span><br><span class="line"># 修改并行度</span><br><span class="line">bin&#x2F;flink modify -p 4 7752ea7b0e7303c780de9d86a5ded3fa</span><br><span class="line"></span><br><span class="line"># 查看Flink任务执行计划(StreamGraph)</span><br><span class="line">bin&#x2F;flink info examples&#x2F;streaming&#x2F;TopSpeedWindowing.jar</span><br><span class="line">粘贴Json内容到http:&#x2F;&#x2F;flink.apache.org&#x2F;visualizer&#x2F;</span><br><span class="line"></span><br><span class="line"># Yarn单任务提交</span><br><span class="line">Attach模式,批处理程序结束才退出,流处理客户端会一直等待不退出</span><br><span class="line">bin&#x2F;flink run -m yarn-cluster .&#x2F;examples&#x2F;batch&#x2F;WordCount.jar</span><br><span class="line">Detached模式,客户端提交完任务就退出</span><br><span class="line">bin&#x2F;flink run -yd -m yarn-cluster .&#x2F;examples&#x2F;streaming&#x2F;TopSpeedWindowing.jar</span><br><span class="line"></span><br><span class="line"># Yarn Session</span><br><span class="line">Attach模式</span><br><span class="line">bin&#x2F;yarn-session.sh -tm 2048 -s 3</span><br><span class="line">Detached模式</span><br><span class="line">bin&#x2F;yarn-session.sh -tm 2048 -s 3 -d</span><br><span class="line"></span><br><span class="line"># Scala Shell</span><br><span class="line">Local</span><br><span class="line">bin&#x2F;start-scala-shell.sh local</span><br><span class="line">Yarn</span><br><span class="line">bin&#x2F;yarn-session.sh  -tm 2048 -s 3</span><br><span class="line">bin&#x2F;start-scala-shell.sh yarn -n 2 -jm 1024 -s 2 -tm 1024 -nm flink-yarn</span><br><span class="line"></span><br><span class="line"># SQL Client</span><br><span class="line">bin&#x2F;sql-client.sh embedded</span><br><span class="line">两种模式维护展示查询结果</span><br><span class="line">table mode: 在内存中物化查询结果,并以分页table形式展示</span><br><span class="line">changlog mode: 不会物化查询结果,而是直接对continuous query产生的添加和撤回(retractions)结果进行展示</span><br><span class="line">SET execution.result-mode&#x3D;table</span><br><span class="line">SET execution.result-mode&#x3D;changelog</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink保证Exactly-Once"><a href="#Flink保证Exactly-Once" class="headerlink" title="Flink保证Exactly-Once"></a>Flink保证Exactly-Once</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">分布式快照</span><br><span class="line">Checkpoint Barrier</span><br><span class="line">TwoPhaseCommitSinkFunction两阶段提交</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之二概念介绍</title>
    <url>/2019/03/14/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E4%BA%8C%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Flink入门之二"><a href="#Flink入门之二" class="headerlink" title="Flink入门之二"></a>Flink入门之二</h1><h2 id="有状态分散式流"><a href="#有状态分散式流" class="headerlink" title="有状态分散式流"></a>有状态分散式流</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">保证精确一次的状态容错</span><br><span class="line">全域一致的快照，更改完所有State，再产生快照 </span><br><span class="line"></span><br><span class="line">分散式快照</span><br><span class="line">CheckPoint Barrier N跟随着数据流完成，并填充CheckPoint N，当填充完成或数据流完成所有计算，代表快照完成</span><br><span class="line"></span><br><span class="line">状态维护</span><br><span class="line">本地状态后端去维护</span><br><span class="line">JVM Heap状态后端</span><br><span class="line">RocksDB状态后端</span><br><span class="line"></span><br><span class="line">WaterMarks</span><br><span class="line">Flink中的特殊事件</span><br><span class="line">一个带有时间戳T的WaterMark会让运算元判定不会再收到任何时间戳&lt;T的事件</span><br><span class="line"></span><br><span class="line">状态保存与迁移</span><br><span class="line">CheckPoint&amp;SavePoint</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink是什么"><a href="#Flink是什么" class="headerlink" title="Flink是什么"></a>Flink是什么</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">状态容错：精确一次保证，分布式快照</span><br><span class="line">可应付极大的状态量：out-of-core状态后端，异步快照</span><br><span class="line">状态迁移：在应用重新平行化&#x2F;更改应用代码的状况下仍然恢复历史状态</span><br><span class="line">Event-Time处理：用以定义何时接收完毕所需数据</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之四DataStreamAPI</title>
    <url>/2019/04/11/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E5%9B%9BDataStreamAPI/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>


<h1 id="分布式流处理的基本模型"><a href="#分布式流处理的基本模型" class="headerlink" title="分布式流处理的基本模型"></a>分布式流处理的基本模型</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">流可以看做一个DAG有向无环图,每一个节点看做一个算子操作</span><br><span class="line">每一个算子操作可能有多个实例</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="DataStream程序结构"><a href="#DataStream程序结构" class="headerlink" title="DataStream程序结构"></a>DataStream程序结构</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">设置运行环境</span><br><span class="line">配置数据源读取数据</span><br><span class="line">进行一系列转换</span><br><span class="line">配置数据Sink写出数据</span><br><span class="line">提交执行</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="操作概览"><a href="#操作概览" class="headerlink" title="操作概览"></a>操作概览</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基于单条记录:filter,map</span><br><span class="line">基于窗口:window</span><br><span class="line">合并多条流:union,join,connect</span><br><span class="line">拆分单条流:split</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="物理分组"><a href="#物理分组" class="headerlink" title="物理分组"></a>物理分组</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataStream.global()全部发往第一个Task</span><br><span class="line">dataStream.broadcast()广播</span><br><span class="line">dataStream.forward()上下游并发度一样时一对一发送,不一致会报错</span><br><span class="line">dataStream.shuffle()随机均匀分配</span><br><span class="line">dataStream.rebalance()轮流分配</span><br><span class="line">dataStream.recale()本地轮流分配</span><br><span class="line">dataStream.partitionCustom()自定义单播</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink入门之六WindowTime</title>
    <url>/2019/04/16/Ververica&amp;Flink%E5%85%A5%E9%97%A8%E4%B9%8B%E5%85%ADWindowTime/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Window示例"><a href="#Window示例" class="headerlink" title="Window示例"></a>Window示例</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Keyed Windows</span><br><span class="line">    stream</span><br><span class="line">        .keyBy</span><br><span class="line">        .window(必须: &quot;assigner分配器&quot;)</span><br><span class="line">        [.trigger](默认: &quot;trigger触发器&quot;)</span><br><span class="line">        [.evictor](默认没有: &quot;evictor过滤&quot;)</span><br><span class="line">        [.allowedLateness](默认为0: &quot;最迟迟到时间&quot;)</span><br><span class="line">        [.sideOutputLateData](默认没有: &quot;迟到数据&quot;)</span><br><span class="line">        .reduce&#x2F;aggregate&#x2F;fold&#x2F;apply</span><br><span class="line">        [getSideOutput](&quot;获取迟到数据&quot;)</span><br><span class="line">Non-Keyed Windows</span><br><span class="line">    stream</span><br><span class="line">        .windowAll</span><br><span class="line">        [.trigger]</span><br><span class="line">        [.evictor]</span><br><span class="line">        [.allowedLateness]</span><br><span class="line">        [.sideOutputLateData]</span><br><span class="line">        .reduce&#x2F;aggregate&#x2F;fold&#x2F;apply</span><br><span class="line">        [getSideOutput]</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Window组件"><a href="#Window组件" class="headerlink" title="Window组件"></a>Window组件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Assigner</span><br><span class="line">    负责将每条输入的数据分发到正确的window中</span><br><span class="line">    一条数据可能同时分发到多个window中</span><br><span class="line">    TumblingWindow,SlidingWindow,SessionWindow,GlobalWindow</span><br><span class="line"></span><br><span class="line">Evictor</span><br><span class="line">    主要用于做一些数据的自定义操作,可以在执行用户代码之前,也可以在执行用户代码之后</span><br><span class="line">    通用Evictor:</span><br><span class="line">        CountEvictor保留指定数量的元素</span><br><span class="line">        DeltaEvictor通过执行用户给定的DeltaFunction以及预设的Threshold,判断是否删除一个元素</span><br><span class="line">        TimeEvictor设定一个阈值interval,删除所有不在max_ts-interval范围内的元素,max_ts是窗口内时间戳的最大值</span><br><span class="line">        </span><br><span class="line">Trigger</span><br><span class="line">    用来判断一个窗口是否需要被触发</span><br><span class="line">    每个WindowAssigner都自带一个默认的Trigger</span><br><span class="line">    如果默认的Trigger不能满足你的需求,则可以自定义一个类,继承自Trigger即可</span><br><span class="line">    Trigger接口:</span><br><span class="line">        onElement:每次往window增加一个元素的时候都会触发</span><br><span class="line">        onEventTime:当Event-Time Timer被触发的时候会调用</span><br><span class="line">        onProcessingTime:当Processing-Time Timer被触发的时候会调用</span><br><span class="line">        onMerge:对两个Trigger的state进行merge操作</span><br><span class="line">        clear:window销毁的时候被调用</span><br><span class="line">    TriggerResult返回的选择:</span><br><span class="line">        CONTINUE:不做任何事情</span><br><span class="line">        FIRE:触发Window</span><br><span class="line">        PURGE:清空整个Window的元素并销毁窗口</span><br><span class="line">        FIRE_AND_PURGE:触发窗口,然后销毁窗口</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之一反压延时</title>
    <url>/2019/08/11/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E4%B8%80%E5%8F%8D%E5%8E%8B%E5%BB%B6%E6%97%B6/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Flink运维基础"><a href="#Flink运维基础" class="headerlink" title="Flink运维基础"></a>Flink运维基础</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对比其他的大数据计算平台,Flink运维更类似于微服务架构(micro-service)运维</span><br><span class="line">    微服务部署 vs 流处理的无限性</span><br><span class="line">    微服务存储 vs state的维护</span><br><span class="line">    微服务请求响应 vs 流处理延时</span><br><span class="line"></span><br><span class="line">Metrics应作为重要的运维参考</span><br><span class="line">    Flink丰富的内置Metrics接口</span><br><span class="line">    Flink针对各种运维场景的运维泛用性</span><br><span class="line">    </span><br><span class="line">定义运维标准</span><br><span class="line">    定义Service-Level Agreement(SLA)</span><br><span class="line">定义基本Metrics</span><br><span class="line">    直观定义,能反应SLA的参数</span><br><span class="line">    延时,反压,吞吐量等</span><br><span class="line">更多衍生Metrics</span><br><span class="line">    深入Flink作业,反应内部的性能&#x2F;异常状态</span><br><span class="line">制定运维策略</span><br><span class="line">    根据Metrics参数制定具体策略</span><br><span class="line">    自动化策略执行</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="图形界面"><a href="#图形界面" class="headerlink" title="图形界面"></a>图形界面</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">实时Metrics</span><br><span class="line">    收集实时Metrics数据</span><br><span class="line">    精确到算子</span><br><span class="line">触发反压计算</span><br><span class="line">    反压计算需要触发</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="程序监控-RESTful-API"><a href="#程序监控-RESTful-API" class="headerlink" title="程序监控-RESTful API"></a>程序监控-RESTful API</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RESTful API</span><br><span class="line">    与WebUI的应用情况类似,使用程序触发Metrics收集</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="第三方Metrics-Reporter"><a href="#第三方Metrics-Reporter" class="headerlink" title="第三方Metrics Reporter"></a>第三方Metrics Reporter</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">分离收集与处理</span><br><span class="line">    Grafana</span><br><span class="line">    JMX</span><br><span class="line">查看历史记录</span><br><span class="line">    第三方Metrics可以视为一个实时OLAP数据库</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="基于Metrics运维的优点"><a href="#基于Metrics运维的优点" class="headerlink" title="基于Metrics运维的优点"></a>基于Metrics运维的优点</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">整合数据</span><br><span class="line">    比起RESTful或者WebUI,Metrics Reporter能更容易整合数据</span><br><span class="line">稳定性</span><br><span class="line">    高可用Metric系统</span><br><span class="line">多维度分析</span><br><span class="line">    JVM基础分析</span><br><span class="line">    与周边系统连调</span><br><span class="line">    与集群系统连调</span><br><span class="line">    整合State后端以及外部DFS</span><br><span class="line">整合第三方资源</span><br><span class="line">    Flink直接支持向第三方Metrics系统</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="延时基本概念"><a href="#延时基本概念" class="headerlink" title="延时基本概念"></a>延时基本概念</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">什么是延时?</span><br><span class="line">    定义两个时间点差值</span><br><span class="line">        最近一个成功处理的数据offset</span><br><span class="line">        最新一个生成的数据offset</span><br><span class="line"></span><br><span class="line">如何测量延时?</span><br><span class="line">    基于流数据系统</span><br><span class="line">        Kafka系统直接返回延时差值</span><br><span class="line">        其他系统可能需要通过Metrics计算</span><br><span class="line"></span><br><span class="line">如何使用延时数据?</span><br><span class="line">    延时是衡量流数据作业是否能够定义为实时的基本参数</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="反压基本概念"><a href="#反压基本概念" class="headerlink" title="反压基本概念"></a>反压基本概念</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">什么是反压?</span><br><span class="line">    定义两个连接的算子之间的差值</span><br><span class="line">        上游算子的处理速度</span><br><span class="line">        下游算子的处理速度</span><br><span class="line"></span><br><span class="line">如何测量反压?</span><br><span class="line">    Flink RESTful API直接触发计算</span><br><span class="line">        Flink(&gt;1.6)使用内部Credit反压机制</span><br><span class="line">        与缓冲区的使用有直接关联</span><br><span class="line"></span><br><span class="line">如何使用反压数据?</span><br><span class="line">    反压计算能够更精确找到系统性错误</span><br><span class="line">        精确到算子级别</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="JVM-Metrics基本设置"><a href="#JVM-Metrics基本设置" class="headerlink" title="JVM Metrics基本设置"></a>JVM Metrics基本设置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JVM Metrics</span><br><span class="line">    适用于几乎所有作业类型</span><br><span class="line"></span><br><span class="line">JVM通用定义</span><br><span class="line">    CPU usage</span><br><span class="line">    Heap commit&#x2F;use&#x2F;max</span><br><span class="line">    GC时间,类型和比例</span><br><span class="line"></span><br><span class="line">定义合理数据区间</span><br><span class="line">    CPU占用比小于50%</span><br><span class="line">    Heap占用比小于50%</span><br><span class="line">    GC比例小于15%</span><br><span class="line">    FullGC时间恒定</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="流数据Metrics"><a href="#流数据Metrics" class="headerlink" title="流数据Metrics"></a>流数据Metrics</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">流数据-接口整合</span><br><span class="line">    Flink接口Metrics反应接受端Metrics</span><br><span class="line">    流数据系统Metrics反应发送端Metrics</span><br><span class="line"></span><br><span class="line">自定义流数据系统</span><br><span class="line">    不同的流数据系统一般会自定义不同类型的数据Metrics</span><br><span class="line">        sleepTimeMillis(Kinesis)接口的休眠延时</span><br><span class="line">        connection-close-rate(Kafka)接口连接断开&#x2F;传输比例</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="State-Metrics"><a href="#State-Metrics" class="headerlink" title="State Metrics"></a>State Metrics</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink的原生支持</span><br><span class="line">    CK对JVM有较大影响</span><br><span class="line">        当前CK的进度,时长,文件大小,频率</span><br><span class="line">        CK的失败恢复比例</span><br><span class="line"></span><br><span class="line">外部分布式存储</span><br><span class="line">    不能忽视外部分布式存储系统(DFS)对于CK与SP的影响</span><br><span class="line">        DFS的设置-冗余,分片</span><br><span class="line">        DFS的管理-配额管理,碎片文件管理,回收机制</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="反压检测"><a href="#反压检测" class="headerlink" title="反压检测"></a>反压检测</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">利用反压</span><br><span class="line">    反压的计算能够提供更多参考</span><br><span class="line">        直接判断问题算子</span><br><span class="line">        确定跨算子之间的联系以及瓶颈</span><br><span class="line"></span><br><span class="line">触发反压计算</span><br><span class="line">    反压的计算需要触发,意味着反压的计算并不是免费运维</span><br><span class="line">        合理分配反压计算的频率</span><br><span class="line">        结合Metrics,只针对需要深入分析的作业进行反压分析</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Metrics的局限性"><a href="#Metrics的局限性" class="headerlink" title="Metrics的局限性"></a>Metrics的局限性</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">难以保证准确性</span><br><span class="line">    Metrics Reporter并不能保证100%合理处理系统故障</span><br><span class="line">    包括客户端和服务端故障</span><br><span class="line">    第三方Metrics意味着存在网络延时的影响</span><br><span class="line"></span><br><span class="line">难以回答统计型分析</span><br><span class="line">    Flink是否合理处理流数据系统的分区平衡问题</span><br><span class="line">    Flink与流数据系统的是否存在合理的延时&#x2F;流量比</span><br><span class="line"></span><br><span class="line">难以融合其他的Metrics</span><br><span class="line">    很多问题必须融合更多的Metrics才能合理运维</span><br><span class="line">        TaskManager重启是否由于集群系统的节点故障导致</span><br><span class="line">        反压的出现是否源于流系统的周期性分区负载平衡作业</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a>时间序列</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">便于历史数据处理</span><br><span class="line">    时段分析</span><br><span class="line">    分辨率压缩</span><br><span class="line"></span><br><span class="line">便于统计型分析</span><br><span class="line">    时间序列源于统计分析</span><br><span class="line">        基本统计函数:整合,差分</span><br><span class="line">        自定义函数分析</span><br><span class="line"></span><br><span class="line">众多第三方软件</span><br><span class="line">    JMX</span><br><span class="line">    Graphite</span><br><span class="line">    Prometheus</span><br><span class="line">    StatsD</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="去除噪声"><a href="#去除噪声" class="headerlink" title="去除噪声"></a>去除噪声</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">去除错误Metrics</span><br><span class="line">    纠正Metrics系统错误</span><br><span class="line">    自定义范围分析</span><br><span class="line">    使用移动平均</span><br><span class="line"></span><br><span class="line">整合多个Metrics</span><br><span class="line">    多个Metrics能更全面反应问题</span><br><span class="line">        关联Heap使用与GC比例</span><br><span class="line">        按比例计算Direct和Heap使用</span><br><span class="line">    异常值&#x2F;极端值分析</span><br><span class="line">        只有去除噪声之后才能正确确认极端值的可靠性</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="整合系统"><a href="#整合系统" class="headerlink" title="整合系统"></a>整合系统</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">整合输入输出数据</span><br><span class="line">    使用统一统计分析</span><br><span class="line">        流数据节点看作虚拟算子</span><br><span class="line">整合集群数据</span><br><span class="line">    跨界对比:多数据中心,跨集群对比时间序列算法</span><br><span class="line">    整合:集群资源分配是否合理-&gt;TM是否能合理连接资源分配</span><br><span class="line">    多维度:集群资源管理,存储管理,网络分配</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="特征分析"><a href="#特征分析" class="headerlink" title="特征分析"></a>特征分析</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">多维度统计</span><br><span class="line">    综合维度:集群,集群节点</span><br><span class="line">    纵向维度:作业,作业节点</span><br><span class="line"></span><br><span class="line">统计算法</span><br><span class="line">    均值,方差,标准差</span><br><span class="line">    自定义函数:奇异值分析,中值偏差</span><br><span class="line"></span><br><span class="line">生成更多Metrics</span><br><span class="line">    使用空间函数:比例分析,排序</span><br><span class="line">    使用时间函数:移动平均</span><br><span class="line">    整合多个Metrics:差分,动态阈值</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Metrics进阶"><a href="#Metrics进阶" class="headerlink" title="Metrics进阶"></a>Metrics进阶</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Metrics设计到更多的方面</span><br><span class="line">并不是所有Metrics都需要与延时&#x2F;反压挂钩</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="运维样例"><a href="#运维样例" class="headerlink" title="运维样例"></a>运维样例</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作业造成资源浪费</span><br><span class="line">    延时一直不存在任何的浮动性</span><br><span class="line">    资源使用率异常的低:平均在10%~20%</span><br><span class="line">    非常规整的GC进程-时间以及次数上不存在任何异常</span><br><span class="line">资源过度浪费</span><br><span class="line"></span><br><span class="line">跨集群故障转移</span><br><span class="line">    出现了非常明显的系统处理速度异常</span><br><span class="line">    明显的下降-接近0的数据吞吐量,而且持续时间长</span><br><span class="line">    恢复速度正常:并没有出现峰值异常</span><br><span class="line">上游端发生跨数据中心冗余处理</span><br><span class="line"></span><br><span class="line">作业的分区异常处理故障</span><br><span class="line">    数据吞吐量正常</span><br><span class="line">    分区分析上:上升沿以及下降沿出现了明显断层</span><br><span class="line">    出现过短时间的延时增长,不过并没有造成积压</span><br><span class="line">分区有非常明显的错误:并且不可自我修复</span><br><span class="line"></span><br><span class="line">下游端响应延时造成反压</span><br><span class="line">    均匀的GC和均匀的上升下降</span><br><span class="line">    吞吐量有一定时间的平滑曲线,之后在两个数值之间来回跳动</span><br><span class="line">    延时数据出现了周期性清零,反压升高</span><br><span class="line">下游的输出系统的接收延时出现爆炸性增长</span><br><span class="line"></span><br><span class="line">作业增长-系统资源匮乏</span><br><span class="line">    数据吞吐量上出现一定程度的噪音</span><br><span class="line">    GC:时间使用非常高,峰值非常不稳定</span><br><span class="line">    延时升高而且不可恢复,反压正常</span><br><span class="line">    上下游系统正常</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="基础调参"><a href="#基础调参" class="headerlink" title="基础调参"></a>基础调参</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基础调参需要根据不同的运维方式调整</span><br><span class="line">    逻辑,作业,平台,集群</span><br><span class="line"></span><br><span class="line">JVM</span><br><span class="line">    适用于单个作业的调整</span><br><span class="line">        增加容器数量 vs 增加内存&#x2F;CPU&#x2F;并发</span><br><span class="line">            根据作业的特性,例如高I&#x2F;O作业,可能需要更多的Container</span><br><span class="line">            根据Metrics反馈,例如周期性峰值数据量,可能需要更多内存缓冲</span><br><span class="line">            根据异常反馈,例如周期性AkkaTimeout,可能需要更多的Locality</span><br><span class="line">        其他的Flink运维调参</span><br><span class="line">            更多的Parallism vs 更多的TaskManager Slot</span><br><span class="line">            内存分配:Direct Heap vs offHeap vs Reserve</span><br><span class="line"></span><br><span class="line">集群设置</span><br><span class="line">    适用于平台&#x2F;集群的运维</span><br><span class="line">        保证作业根据作业间的集群分离</span><br><span class="line">            如何进行资源隔离:CPU,内存,disk</span><br><span class="line">            是否Over-Subscribe</span><br><span class="line">        保证集群的整体负载平衡</span><br><span class="line">            如何分配资源配额</span><br><span class="line">            如何保证不出现I&#x2F;O Hotspot</span><br><span class="line">            如何保证container的初始化对DFS的负载平衡</span><br><span class="line"></span><br><span class="line">调参进阶</span><br><span class="line">    Flink调参</span><br><span class="line">        调参是一个多维度协调的过程</span><br><span class="line">        Flink的特殊集群接口</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">判断运维方式</span><br><span class="line">    优化集群?</span><br><span class="line">    优化流处理?</span><br><span class="line"></span><br><span class="line">Metrics</span><br><span class="line">    基础JVM</span><br><span class="line">    流系统,I&#x2F;O</span><br><span class="line">    State管理</span><br><span class="line">    自定义</span><br><span class="line"></span><br><span class="line">时间序列分析</span><br><span class="line">    整合</span><br><span class="line">    除噪&#x2F;特征值</span><br><span class="line"></span><br><span class="line">调参</span><br><span class="line">    反压分析</span><br><span class="line">    增加资源</span><br><span class="line">    调整负载</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之三深度学习垃圾图片分类(特别场)</title>
    <url>/2019/08/23/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E4%B8%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9E%83%E5%9C%BE%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB(%E7%89%B9%E5%88%AB%E5%9C%BA)/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之九作业问题分析调优</title>
    <url>/2019/10/18/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E4%B9%9D%E4%BD%9C%E4%B8%9A%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="原理剖析"><a href="#原理剖析" class="headerlink" title="原理剖析"></a>原理剖析</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CK机制</span><br><span class="line">    什么是CK</span><br><span class="line">        简单的说就是Flink为了达到容错和Exactly-Once语义的功能</span><br><span class="line">        定期把State持久化下来,而这一持久化过程就叫CK</span><br><span class="line">        它是FlinkJob在某一时刻全局状态的快照</span><br><span class="line">    </span><br><span class="line">统一时钟-广播</span><br><span class="line">    由中心点生成时钟并广播给各个Slave节点,达到时间的统一性</span><br><span class="line">    缺点:</span><br><span class="line">        单点故障</span><br><span class="line">        数据不一致,出现脑裂现象</span><br><span class="line">        系统复杂,不稳定</span><br><span class="line"></span><br><span class="line">Flink之Barrier</span><br><span class="line">    定时向数据流中发送Barrier</span><br><span class="line"></span><br><span class="line">CK注意</span><br><span class="line">    频率不宜过高,一般需求1-5分钟,精确性要求高20-30秒</span><br><span class="line">    超时时间不宜过长,一般在频率的一半即可</span><br><span class="line">    异步化:在Operator的自定义snapshotState尽量异步化</span><br><span class="line">    </span><br><span class="line">Flink背压</span><br><span class="line">    Flink不同TaskManager之间的数据交换就是采用有界的buffer实现</span><br><span class="line">    当上下游计算速度不一致时就会出现正背压和反背压的情况</span><br><span class="line"></span><br><span class="line">反压-静态控流</span><br><span class="line">    预估producer和consumer的处理速度,静态控制快速组件的速度</span><br><span class="line">    达到系统整体的平衡</span><br><span class="line"></span><br><span class="line">反压处理-Flink</span><br><span class="line">    静态流控的缺陷:</span><br><span class="line">        事先预估系统中流速</span><br><span class="line">        系统的速度是静态的,不能适应动态变化的速度</span><br><span class="line">    动态流控:</span><br><span class="line">        TCP流控(Flink1.5之前)</span><br><span class="line">            主要通过TCP协议的滑动窗口实现</span><br><span class="line">            缺点:</span><br><span class="line">                Flink上游对下游情况一无所知,导致上游对数据反压的感知过于迟钝</span><br><span class="line">                TCP的复用导致一个channel阻塞住其他channel,尤其是CKBarrier的传递</span><br><span class="line">        Credit流控</span><br><span class="line">            根据TCP的流量控制原理实现一套自身的流控机制</span><br><span class="line">            在数据交换是会返回一个Credit,当为0时将不会再发送数据给下游</span><br><span class="line">            放空了Netty层与Socket层,作业的其他信息交换得到了保障</span><br><span class="line">            </span><br><span class="line">Java内存模型的问题</span><br><span class="line">    JVM复杂,GC成本高</span><br><span class="line">        使用JVM对开发人员来说是比较复杂的,垃圾回收的成本高</span><br><span class="line">    对象存储开销大密度低</span><br><span class="line">        对象存储到Heap中,需要序列化</span><br><span class="line">        其中对象Header会占用比较大的空间导致了对象的存储密度低</span><br><span class="line">    OOM引起稳定性问题</span><br><span class="line">        当内存不足时,会引发OOM异常,导致系统崩溃</span><br><span class="line"></span><br><span class="line">Flink内存组成</span><br><span class="line">    cutoff:预留内存</span><br><span class="line">    NetworkBuffers:用于Task间的数据交换</span><br><span class="line">    MomeryManager:主要用于算法上,目前用于batch作业中,可以是堆外</span><br><span class="line">    free:用户在算子中new的对象以及TM的数据结构使用的</span><br><span class="line">    </span><br><span class="line">内存计算</span><br><span class="line">    TM 8G,Net 0.1,MF 0.2</span><br><span class="line">    可用内存:8192M * 0.75 &#x3D; 6144M</span><br><span class="line">    Network:6144M * 0.1 &#x3D; 614.4M</span><br><span class="line">    Heap:6144M * 0.9 * 0.8 &#x3D; 4424M</span><br><span class="line">    Flink:6144M * 0.9 * 0.2 &#x3D; 796M</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="性能定位"><a href="#性能定位" class="headerlink" title="性能定位"></a>性能定位</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">定位口诀</span><br><span class="line">    一压二查三指标,延迟吞吐是核心</span><br><span class="line">    时刻关注资源量,排查首先看GC</span><br><span class="line"></span><br><span class="line">看反压</span><br><span class="line">    通常最后一个被压高的subTask的下游就是Job的瓶颈之一</span><br><span class="line">看CK时长</span><br><span class="line">    CK时长能在一定程度影响Job的整体吞吐</span><br><span class="line">看核心指标</span><br><span class="line">    指标是对一个任务性能精准判断的依据</span><br><span class="line">    延迟指标和吞吐则是其中最为关键的指标</span><br><span class="line">资源的使用率</span><br><span class="line">    提高资源的利用率是最终的目的</span><br><span class="line">    </span><br><span class="line">常见的性能问题</span><br><span class="line">    JSON序列化和反序列化</span><br><span class="line">        常是在Source和Sink的Task上,在指标上没有体现,容易被忽略</span><br><span class="line">    Map和Set的Hash冲突</span><br><span class="line">        由于HashMap,HashSet等随数据负载因子增高,引起的插入和查询性能下降</span><br><span class="line">    数据倾斜</span><br><span class="line">        数据倾斜大大影响系统的吞吐</span><br><span class="line">    和低速的系统交互</span><br><span class="line">        在高速的计算系统中,低速的外部系统,比如MySQL,HBase,传统的单机系统等</span><br><span class="line">    频繁的GC</span><br><span class="line">        内内存或是比例分配不合理导致频繁GC,甚至是TM失联</span><br><span class="line">    大窗口</span><br><span class="line">        窗口size大,数据量大,或是滑动窗口size和step比值比较大如size&#x3D;5min,step&#x3D;1s</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="经典场景调优"><a href="#经典场景调优" class="headerlink" title="经典场景调优"></a>经典场景调优</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">数据去重</span><br><span class="line">    通过Set,Map等数据结构结合Flink的State实现</span><br><span class="line">        缺陷:</span><br><span class="line">            随时间的推移,Set,Map等数据量增大,Hash冲突导致对写性能急剧下降</span><br><span class="line">            内存直线上升,导致频繁GC</span><br><span class="line">            资源耗尽吞吐低,TM失联,任务异常</span><br><span class="line">    精准去重:通过bitMap&#x2F;roaring bitMap</span><br><span class="line">    近似去重:bloomFilter</span><br><span class="line"></span><br><span class="line">数据倾斜</span><br><span class="line">    单点问题</span><br><span class="line">        数据集中在某些partition上,致使数据严重不平衡</span><br><span class="line">    GC频繁</span><br><span class="line">        过多的数据集中在某些JVM中导致其内存资源短缺,进而引起频繁的GC</span><br><span class="line">    吞吐下降,延迟增大</span><br><span class="line">        频繁的GC和数据单点导致系统吞吐下降,数据延迟</span><br><span class="line">    系统崩溃</span><br><span class="line">        严重情况下过长的GC会导致TM和JM失联,系统崩溃</span><br><span class="line">倾斜-源头</span><br><span class="line">    数据源的消费不均匀:调整并发度</span><br><span class="line">        对于数据源消费不均匀,比如kafka数据源,尝试通过调整数据源算子的并发度实现</span><br><span class="line">        原则:通常情况下是Source的并发度和Kafka的分区数一致或是Kafka分区数是Source并发度的正整数倍</span><br><span class="line"></span><br><span class="line">无统计场景</span><br><span class="line">    改变Key的分布</span><br><span class="line">聚合场景</span><br><span class="line">    预聚合</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="内存调优"><a href="#内存调优" class="headerlink" title="内存调优"></a>内存调优</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink内存主要是三部分,NetworkBuffer和ManagerPool都是由Flink管理</span><br><span class="line">ManagerPool也已经走向堆外内存,所以内存调优分为两部分</span><br><span class="line">    非堆内存NetWorkBuffer和ManagerPool调优</span><br><span class="line">    Flink系统中Heap内存调优</span><br><span class="line"></span><br><span class="line">非堆内存</span><br><span class="line">    调整NetworkBuffer和ManagerPool的比例</span><br><span class="line">        NetworkBuffer:</span><br><span class="line">            taskmanager.network.memory.fraction(默认0.1)</span><br><span class="line">            taskmanager.network.memory.min(默认64M)</span><br><span class="line">            taskmanager.network.memory.max(默认1G)</span><br><span class="line">            原则:默认是0.1或是小于0.1可以根据使用的情况进行调整</span><br><span class="line">        ManagerPool:</span><br><span class="line">            taskmanager.memory.off-heap:true(默认是false)</span><br><span class="line">            taskmanager.memory.fraction(默认是0.7)</span><br><span class="line">            原则:在流计算中建议调整成小于0.3</span><br><span class="line">            </span><br><span class="line">堆内存</span><br><span class="line">    Flink是运行在JVM上的,所以堆内存调优和传统JVM调优无差别</span><br><span class="line">    默认Flink使用的ParallelScavenge的垃圾回收器,可以改用G1垃圾回收器</span><br><span class="line">    启动参数</span><br><span class="line">        env.java.opts&#x3D; -server -XX:+UseG1GC -XX:MaxGCPauseMillis&#x3D;300 -XX:+PrintGCDetails</span><br><span class="line">        </span><br><span class="line">G1的优势</span><br><span class="line">    无空间碎片</span><br><span class="line">        G1基于标记-整理算法,不会产生空间碎片,分配大对象时不会无法得到连续的空间而提前触发一次FULLGC</span><br><span class="line">    可控制的暂停时间</span><br><span class="line">        G1通过动态调整young代空间并根据回收代价和空间率选择部分回收Old代垃圾达到可预测的暂停时间</span><br><span class="line">    并行与并发</span><br><span class="line">        G1能更充分的利用CPU,多核环境下的硬件优势来缩短stop the world的停顿时间</span><br><span class="line">        </span><br><span class="line">G1参数</span><br><span class="line">    -XX:MaxGCPauseMillis:设置允许的最大GC停顿时间(GC Pause Time),默认是200ms</span><br><span class="line">    -XX:G1HeapRegionSize:每个分区的大小,默认值是会根据整个堆区的大小计算出来,范围是1M~32M,取值是2的幂,计算的倾向是尽量有2048个分区数</span><br><span class="line">    -XX:MaxTenuringThreshold&#x3D;n:晋升老年代的年龄阈值,默认值为15</span><br><span class="line">    -XX:InitiatingHeapOccupancyPercent:</span><br><span class="line">        一般会简写IHOP,默认是45%,这个占比跟并发周期的启动相关,当空间占比达到这个值时,会启动并发周期</span><br><span class="line">        如果经常出现FULLGC,可以调低该值,尽早的回收可以减少FULLGC的触发</span><br><span class="line">        但如果过低,则并发阶段会更加频繁,降低应用的吞吐</span><br><span class="line">    -XX:G1NewSizePercent:年轻代最小的堆空间占比,默认5%</span><br><span class="line">    -XX:G1MaxNewSizePercent:年轻代最大的堆空间占比,默认60%</span><br><span class="line">    -XX:CountGCThreads:并发执行的线程数,默认值接近整个应用线程数的1&#x2F;4</span><br><span class="line">    -XX:-XX:G1HeapWastePercent:允许的浪费堆空间的占比,默认5%,如果并发标记可回收的空间小于5%,则不会触发MixedGC</span><br><span class="line">    -XX:G1MixedGCCountTarget:一次全局并发标记之后,后续最多执行的MixedGC次数,默认值是8</span><br><span class="line">    </span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之五实时数仓</title>
    <url>/2019/09/07/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E4%BA%94%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="实时数仓建设目的"><a href="#实时数仓建设目的" class="headerlink" title="实时数仓建设目的"></a>实时数仓建设目的</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决由于传统数据仓库数据时效性低解决不了的问题</span><br><span class="line">    面向主题的</span><br><span class="line">    集成的</span><br><span class="line">    相对稳定的</span><br><span class="line">    处理上一次批处理流程到当前的数据</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">实时OLAP分析</span><br><span class="line">    扩展现有OLAP分析工具支持实时数据分析</span><br><span class="line">实时数据看板</span><br><span class="line">    实时播报核心数据</span><br><span class="line">实时特征</span><br><span class="line">    实时计算实体特征,进行精准运营</span><br><span class="line">实时业务监控</span><br><span class="line">    核心业务指标实时监控,预警</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="如何建设"><a href="#如何建设" class="headerlink" title="如何建设"></a>如何建设</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">概念映射</span><br><span class="line">            离线数仓        实时数仓</span><br><span class="line">编程接口     HQL+UDF        FlinkSQL+UDF</span><br><span class="line">Runtime     MR&#x2F;SparkJob    FlinkStreaming</span><br><span class="line">数仓抽象     HiveTable      StreamTable</span><br><span class="line">物理存储     HDFS           Kafka</span><br><span class="line"></span><br><span class="line">整体架构</span><br><span class="line">    数仓层次更少</span><br><span class="line">        尽量减少层次的划分,应用层数据直接写入应用数据库,仓库内不维护应用层</span><br><span class="line">    多种数据源存储</span><br><span class="line">        实时数仓使用Kafka存储明细与汇总数据</span><br><span class="line">        Tair,HBase等缓存存储维度数据</span><br><span class="line">        </span><br><span class="line">ODS层建设</span><br><span class="line">    Binlog,流量日志,系统日志</span><br><span class="line">    数据来源尽可能统一</span><br><span class="line">    利用分区保证数据局部有序</span><br><span class="line"></span><br><span class="line">DW层的建设</span><br><span class="line">    解决原始数据中数据存在噪声,不完整和数据形式不统一的情况</span><br><span class="line">    形成规范,统一的数据,如果可能的话尽可能和离线保持一致</span><br><span class="line">    数据解析-&gt;业务整合-&gt;脏数据清洗-&gt;模型规范化</span><br><span class="line"></span><br><span class="line">附带信息应对实时数据的常见问题</span><br><span class="line">内容-&gt;生成逻辑-&gt;解决问题</span><br><span class="line">唯一键-&gt;标记唯一一条数-&gt;解决重复数据问题</span><br><span class="line">主键-&gt;标记唯一一行数据-&gt;分区保证数据有序</span><br><span class="line">版本-&gt;对应表结构的版本-&gt;解决表结构变化的问题</span><br><span class="line">批次-&gt;当数据发生重导时更新批次-&gt;解决数据重导</span><br><span class="line"></span><br><span class="line">维度数据建设</span><br><span class="line">    变化频率低的维度</span><br><span class="line">        可以通过将离线仓库的维表数据,同步到缓存</span><br><span class="line">        或者通过公共服务提数据</span><br><span class="line">        通过维表服务查询,对用户屏蔽细节</span><br><span class="line">    变化频率高的维度</span><br><span class="line">        通过维度数据的变化的消息构建拉链表</span><br><span class="line">        通过事实数据计算衍生维度构建拉链表</span><br><span class="line">        注意:可以利用HBase的MIN_VERSIONS方便的构建类似于拉链表的结构</span><br><span class="line">        </span><br><span class="line">维度的使用</span><br><span class="line">    利用UDTF关联</span><br><span class="line">        通过使用开发UDTF,利用LATERAL TABLE进行关联</span><br><span class="line">    通过重新生成SQL</span><br><span class="line">        解析SQL识别维表,以及维表中的字段将原查询转化为:原表.flatMap(维表)</span><br><span class="line">        </span><br><span class="line">汇总层的建设</span><br><span class="line">    对共性指标统一加工</span><br><span class="line">        大数据去重指标计算时,可以考虑使用非精确去重减少内存使用</span><br><span class="line">    Flink丰富的时间窗口</span><br><span class="line">        使用时间窗口后,要设置对应的State的TTL设置</span><br><span class="line">    根据主题进行多维汇总</span><br><span class="line">        直接使用GroupBy语句,需要转化成append流才能写入Kafka</span><br><span class="line">    衍生维度的统一加工</span><br><span class="line">        可以利用HBase的版本机制构建实时维表</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="数仓质量保证"><a href="#数仓质量保证" class="headerlink" title="数仓质量保证"></a>数仓质量保证</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">元数据与血缘关系</span><br><span class="line">    通过元数据服务生成Catalog</span><br><span class="line">    解析DDL语句创建更新表</span><br><span class="line">    作业信息和运行状态写入元数据</span><br><span class="line"></span><br><span class="line">数据质量验证</span><br><span class="line">    实时数据写入Hive,使用离线数据持续验证实时数据的准确性</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之七常见问题诊断</title>
    <url>/2019/09/21/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E4%B8%83%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="作业运行环境"><a href="#作业运行环境" class="headerlink" title="作业运行环境"></a>作业运行环境</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Yarn Per-Job</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="为什么作业延时了"><a href="#为什么作业延时了" class="headerlink" title="为什么作业延时了"></a>为什么作业延时了</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">时间类型</span><br><span class="line">    Processing Time</span><br><span class="line">    Event Time</span><br><span class="line">    Ingestion Time</span><br><span class="line"></span><br><span class="line">延时特殊定义</span><br><span class="line">    Delay &#x3D; 当前系统时间 - Event Time</span><br><span class="line">    反应处理数据的进度情况</span><br><span class="line">    </span><br><span class="line">延时定义</span><br><span class="line">    自定义Source源解析中加入Gauge类型指标埋点,汇报如下指标:</span><br><span class="line">        记录最新的一条数据中的EventTime,在汇报指标时使用当前系统时间-EventTime</span><br><span class="line">        记录读取到数据的系统时间-数据中的EventTime,直接汇报差值</span><br><span class="line">    Fetch_delay &#x3D; 读取到数据的系统时间 - Event Time</span><br><span class="line">    反应实时计算的实际处理能力</span><br><span class="line"></span><br><span class="line">延时分析</span><br><span class="line">    几个上游源头,每个源头的并发问题</span><br><span class="line">    是否上游数据稀疏导致</span><br><span class="line">    作业性能问题</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="为什么作业Failover了"><a href="#为什么作业Failover了" class="headerlink" title="为什么作业Failover了"></a>为什么作业Failover了</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">JM</span><br><span class="line">    ZK访问超时</span><br><span class="line">    长时间GC</span><br><span class="line">    资源问题</span><br><span class="line">    主机层面问题</span><br><span class="line">TM</span><br><span class="line">    上下游异常</span><br><span class="line">    数据问题</span><br><span class="line">    Runtime异常</span><br><span class="line">    主机层面异常</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="作业无法提交-异常停止"><a href="#作业无法提交-异常停止" class="headerlink" title="作业无法提交,异常停止"></a>作业无法提交,异常停止</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">无法提交</span><br><span class="line">    Yarn问题-资源限制</span><br><span class="line">    HDFS问题-Jar包过大,DFS异常</span><br><span class="line">    JM资源不足,无法响应TM注册</span><br><span class="line">    TM启动过程中异常</span><br><span class="line"></span><br><span class="line">异常停止-指标监控无法覆盖</span><br><span class="line">    重启策略配置错误</span><br><span class="line">    重启次数达到上限</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="延时问题处理方式"><a href="#延时问题处理方式" class="headerlink" title="延时问题处理方式"></a>延时问题处理方式</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">延时与吞吐</span><br><span class="line">    确定延时节点以及时间</span><br><span class="line">反压分析</span><br><span class="line">    找到反压源节点</span><br><span class="line">指标分析</span><br><span class="line">    查看一段时间相关指标</span><br><span class="line">堆栈</span><br><span class="line">    找到指定节点JVM进程,分析Jstack等堆栈信息</span><br><span class="line">相关日志</span><br><span class="line">    查看TM相关日志是否有异常</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="作业性能问题"><a href="#作业性能问题" class="headerlink" title="作业性能问题"></a>作业性能问题</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">延时与吞吐</span><br><span class="line">    延时指标</span><br><span class="line">    TPS输出</span><br><span class="line">    节点输入输出</span><br><span class="line"></span><br><span class="line">反压</span><br><span class="line">    反压源头节点</span><br><span class="line">    节点连接方式Shuffle&#x2F;Rebalance&#x2F;Hash</span><br><span class="line">    节点各并发情况</span><br><span class="line">    业务逻辑,是否有正则,外部系统访问等</span><br><span class="line"></span><br><span class="line">指标</span><br><span class="line">    GC时间</span><br><span class="line">    GC次数</span><br><span class="line">    State性能,CK情况</span><br><span class="line">    外部系统访问延时</span><br><span class="line"></span><br><span class="line">堆栈</span><br><span class="line">    节点所在TM进行</span><br><span class="line">    查看线程TID CPU使用情况,确定CPU还是IO问题</span><br><span class="line">    ps H -p $&#123;javapid&#125; -o user,pid,ppid,tid,time,%cpu,cmd</span><br><span class="line">    转换为16进制后查看tid具体堆栈</span><br><span class="line">    jstack $&#123;javapid&#125; &gt; jstack.log</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="常见处理方式"><a href="#常见处理方式" class="headerlink" title="常见处理方式"></a>常见处理方式</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">调整节点并发</span><br><span class="line">    性能瓶颈节点增加并发</span><br><span class="line">调整节点资源</span><br><span class="line">    增加节点CPU,内存</span><br><span class="line">拆分节点</span><br><span class="line">    将chain起来的消耗资源较多的operator拆开,增加并发</span><br><span class="line">作业&#x2F;集群优化</span><br><span class="line">    主键设置,数据去重,数据倾斜</span><br><span class="line">    GC参数</span><br><span class="line">    JM参数</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="作业Failover分析"><a href="#作业Failover分析" class="headerlink" title="作业Failover分析"></a>作业Failover分析</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Failover信息</span><br><span class="line">是否频繁Failover节点SubTask-&gt;TM</span><br><span class="line">Job&#x2F;TaskManager日志</span><br><span class="line">Yarn&#x2F;OS相关日志</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="作业生命周期"><a href="#作业生命周期" class="headerlink" title="作业生命周期"></a>作业生命周期</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作业状态变化JobStatus</span><br><span class="line">    Created</span><br><span class="line">    Running</span><br><span class="line">        Finished</span><br><span class="line">    Cancelling</span><br><span class="line">        Canceled</span><br><span class="line">    Failing</span><br><span class="line">        Failed</span><br><span class="line">    Restarting</span><br><span class="line">    Suspended</span><br><span class="line">    </span><br><span class="line">Task状态变化ExecutionState</span><br><span class="line">    Created</span><br><span class="line">    Scheduled</span><br><span class="line">    Deploying</span><br><span class="line">    Running</span><br><span class="line">    Failed</span><br><span class="line">    Finished</span><br><span class="line">    Canceling</span><br><span class="line">    Canceled</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="工具化经验"><a href="#工具化经验" class="headerlink" title="工具化经验"></a>工具化经验</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">指标</span><br><span class="line">    延时与吞吐</span><br><span class="line">        衡量Flink作业的黄金指标</span><br><span class="line">    外部系统调用</span><br><span class="line">        外部系统耗时</span><br><span class="line">        缓存名字</span><br><span class="line">        排除外部系统因素</span><br><span class="line">    基线管理</span><br><span class="line">        State访问延时</span><br><span class="line">        CK耗时</span><br><span class="line">        排查异常问题</span><br><span class="line">        </span><br><span class="line">日志</span><br><span class="line">    错误日志</span><br><span class="line">        关键字及错误日志报警</span><br><span class="line">    事件日志</span><br><span class="line">        采集关键日志信息,形成关键事件</span><br><span class="line">    日志收集</span><br><span class="line">        存储关键信息</span><br><span class="line">    日志分析</span><br><span class="line">        日志聚类,Failover建议等</span><br><span class="line"></span><br><span class="line">关联分析</span><br><span class="line">    集群环境各组件关联分析</span><br><span class="line">        作业指标&#x2F;事件-TM,JM</span><br><span class="line">        Yarn事件-资源抢占,NodeManager Decommission</span><br><span class="line">        机器异常-宕机,替换</span><br><span class="line">        Failover日志聚类</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之八大规模运维</title>
    <url>/2019/10/10/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E5%85%AB%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%BF%90%E7%BB%B4/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">规模大,架构复杂,增长迅速</span><br><span class="line"></span><br><span class="line">集群规模大:几万个计算节点,几百个集群</span><br><span class="line">用户规模大:几万个作业,几千个用户</span><br><span class="line">系统复杂:几十个上下游模块,分布式系统原理复杂</span><br><span class="line">部署环境多:底层架构多样,出口多元化</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">稳定</span><br><span class="line">    如何减少服务故障</span><br><span class="line">    如何保障大促稳定</span><br><span class="line">    大量运维操作如何保持一致性</span><br><span class="line">成本</span><br><span class="line">    如何管理硬件资源</span><br><span class="line">    如何管理用户资源</span><br><span class="line">    如何降低运维人力成本</span><br><span class="line">效率</span><br><span class="line">    值班答疑</span><br><span class="line">    问题排查</span><br><span class="line">    如何减少人肉运维</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink运维管控"><a href="#Flink运维管控" class="headerlink" title="Flink运维管控"></a>Flink运维管控</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">定位</span><br><span class="line">    着力于稳定,成本,效率,一站式支撑超大规模Flink集群运维</span><br><span class="line"></span><br><span class="line">架构</span><br><span class="line">    高效开发,通用</span><br><span class="line">    数据层</span><br><span class="line">    服务层</span><br><span class="line">    功能层</span><br><span class="line"></span><br><span class="line">运维解决方案</span><br><span class="line">    稳定:软件生命周期</span><br><span class="line">        发布变更:集群规模大,场景多,流程复杂,如何稳定,高效的发布</span><br><span class="line">            Flink作业升级:大规模Flink作业版本如何升级</span><br><span class="line">        服务故障:如何减少故障,低成本维持稳定</span><br><span class="line">            故障隐患自愈</span><br><span class="line">            服务故障自愈</span><br><span class="line">        大促压测:如何高效支撑大规模作业压测</span><br><span class="line">    成本:资源生命周期</span><br><span class="line">        用户资源管理:如何高效支撑用户规模大,场景复杂的资源管理需求</span><br><span class="line">    效率:日常运维琐事</span><br><span class="line">        作业诊断:如何高效分析作业异常根因,并给出有效建议</span><br><span class="line">        值班答疑:如何高效应对大量日常答疑</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之六StateProcessorAPI</title>
    <url>/2019/09/17/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E5%85%ADStateProcessorAPI/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="StateProcessorAPI"><a href="#StateProcessorAPI" class="headerlink" title="StateProcessorAPI"></a>StateProcessorAPI</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Apache Flink 1.9.0推出的新功能之一</span><br><span class="line">用于读取&#x2F;分析&#x2F;生成Flink的SavePoint与CheckPoint</span><br><span class="line"></span><br><span class="line">状态的运算与分析</span><br><span class="line">    读取保存点中的状态数据并且加以分析</span><br><span class="line">StateBootstraping</span><br><span class="line">    用历史数据生成新流式应用的起始状态保存点</span><br><span class="line">状态修正</span><br><span class="line">    仅修正错误的状态值,其余算子的状态保留并且生成一个新的Flink保存点</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="范例"><a href="#范例" class="headerlink" title="范例"></a>范例</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">UserPurchase&#123;</span><br><span class="line">    String userId;</span><br><span class="line">    <span class="keyword">long</span> timestamp;</span><br><span class="line">    Item item;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">UserItems <span class="title">KeyedProcessFunction</span><span class="params">(uid=<span class="string">&quot;user_items&quot;</span>)</span></span></span><br><span class="line"><span class="function">ValueState&lt;String&gt; userId</span>;</span><br><span class="line">ValueState&lt;Long&gt; lastSeenTimestamp;</span><br><span class="line">ListState&lt;Item&gt; purchasedItems;</span><br><span class="line"></span><br><span class="line"><span class="comment">// SP&amp;CK读取</span></span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">ExistingSavepoint existingSavepoint = Savepoint.load(env,<span class="string">&quot;hdfs://path/&quot;</span>,<span class="keyword">new</span> RocksDBStateBackend());</span><br><span class="line"><span class="comment">// read keyed state in operator &quot;user_items&quot;</span></span><br><span class="line">DataSet&lt;UserState&gt; userStates = existingSavepoint.readKeyedState(<span class="string">&quot;user_items&quot;</span>,<span class="keyword">new</span> UserKeyedStateReaderFunction());</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserKeyedStateReaderFunction</span> <span class="keyword">extends</span> <span class="title">KeyedStateReaderFunction</span>&lt;<span class="title">String</span>,<span class="title">UserState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;String&gt; userId;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;Long&gt; lastSeenTimestamp;</span><br><span class="line">    <span class="keyword">private</span> ListState&lt;Item&gt; purchasedItems;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userId = getRuntimeContext.getState(...);</span><br><span class="line">        <span class="keyword">this</span>.lastSeenTimestamp = getRuntimeContext.getState(...);</span><br><span class="line">        <span class="keyword">this</span>.purchasedItems = getRuntimeContext.getListState(...);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readKey</span><span class="params">(String key,Context cxt,Collector&lt;OUT&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        out.collect(<span class="keyword">new</span> UserState(userId.get(),lastSeenTimestamp.get(),purchasedItems.get()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// SP&amp;CK运算</span></span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">ExistingSavepoint existingSavepoint = Savepoint.load(env,<span class="string">&quot;hdfs://path/&quot;</span>,<span class="keyword">new</span> RocksDBStateBackend());</span><br><span class="line"><span class="comment">// read keyed state in operator &quot;user_items&quot;</span></span><br><span class="line">DataSet&lt;UserState&gt; userStates = existingSavepoint.readKeyedState(<span class="string">&quot;user_items&quot;</span>,<span class="keyword">new</span> UserKeyedStateReaderFunction());</span><br><span class="line"><span class="comment">// process userStates as you normally would with the DataSetAPI ...</span></span><br><span class="line"><span class="comment">// e.g. count total number of items purchased for each category,计算所有被购买过的商品中,各商品种类的购买次数</span></span><br><span class="line">userStates.flatMap(<span class="comment">/* split purchased items list into individual items */</span>)</span><br><span class="line">    .groupBy(<span class="string">&quot;itemCategory&quot;</span>)</span><br><span class="line">    .reduce(<span class="keyword">new</span> ItemCounter());</span><br><span class="line">env.execute();</span><br><span class="line"></span><br><span class="line"><span class="comment">// SP&amp;CK状态修正</span></span><br><span class="line"><span class="comment">// 某商品的隶属分类被改动</span></span><br><span class="line"><span class="comment">// UserItems算子中状态内的ListState&lt;Item&gt;所有的状态值则需要被过滤且修正分类类别</span></span><br><span class="line">ExistingSavepoint existingSavepoint = Savepoint.load(env,<span class="string">&quot;hdfs://path/&quot;</span>,<span class="keyword">new</span> RocksDBStateBackend());</span><br><span class="line"><span class="comment">// read keyed state in operator &quot;user_items&quot;</span></span><br><span class="line">DataSet&lt;UserState&gt; userStates = existingSavepoint.readKeyedState(<span class="string">&quot;user_items&quot;</span>,<span class="keyword">new</span> UserKeyedStateReaderFunction());</span><br><span class="line"><span class="comment">// perpare a DataSet with the correct state values</span></span><br><span class="line">DataSet&lt;UserState&gt; correctedUserStates = userStates.map(<span class="keyword">new</span> PurchasedItemCategoryPatcher());</span><br><span class="line"><span class="comment">// bootstrap a new operator with the correctedUserStates</span></span><br><span class="line">BootstrapTransformation bootstrapTransformation = OperatorTransformation.bootstrapWith(correctedUserStates)</span><br><span class="line">    .keyBy(<span class="string">&quot;userId&quot;</span>)</span><br><span class="line">    .transform(<span class="keyword">new</span> UserKeyedStateBootstrapFunction());</span><br><span class="line"><span class="comment">// replace the old operator with the new one</span></span><br><span class="line">existingSavepoint.withOperator(<span class="string">&quot;user_items&quot;</span>,bootstrapTransformation)</span><br><span class="line">    .write(<span class="string">&quot;hdfs://path/for/corrected/savepoint&quot;</span>);</span><br><span class="line">env.execute();</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserKeyedStateBootstrapFunction</span> <span class="keyword">extends</span> <span class="title">KeyedStateBootstrapFunction</span>&lt;<span class="title">String</span>,<span class="title">UserState</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;String&gt; userId;</span><br><span class="line">    <span class="keyword">private</span> ValueState&lt;Long&gt; lastSeenTimestamp;</span><br><span class="line">    <span class="keyword">private</span> ListState&lt;Item&gt; purchasedItems;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userId = getRuntimeContext.getState(...);</span><br><span class="line">        <span class="keyword">this</span>.lastSeenTimestamp = getRuntimeContext.getState(...);</span><br><span class="line">        <span class="keyword">this</span>.purchasedItems = getRuntimeContext.getListState(...);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(UserState userStates,Context cxt)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.userId.update(userStates.userId);</span><br><span class="line">        <span class="keyword">this</span>.lastSeenTimestamp(userStates.lastSeenTimestamp);</span><br><span class="line">        <span class="keyword">for</span>(Item purchasedItem : userStates.purchasedItems) &#123;</span><br><span class="line">            <span class="keyword">this</span>.purchasedItems.add(purchasedItem);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="State-Bootstrapping"><a href="#State-Bootstrapping" class="headerlink" title="State Bootstrapping"></a>State Bootstrapping</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 新部署的流式运算应用的初始状态往往存在于其他现有数据库&#x2F;档案存储系统</span><br><span class="line">&#x2F;&#x2F; 先以DataSet读取历史资料,处理完毕后生成新的Flink SavePoint</span><br><span class="line">ExecutionEnvironment env &#x3D; ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">&#x2F;&#x2F; read a DataSet from any existing data source</span><br><span class="line">DataSet&lt;UserState&gt; historicUserStates &#x3D; env.readFile(new MyFileInputFormat&lt;&gt;(...), &quot;hdfs:&#x2F;&#x2F;history&#x2F;user&#x2F;files&quot;);</span><br><span class="line">&#x2F;&#x2F; bootstarp a new operator with the historicUserStates</span><br><span class="line">BootstrapTransformation bootstrapTransformation &#x3D; OperatorTransformation.bootstrapWith(correctedUserStates)</span><br><span class="line">    .keyBy(&quot;userId&quot;)</span><br><span class="line">    .transform(new UserKeyedStateBootstrapFunction());</span><br><span class="line">&#x2F;&#x2F; create a new savepoint, and register the bootstrapped operator</span><br><span class="line">NewSavepoint newSavepoint &#x3D; Savepoint.create(new RocksDBStateBackend(), 128);</span><br><span class="line">newSavepoint.withOperator(&quot;user_items&quot;,bootstrapTransformation)</span><br><span class="line">    .write(&quot;hdfs:&#x2F;&#x2F;new&#x2F;flink&#x2F;savepoint&quot;);</span><br><span class="line">env.execute();</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="未来计划"><a href="#未来计划" class="headerlink" title="未来计划"></a>未来计划</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DataSet API即将被移除,未来State Processor API会直接使用DataStream API</span><br><span class="line">更便利的直接更改MaxParallelism</span><br><span class="line">更便利的去生成WindowState</span><br><span class="line">增加查询应用保存点中有的Operator与所有注册过的状态MetaInfomation</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之十二知识图谱</title>
    <url>/2020/02/04/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E5%8D%81%E4%BA%8C%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/</url>
    <content><![CDATA[<blockquote>
<p>学习知识图谱</p>
</blockquote>
<span id="more"></span>

<p>由 Apache Flink Committer 执笔，四位 PMC 成员审核，将 Flink 9 大技术版块详细拆分，突出重点内容并搭配全面的学习素材。PDF 版本内含大量补充链接，点击即可跳转。</p>
<p><a href="https://files.alicdn.com/tpsservice/7ee4adfd0d54825bb3404133b0f25a0d.pdf">传送门</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之二指标监控报警</title>
    <url>/2019/08/16/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E4%BA%8C%E6%8C%87%E6%A0%87%E7%9B%91%E6%8E%A7%E6%8A%A5%E8%AD%A6/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="为什么关注指标"><a href="#为什么关注指标" class="headerlink" title="为什么关注指标"></a>为什么关注指标</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">指标</span><br><span class="line">    标准化</span><br><span class="line">    可量化</span><br><span class="line">    多维度</span><br><span class="line">监控</span><br><span class="line">    易用</span><br><span class="line">    实时</span><br><span class="line">    可查询历史</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="监控报警链路"><a href="#监控报警链路" class="headerlink" title="监控报警链路"></a>监控报警链路</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">日志指标收集</span><br><span class="line">    统一化,集中化</span><br><span class="line">解析展示</span><br><span class="line">    多维度,多种方式聚合</span><br><span class="line">监控报警</span><br><span class="line">    个性化,可配置</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="指标展示"><a href="#指标展示" class="headerlink" title="指标展示"></a>指标展示</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grafana</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="常用的指标"><a href="#常用的指标" class="headerlink" title="常用的指标"></a>常用的指标</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">系统指标</span><br><span class="line">    可用性,流量,CPU,内存,GC,网络,CK,Connector</span><br><span class="line">    </span><br><span class="line">自定义指标</span><br><span class="line">    处理逻辑耗时打点</span><br><span class="line">    外部服务调用性能</span><br><span class="line">    缓存命中率</span><br><span class="line">    处理失败的数据占比</span><br><span class="line">    filter过滤的数据占比</span><br><span class="line">    超时丢弃的数据量</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="如何确定关注哪些监控项"><a href="#如何确定关注哪些监控项" class="headerlink" title="如何确定关注哪些监控项"></a>如何确定关注哪些监控项</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作业状态相关</span><br><span class="line">    作业故障</span><br><span class="line">    运行不稳定</span><br><span class="line">    影响可用性的风险因素</span><br><span class="line"></span><br><span class="line">作业性能相关</span><br><span class="line">    处理延迟</span><br><span class="line">    数据倾斜</span><br><span class="line">    性能瓶颈</span><br><span class="line"></span><br><span class="line">业务逻辑相关</span><br><span class="line">    上游数据问题</span><br><span class="line">    新上逻辑问题</span><br><span class="line">    数据丢失</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="指标聚合方式"><a href="#指标聚合方式" class="headerlink" title="指标聚合方式"></a>指标聚合方式</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">聚合维度 </span><br><span class="line">    Task</span><br><span class="line">    Operator</span><br><span class="line">    Job</span><br><span class="line">    Host</span><br><span class="line">        物理指标</span><br><span class="line">    Cluster</span><br><span class="line">        容量</span><br><span class="line">        流量</span><br><span class="line">    业务维度</span><br><span class="line">    大盘展示</span><br><span class="line">        粗粒度</span><br><span class="line">    故障排除</span><br><span class="line">        由粗到细</span><br><span class="line">    性能测试</span><br><span class="line">        细粒度</span><br><span class="line"></span><br><span class="line">聚合方式</span><br><span class="line">    总和,均值,最大,最小</span><br><span class="line">        常规指标</span><br><span class="line">        消除统计误差</span><br><span class="line">    差值</span><br><span class="line">        上游数据量与下游处理量的差</span><br><span class="line">        最新offset与消费offset的差</span><br><span class="line">    99线</span><br><span class="line">        xx率</span><br><span class="line">        xx耗时</span><br><span class="line">    指标缺失</span><br><span class="line">        单个指标缺失</span><br><span class="line">        整个作业没有指标</span><br><span class="line">        </span><br><span class="line">多指标复杂聚合</span><br><span class="line">    时间线对比</span><br><span class="line">        同比</span><br><span class="line">        环比</span><br><span class="line">        持续时间</span><br><span class="line">        周期性</span><br><span class="line">    结合外部系统计算</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="指标监控的用途"><a href="#指标监控的用途" class="headerlink" title="指标监控的用途"></a>指标监控的用途</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">作业异常报警</span><br><span class="line">    作业状态异常</span><br><span class="line">    作业无指标上报</span><br><span class="line">    指标达到阈值</span><br><span class="line">        处理量跌0</span><br><span class="line">        消费延迟</span><br><span class="line">            数量</span><br><span class="line">            时间</span><br><span class="line">        失败率,丢失率</span><br><span class="line">    个性化</span><br><span class="line">        报警时段</span><br><span class="line">        聚合方式</span><br><span class="line">    错误日志,关键词日志</span><br><span class="line">需要考虑报警系统本身的稳定性</span><br><span class="line">    误报</span><br><span class="line">    漏报</span><br><span class="line">    延迟</span><br><span class="line"></span><br><span class="line">指标大盘</span><br><span class="line">    反映平台整体的现状</span><br><span class="line">        异常值高亮</span><br><span class="line">        多维度聚合</span><br><span class="line">        时间线对比</span><br><span class="line">    及时发现并快速定位到故障</span><br><span class="line">    给出平台可优化的方向</span><br><span class="line">    便于统筹资源分配</span><br><span class="line">    </span><br><span class="line">自动化运维</span><br><span class="line">    无法运维</span><br><span class="line">        没有指标</span><br><span class="line">        黑盒作业</span><br><span class="line">        一群人围着看问题</span><br><span class="line">    手动运维</span><br><span class="line">        重启,扩容,回滚,迁移,降级</span><br><span class="line">        纠正错误代码</span><br><span class="line">        优化处理逻辑</span><br><span class="line">    辅助运维</span><br><span class="line">        使用指标量化</span><br><span class="line">        经验转化为建议</span><br><span class="line">            GC频繁-&gt;增大内存</span><br><span class="line">            数据量大-&gt;增大并发</span><br><span class="line">    智能运维</span><br><span class="line">        故障自动拉起</span><br><span class="line">        资源不足自动扩容</span><br><span class="line">        自动切换备用作业</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之十一监控告警系统</title>
    <url>/2019/12/25/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E5%8D%81%E4%B8%80%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之十生产配置</title>
    <url>/2019/10/19/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E5%8D%81%E7%94%9F%E4%BA%A7%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="基础配置"><a href="#基础配置" class="headerlink" title="基础配置"></a>基础配置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#JM的IP地址</span><br><span class="line">jobmanager.rpc.address: localhost</span><br><span class="line"></span><br><span class="line">#JM的端口号</span><br><span class="line">jobmanager.rpc.port: 6123</span><br><span class="line"></span><br><span class="line">#JM JVM heap内存大小(任务提交阶段可以再设置)</span><br><span class="line">jobmanager.heap.mb: 1024</span><br><span class="line"></span><br><span class="line">#TM JVM heap内存大小(任务提交阶段可以再设置)</span><br><span class="line">taskmanager.heap.mb: 2048</span><br><span class="line"></span><br><span class="line">#每个TM提供的任务slots数量大小(任务提交阶段可以再设置)</span><br><span class="line">taskmanager.numberOfTaskSlots: 2048</span><br><span class="line"></span><br><span class="line">#Flink任务默认并行度(如果是Kafka按照Kafka分区数即可,p&#x3D;slot*tm)</span><br><span class="line">parallelism.default: 1</span><br><span class="line"></span><br><span class="line">#Web的运行监视器端口</span><br><span class="line">web.port: 8081</span><br><span class="line"></span><br><span class="line">#将已完成的作业上传到的目录(用于帮助发现任务运行阶段日志信息)</span><br><span class="line">jobmanager.archive.fs.dir: hdfs:&#x2F;&#x2F;nameservice&#x2F;flink&#x2F;flink-jobs&#x2F;</span><br><span class="line"></span><br><span class="line">#基于Web的HistoryServer的端口号</span><br><span class="line">historyserver.web.port:8082</span><br><span class="line"></span><br><span class="line">#以逗号分割的目录列表,将作业归档到目录中</span><br><span class="line">historyserver.archive.fs.dir: hdfs:&#x2F;&#x2F;nameservice&#x2F;flink&#x2F;flink-jobs&#x2F;</span><br><span class="line"></span><br><span class="line">#刷新存档的作业目录的时间间隔(毫秒)</span><br><span class="line">historyserver.archive.fs.refresh-interval:10000</span><br><span class="line"></span><br><span class="line">#用于存储和检查点状态的存储类型:filesystem,hdfs,rocksdb</span><br><span class="line">state.backend: rocksdb</span><br><span class="line"></span><br><span class="line">#存储检查点的数据文件和元数据的默认目录</span><br><span class="line">state.backend.fs.checkpointdir:hdfs:&#x2F;&#x2F;nameservice&#x2F;flink&#x2F;pointsdata&#x2F;</span><br><span class="line"></span><br><span class="line">#用于保存检查点的目录</span><br><span class="line">state.checkpoints.dir: hdfs:&#x2F;&#x2F;&#x2F;flink&#x2F;checkpoints&#x2F;</span><br><span class="line"></span><br><span class="line">#savepoint的目录</span><br><span class="line">state.savepoints.dir: hdfs:&#x2F;&#x2F;&#x2F;flink&#x2F;checkpoints&#x2F;</span><br><span class="line"></span><br><span class="line">#保留最近的检查点数量</span><br><span class="line">state.checkpoints.num-retained: 20</span><br><span class="line"></span><br><span class="line">#开启增量CK</span><br><span class="line">state.backend.incremental: true</span><br><span class="line"></span><br><span class="line"># 超时</span><br><span class="line">akka.ask.timeout: 300s</span><br><span class="line"></span><br><span class="line">#akka心跳间隔,用于检测失效的TM,误报减小此值</span><br><span class="line">akka.watch.heartbeat.interval: 30s</span><br><span class="line"></span><br><span class="line">#如果由于丢失或延迟的心跳信息而错误的将TM标记为无效,增加此值</span><br><span class="line">akka.watch.hearbeat.pause: 120s</span><br><span class="line"></span><br><span class="line">#网络缓冲区的最大内存大小</span><br><span class="line">taskmanager.network.memeory.max: 4gb</span><br><span class="line"></span><br><span class="line">#网络缓冲区的最小内存大小</span><br><span class="line">taskmanager.network.memeory.min: 256mb</span><br><span class="line"></span><br><span class="line">#用于网络缓冲区的JVM内存的分数.决定TM可以同时具有多少个流数据交换通道以及通道的缓冲程度</span><br><span class="line">taskmanager.network.memory.fraction: 0.5</span><br><span class="line"></span><br><span class="line">#hadoop配置文件地址</span><br><span class="line">fs.hdfs.hadoopconf: &#x2F;etc&#x2F;ecm&#x2F;hadoop-conf&#x2F;</span><br><span class="line"></span><br><span class="line">#任务失败尝试次数</span><br><span class="line">yarn.application-attempts: 10</span><br><span class="line"></span><br><span class="line">#高可用</span><br><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.path.root: &#x2F;flink</span><br><span class="line">high-availability.zookeeper.quorum: zk1,zk2,zk3</span><br><span class="line">high-availability.storageDir: hdfs:&#x2F;&#x2F;nameservice&#x2F;flink&#x2F;ha&#x2F;</span><br><span class="line"></span><br><span class="line">#Metric收集</span><br><span class="line">metrics.reporters: prom</span><br><span class="line"></span><br><span class="line">#收集器</span><br><span class="line">metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter</span><br><span class="line"></span><br><span class="line">#metric对外暴露端口</span><br><span class="line">metrics.reporter.prom.port: 9250-9269</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Hadoop参数调优"><a href="#Hadoop参数调优" class="headerlink" title="Hadoop参数调优"></a>Hadoop参数调优</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#每个节点最大使用的vcore数,适当放大</span><br><span class="line">yarn.nodemanager.resource.cpu-vcores: 64</span><br><span class="line"></span><br><span class="line">#yarn-site.xml中设置container</span><br><span class="line">yarn.shceduler.minimum-allocation-mb:最小可申请内存量,默认1024</span><br><span class="line">yarn.shceduler.minimum-allocation-vcores:最小可申请CPU数,默认1</span><br><span class="line">yarn.shceduler.maximum-allocation-mb:最大可申请内存量,默认8096</span><br><span class="line">yarn.shceduler.maximum-allocation-vcores:最大可申请CPU数(也是TM最大可设置的Slot数)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">发布到多个Yarn集群</span><br><span class="line">    不同版本或集群bin&#x2F;conf隔离不同路径,通过export实现多集群操作</span><br><span class="line">    注意通过CK恢复的任务,需要设置CK到相对路径</span><br><span class="line"></span><br><span class="line">CK在A集群,数据写往B集群</span><br><span class="line">    Hadoop做多集群访问互通</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink运维之四FlinkCEP</title>
    <url>/2019/08/31/Ververica&amp;Flink%E8%BF%90%E7%BB%B4%E4%B9%8B%E5%9B%9BFlinkCEP/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="传送门"><a href="#传送门" class="headerlink" title="传送门"></a><a href="https://files.alicdn.com/tpsservice/94d409d9679d1b46034f7d00161d99a7.pdf">传送门</a></h1><hr>
<h1 id="什么是CEP"><a href="#什么是CEP" class="headerlink" title="什么是CEP"></a>什么是CEP</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CEP:复杂事件处理</span><br><span class="line">异常行为检测 -&gt; 运维维修 -&gt; 无翻台 -&gt; 运维报障</span><br><span class="line">策略营销 -&gt; 规划行程 -&gt; 下单 -&gt; 未被接单</span><br><span class="line">运维监控 -&gt; 流量抖动 -&gt; 等待5min -&gt; 流量抖动</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">风控检测</span><br><span class="line">    对用户异常行为模式,数据异常流向实时检测</span><br><span class="line">策略营销</span><br><span class="line">    向特定行为的用户进行实时的精准营销</span><br><span class="line">运维监控</span><br><span class="line">    监控设备运行参数,灵活配置多指标的发生规则</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="如何使用CEP"><a href="#如何使用CEP" class="headerlink" title="如何使用CEP"></a>如何使用CEP</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">流程</span><br><span class="line">    定义事件模式</span><br><span class="line">    匹配结果处理</span><br><span class="line"></span><br><span class="line">构成</span><br><span class="line">    Pattern</span><br><span class="line">        next&#x2F;notNext</span><br><span class="line">        followedBy&#x2F;noFollowedBy</span><br><span class="line">        followedByAny</span><br><span class="line">            Pattern</span><br><span class="line">                next&#x2F;notNext</span><br><span class="line">                followedBy&#x2F;noFollowedBy</span><br><span class="line">                followedByAny</span><br><span class="line">                    Pattern</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之一Runtime核心机制</title>
    <url>/2019/05/14/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E4%B8%80Runtime%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="整理架构"><a href="#整理架构" class="headerlink" title="整理架构"></a>整理架构</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">物理资源层</span><br><span class="line">Runtime统一执行引擎</span><br><span class="line">API层</span><br><span class="line">High-Level API层</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Runtime层总体架构"><a href="#Runtime层总体架构" class="headerlink" title="Runtime层总体架构"></a>Runtime层总体架构</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Client</span><br><span class="line">AM</span><br><span class="line">    Dispatcher</span><br><span class="line">    ResourceMananger</span><br><span class="line">    JobManager</span><br><span class="line">        JobGraph</span><br><span class="line">TaskManager</span><br><span class="line">    StateBackend</span><br><span class="line">    Tasks</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="资源管理与任务调度"><a href="#资源管理与任务调度" class="headerlink" title="资源管理与任务调度"></a>资源管理与任务调度</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ResourceManager</span><br><span class="line">    Slot Manager</span><br><span class="line">    管理Slot状态</span><br><span class="line">    分配Slot资源</span><br><span class="line">TaskExecutor</span><br><span class="line">    实际持有Slot资源</span><br><span class="line">JobMaster</span><br><span class="line">    Slot资源的申请者</span><br><span class="line">    </span><br><span class="line"># 调度策略</span><br><span class="line">Eager调度</span><br><span class="line">    适用于流作业</span><br><span class="line">    一次性调度所有的Task</span><br><span class="line">LAZY_FROM_SOURCE</span><br><span class="line">    适用于批作业</span><br><span class="line">    上游作业执行完成后,调度下游的作业</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="错误恢复"><a href="#错误恢复" class="headerlink" title="错误恢复"></a>错误恢复</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TaskFailover</span><br><span class="line">    单个Task执行失败或TM出错退出等</span><br><span class="line">    可以有多种不同的恢复策略</span><br><span class="line">        Restart-all:重启所有Task,从上次的CK开始重新执行</span><br><span class="line">        Restart-individual:只重启出错Task,只能用于Task间无连接的情况,应用极为有限</span><br><span class="line">        Restart-Region:重启PipelineRegion,Blocking数据落盘,可以直接读取,逻辑上仅需要重启通过Pipeline边关联的Task </span><br><span class="line"></span><br><span class="line">MasterFailover</span><br><span class="line">    AM执行失败</span><br><span class="line">    多个Master通过ZK进行选主</span><br><span class="line">    目前MasterFailover要求全图重启</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之三CK快照</title>
    <url>/2019/05/31/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E4%B8%89CK%E5%BF%AB%E7%85%A7/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="CK与State"><a href="#CK与State" class="headerlink" title="CK与State"></a>CK与State</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CK是从source触发到下游所有节点完成的一次全局操作</span><br><span class="line">State是构成CK的数据构成</span><br><span class="line"></span><br><span class="line">Key与NoKey维度</span><br><span class="line">    KeyedState</span><br><span class="line">    OperatorState</span><br><span class="line">Flink管理维度</span><br><span class="line">    ManagedState</span><br><span class="line">    RawState</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="CK执行机制"><a href="#CK执行机制" class="headerlink" title="CK执行机制"></a>CK执行机制</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">三类存储机制</span><br><span class="line">    Memory</span><br><span class="line">    Fs</span><br><span class="line">    RocksDB</span><br><span class="line"></span><br><span class="line">执行流程</span><br><span class="line">    a.Checkpoint Coordinator向所有source节点trigger Checkpoint</span><br><span class="line">    b.source节点向下游广播barrier,这个barrier就是实现Chandy-Lamport分布式快照算法的核心,下游的task只有收到所有input的barrier才会执行相应的Checkpoint</span><br><span class="line">    c.当task完成state备份后,会将备份数据的地址(state handle)通知给Checkpoint coordinator</span><br><span class="line">    d.下游的sink节点收集齐上游两个input的barrier之后,会执行本地快照</span><br><span class="line">        RocksDB会全量刷数据到磁盘上,然后Flink框架会从中选择没有上传的文件进行持久化备份</span><br><span class="line">    e.sink节点在完成自己的Checkpoint之后,会将state handle返回通知Coordinator</span><br><span class="line">    f.当Checkpoint coordinator收集齐所有task的state handle,就认为这一次的Checkpoint全局完成了,向持久化存储中再备份一个Checkpoint meta文件</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="CK的Exactly-Once"><a href="#CK的Exactly-Once" class="headerlink" title="CK的Exactly_Once"></a>CK的Exactly_Once</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">为了实现EXACTLY ONCE语义,Flink通过一个input buffer将在对齐阶段收到的数据缓存起来,等对齐完成之后再进行处理</span><br><span class="line">而对于AT LEAST ONCE语义,无需缓存收集到的数据</span><br><span class="line">会对后续直接处理,所以导致restore时,数据可能会被多次处理</span><br><span class="line"></span><br><span class="line">Flink的Checkpoint机制只能保证Flink的计算过程可以做到EXACTLY ONCE</span><br><span class="line">端到端的EXACTLY ONCE需要source和sink支持</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之九Connector分享</title>
    <url>/2019/06/27/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E4%B9%9DConnector%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="预定义的Source和Sink"><a href="#预定义的Source和Sink" class="headerlink" title="预定义的Source和Sink"></a>预定义的Source和Sink</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基于文件的Source</span><br><span class="line">    readTextFile(path)</span><br><span class="line">    readFile(fileInputFormat,path)</span><br><span class="line">基于文件的Sink</span><br><span class="line">    writeAsText</span><br><span class="line">    writeAsCsv</span><br><span class="line">基于Socket的Source</span><br><span class="line">    socketTextStream</span><br><span class="line">基于Socket的Sink</span><br><span class="line">    writeToSocket</span><br><span class="line">基于Collections,Iterators的Source</span><br><span class="line">    fromCollection</span><br><span class="line">    fromElements</span><br><span class="line">标准输出,标准错误</span><br><span class="line">    print</span><br><span class="line">    printToError</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Bundled-Connectors"><a href="#Bundled-Connectors" class="headerlink" title="Bundled Connectors"></a>Bundled Connectors</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Apache Kafka(source&#x2F;sink)</span><br><span class="line">Apache Cassandra(sink)</span><br><span class="line">Amazon Kinesis Streams(source&#x2F;sink)</span><br><span class="line">ElasticSearch(sink)</span><br><span class="line">Hadoop FileSystem(sink)</span><br><span class="line">RabbitMQ(source&#x2F;sink)</span><br><span class="line">Apache NiFi(source&#x2F;sink)</span><br><span class="line">Twitter Streaming API(source)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Apache-Bahir中的连接器"><a href="#Apache-Bahir中的连接器" class="headerlink" title="Apache Bahir中的连接器"></a>Apache Bahir中的连接器</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Apache ActiveMQ(source&#x2F;sink)</span><br><span class="line">Apache Flume(sink)</span><br><span class="line">Redis(sink)</span><br><span class="line">Akka(sink)</span><br><span class="line">Netty(source)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Async-I-O"><a href="#Async-I-O" class="headerlink" title="Async I/O"></a>Async I/O</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用connector并不是数据输入输出Flink的唯一方式</span><br><span class="line">在Map,FlatMap中使用Async I&#x2F;O方式读取外部数据库等</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Kafka-Consumer反序列化数据"><a href="#Flink-Kafka-Consumer反序列化数据" class="headerlink" title="Flink Kafka Consumer反序列化数据"></a>Flink Kafka Consumer反序列化数据</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将kafka中二进制数据转化为具体的java,scala对象</span><br><span class="line">DeserializationSchema,T deserialize(byte[] message)</span><br><span class="line">KeyedDeserializationSchema,T deserialize(byte[] messageKey,byte[] message,String topic,int partition,long offset):对于访问kafka key&#x2F;value</span><br><span class="line"></span><br><span class="line">常用</span><br><span class="line">SimpleStringSchema:按字符串方式进行序列化,反序列化</span><br><span class="line">TypeInformationSerializationSchema:基于Flink的TypeInformation来创建schema</span><br><span class="line">JsonDeserializationSchema:使用jackson反序列化json格式消息,并返回ObjectNode,可以使用.get(&quot;property&quot;)方法来访问字段</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Kafka-Consumer消费起始位置"><a href="#Flink-Kafka-Consumer消费起始位置" class="headerlink" title="Flink Kafka Consumer消费起始位置"></a>Flink Kafka Consumer消费起始位置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">setStartFromGroupOffsets(默认)</span><br><span class="line">    从Kafka记录的group.id的位置开始读取,如果没有根据anto.offset.reset设置</span><br><span class="line">setStratFromEarliest</span><br><span class="line">    从Kafka最早的位置读取</span><br><span class="line">setStartFromLatest</span><br><span class="line">    从Kafka最新数据开始读取</span><br><span class="line">setStartFromTimestamp(long)</span><br><span class="line">    从时间戳大于或等于指定时间戳的位置开始读取</span><br><span class="line">setStartFromSpecificOffsets</span><br><span class="line">    从指定的分区的offset位置开始读取,如指定的offsets中不存在某个分区,该分区从group offset位置开始读取</span><br><span class="line"></span><br><span class="line">注意</span><br><span class="line">    作业故障从CK自动恢复,以及手动做SP时,消费的位置从保存状态中恢复,与该配置无关</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Kafka-Consumer-Topic-Partition自动发现"><a href="#Flink-Kafka-Consumer-Topic-Partition自动发现" class="headerlink" title="Flink Kafka Consumer Topic Partition自动发现"></a>Flink Kafka Consumer Topic Partition自动发现</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原理:</span><br><span class="line">    内部单独的线程获取kafka meta信息进行更新</span><br><span class="line">flink.partition-discovery.interval-millis:发现时间间隔.默认false,设置为非负值开启</span><br><span class="line"></span><br><span class="line">分区发现</span><br><span class="line">    消费的Source Kafka Topic进行partition扩容</span><br><span class="line">    新发现的分区,从earliest位置开始读取</span><br><span class="line">Topic发现</span><br><span class="line">    支持正则表达式描述topic名字</span><br><span class="line">Pattern topicPattern &#x3D; java.util.regex.Pattern.compile(&quot;topic[0-9]&quot;);</span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer &#x3D; new FlinkKafkaConsumer010(topicPattern,new SimpleStringSchema(),properties);</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Kafka-Consumer-Commit-Offset方式"><a href="#Flink-Kafka-Consumer-Commit-Offset方式" class="headerlink" title="Flink Kafka Consumer Commit Offset方式"></a>Flink Kafka Consumer Commit Offset方式</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CK关闭</span><br><span class="line">    依赖kafka客户端的auto commit定期提交offset</span><br><span class="line">    需设置enable.anto.commit,auto.commit.interval.ms参数到consumer properties</span><br><span class="line"></span><br><span class="line">CK开启</span><br><span class="line">    Offset自己在ck state中管理和容错,提交Kafka仅作为外部监视消费进度</span><br><span class="line">    通过setCommitOffsetsOnCheckpoints控制,CK成功之后,是否提交offset到kafka</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Kafka-Consumer时间戳提取-水位生成"><a href="#Flink-Kafka-Consumer时间戳提取-水位生成" class="headerlink" title="Flink Kafka Consumer时间戳提取/水位生成"></a>Flink Kafka Consumer时间戳提取/水位生成</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">per Kafka Partition WaterMark</span><br><span class="line">    assignTimestampsAndWatermarks,每个partition一个assigner,水位为对个partition对齐后值</span><br><span class="line">    不在KafkaSource生成WaterMark,会出现扔掉部分数据情况</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Kafka-Producer-Producer分区"><a href="#Flink-Kafka-Producer-Producer分区" class="headerlink" title="Flink Kafka Producer Producer分区"></a>Flink Kafka Producer Producer分区</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Producer分区</span><br><span class="line">    FlinkFixedPartitioner(默认):parallellnstanceld % partitions.length</span><br><span class="line">    Partitioner设置为null:round-robin kafka partitioner,维持过多链接</span><br><span class="line">    Custom Partitioner:自定义分区</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink-Kafka-Producer-Producer容错"><a href="#Flink-Kafka-Producer-Producer容错" class="headerlink" title="Flink Kafka Producer Producer容错"></a>Flink Kafka Producer Producer容错</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kafka 0.9 and 0.10</span><br><span class="line">    setLogFailuresOnly:默认false.写失败时,是否打印失败log,不抛异常</span><br><span class="line">    setFlushOnCheckpoint:默认true.ck时保证数据写到kafka</span><br><span class="line">    at-least-once语义:setLogFailuresOnly(false)+setFlushOnCheckPoint(true)</span><br><span class="line"></span><br><span class="line">Kafka 0.11</span><br><span class="line">    FlinkKafkaProducer011,两阶段提交Sink结合Kafka事务,保证端到端精准一次</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之二Time深度解析</title>
    <url>/2019/05/23/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E4%BA%8CTime%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当你的应用遇到某些问题要从上一个checkpoint或者savepoint进行重放,是不是希望结果完全相同</span><br><span class="line">如果希望结果完全相同,就只能用Event Time</span><br><span class="line">如果接受结果不同,则可以用Processing Time</span><br><span class="line"></span><br><span class="line">Processing Time</span><br><span class="line">    对于Processing Time,因为我们是使用的是本地节点的时间(假设这个节点的时钟同步没有问题)</span><br><span class="line">    我们每一次取到的Processing Time肯定都是递增的</span><br><span class="line">    递增就代表着有序,所以说我们相当于拿到的是一个有序的数据流</span><br><span class="line"></span><br><span class="line">Event Time</span><br><span class="line">    而用Event Time的时候因为时间是绑定在每一条的记录上的</span><br><span class="line">    由于网络延迟,程序内部逻辑,或者其他一些分布式系统的原因</span><br><span class="line">    数据的时间可能会存在一定程度的乱序</span><br><span class="line">    在Event Time场景下,我们把每一个记录所包含的时间称作Record Timestamp</span><br><span class="line">    如果Record Timestamp所得到的时间序列存在乱序</span><br><span class="line">    我们就需要去处理这种情况</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="WaterMark生成"><a href="#WaterMark生成" class="headerlink" title="WaterMark生成"></a>WaterMark生成</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个watermark本质上就代表了这个watermark所包含的timestamp数值</span><br><span class="line">表示以后到来的数据已经再也没有小于或等于这个时间的了</span><br><span class="line"></span><br><span class="line"># WaterMark生成</span><br><span class="line">SourceFunction生成</span><br><span class="line">    collectWithTimestamp,发送一条数据</span><br><span class="line">        第一个参数就是我们要发送的数据</span><br><span class="line">        第二个参数就是这个数据对应的时间戳</span><br><span class="line">    emitWaterMark去产生一条watermark</span><br><span class="line">        表示接下来不会再有时间戳小于等于这个数值记录</span><br><span class="line">DataStreamAPI指定</span><br><span class="line">    DataStream.assignTimestampsAndWatermarks能接收不同的timestamp和watermark生成器</span><br><span class="line">    </span><br><span class="line">生成器可以分为两类,定期生成器,特殊记录生成器</span><br><span class="line">两者的区别主要有三个方面</span><br><span class="line">    首先定期生成是现实时间驱动的,这里的&quot;定期生成&quot;主要是指watermark(因为timestamp是每一条数据都需要有的)</span><br><span class="line">    即定期会调用生成逻辑去产生一个watermark</span><br><span class="line">    而根据特殊记录生成是数据驱动的,即是否生成watermark不是由现实时间来决定</span><br><span class="line">    而是当看到一些特殊的记录就表示接下来可能不会有符合条件的数据再发过来了</span><br><span class="line">    这个时候相当于每一次分配Timestamp之后都会调用用户实现的watermark生成方法</span><br><span class="line">    用户需要在生成方法中去实现watermark的生成逻辑</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="WaterMark传播"><a href="#WaterMark传播" class="headerlink" title="WaterMark传播"></a>WaterMark传播</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">广播</span><br><span class="line">Long.MAX_VALUE</span><br><span class="line">单输入取其大,多输入取其小(最小化其最大值)</span><br><span class="line"></span><br><span class="line">局限</span><br><span class="line">没有区分逻辑上的单流和多流,强制同步时钟</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="ProcessFunction"><a href="#ProcessFunction" class="headerlink" title="ProcessFunction"></a>ProcessFunction</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">获取记录的Timestamp或当前的ProcessTime</span><br><span class="line">获取算子时间(WaterMark)</span><br><span class="line">注册Timer并提供回调逻辑</span><br><span class="line">    registerEventTimeTimer()</span><br><span class="line">    registerProcessingTimeTimer()</span><br><span class="line">    onTimer()</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="WaterMark处理"><a href="#WaterMark处理" class="headerlink" title="WaterMark处理"></a>WaterMark处理</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">更新算子时间</span><br><span class="line">便利计时器队列触发回调</span><br><span class="line">将WaterMark发送至下游</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Table指定时间列"><a href="#Table指定时间列" class="headerlink" title="Table指定时间列"></a>Table指定时间列</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Processing Time</span><br><span class="line">    DataStream</span><br><span class="line">        tEnv.fromDataStream(stream,&quot;f1,f2,f3.proctime&quot;)</span><br><span class="line">    TableSource</span><br><span class="line">        TableSource实现DefinedProctimeAttributes接口</span><br><span class="line"></span><br><span class="line">Event Time</span><br><span class="line">    DataStream(原始DS必须有Timestamp及WaterMark)</span><br><span class="line">        tEnv.fromDataStream(stream,&quot;f1,f2,f3.rowtime&quot;)</span><br><span class="line">        tEnv.fromDataStream(stream,&quot;f1,f2.rowtime,f3&quot;)</span><br><span class="line">    TableSource(数据中存在类型为long或timestamp的时间字段)</span><br><span class="line">        TableSource实现DefinedProctimeAttributes接口</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="时间列和Table操作"><a href="#时间列和Table操作" class="headerlink" title="时间列和Table操作"></a>时间列和Table操作</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Over窗口聚合</span><br><span class="line">Group By窗口聚合</span><br><span class="line">时间窗口连接</span><br><span class="line">排序</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之五数据类型及序列化</title>
    <url>/2019/05/31/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E4%BA%94%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%8A%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="定制的序列化框架"><a href="#定制的序列化框架" class="headerlink" title="定制的序列化框架"></a>定制的序列化框架</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基于JVM的数据分析引擎</span><br><span class="line">大数据时代的JVM - 显式的内存管理</span><br><span class="line">定制的序列化框架</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink的数据类型"><a href="#Flink的数据类型" class="headerlink" title="Flink的数据类型"></a>Flink的数据类型</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基础类型</span><br><span class="line">    所有Java的基础类型</span><br><span class="line">数组</span><br><span class="line">    基础类型构成的数组</span><br><span class="line">    Object[]</span><br><span class="line">复合类型</span><br><span class="line">    Flink Java Tuple 1~25个字段</span><br><span class="line">    Scala Tuple 1~22个字段</span><br><span class="line">    Row</span><br><span class="line">    POJO</span><br><span class="line">辅助类型</span><br><span class="line">    Option</span><br><span class="line">    Either</span><br><span class="line">    Lists</span><br><span class="line">    Maps</span><br><span class="line">泛型和其他类</span><br><span class="line">    由Kryo提供序列化支持</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">注册子类型</span><br><span class="line">注册自定义序列化器</span><br><span class="line">添加类型提示</span><br><span class="line">手动创建TypeInfomation</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Kryo序列化"><a href="#Kryo序列化" class="headerlink" title="Kryo序列化"></a>Kryo序列化</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对于Flink无法序列化的类型,默认会交给Kryo处理,如果Kryo仍然无法处理</span><br><span class="line">    1.强制使用Avro来代替Kryo</span><br><span class="line">        env.getConfig().enableForceAvro();</span><br><span class="line">    2.为Kryo增加自定义的Serializer以增强Kryo的功能</span><br><span class="line">        env.getConfig().addDefaultKryoSerializer(clazz,serializer);</span><br><span class="line"></span><br><span class="line">禁用Kryo</span><br><span class="line">    env.getConfig().disableGenericTypers();</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink通信层的序列化"><a href="#Flink通信层的序列化" class="headerlink" title="Flink通信层的序列化"></a>Flink通信层的序列化</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink的Task之间如果需要跨网络传输数据记录,那么就需要将数据序列化之后写入NetworkBufferPool,然后下层的Task读出之后再进行反序列操作,最后进行逻辑处理</span><br><span class="line"></span><br><span class="line">为了使得记录以及时间能够被写入Buffer随后在消费时在从Buffer中读出,Flink提供了数据记录序列化器(RecordSerializer)与反序列化器(RecordDeserializer)以及事件序列化器(EventSerializer)</span><br><span class="line"></span><br><span class="line">Function发送的数据被封装成SerialzationDelegate,它将任意元素公开为IOReadableWritable以进行序列化,通过setInstance()来传入要序列化的数据</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之八Metrics监控</title>
    <url>/2019/06/22/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%85%ABMetrics%E7%9B%91%E6%8E%A7/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Meteric-Type"><a href="#Meteric-Type" class="headerlink" title="Meteric Type"></a>Meteric Type</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Counter:计数器</span><br><span class="line">Gauge:最简单的Metric,反映一个值</span><br><span class="line">Meter:统计吞吐量,单位时间内发生&quot;事件&quot;的次数</span><br><span class="line">Histogram:统计数据分布,Quantile,Mean,StdDev,Max,Min</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Meteric-Group"><a href="#Meteric-Group" class="headerlink" title="Meteric Group"></a>Meteric Group</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Metric在Flink内部有多层结构,以Group的方式组织</span><br><span class="line">Metric Group + Metric Name是Metrics的唯一标识</span><br><span class="line">TaskManagerMetricGroup</span><br><span class="line">    TaskManagerJobMetricGroup</span><br><span class="line">        TaskMetricGroup</span><br><span class="line">            TaskIOMetricGroup</span><br><span class="line">            OperatorMetricGroup</span><br><span class="line">                User-defined Group &#x2F; User-defined Metrics</span><br><span class="line">                OperatorIOMetricGroup</span><br><span class="line">JobManagerMetricGroup</span><br><span class="line">    JobManagerJobMetricGroup</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="System-Metrics"><a href="#System-Metrics" class="headerlink" title="System Metrics"></a>System Metrics</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CPU</span><br><span class="line">Memory</span><br><span class="line">Threads</span><br><span class="line">Garbage Collection</span><br><span class="line">Network</span><br><span class="line">ClassLoader</span><br><span class="line">Cluster</span><br><span class="line">Availability</span><br><span class="line">CheckPointing</span><br><span class="line">StateBackend</span><br><span class="line">IO</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="User-defined-Metrics"><a href="#User-defined-Metrics" class="headerlink" title="User-defined Metrics"></a>User-defined Metrics</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">除了系统的Metrics之外,Flink支持自定义Metrics</span><br><span class="line">继承RichFunction</span><br><span class="line">    Register User-defined Metric Group:</span><br><span class="line">        getRuntimeContext().getMetricGroup().addGroup()</span><br><span class="line">    Register User-defined Metric:</span><br><span class="line">        getRuntimeContext().getMetricGroup().counter&#x2F;gauge&#x2F;meter&#x2F;histogram()</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="User-defined-Metrics-Example"><a href="#User-defined-Metrics-Example" class="headerlink" title="User-defined Metrics Example"></a>User-defined Metrics Example</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Counter processedCount = getRuntimeContext().getMetricGroup().counter(<span class="string">&quot;processed_count&quot;</span>);</span><br><span class="line">processedCount.inc();</span><br><span class="line">Meter processRate = getRuntimeContext().getMetricGroup().meter(<span class="string">&quot;rate&quot;</span>,<span class="keyword">new</span> MeterView(<span class="number">60</span>));</span><br><span class="line">getRuntimeContext().getMetricGroup().gauge(<span class="string">&quot;current_timestamp&quot;</span>,System::currentTimeMaillis);</span><br><span class="line">Histogram histogram = getRuntimeContext().getMetricGroup().histogram(<span class="string">&quot;histogram&quot;</span>,newDescriptiveStatisticsHistogram(<span class="number">1000</span>));</span><br><span class="line">histogram.update(<span class="number">1024</span>);</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="获取Metrics"><a href="#获取Metrics" class="headerlink" title="获取Metrics"></a>获取Metrics</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">WebUI</span><br><span class="line">RESTful API</span><br><span class="line">Metric Reporter</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="多维度分析"><a href="#多维度分析" class="headerlink" title="多维度分析"></a>多维度分析</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">业务维度</span><br><span class="line">    并发度是否合理</span><br><span class="line">    数据波峰波谷</span><br><span class="line">    数据倾斜</span><br><span class="line">Garbage Collection</span><br><span class="line">    GC log</span><br><span class="line">CK Alignment</span><br><span class="line">StateBackend性能</span><br><span class="line">    RockDB</span><br><span class="line">系统性能</span><br><span class="line">    CPU</span><br><span class="line">    内存,Swap</span><br><span class="line">    Disk IO,吞吐量,容量</span><br><span class="line">    Network IO,带宽</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之六流作业执行解析</title>
    <url>/2019/06/08/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%85%AD%E6%B5%81%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一层: Program -&gt; StreamGraph</span><br><span class="line">第二层: StreamGraph -&gt; JobGraph</span><br><span class="line">第三层: JobGraph -&gt; ExecutionGraph</span><br><span class="line">第四层: ExecutionGraph -&gt; 物理执行计划</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="StreamGraph转换"><a href="#StreamGraph转换" class="headerlink" title="StreamGraph转换"></a>StreamGraph转换</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从StreamExecutionEnvironment.execute开始执行程序</span><br><span class="line">    将transform添加到StreamExecutionEnvironment的transformations</span><br><span class="line">调用StreamGraphGenerator的generateInternal</span><br><span class="line">    遍历transformations构建StreamNode及StreamEage</span><br><span class="line">通过streamEdge连接StreamNode</span><br><span class="line"></span><br><span class="line">StreamNode</span><br><span class="line">    描述Operator的逻辑节点</span><br><span class="line">    slotSharingGroup</span><br><span class="line">    jobVertexClass</span><br><span class="line">    inEdges</span><br><span class="line">    outEdges</span><br><span class="line">    transformationUID</span><br><span class="line"></span><br><span class="line">StreamEdge</span><br><span class="line">    描述两个Operator逻辑的连接边</span><br><span class="line">    sourceVertex</span><br><span class="line">    targetVertex</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="StreamGraph到JobGraph的转化"><a href="#StreamGraph到JobGraph的转化" class="headerlink" title="StreamGraph到JobGraph的转化"></a>StreamGraph到JobGraph的转化</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">设置调度模式,Eager所有节点立即启动</span><br><span class="line">广度优先遍历StreamGraph,为每个StreamNode生成byte数组类型的hash值</span><br><span class="line">从source节点开始递归寻找chain到一起的operator,不能chain到一起的节点单独生成JobVertex,能够chain到一起的,开始节点生成JobVertex,其他节点以序列化的形式写入到StreamConfig,然后merge到CHAINED_TAS_CONFIG,然后通过JobEdge链接上下游JobVertex</span><br><span class="line">将每个JobVertex的入边(StreamEdge)序列化到该StreamConfig</span><br><span class="line">根据GroupName为每个JobVertex指定SlotSharingGroup</span><br><span class="line">配置CK</span><br><span class="line">将缓存文件存文件的配置添加到configuration中</span><br><span class="line">设置ExecutionConfig</span><br><span class="line"></span><br><span class="line"># Chain的条件</span><br><span class="line">下游节点只有一个输入</span><br><span class="line">下游节点的操作符不为null</span><br><span class="line">上游节点的操作符不为null</span><br><span class="line">上下游节点在一个槽位共享组中</span><br><span class="line">下游节点的连接策略是ALWAYS</span><br><span class="line">上游节点的连接策略是HEAD或者ALWAYS</span><br><span class="line">edge的分区函数是ForwardPartitioner的实例</span><br><span class="line">上下游节点的并行度相等</span><br><span class="line">可以进行节点连接操作</span><br><span class="line"></span><br><span class="line"># 为什么要为每个Operator生成hash值</span><br><span class="line">Flink任务失败时,各个Operator是能够从CK中恢复到失败之前的状态的,恢复时是依据JobVertexID(hash值)进行状态恢复的,相同的任务在恢复时要求Operator的hash值不变</span><br><span class="line"></span><br><span class="line"># 怎么生成Operator的hash值</span><br><span class="line">如果用户对节点指定了一个散列值,则基于用户指定的值,产生一个长度为16的字节数组</span><br><span class="line">如果用户没有指定,则根据当前节点所处的位置,产生一个散列值,需要考虑的因素有:</span><br><span class="line">    在当前StreamNode之前已经处理过的节点的个数,作为当前StreamNode的id,添加到hasher中</span><br><span class="line">    遍历当前StreamNode输出的每个StreamEdge,并判断当前StreamNode与这个StreamEdge的目标StreamNode是否可以进行连接,如果可以,则将目标StreamNode的id也放入hasher中,且这个目标StreamNode的id与当前StreamNode的id取相同的值</span><br><span class="line">    将上述步骤后产生的字节数据,与当前StreamNode的所有输入StreamNode对应的字节数据,进行相应的位操作,最终得到的字节数据,就是当前StreamNode对应的长度为16的字节数组</span><br><span class="line">    </span><br><span class="line"># 为什么不用StreamNode ID</span><br><span class="line">静态累加器,相同处理逻辑,可以产生不同的id组合</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="JobGraph到ExecutionGraph以及物理执行计划"><a href="#JobGraph到ExecutionGraph以及物理执行计划" class="headerlink" title="JobGraph到ExecutionGraph以及物理执行计划"></a>JobGraph到ExecutionGraph以及物理执行计划</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主要为ExecutionGraphBuilder的buildGraph方法</span><br><span class="line">关键流程</span><br><span class="line">    将JobGraph里面的JobVertex从source节点开始排序</span><br><span class="line">    executionGraph.attachJobGraph(sortedTopology)方法内部</span><br><span class="line">        根据JobVertex生成ExecutionJobVertex,在ExecutionJobVertex构造方法里面</span><br><span class="line">        根据JobVertex的IntermediateDataSet构建IntermediateResult</span><br><span class="line">        根据JobVertex并发构建ExecutionVertex</span><br><span class="line">        ExecutionVertex构建的时候,构建IntermediateResultPartition(每一个Execution构建IntermediateResult个数对应的IntermediateResultPartition)</span><br><span class="line">        将创建的ExecutionJobVertex与前置的IntermediateResult连接起来</span><br><span class="line">    构建ExecutionEdge,连接到前面的IntermediateResultPartition</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之十Zeppelin开发</title>
    <url>/2019/07/12/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%8D%81Connector%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Zeppelin介绍"><a href="#Zeppelin介绍" class="headerlink" title="Zeppelin介绍"></a>Zeppelin介绍</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是一个让交互式数据分析变得可行的基于网页的notebook</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之十一State实践</title>
    <url>/2019/07/12/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%8D%81%E4%B8%80State%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="State-Overview"><a href="#State-Overview" class="headerlink" title="State Overview"></a>State Overview</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">State:流式计算中持久化了的状态</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="OperatorState-VS-KeyedState"><a href="#OperatorState-VS-KeyedState" class="headerlink" title="OperatorState VS KeyedState"></a>OperatorState VS KeyedState</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">current key</span><br><span class="line">    OperatorState没有current key概念</span><br><span class="line">    KeyedState的数值总是与一个current key对应的</span><br><span class="line">heap</span><br><span class="line">    OperatorState只有堆内存一种实现</span><br><span class="line">    KeyedState有堆内存和RocksDB两种实现</span><br><span class="line">snapshot</span><br><span class="line">    OperatorState需要手动实现snapshot和restore方法</span><br><span class="line">    KeyedState由backend实现,对用户透明</span><br><span class="line">Size</span><br><span class="line">    OperatorState一般被认为是规模比较小的</span><br><span class="line">    KeyedState一般是相对规模较大的</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="StateBackend的选择"><a href="#StateBackend的选择" class="headerlink" title="StateBackend的选择"></a>StateBackend的选择</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">三种:</span><br><span class="line">    MemoryStateBackend(CK数据直接返回给Master节点)</span><br><span class="line">    FsStateBackend(CK数据写入文件中,将文件路径传递给Master)</span><br><span class="line">    RocksDBStateBackend(CK数据写入文件中,将文件路径传递给Master)</span><br><span class="line"></span><br><span class="line">OperatorStateBackend分类</span><br><span class="line">    DefaultOperatorStateBackend(数据存储在内存中)</span><br><span class="line"></span><br><span class="line">KeyedStateBackend分类</span><br><span class="line">    HeapKeyedStateBackend(数据存储在内存中)</span><br><span class="line">    RocksDBKeyedStateBackend((数据存储在RocksDB中)</span><br><span class="line">    </span><br><span class="line">选择FsStateBackend:</span><br><span class="line">    性能更好,日常存储是在堆内存中,面临OOM风险,不支持增量的CK</span><br><span class="line">选择RocksDBStateBackend:</span><br><span class="line">    无需担心OOM风险</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="RocksDB的state存储"><a href="#RocksDB的state存储" class="headerlink" title="RocksDB的state存储"></a>RocksDB的state存储</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RocksDB中,每个state使用一个Column Family</span><br><span class="line">每个column family使用独占writebuffer,整个DB共享一个block cache</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="RocksDB的相关参数"><a href="#RocksDB的相关参数" class="headerlink" title="RocksDB的相关参数"></a>RocksDB的相关参数</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink1.8开始支持ConfigurableOptionsFactory</span><br><span class="line">state.backend.rocksdb.block.blocksize数据块大小,默认4KB,增大会影响减少内存使用,但是会影响读性能</span><br><span class="line">state.backend.rocksdb.block.cache-size整个DB的block size大小,默认8MB,建议调大</span><br><span class="line">state.backend.rocksdb.compaction.level.use-dynamic-size如果使用LEVEL compaction,在SATA磁盘上,建议配置成true,默认false</span><br><span class="line">state.backend.rocksdb.files.open最大打开文件数目,-1意味着没有限制,默认值5000</span><br><span class="line">state.backend.rocksdb.thread.num后台flush和compaction的线程数,默认1,建议调大</span><br><span class="line">state.backend.rocksdb.writebuffer.count每个column family的writebuffer数目,默认值2,建议调大</span><br><span class="line">state.backend.rocksdb.writebuffer.number-to-merge写之前的writebuffer merge数目,默认值1,建议调大</span><br><span class="line">state.backend.rocksdb.writebuffer.size每个writebuffer的size,默认4MB,建议调大</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="OperatorState使用建议"><a href="#OperatorState使用建议" class="headerlink" title="OperatorState使用建议"></a>OperatorState使用建议</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">慎重使用长List</span><br><span class="line">正确使用UnionListState</span><br><span class="line">    restore后,每个subTask均恢复了与之前所有并发的state</span><br><span class="line">    目前Flink内部的使用都是为了获取之前的全局信息,在下一次snapshot时,仅使用其中一部分做snapshot</span><br><span class="line">    切勿在下一次snapshot时进行全局snapshot</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="KeyedState使用建议"><a href="#KeyedState使用建议" class="headerlink" title="KeyedState使用建议"></a>KeyedState使用建议</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如何清空当前state</span><br><span class="line">    state.clear只能清理当前key对应的value值</span><br><span class="line">    需要借助KeyedStateBackend的applyToAllKeys方法</span><br><span class="line">考虑value值很大的极限场景(RocksDB)</span><br><span class="line">    受限于JNI bridge API的限制,单个value只支持2^31bytes</span><br><span class="line">    考虑使用MapState来代替ListState或者ValueState</span><br><span class="line">如何知道当前RocksDB的使用情况</span><br><span class="line">    RocksDB的日志可以观察到一些compaction信息,默认存储位置在flink-io目录下,需要登录到TaskManager里面才能找到</span><br><span class="line">    考虑打开RocksDB的native metrics</span><br><span class="line">配置了StateTTL,可能存储空间并没有减少</span><br><span class="line">    默认情况下,只有在下次读访问时才会触发清理那条过期数据</span><br><span class="line">    如果那条数据之后不再访问,则也不会清理</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="RawState-timer-使用建议"><a href="#RawState-timer-使用建议" class="headerlink" title="RawState(timer)使用建议"></a>RawState(timer)使用建议</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TimerState太大怎么办</span><br><span class="line">    考虑存储到RocksDB中</span><br><span class="line">        state.backend.rocksdb.timer-service.factory: ROCKSDB</span><br><span class="line">    Trade off</span><br><span class="line">        存储到Heap中,面临OOM风险,CK的同步阶段耗时大</span><br><span class="line">        存储到RocksDB中,影响timer的读写性能</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="RocksDBState使用建议"><a href="#RocksDBState使用建议" class="headerlink" title="RocksDBState使用建议"></a>RocksDBState使用建议</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">不要创建过多的state</span><br><span class="line">    每个state一个column family,独占writebuffer,过多的state会导致占据过多的writebuffer</span><br><span class="line">    根本上还是RocksDBStateBackend的native内存无法直接管理</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="CK的使用建议"><a href="#CK的使用建议" class="headerlink" title="CK的使用建议"></a>CK的使用建议</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CK间隔不要太短</span><br><span class="line">    一般5min级别足够</span><br><span class="line">    CK与record处理共抢一把锁,CK的同步阶段会影响record的处理</span><br><span class="line">设置合理超时时间</span><br><span class="line">    默认的超时时间是10min,如果state规模大,则需要合理配置</span><br><span class="line">    最坏情况是创建速度大于删除速度,导致磁盘空间不可用</span><br><span class="line">FsStateBackend可以考虑文件压缩</span><br><span class="line">    对于刷出去的文件可以考虑使用压缩来减少CK体积</span><br><span class="line">    ExecutionConfig executionConfig &#x3D; new ExecutionConfig();</span><br><span class="line">    executionConfig.setUseSnapshotCompression(true);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之十三深度探索FlinkSQL</title>
    <url>/2019/08/03/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%8D%81%E4%B8%89%E6%B7%B1%E5%BA%A6%E6%8E%A2%E7%B4%A2FlinkSQL/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="TableEnvironments"><a href="#TableEnvironments" class="headerlink" title="TableEnvironments"></a>TableEnvironments</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两种planner</span><br><span class="line">    Blink planner</span><br><span class="line">        flink-table-runtime-blink</span><br><span class="line">        flink-table-planner-blink</span><br><span class="line">    Flink planner</span><br><span class="line">        BatchTableEnvironment</span><br><span class="line">            flink-table-planner(java&#x2F;scala)</span><br><span class="line"></span><br><span class="line">scala版本</span><br><span class="line">    StreamTableEnvironment</span><br><span class="line">        flink-table-api-scala-bridge</span><br><span class="line">            flink-table-api-scala</span><br><span class="line"></span><br><span class="line">java版本</span><br><span class="line">    StreamTableEnvironment</span><br><span class="line">        flink-table-api-java-bridge</span><br><span class="line">            TableEnvironment</span><br><span class="line">                flink-table-api-java</span><br><span class="line">                    flink-table-common</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.FlinkStream</span><br><span class="line">b.FlinkBatch</span><br><span class="line">c.BlinkStream</span><br><span class="line">d.BlinkBatch</span><br><span class="line">e.From&#x2F;ToDataStream</span><br><span class="line">f.From&#x2F;ToDataSet</span><br><span class="line">g.UDAF&#x2F;UDTF</span><br><span class="line"></span><br><span class="line">TableEnvironment</span><br><span class="line">    a,c,d</span><br><span class="line">(Java&#x2F;Scala)StreamTableEnvironment</span><br><span class="line">    a,c(不支持分段优化),e,g</span><br><span class="line">(Java&#x2F;Scala)BatchTableEnvironment</span><br><span class="line">    b,f,g</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Blink Batch</span></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();</span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line">tEnv...</span><br><span class="line">tEnv.execute(<span class="string">&quot;JobName&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Blink Stream</span></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();</span><br><span class="line">StreamExecutionEnvironment execEnv = ...</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(execEnv,settings);</span><br><span class="line">tEnv...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink Batch</span></span><br><span class="line">ExecutionEnvironment execEnv = ...</span><br><span class="line">BatchTableEnvironment tEnv = BatchTableEnvironment.create(execEnv);</span><br><span class="line">tEnv...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Flink Stream</span></span><br><span class="line">EnvironmentSettings settings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();</span><br><span class="line">TableEnvironment tEnv = TableEnvironment.create(settings);</span><br><span class="line">tEnv...</span><br><span class="line">tEnv.execute(<span class="string">&quot;JobName&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="New-Catalog"><a href="#New-Catalog" class="headerlink" title="New Catalog"></a>New Catalog</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ExternalCatalog已废弃,Blink planner不支持</span><br><span class="line"></span><br><span class="line">GenericInMemoryCatalog+HiveCatalog</span><br><span class="line">    Catalog</span><br><span class="line">        CatalogManager</span><br><span class="line">            TableEnvironment</span><br><span class="line">                SQLClient</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># in tableEnv#sqlUpdate</span><br><span class="line">CREATE TABLE</span><br><span class="line">CREATE VIEW</span><br><span class="line">DROP TABLE</span><br><span class="line">DROP VIEW</span><br><span class="line">创建表时的With属性,参考各个connector factory里的定义</span><br><span class="line">    CsvAppendTableSourceFactory</span><br><span class="line">    KafkaTableSourceSinkFactory</span><br><span class="line">    </span><br><span class="line"># in sqlClient</span><br><span class="line">CREATE VIEW</span><br><span class="line">DROP VIEW</span><br><span class="line">SHOW CATALOGS&#x2F;DATABASES&#x2F;TABLES&#x2F;FUNCTIONS</span><br><span class="line">USE CATALOG xxx</span><br><span class="line">SET xxx&#x3D;yyy</span><br><span class="line">DESCRIBE table_name</span><br><span class="line">EXPLAN SELECT xxx</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Blink-planner"><a href="#Blink-planner" class="headerlink" title="Blink planner"></a>Blink planner</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">改进</span><br><span class="line">    功能相关</span><br><span class="line">        更完整的SQL语法的支持(subquery,over,group sets等)</span><br><span class="line">        更丰富,高效的算子</span><br><span class="line">        更完善的cost模型,对接了catalog中的statistics</span><br><span class="line">        join reorder</span><br><span class="line">        shuffle service(only for batch)</span><br><span class="line">    性能相关</span><br><span class="line">        分段优化 &amp; sub-plan reuse</span><br><span class="line">        更丰富的优化rule</span><br><span class="line">        更高效的数据结构BinaryRow</span><br><span class="line">        mini-batch(only for stream)</span><br><span class="line">        省多余的shuffle &amp; sort(for batch now)</span><br><span class="line">        </span><br><span class="line">分段优化</span><br><span class="line">    Flink planner按每个Sink单独优化,各个Sink的计算链路相互独立,有重复计算</span><br><span class="line">    Blink planner先分段(能优化的最大公共子图),每段独立优化</span><br><span class="line">分段优化解决的是多Sink优化问题(DAG优化)</span><br><span class="line"></span><br><span class="line">sub-plan reuse</span><br><span class="line">    相关配置</span><br><span class="line">        table.optimizer.reuse-sub-plan-enabled,默认开启</span><br><span class="line">        table.optimizer.reuse-source-enabled,默认开启</span><br><span class="line">    在Batch模式下,sub-plan reuse可能造成死锁(hash-join,nested-loop-join先读build端再读probe端)</span><br><span class="line">    框架会将probe端数据落盘来解死锁</span><br><span class="line">    落盘会有额外开销,此时用户可根据情况来调整配置</span><br><span class="line">sub-plan reuse解决的是优化结果的子图复用问题</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="AGG分类"><a href="#AGG分类" class="headerlink" title="AGG分类"></a>AGG分类</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">group agg:   select count(a) from t group by b</span><br><span class="line">over agg:    select count(a) over (partition by b order by c) from t</span><br><span class="line">window agg:  select count(a) from t group by tumble(ts, interval &#39;10&#39; second), b</span><br><span class="line">table agg:   tEnv.scan(&quot;t&quot;).groupBy(&#39;a).flatAggregate(flatAggFunc(&#39;b as (&#39;c, &#39;d)))</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Group-AGG优化"><a href="#Group-AGG优化" class="headerlink" title="Group AGG优化"></a>Group AGG优化</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local&#x2F;global</span><br><span class="line">    减少网络shuffle数据</span><br><span class="line">    agg function可merge(sum(a) -&gt; local sum(a) + global sum(local result))</span><br><span class="line">distinct agg</span><br><span class="line">    改写为两层agg</span><br><span class="line">    Stream下是为了解决数据热点问题(state需要存所有input数据)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Local-Global-AGG"><a href="#Local-Global-AGG" class="headerlink" title="Local/Global AGG"></a>Local/Global AGG</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">必要条件</span><br><span class="line">    agg的所有agg function都是mergeable(实现了merge方法)</span><br><span class="line">    table.optimizer.agg-phase-strategy为AUTO或TWO_PHASE</span><br><span class="line">    Stream下,mini-batch开启</span><br><span class="line">    Batch下,AUTO会根据cost选择</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Distinct-AGG"><a href="#Distinct-AGG" class="headerlink" title="Distinct AGG"></a>Distinct AGG</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Batch下,强制改写</span><br><span class="line">    第一层求distinct值和非distinct agg function的值</span><br><span class="line">    第二层求distinct agg function的值</span><br><span class="line">    示例:</span><br><span class="line">        select color,count(distinct id),count(*)</span><br><span class="line">        from t</span><br><span class="line">        group by color</span><br><span class="line">        改写</span><br><span class="line">        select color,count(id),min(cnt)</span><br><span class="line">        from (</span><br><span class="line">            select color,id,count(*) filter (where $e &#x3D; 2) as cnt</span><br><span class="line">            from (</span><br><span class="line">                select color,id,1 as $e from t -- for distinct id</span><br><span class="line">                union all</span><br><span class="line">                select color,null as id, 2 as $e from t -- for count(*)</span><br><span class="line">            ) group by color,id,$e</span><br><span class="line">        ) group by color</span><br><span class="line">            </span><br><span class="line">    </span><br><span class="line">Stream下,必要条件</span><br><span class="line">    必须是支持的agg function</span><br><span class="line">        avg&#x2F;count&#x2F;min&#x2F;max&#x2F;sum&#x2F;first_value&#x2F;last_value&#x2F;concat_agg&#x2F;single_value</span><br><span class="line">    table.optimizer.distinct-agg.split.enabled开启(默认关闭)</span><br><span class="line">    示例:</span><br><span class="line">        select color,count(distinct id),count(*)</span><br><span class="line">        from t</span><br><span class="line">        group by color</span><br><span class="line">        改写</span><br><span class="line">        select color,sum(dcnt),sum(cnt)</span><br><span class="line">        from (</span><br><span class="line">            select color,count(distinct id) as dcnt,count(*) as cnt</span><br><span class="line">            from t</span><br><span class="line">            group by color,mod(hash_code(id),1024)</span><br><span class="line">        ) group by color</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之十二TensorFlow</title>
    <url>/2019/07/27/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%8D%81%E4%BA%8CTensorFlow/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之四FlinkOn(Yarn|K8s)原理</title>
    <url>/2019/05/29/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%9B%9BFlinkOn(Yarn-K8s)%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Flink架构概览"><a href="#Flink架构概览" class="headerlink" title="Flink架构概览"></a>Flink架构概览</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Job层面</span><br><span class="line">    DataStreamAPI,DataSetAPI,TableAPI,SQL编写Flink任务,生成StreamGraph和JobGraph</span><br><span class="line">    JobGraph由算子组成,提交给Flink后可以以Local,Standalone,Yarn,K8s四种模式运行</span><br><span class="line">    </span><br><span class="line">JobManager层面</span><br><span class="line">    将JobGraph转换成ExecutionGraph</span><br><span class="line">    Scheduler组件负责Task的调度</span><br><span class="line">    CK Coordinator组件负责协调整个任务的CK,包括CK的开始和完成</span><br><span class="line">    Actor System与TM进行通信</span><br><span class="line">    Recovery Metadata用于进行故障恢复时,可以从Metadata里面读取数据</span><br><span class="line"></span><br><span class="line">TaskManager层面</span><br><span class="line">    Memory &amp; I&#x2F;O Manager,内存I&#x2F;O的管理</span><br><span class="line">    Network Manager,用来对网络方面的管理</span><br><span class="line">    Actor System,负责网络通信</span><br><span class="line">    TaskManager被分成很多个TaskSlot,每个任务都要运行在一个TaskSlot里面,TaskSlot是调度资源里的最小单位</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在Standalone模式下,Master和TaskManager可以运行在同一台机器上,也可以运行在不同的机器上</span><br><span class="line"></span><br><span class="line">在Master进程中,Standalone ResourceManager的作用是对资源进行管理</span><br><span class="line">当用户通过Flink Cluster Client将JobGraph提交给Master时,JobGraph先经过Dispatcher</span><br><span class="line"></span><br><span class="line">当Dispatcher收到客户端的请求之后,生成一个JobManager</span><br><span class="line">接着JobManager进程向Standalone ResourceManager申请资源,最终再启动TaskManager</span><br><span class="line"></span><br><span class="line">TaskManager启动之后,会有一个注册的过程,注册之后JobManager再将具体的Task任务分发给这个TaskManager去执行</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Per-Job</span><br><span class="line">首先Client提交Yarn App,比如JobGraph或者JARs</span><br><span class="line"></span><br><span class="line">接下来Yarn的ResourceManager会申请第一个Container</span><br><span class="line">这个Container通过Application Master启动进程</span><br><span class="line">Application Master里面运行的是Flink程序</span><br><span class="line">即Flink-Yarn ResourceManager和JobManager</span><br><span class="line"></span><br><span class="line">最后Flink-Yarn ResourceManager向Yarn ResourceManager申请资源</span><br><span class="line">当分配到资源后,启动TaskManager</span><br><span class="line">TaskManager启动后向Flink-Yarn ResourceManager进行注册</span><br><span class="line">注册成功后JobManager就会分配具体的任务给TaskManager开始执行</span><br><span class="line"></span><br><span class="line"># Session</span><br><span class="line">在Per Job模式中,执行完任务后整个资源就会释放,包括JobManager,TaskManager都全部退出</span><br><span class="line">而Session模式则不一样,它的Dispatcher和ResourceManager是可以复用的</span><br><span class="line">Session模式下,当Dispatcher在收到请求之后,会启动JobManager(A),让JobManager(A)来完成启动TaskManager</span><br><span class="line">接着会启动JobManager(B)和对应的TaskManager的运行</span><br><span class="line">当A、B任务运行完成后,资源并不会释放</span><br><span class="line">Session模式也称为多线程模式,其特点是资源会一直存在不会释放</span><br><span class="line">多个JobManager共享一个Dispatcher,而且还共享Flink-YARN ResourceManager</span><br><span class="line"></span><br><span class="line"># 应用场景</span><br><span class="line">Session模式和Per Job模式的应用场景不一样</span><br><span class="line">Per Job模式比较适合那种对启动时间不敏感,运行时间较长的任务</span><br><span class="line">Seesion模式适合短时间运行的任务,一般是批处理任务</span><br><span class="line">若用Per Job模式去运行短时间的任务,那就需要频繁的申请资源</span><br><span class="line">运行结束后,还需要资源释放,下次还需再重新申请资源才能运行</span><br><span class="line">显然,这种任务会频繁启停的情况不适用于Per Job模式,更适合用Session模式</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="K8S"><a href="#K8S" class="headerlink" title="K8S"></a>K8S</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kubernetes 是 Google 开源的容器集群管理系统，其提供应用部署、维护、扩展机制等功能，利用 Kubernetes 能方便地管理跨机器运行容器化的应用。</span><br><span class="line">Kubernetes 和 Yarn 相比，相当于下一代的资源管理系统，但是它的能力远远不止这些。</span><br><span class="line"></span><br><span class="line"># Kubernetes–基本概念</span><br><span class="line">Kubernetes（k8s）中的 Master 节点，负责管理整个集群，含有一个集群的资源数据访问入口，还包含一个 Etcd 高可用键值存储服务。</span><br><span class="line"></span><br><span class="line">Master 中运行着 API Server，Controller Manager 及 Scheduler 服务。</span><br><span class="line"></span><br><span class="line">Node 为集群的一个操作单元，是 Pod 运行的宿主机。</span><br><span class="line">Node 节点里包含一个 agent 进程，能够维护和管理该 Node 上的所有容器的创建、启停等。</span><br><span class="line">Node 还含有一个服务端 kube-proxy，用于服务发现、反向代理和负载均衡。</span><br><span class="line">Node 底层含有 docker engine，docker 引擎主要负责本机容器的创建和管理工作。</span><br><span class="line"></span><br><span class="line">Pod 运行于 Node 节点上，是若干相关容器的组合。在 K8s 里面 Pod 是创建、调度和管理的最小单位。</span><br><span class="line"></span><br><span class="line"># Kubernetes–架构图</span><br><span class="line">Kubernetes 的架构如图所示，从这个图里面能看出 Kubernetes 的整个运行过程。</span><br><span class="line"></span><br><span class="line">API Server 相当于用户的一个请求入口，用户可以提交命令给 Etcd，这时会将这些请求存储到 Etcd 里面去。</span><br><span class="line"></span><br><span class="line">Etcd 是一个键值存储，负责将任务分配给具体的机器，在每个节点上的 Kubelet 会找到对应的 container 在本机上运行。</span><br><span class="line"></span><br><span class="line">用户可以提交一个 Replication Controller 资源描述，Replication Controller 会监视集群中的容器并保持数量；</span><br><span class="line">用户也可以提交 service 描述文件，并由 kube proxy 负责具体工作的流量转发。</span><br><span class="line"></span><br><span class="line"># Kubernetes–核心概念</span><br><span class="line">Kubernetes 中比较重要的概念有：</span><br><span class="line">    Replication Controller (RC) 用来管理 Pod 的副本。</span><br><span class="line">    RC 确保任何时候 Kubernetes 集群中有指定数量的 pod 副本(replicas) 在运行， 如果少于指定数量的 pod 副本，RC 会启动新的 Container，反之会杀死多余的以保证数量不变。</span><br><span class="line">    </span><br><span class="line">    Service 提供了一个统一的服务访问入口以及服务代理和发现机制</span><br><span class="line">    </span><br><span class="line">    Persistent Volume(PV) 和 Persistent Volume Claim(PVC) 用于数据的持久化存储。</span><br><span class="line">    </span><br><span class="line">    ConfigMap 是指存储用户程序的配置文件，其后端存储是基于 Etcd。</span><br><span class="line"></span><br><span class="line"># Flink on Kubernetes–架构</span><br><span class="line">Flink on Kubernetes 的架构如图所示，Flink 任务在 Kubernetes 上运行的步骤有：</span><br><span class="line"></span><br><span class="line">首先往 Kubernetes 集群提交了资源描述文件后，会启动 Master 和 Worker 的 container。</span><br><span class="line"></span><br><span class="line">Master Container 中会启动 Flink Master Process，包含 Flink-Container ResourceManager、JobManager 和 Program Runner。</span><br><span class="line"></span><br><span class="line">Worker Container 会启动 TaskManager，并向负责资源管理的 ResourceManager 进行注册，注册完成之后，由 JobManager 将具体的任务分给 Container，再由 Container 去执行。</span><br><span class="line"></span><br><span class="line">需要说明的是，在 Flink 里的 Master 和 Worker 都是一个镜像，只是脚本的命令不一样，通过参数来选择启动 master 还是启动 Worker。</span><br><span class="line"></span><br><span class="line"># Flink on Kubernetes–JobManager</span><br><span class="line">JobManager 的执行过程分为两步:</span><br><span class="line">    首先，JobManager 通过 Deployment 进行描述，保证 1 个副本的 Container 运行 JobManager，可以定义一个标签，例如 flink-jobmanager。</span><br><span class="line">    </span><br><span class="line">    其次，还需要定义一个 JobManager Service，通过 service name 和 port 暴露 JobManager 服务，通过标签选择对应的 pods。</span><br><span class="line"></span><br><span class="line"># Flink on Kubernetes–TaskManager</span><br><span class="line">TaskManager 也是通过 Deployment 来进行描述，保证 n 个副本的 Container 运行 TaskManager，同时也需要定义一个标签，例如 flink-taskmanager。</span><br><span class="line"></span><br><span class="line">对于 JobManager 和 TaskManager 运行过程中需要的一些配置文件，如：flink-conf.yaml、hdfs-site.xml、core-site.xml，可以通过将它们定义为 ConfigMap 来实现配置的传递和读取。</span><br><span class="line"></span><br><span class="line"># Flink on Kubernetes–交互</span><br><span class="line">整个交互的流程比较简单，用户往 Kubernetes 集群提交定义好的资源描述文件即可，例如 deployment、configmap、service 等描述。</span><br><span class="line">后续的事情就交给 Kubernetes 集群自动完成。Kubernetes 集群会按照定义好的描述来启动 pod，运行用户程序。</span><br><span class="line">各个组件的具体工作如下：</span><br><span class="line">    Service: 通过标签(label selector)找到 job manager 的 pod 暴露服务。</span><br><span class="line">    Deployment：保证 n 个副本的 container 运行 JM&#x2F;TM，应用升级策略。</span><br><span class="line">    ConfigMap：在每个 pod 上通过挂载 &#x2F;etc&#x2F;flink 目录，包含 flink-conf.yaml 内容。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之十四PyFlink</title>
    <url>/2019/08/09/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E5%8D%81%E5%9B%9BPyFlink/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="Python-Table-API架构"><a href="#Python-Table-API架构" class="headerlink" title="Python Table API架构"></a>Python Table API架构</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Python VM</span><br><span class="line">    Python Table API</span><br><span class="line">    Python Gateway</span><br><span class="line">        Table TableEnv</span><br><span class="line">Py4J</span><br><span class="line">Socket</span><br><span class="line">JVM</span><br><span class="line">    CliFrontend</span><br><span class="line">    Java GatewayServer</span><br><span class="line">        Table TableEnv</span><br><span class="line">Flink Cluster</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Job开发"><a href="#Job开发" class="headerlink" title="Job开发"></a>Job开发</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建执行环境</span></span><br><span class="line">exec_env = ExecutionEnvironment.get_execution_environment()</span><br><span class="line"><span class="comment">#创建配置对象(IdleState TTL,NULL check,timezone等)</span></span><br><span class="line">t_config = TableConfig()</span><br><span class="line"><span class="comment">#创建一个TableEnv</span></span><br><span class="line">t_env = BatchTableEnvironment.create(exec_env,t_config)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建数据源</span></span><br><span class="line">t_env.connect(FileSystem().path(source_file)) \</span><br><span class="line">    <span class="comment">#Csv/Json/Avro</span></span><br><span class="line">    .with_format(OldCsv()</span><br><span class="line">        .line_delimiter(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        .field(<span class="string">&#x27;word&#x27;</span>,DataTypes.STRING())) \</span><br><span class="line">    <span class="comment">#定义字段名和类型</span></span><br><span class="line">    .with_schema(Schema()</span><br><span class="line">        .field(<span class="string">&#x27;word&#x27;</span>,DataTypes.STRING())) \</span><br><span class="line">    <span class="comment">#注册Source</span></span><br><span class="line">    .register_table_source(<span class="string">&#x27;mySource&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#创建结果表</span></span><br><span class="line">t_env.connect(FileSystem().path(sink_file)) \</span><br><span class="line">    <span class="comment">#Csv/Json/Avro</span></span><br><span class="line">    .with_format(OldCsv()</span><br><span class="line">        .line_delimiter(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        .field(<span class="string">&#x27;word&#x27;</span>,DataTypes.STRING())</span><br><span class="line">        .field(<span class="string">&#x27;count&#x27;</span>,DataTypes.BIGINT())) \</span><br><span class="line">    <span class="comment">#定义字段名和类型</span></span><br><span class="line">    .with_schema(Schema()</span><br><span class="line">        .field(<span class="string">&#x27;word&#x27;</span>,DataTypes.STRING())</span><br><span class="line">        .field(<span class="string">&#x27;count&#x27;</span>,DataTypes.BIGINT())) \</span><br><span class="line">    <span class="comment">#注册Sink</span></span><br><span class="line">    .register_table_sink(<span class="string">&#x27;mySink&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#编辑业务逻辑和执行</span></span><br><span class="line"><span class="comment">#word_count计算逻辑</span></span><br><span class="line"><span class="comment">#读取数据源</span></span><br><span class="line">t_env.scan(<span class="string">&#x27;mySource&#x27;</span>) \</span><br><span class="line">    <span class="comment">#按word进行分组</span></span><br><span class="line">    .group_by(<span class="string">&#x27;word&#x27;</span>) \</span><br><span class="line">    <span class="comment">#进行count计数统计</span></span><br><span class="line">    .select(<span class="string">&#x27;word,count(1)&#x27;</span>) \</span><br><span class="line">    <span class="comment">#将计算结果插入到结果表</span></span><br><span class="line">    .insert_into(<span class="string">&#x27;mySink&#x27;</span>)</span><br><span class="line"><span class="comment">#执行Job</span></span><br><span class="line">t_env.execute(<span class="string">&quot;wc&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">依赖</span><br><span class="line">    JDK1.8+</span><br><span class="line">    Maven3.x+</span><br><span class="line">    Scala2.11+</span><br><span class="line">    Python2.7+</span><br><span class="line">    Git2.20+</span><br><span class="line">    Pip19.1+</span><br><span class="line">构建Java二进制发布包</span><br><span class="line">    下载源代码</span><br><span class="line">    git clone https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;flink.git</span><br><span class="line">    拉取1.9分支</span><br><span class="line">    cd flink</span><br><span class="line">    git fetch origin release-1.9</span><br><span class="line">    git checkout -b release-1.9 origin&#x2F;release-1.9</span><br><span class="line">    构建二进制发布包</span><br><span class="line">    mvn clean install -DskipTests -Dfast</span><br><span class="line">构建Python发布包</span><br><span class="line">    cd flink-python</span><br><span class="line">    python setup.py sdist</span><br><span class="line">    在dist目录的apache-flink-1.9.dev0.tar.gz就是可以用于pip install的pyFlink包</span><br><span class="line">安装PyFlink</span><br><span class="line">    pip install dist&#x2F;*.tar.gz</span><br><span class="line">    pip list</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CLI方式</span><br><span class="line">    .&#x2F;bin&#x2F;flink run -py wc.py</span><br><span class="line">    -py指定python文件</span><br><span class="line">    -pym指定python的module</span><br><span class="line">    -pyfs指定python依赖的资源文件</span><br><span class="line">    -j指定依赖的jar包</span><br><span class="line">    </span><br><span class="line">Python-Shell</span><br><span class="line">    Local</span><br><span class="line">        bin&#x2F;pyflink-shell.sh local(会启动一个mini Cluster)</span><br><span class="line">    Remote</span><br><span class="line">        bin&#x2F;pyflink-shell.sh remote 127.0.0.1 4000(需要一个已经存在的Cluster)</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="PythonTableAPI算子"><a href="#PythonTableAPI算子" class="headerlink" title="PythonTableAPI算子"></a>PythonTableAPI算子</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PythonAPI(1.9) &#x3D;&#x3D; JavaTableAPI</span><br><span class="line">select</span><br><span class="line">alias</span><br><span class="line">filter</span><br><span class="line">where</span><br><span class="line">group_by -&gt; GroupedTable -&gt; select -&gt; Table</span><br><span class="line">distinct</span><br><span class="line">join(inner,left,right,full)</span><br><span class="line">minus</span><br><span class="line">minus_all</span><br><span class="line">union</span><br><span class="line">union_all</span><br><span class="line">over_window -&gt; OverWindowedTable -&gt; select -&gt; Table</span><br><span class="line">window -&gt; GroupWindowedTable -&gt; groupBy -&gt; WindowGroupedTable -&gt; select -&gt; Table</span><br><span class="line">add_columns</span><br><span class="line">drop_columns</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Win10系统专业版激活</title>
    <url>/2019/11/29/Win10%E7%B3%BB%E7%BB%9F%E4%B8%93%E4%B8%9A%E7%89%88%E6%BF%80%E6%B4%BB%E4%BB%A5%E5%8F%8A%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<blockquote>
<p>激活专业版Win10以达到使用远程连接功能</p>
</blockquote>
<span id="more"></span>

<h2 id="激活Win10"><a href="#激活Win10" class="headerlink" title="激活Win10"></a>激活Win10</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 有效的kms服务器</span><br><span class="line">zh.us.to 有效</span><br><span class="line">kms.03k.org 有效</span><br><span class="line">kms.chinancce.com 有效</span><br><span class="line">kms.shuax.com 有效</span><br><span class="line">kms.dwhd.org 有效</span><br><span class="line">kms.luody.info 有效</span><br><span class="line">kms.digiboy.ir 有效</span><br><span class="line">kms.lotro.cc 有效</span><br><span class="line">ss.yechiu.xin 有效</span><br><span class="line">www.zgbs.cc 有效</span><br><span class="line">cy2617.jios.org 有效</span><br><span class="line"></span><br><span class="line"># 右键开始图标,选择[windows powershell(管理员)]</span><br><span class="line"></span><br><span class="line"># 安装秘钥</span><br><span class="line">slmgr &#x2F;ipk W269N-WFGWX-YVC9B-4J6C9-T83GX</span><br><span class="line"></span><br><span class="line"># 设置kms服务器</span><br><span class="line">slmgr &#x2F;skms zh.us.to</span><br><span class="line"></span><br><span class="line"># 激活系统</span><br><span class="line">slmgr &#x2F;ato</span><br><span class="line"></span><br><span class="line"># 查看激活状态</span><br><span class="line">slmgr &#x2F;xpr</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>Ververica&amp;Flink进阶之七网络流控及反压(精)</title>
    <url>/2019/06/14/Ververica&amp;Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E4%B8%83%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%8F%8A%E5%8F%8D%E5%8E%8B(%E7%B2%BE)/</url>
    <content><![CDATA[<blockquote>
<p>B站Flink教程视频观看</p>
</blockquote>
<span id="more"></span>

<h1 id="网络流控"><a href="#网络流控" class="headerlink" title="网络流控"></a>网络流控</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">当生产数据的速率远高于消费数据的速率</span><br><span class="line">    消费端丢弃新到达的数据</span><br><span class="line">    消费端的接收buffer持续扩张,最终耗尽消费端内存</span><br><span class="line"></span><br><span class="line">静态限速</span><br><span class="line">    限制住生产端的速率与消费端保持一致</span><br><span class="line">    通常无法事先预估消费端能承受的最大速率</span><br><span class="line">    消费端承受能力通常会动态的波动</span><br><span class="line"></span><br><span class="line">动态反馈&#x2F;自动反压</span><br><span class="line">    负反馈</span><br><span class="line">        接收速率小于发送速率时发生</span><br><span class="line">    正反馈</span><br><span class="line">        发送速率小于接收速率时发生</span><br><span class="line">        </span><br><span class="line">Storm和SparkStreaming都有反压机制</span><br><span class="line">Flink1.5之前没有反压机制,为什么?</span><br><span class="line">    TCP天然具备feedback流控机制,Flink基于它来实现反压</span><br><span class="line"></span><br><span class="line">TCP流控:滑动窗口方式</span><br><span class="line">    发送端初始3packets每秒</span><br><span class="line">    消费端1packets每秒,window固定为5</span><br><span class="line">    第一次</span><br><span class="line">        P:[123]456789</span><br><span class="line">        C:[12345]6789    #接收到123,窗口还剩2个</span><br><span class="line">    第二次</span><br><span class="line">        P:123[456]789</span><br><span class="line">        C:1[23456]789    #消费了1,窗口还剩3个,刚好接收456</span><br><span class="line">    第三次</span><br><span class="line">        P:123456[7]89</span><br><span class="line">        C:12[34567]89    #消费了2,窗口还剩1个,限定发送端速率降为1</span><br><span class="line">    第四次</span><br><span class="line">        P:1234567[]89    #定期发送zeroWindowProbe探测消息</span><br><span class="line">        C:12[34567]89    #消费端出现问题,速率降为0了,发送端也会降为0</span><br><span class="line"></span><br><span class="line">对应在Flink中</span><br><span class="line">    Buffer被分为两种</span><br><span class="line">        InputGate(InputChannel)    接收Buffer</span><br><span class="line">        ResultPartition(ResultSubPartition)    发送Buffer</span><br><span class="line">    跨TaskManager,反压如何从IC传播到RS</span><br><span class="line">    TaskManager内,反压如何从RS传播到IC</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="跨TaskManager反压过程TCP"><a href="#跨TaskManager反压过程TCP" class="headerlink" title="跨TaskManager反压过程TCP"></a>跨TaskManager反压过程TCP</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">消费端ConsumerOperator在消费时速度匹配不上了</span><br><span class="line">RecordReader读取数据过慢会导致InputChannel被写满</span><br><span class="line">IC去LocalBufferPool申请位置,直到LBP写满</span><br><span class="line">LBP写满会去向NetworkBufferPool申请位置</span><br><span class="line">当IC,LBP,NBP都满了之后,Netty的AutoRead会被设置成disable</span><br><span class="line">Netty将不再向Socket读取数据</span><br><span class="line">当Socket也满了,将ACK Window &#x3D; 0发送给发送端的Socket</span><br><span class="line"></span><br><span class="line">发送端Socket接受到ACK Window &#x3D; 0就会停止向消费端Socket发送数据</span><br><span class="line">Socket慢慢也会变满,Netty就会停止写入Socket</span><br><span class="line">Netty是无界的,有一个Watermark机制,当Netty内部数据超过Watermark</span><br><span class="line">Netty的isWritable会返回false</span><br><span class="line">RS就无法向Netty写入数据</span><br><span class="line">RS写满后,向LBP申请</span><br><span class="line">LBP满了之后,向NBP申请位置</span><br><span class="line">NBP也满了,RecordWriter就会等待空闲</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="TaskManager内反压过程TCP"><a href="#TaskManager内反压过程TCP" class="headerlink" title="TaskManager内反压过程TCP"></a>TaskManager内反压过程TCP</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">假设RecordWriter被堵住了</span><br><span class="line">那么RecordReader就不会向IC中读取数据</span><br><span class="line">IC被打满数据,向LBP申请</span><br><span class="line">LBP满了向NBP申请</span><br><span class="line">NBP也满了,NettyAutoRead会被设置成disable</span><br><span class="line">Netty将不再向Socket读取数据</span><br><span class="line">当Socket也满了,将ACK Window &#x3D; 0</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="TCP反压的弊端"><a href="#TCP反压的弊端" class="headerlink" title="TCP反压的弊端"></a>TCP反压的弊端</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">单个Task导致的反压,会阻断整个TM-TM的Socket</span><br><span class="line">连CK Barrier也无法发出</span><br><span class="line">反压传播路径太长,导致生效延迟比较大</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Credit反压"><a href="#Credit反压" class="headerlink" title="Credit反压"></a>Credit反压</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在Flink层面实现类似TCP流控的反压机制</span><br><span class="line">Credit可类比TCP Window</span><br><span class="line">ResultSubPartition发送Buffers的时候会附带一个BacklogSize积压数据大小</span><br><span class="line">InputChannel会像ACK一样返回一个Credit</span><br><span class="line">这个时候RS收到这么一个Credit之后才会发送对应Credit的数据</span><br><span class="line"></span><br><span class="line">当Credit为0,RS就不会向Netty发送任何数据了</span><br><span class="line">但是一样会有探测机制</span><br><span class="line"></span><br><span class="line">可以对比TCP机制是类似的,但是比直接使用TCP反压要好</span><br><span class="line">Netty和Socket不需要等待变满</span><br><span class="line">Socket永远不会变满,TCP通信不会发生不了,CK机制不会被堵塞</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">网络流控是为了在上下游速度不匹配的情况下,如何防止下游出现过载</span><br><span class="line">网络流控有静态限速和动态反压两种手段</span><br><span class="line">Flink1.5以前基于TCP流程控+BoundedBuffer来实现反压</span><br><span class="line">Flink1.5之后实现自己托管的Credit流控机制,在应用层模拟TCP流控的机制</span><br></pre></td></tr></table></figure>

<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">外部数据存储到Sink的反压源头是否会触发?</span><br><span class="line">    反压不一定会触发,得看Storage是否有限流机制,能不能很好的触发反压</span><br><span class="line">静态限流的必要性</span><br><span class="line">    向上一种情况,就可以通过静态限流来解决反压问题</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>XShell5和XFtp5的永久使用</title>
    <url>/2018/08/20/XShell5%E5%92%8CXFtp5%E7%9A%84%E6%B0%B8%E4%B9%85%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>如何永久使用XShell和XFtp</p>
</blockquote>
<span id="more"></span>

<h2 id="改二进制文件"><a href="#改二进制文件" class="headerlink" title="改二进制文件"></a>改二进制文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改安装目录下的配置文件nslicense.dll</span><br><span class="line"># 原始</span><br><span class="line">7F0C81F98033E1010F8680</span><br><span class="line"># 修改</span><br><span class="line">7F0C81F98033E1010F8380</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="二进制编辑器"><a href="#二进制编辑器" class="headerlink" title="二进制编辑器"></a>二进制编辑器</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 可以使用NotePad++,需要去官网下载插件</span><br><span class="line">Hex-Edit</span><br><span class="line"># 设置中文</span><br><span class="line">Settings-&gt;Preferences-&gt;General-&gt;Localization</span><br><span class="line"># 导入插件</span><br><span class="line">设置-&gt;导入-&gt;导入插件</span><br><span class="line"># 使用插件</span><br><span class="line">插件-&gt;Hex-Edit</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>XWiki的使用与修改</title>
    <url>/2019/06/28/XWiki%E7%9A%84%E4%BD%BF%E7%94%A8%E4%B8%8E%E4%BF%AE%E6%94%B9/</url>
    <content><![CDATA[<blockquote>
<p>Xwiki教程</p>
</blockquote>
<span id="more"></span>

<h2 id="安装搭建"><a href="#安装搭建" class="headerlink" title="安装搭建"></a>安装搭建</h2><blockquote>
<ul>
<li>下载官方最新版本…war</li>
<li>部署到Tomcat</li>
<li>启动 localhost:8080/xwiki</li>
</ul>
</blockquote>
<hr>
<h2 id="启动管理员"><a href="#启动管理员" class="headerlink" title="启动管理员"></a>启动管理员</h2><blockquote>
<p>修改Tomcat下的webapps\wiki\WEB-INF\xwiki.cfg</p>
</blockquote>
<blockquote>
<p><strong>xwiki.superadminpassword=system</strong></p>
</blockquote>
<hr>
<h2 id="整理样式"><a href="#整理样式" class="headerlink" title="整理样式"></a>整理样式</h2><blockquote>
<ul>
<li>可以下载官方一体包,然后将data/extension目录下的文件复制到Tomcat下的work\Catalina\localhost\xwiki</li>
<li>将上述目录下的xar文件复制到Tomcat下的webapps\xwiki\WEB-INF\extensions</li>
<li>使用import功能导入xar文件</li>
</ul>
</blockquote>
<hr>
<h2 id="全面汉化"><a href="#全面汉化" class="headerlink" title="全面汉化"></a>全面汉化</h2><h3 id="基础汉化"><a href="#基础汉化" class="headerlink" title="基础汉化"></a>基础汉化</h3><blockquote>
<ul>
<li>进入Tomcat下的webapps\wiki\WEB-INF\lib目录</li>
<li>找到xwiki-platform-legacy-oldcore-x.x.x.jar文件</li>
<li>将ApplicationResources_zh.properties文件复制出来</li>
<li>使用<strong>native2ascii -reverse ApplicationResources_zh.properties ApplicationResources_zh1.properties</strong>命令将unicode编码文件转成本地编码文件</li>
<li>对文件内容进行汉化</li>
<li>使用<strong>native2ascii ApplicationResources_zh1.properties ApplicationResources_zh.properties</strong>命令将本地文件转换成unicode编码文件</li>
<li>替换jar包中的文件</li>
</ul>
</blockquote>
<h3 id="xar汉化"><a href="#xar汉化" class="headerlink" title="xar汉化"></a>xar汉化</h3><blockquote>
<ul>
<li>进入Tomcat下的work\Catalina\localhost\xwiki目录</li>
<li>修改xar文件中的xml文件</li>
<li>将文件内容汉化</li>
<li>替换xar包中的文件</li>
<li>重新导入xar文件</li>
</ul>
</blockquote>
<hr>
<h2 id="修改SQL"><a href="#修改SQL" class="headerlink" title="修改SQL"></a>修改SQL</h2><blockquote>
<ul>
<li>进入Tomcat下的webapps\wiki\WEB-INF\lib目录</li>
<li>修改hbm.xml类似文件</li>
<li>queries.hbm.xml可以控制查询的排序</li>
<li>xwiki.hbm.xml是数据库表的映射</li>
<li>修改xwiki.hbm.xml和xwiki-platform-legacy-oldcore-x.x.x.jar中的class文件可以添加表映射</li>
</ul>
</blockquote>
<hr>
<h2 id="修改ckeditor"><a href="#修改ckeditor" class="headerlink" title="修改ckeditor"></a>修改ckeditor</h2><blockquote>
<ul>
<li>进入Tomcat下的work\Catalina\localhost\wiki\extension\repository目录</li>
<li>找到org%2Exwiki%2Econtrib%3Aapplication-ckeditor-webjar.jar文件</li>
<li>修改META-INF\resources\webjars\application-ckeditor-webjar\1.33\plugins\xwiki-resource目录下的resourcePicker.bundle.min.js文件</li>
<li>注释掉<strong>c.prop(“disabled”,g);g&amp;&amp;c.attr(“disabled”,”disabled”);</strong></li>
<li>可以添加新的点击事件</li>
</ul>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 新点击事件</span><br><span class="line">m &#x3D; function (h) &#123;</span><br><span class="line">    h &#x3D; a(this);</span><br><span class="line">    var b &#x3D; h.closest(&quot;.resourcePicker&quot;), c &#x3D; k(b.prev(&quot;input&quot;));</span><br><span class="line">    c.type !&#x3D;&#x3D; h.val() &amp;&amp; (c &#x3D; &#123;type: h.val(), reference: b.find(&quot;input.resourceReference&quot;).val()&#125;);</span><br><span class="line">    if(c.type &#x3D;&#x3D; &#39;url&#39;)&#123;</span><br><span class="line">        window.open(&quot;http:&#x2F;&#x2F;localhost:1114&#x2F;re&quot;,window,&quot;height&#x3D;400,width&#x3D;400&quot;);</span><br><span class="line">    &#125;else if(c.type &#x3D;&#x3D; &#39;mailto&#39;)&#123;</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">        e.pickers[c.type](c).done(a.proxy(l, b))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># http:&#x2F;&#x2F;localhost:1114&#x2F;re对应页面</span><br><span class="line">&lt;%@ page contentType&#x3D;&quot;text&#x2F;html;charset&#x3D;UTF-8&quot; language&#x3D;&quot;java&quot; %&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;title&gt;Title&lt;&#x2F;title&gt;</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">    function re() &#123;</span><br><span class="line">        window.opener.postMessage(document.getElementById(&quot;url&quot;).value,&quot;*&quot;);</span><br><span class="line">        window.close();</span><br><span class="line">    &#125;</span><br><span class="line">    function test1() &#123;</span><br><span class="line">        alert(document.getElementById(&quot;url&quot;).value)</span><br><span class="line">    &#125;</span><br><span class="line">&lt;&#x2F;script&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;input type&#x3D;&quot;button&quot; value&#x3D;&quot;返回值&quot; onclick&#x3D;&quot;re()&quot;&gt;</span><br><span class="line">&lt;br&gt;</span><br><span class="line">&lt;input type&#x3D;&quot;button&quot; value&#x3D;&quot;显示返回值&quot; onclick&#x3D;&quot;test1()&quot;&gt;</span><br><span class="line">&lt;br&gt;</span><br><span class="line">&lt;div id&#x3D;&quot;root&quot;&gt;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br><span class="line">&lt;script src&#x3D;&quot;..&#x2F;demo&#x2F;js&#x2F;jquery-2.2.4.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&lt;script src&#x3D;&quot;..&#x2F;demo&#x2F;js&#x2F;bootstrap-3.3.7.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&lt;script src&#x3D;&quot;..&#x2F;window&#x2F;demo.js&quot;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br><span class="line"></span><br><span class="line"># demo.js</span><br><span class="line">$(&quot;#root&quot;).html(&quot;&lt;select name&#x3D;&#39;url&#39; id&#x3D;&#39;url&#39; style&#x3D;&#39;margin-right: 2.2em;&#39;&gt;&lt;&#x2F;select&gt;&quot;)</span><br><span class="line">FillUnit();</span><br><span class="line">function FillUnit() &#123;</span><br><span class="line">    var url &#x3D; &quot;&#x2F;mysql&#x2F;selectUrlData&quot;;</span><br><span class="line">    $.ajax(&#123;</span><br><span class="line">        async: false,</span><br><span class="line">        type: &quot;get&quot;,</span><br><span class="line">        url: url,</span><br><span class="line">        dataType: &quot;json&quot;,</span><br><span class="line">        success: function (data) &#123;</span><br><span class="line">            var str &#x3D; &quot;&lt;option value&#x3D;&#39;&#39;&gt;---&lt;&#x2F;option&gt;&quot;;</span><br><span class="line">            for (var i &#x3D; 0; i &lt; data.length; i++) &#123;</span><br><span class="line">                str +&#x3D; &quot;&lt;option value&#x3D;&#39;&quot; + data[i].id + &quot;&#39;&gt;&quot; + data[i].name + &quot;&lt;&#x2F;option&gt;&quot;</span><br><span class="line">            &#125;</span><br><span class="line">            $(&quot;#url&quot;).html(str)</span><br><span class="line">        &#125;,</span><br><span class="line">        error: function () &#123;</span><br><span class="line">            alert(&#39;ERROR&#39;)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># &#x2F;mysql&#x2F;selectUrlData接口实现</span><br><span class="line">@RequestMapping(&quot;&#x2F;selectUrlData&quot;)</span><br><span class="line">public String selectUrlData() &#123;</span><br><span class="line">    DBContextHolder.setDbType(&quot;secondary&quot;);</span><br><span class="line">    String sql &#x3D; &quot;SELECT id, file_name &#96;name&#96; FROM url_sample;&quot;;</span><br><span class="line">    List&lt;JSONObject&gt; list &#x3D; jdbcTemplate.query(sql, new RowMapper&lt;JSONObject&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public JSONObject mapRow(ResultSet resultSet, int i) throws SQLException &#123;</span><br><span class="line">            JSONObject json &#x3D; new JSONObject();</span><br><span class="line">            json.put(&quot;id&quot;, resultSet.getString(&quot;id&quot;));</span><br><span class="line">            json.put(&quot;name&quot;, resultSet.getString(&quot;name&quot;));</span><br><span class="line">            return json;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    JSONArray array &#x3D; new JSONArray();</span><br><span class="line">    for (JSONObject json : list) &#123;</span><br><span class="line">        array.put(json);</span><br><span class="line">    &#125;</span><br><span class="line">    return array.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="增加目录树排序功能"><a href="#增加目录树排序功能" class="headerlink" title="增加目录树排序功能"></a>增加目录树排序功能</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># changeSort接口实现</span><br><span class="line">@RequestMapping(&quot;&#x2F;changeSort&quot;)</span><br><span class="line">public void changeSort(String value) &#123;</span><br><span class="line">    String[] arr &#x3D; value.split(&quot;\\|&quot;);</span><br><span class="line">    ArrayList&lt;String&gt; list &#x3D; new ArrayList();</span><br><span class="line">    System.out.println(value);</span><br><span class="line">    for (String v : arr) &#123;</span><br><span class="line">        String s &#x3D; v.split(&quot;:&quot;)[2];</span><br><span class="line">        System.out.println(s.substring(0,s.lastIndexOf(&quot;.&quot;)));</span><br><span class="line">        list.add(s.substring(0,s.lastIndexOf(&quot;.&quot;)));</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 将数据库中的sort字段修改</span><br><span class="line">    DBContextHolder.setDbType(&quot;primary&quot;);</span><br><span class="line">    for (int i &#x3D; 0; i &lt; list.size(); i++) &#123;</span><br><span class="line">        String sql &#x3D; &quot;UPDATE xwikispace SET XWS_RANK &#x3D; &#39;&quot; + (list.size() - i) + &quot;&#39; WHERE XWS_REFERENCE &#x3D; &#39;&quot; + list.get(i) + &quot;&#39;;&quot;;</span><br><span class="line">        jdbcTemplate.execute(sql);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 在wiki&#x2F;resources&#x2F;js&#x2F;custom目录下实现,并在changesort方法之上重写jquery-2.2.4.js,将jQuery改为kQuery</span><br><span class="line">function changesort()&#123;</span><br><span class="line">	var list &#x3D; new Array()</span><br><span class="line">	kQuery.each($(&quot;#sorttree li&quot;), function(k,v)&#123;</span><br><span class="line">		list.push(v.getAttribute(&#39;id&#39;))</span><br><span class="line">	&#125;)</span><br><span class="line">	var data &#x3D; &quot;&quot;</span><br><span class="line">	for(var i&#x3D;0;i&lt;list.size();i++)&#123;</span><br><span class="line">		if(i !&#x3D; list.size()-1)&#123;</span><br><span class="line">			data &#x3D; data + list[i] + &quot;|&quot;</span><br><span class="line">		&#125;else&#123;</span><br><span class="line">			data &#x3D; data + list[i]</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	var url &#x3D; &quot;http:&#x2F;&#x2F;localhost:1114&#x2F;mysql&#x2F;changeSort&quot;;</span><br><span class="line">    kQuery.ajax(&#123;</span><br><span class="line">        async: false,</span><br><span class="line">        type: &quot;get&quot;,</span><br><span class="line">        url: url,</span><br><span class="line">        data: &#123;value: data&#125;,</span><br><span class="line">        dataType: &quot;jsonp&quot;,</span><br><span class="line">        success: function (data) &#123;</span><br><span class="line">        &#125;,</span><br><span class="line">        error: function () &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 修改xar文件</span><br><span class="line">xwiki-platform-panels-ui的NavigationConfigurationSection.xml</span><br><span class="line">&#123;&#123;html clean&#x3D;false&#125;&#125;</span><br><span class="line">  &lt;fieldset&gt;</span><br><span class="line">    &lt;input type&#x3D;&quot;hidden&quot; name&#x3D;&quot;form_token&quot; value&#x3D;&quot;$!services.csrf.token&quot; &#x2F;&gt;</span><br><span class="line">    &lt;input type&#x3D;&quot;hidden&quot; name&#x3D;&quot;comment&quot; value&#x3D;&quot;$escapetool.xml($services.localization.render(</span><br><span class="line">      &#39;platform.panels.navigation.configuration.saveComment&#39;))&quot; &#x2F;&gt;</span><br><span class="line">  &lt;&#x2F;fieldset&gt;</span><br><span class="line">  &lt;div class&#x3D;&quot;bottombuttons&quot;&gt;</span><br><span class="line">    &lt;p class&#x3D;&quot;admin-buttons&quot;&gt;</span><br><span class="line">      &lt;span class&#x3D;&quot;buttonwrapper&quot;&gt;</span><br><span class="line">        &lt;input class&#x3D;&quot;button&quot; type&#x3D;&quot;submit&quot; name&#x3D;&quot;action_saveandcontinue&quot;</span><br><span class="line">          value&#x3D;&quot;$services.localization.render(&#39;admin.save&#39;)&quot; &#x2F;&gt;</span><br><span class="line">		  &lt;input class&#x3D;&quot;button&quot; type&#x3D;&quot;button&quot; name&#x3D;&quot;提交修改&quot; onclick&#x3D;&quot;changesort()&quot;</span><br><span class="line">          value&#x3D;&quot;提交修改&quot; &#x2F;&gt;</span><br><span class="line">      &lt;&#x2F;span&gt;</span><br><span class="line">    &lt;&#x2F;p&gt;</span><br><span class="line">  &lt;&#x2F;div&gt;</span><br><span class="line">&lt;&#x2F;form&gt;</span><br><span class="line">&lt;script type&#x3D;&#39;text&#x2F;javascript&#39; src&#x3D;&#39;&#x2F;wiki&#x2F;resources&#x2F;js&#x2F;custom&#x2F;demo.js&#39;&gt;&lt;&#x2F;script&gt;</span><br><span class="line">&#123;&#123;&#x2F;html&#125;&#125;</span><br><span class="line"></span><br><span class="line"># 重新导入xar文件</span><br><span class="line"></span><br><span class="line"># 修改xwiki-platform-index-tree-api下的hbm.xml文件</span><br><span class="line"># 根据XWS_RANK进行排序</span><br></pre></td></tr></table></figure>

<h2 id="XWiki整合Cas"><a href="#XWiki整合Cas" class="headerlink" title="XWiki整合Cas"></a>XWiki整合Cas</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 准备jar</span><br><span class="line">cas-client-core-3.3.3.jar</span><br><span class="line">xmlsec-1.3.0.jar</span><br><span class="line">xwiki-cas.jar</span><br><span class="line"># 拷贝进wiki&#x2F;WEB_INF&#x2F;lib下</span><br><span class="line"># 修改xwiki.conf</span><br><span class="line">xwiki.authentication.authclass&#x3D;org.xwiki.contrib.authentication.cas.XWikiCASAuthenticator</span><br><span class="line">xwiki.authentication.cas.server&#x3D;http:&#x2F;&#x2F;localhost:8443&#x2F;cas</span><br><span class="line">xwiki.authentication.cas.protocol&#x3D;CAS20</span><br><span class="line">xwiki.authentication.cas.create_user&#x3D;1</span><br><span class="line">xwiki.authentication.cas.update_user&#x3D;1</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>xwiki</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper的领导者选举和原子广播</title>
    <url>/2017/08/28/Zookeeper%E7%9A%84%E9%A2%86%E5%AF%BC%E8%80%85%E9%80%89%E4%B8%BE%E5%92%8C%E5%8E%9F%E5%AD%90%E5%B9%BF%E6%92%AD/</url>
    <content><![CDATA[<blockquote>
<p>简单的说一下zookeeper工作的过程,如果对这个过程还不太清楚,或者说对它如何使用等不太清楚的,可以参考一下其他的文章</p>
</blockquote>
<span id="more"></span>

<h2 id="Zookeeper工作原理概述"><a href="#Zookeeper工作原理概述" class="headerlink" title="Zookeeper工作原理概述"></a>Zookeeper工作原理概述</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zookeeper有两种工作的模式,一种是单机方式,另一种是集群方式.单机方式不属于这里分析的范畴,因为研究zookeeper的目的就在于研究一个zookeeper集群的机器如何协调起来工作的.</span><br><span class="line">要配置几台zookeeper一起工作,大家在开始必须使用相同的配置文件,配置文件中有一些配置项</span><br><span class="line">但是与集群相关的是这一项:</span><br><span class="line">    server.1&#x3D;192.168.211.1:2888:3888</span><br><span class="line">    server.2&#x3D;192.168.211.2:2888:3888</span><br><span class="line">    server.3&#x3D;192.168.211.3:2888:3888</span><br><span class="line">这里定义了两台服务器的配置,格式为:</span><br><span class="line">    server.serverid&#x3D;serverhost:leader_listent_port:quorum_port</span><br><span class="line">    serverid是本服务器的id</span><br><span class="line">    leader_listen_port是该服务器一旦成为leader之后需要监听的端口,用于接收来自follower的请求</span><br><span class="line">    quorum_port是集群中的每一个服务器在最开始选举leader时监听的端口,用于服务器互相之间通信选举leader</span><br><span class="line">需要注意的是,server id并没有写在这个配置文件中,而是在datadir中的myid文件中指定</span><br><span class="line">我理解这么做的目的是:</span><br><span class="line">    所有的服务器统一使用一个配置文件,该配置文件里面没有任何与特定服务器相关的信息</span><br><span class="line">    这样便于发布服务的时候不会出错,而独立出来一个文件专门存放这个server id值</span><br><span class="line">Zookeeper集群工作的过程包括如下几步:</span><br><span class="line">    recovery,这个过程泛指集群服务器的启动和恢复,因为恢复也可以理解为另一种层面上的&quot;启动&quot;–需要恢复历史数据的启动,后面会详细讲解.</span><br><span class="line">    broadcast,这是启动完毕之后,集群中的服务器开始接收客户端的连接一起工作的过程,如果客户端有修改数据的改动,那么一定会由leader广播给follower,所以称为&quot;broadcast&quot;.</span><br><span class="line">Zookeeper集群大概是这样工作的:</span><br><span class="line">    a.首先每个服务器读取配置文件和数据文件,根据serverid知道本机对应的配置(就是前面那些地址和端口),并且将历史数据加载进内存中.</span><br><span class="line">    b.集群中的服务器开始根据前面给出的quorum port监听集群中其他服务器的请求,并且把自己选举的leader也通知其他服务器,来来往往几回,选举出集群的一个leader.</span><br><span class="line">    c.选举完leader其实还不算是真正意义上的&quot;leader&quot;,因为到了这里leader还需要与集群中的其他服务器同步数据,如果这一步出错,将返回b中重新选举leader.</span><br><span class="line">    在leader选举完毕之后,集群中的其他服务器称为&quot;follower&quot;,也就是都要听从leader的指令.</span><br><span class="line">    d.到了这里,集群中的所有服务器,不论是leader还是follower,大家的数据都是一致的了,可以开始接收客户端的连接了.</span><br><span class="line">    如果是读类型的请求,那么直接返回就是了,因为并不改变数据;</span><br><span class="line">    否则,都要向leader汇报,如何通知leader呢?就是通过前面讲到的leader_listen_port.</span><br><span class="line">    leader收到这个修改数据的请求之后,将会广播给集群中其他follower,当超过一半数量的follower有了回复,那么就相当于这个修改操作完成了,这时leader可以告诉之前的那台服务器可以给客户端一个回应了.</span><br><span class="line">可以看到,上面a,b,c对应的recovery过程,而d对应的broadcast过程.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Fast-Leader选举算法-领导者选举"><a href="#Fast-Leader选举算法-领导者选举" class="headerlink" title="Fast Leader选举算法(领导者选举)"></a>Fast Leader选举算法(领导者选举)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">如何在Zookeeper集群中选举出一个leader?</span><br><span class="line">Zookeeper使用了三种算法,具体使用哪种算法,在配置文件中是可以配置的,对应的配置项是&quot;electionAlg&quot;,其中:</span><br><span class="line">1对应的是LeaderElection算法</span><br><span class="line">2对应的是AuthFastLeaderElection算法</span><br><span class="line">3对应的是FastLeaderElection算法</span><br><span class="line">默认使用FastLeaderElection算法</span><br><span class="line"></span><br><span class="line">数据恢复阶段</span><br><span class="line">    每个在zookeeper服务器先读取当前保存在磁盘的数据,zookeeper中的每份数据,都有一个对应的id值</span><br><span class="line">    这个值是依次递增的,换言之,越新的数据,对应的ID值就越大</span><br><span class="line"></span><br><span class="line">向其他节点发送投票值</span><br><span class="line">    在读取数据完毕之后,每个zookeeper服务器发送自己选举的leader（首次选自己）</span><br><span class="line">    这个协议中包含了以下几部分的数据:</span><br><span class="line">        所选举leader的id(就是配置文件中写好的每个服务器的id) ,在初始阶段,每台服务器的这个值都是自己服务器的id,也就是它们都选举自己为leader.</span><br><span class="line">        服务器最大数据的id,这个值大的服务器,说明存放了更新的数据.</span><br><span class="line">        逻辑时钟的值,这个值从0开始递增,每次选举对应一个值,也就是说:如果在同一次选举中,那么这个值应该是一致的;逻辑时钟值越大,说明这一次选举leader的进程更新.</span><br><span class="line">        本机在当前选举过程中的状态,有以下几种:LOOKING,FOLLOWING,OBSERVING,LEADING</span><br><span class="line"></span><br><span class="line">接受来自其他节点的数据</span><br><span class="line">    每台服务器将自己服务器的以上数据发送到集群中的其他服务器之后,同样的也需要接收来自其他服务器的数据</span><br><span class="line">    它将做以下的处理:</span><br><span class="line">        如果所接收数据中服务器的状态还是在选举阶段(LOOKING 状态),那么首先判断逻辑时钟值,又分为以下三种情况:</span><br><span class="line">            如果发送过来的逻辑时钟大于目前的逻辑时钟,那么说明这是更新的一次选举,此时需要更新一下本机的逻辑时钟值,同时将之前收集到的来自其他服务器的选举清空,因为这些数据已经不再有效了.</span><br><span class="line">                然后判断是否需要更新当前自己的选举情况.在这里是根据选举leader id,保存的最大数据id来进行判断的</span><br><span class="line">                这两种数据之间对这个选举结果的影响的权重关系是:</span><br><span class="line">                首先看数据id,数据id大者胜出;</span><br><span class="line">                其次再判断leader id,leader id大者胜出.</span><br><span class="line">                然后再将自身最新的选举结果(也就是上面提到的三种数据）广播给其他服务器).</span><br><span class="line">            发送过来数据的逻辑时钟小于本机的逻辑时钟，说明对方在一个相对较早的选举进程中,这里只需要将本机的数据发送过去就是了</span><br><span class="line">            两边的逻辑时钟相同,此时也只是调用totalOrderPredicate函数判断是否需要更新本机的数据,如果更新了再将自己最新的选举结果广播出去就是了.</span><br><span class="line">        再处理两种情况:</span><br><span class="line">            服务器判断是不是已经收集到了所有服务器的选举状态,如果是，那么这台服务器选举的leader就定下来了,然后根据选举结果设置自己的角色(FOLLOWING还是LEADER),然后退出选举过程就是了.</span><br><span class="line">            即使没有收集到所有服务器的选举状态,也可以根据该节点上选择的最新的leader是不是得到了超过半数以上服务器的支持</span><br><span class="line">                如果是,那么当前线程将被阻塞等待一段时间(这个时间在finalizeWait定义)看看是不是还会收到当前leader的数据更优的leader</span><br><span class="line">                如果经过一段时间还没有这个新的leader提出来，那么这台服务器最终的leader就确定了,否则进行下一次选举.</span><br><span class="line">    如果所接收服务器不在选举状态,也就是在FOLLOWING或者LEADING状态</span><br><span class="line">    做以下两个判断:</span><br><span class="line">        如果逻辑时钟相同,将该数据保存到recvset,如果所接收服务器宣称自己是leader,那么将判断是不是有半数以上的服务器选举它,如果是则设置选举状态退出选举过程</span><br><span class="line">        否则这是一条与当前逻辑时钟不符合的消息,那么说明在另一个选举过程中已经有了选举结果,于是将该选举结果加入到outofelection集合中,再根据outofelection来判断是否可以结束选举,如果可以也是保存逻辑时钟,设置选举状态,退出选举过程.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Leader与Follower同步数据（原子广播）"><a href="#Leader与Follower同步数据（原子广播）" class="headerlink" title="Leader与Follower同步数据（原子广播）"></a>Leader与Follower同步数据（原子广播）</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">根据 Fast Leader选举算法中的分析,如果一台zookeeper服务器成为集群中的leader,那么一定是当前所有服务器中保存数据最多（不是最新？？）的服务器</span><br><span class="line">所以在这台服务器成为leader之后,首先要做的事情就是与集群中的其它服务器(现在是follower)同步数据,保证大家的数据一致,这个过程完毕了才开始正式处理来自客户端的连接请求</span><br><span class="line">Fast Leader选举算法中提到的同步数据时使用的逻辑时钟,它的初始值是0,每次选举过程都会递增的,在leader正式上任之后做的第一件事情,就是根据当前保存的数据id值,设置最新的逻辑时钟值</span><br><span class="line">随后,leader构建NEWLEADER封包,该封包的数据是当前最大数据的id,广播给所有的follower,也就是告知follower leader保存的数据id是多少,大家看看是不是需要同步</span><br><span class="line">然后,leader根据follower数量给每个follower创建一个线程LearnerHandler,专门负责接收它们的同步数据请求</span><br><span class="line">leader主线程开始阻塞在这里,等待其他follower的回应(也就是LearnerHandler线程的处理结果),同样的,只有在超过半数的follower已经同步数据完毕,这个过程才能结束,leader才能正式成为leader.</span><br><span class="line"></span><br><span class="line">Leader所做的工作</span><br><span class="line">    所以其实leader与follower同步数据的大部分操作都在LearnerHandler线程中处理的,接着看这一块</span><br><span class="line">    leader接收到的来自某个follower封包一定是FOLLOWERINFO,该封包告知了该服务器保存的数据id</span><br><span class="line">    之后根据这个数据id与本机保存的数据进行比较:</span><br><span class="line">        如果数据完全一致,则发送DIFF封包告知follower当前数据就是最新的了</span><br><span class="line">        判断这一阶段之内有没有已经被提交的提议值,如果有,那么:</span><br><span class="line">            如果有部分数据没有同步,那么会发送DIFF封包将有差异的数据同步过去.同时将follower没有的数据逐个发送COMMIT封包给follower要求记录下来.</span><br><span class="line">            如果follower数据id更大,那么会发送TRUNC封包告知截除多余数据.（一台leader数据没同步就宕掉了，选举之后恢复了，数据比现在leader更新）</span><br><span class="line">        如果这一阶段内没有提交的提议值,直接发送SNAP封包将快照同步发送给follower</span><br><span class="line">        消息完毕之后,发送UPTODATE封包告知follower当前数据就是最新的了,再次发送NEWLEADER封包宣称自己是leader,等待follower的响应</span><br><span class="line">    </span><br><span class="line">Follower做的工作:</span><br><span class="line">    会尝试与leader建立连接,这里有一个机制,如果一定时间内没有连接上,就报错退出,重新回到选举状态.</span><br><span class="line">    其次在发送FOLLOWERINFO封包,该封包中带上自己的最大数据id,也就是会告知leader本机保存的最大数据id.</span><br><span class="line">    根据前面对LeaderHandler的分析,leader会根据不同的情况发送DIFF,UPTODATE,TRUNC,SNAP,依次进行处理就是了,此时follower跟leader的数据也就同步上了.</span><br><span class="line">    由于leader端发送的最后一个封包是UPTODATE,因此在接收到这个封包之后follower结束同步数据过程,发送ACK封包回复leader.</span><br><span class="line"></span><br><span class="line">以上过程中,任何情况出现的错误,服务器将自动将选举状态切换到LOOKING状态,重新开始进行选举.</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>zk</tag>
      </tags>
  </entry>
  <entry>
    <title>一些出现在业务需求上的小点子</title>
    <url>/2019/06/04/%E4%B8%80%E4%BA%9B%E5%87%BA%E7%8E%B0%E5%9C%A8%E4%B8%9A%E5%8A%A1%E9%9C%80%E6%B1%82%E4%B8%8A%E7%9A%84%E5%B0%8F%E7%82%B9%E5%AD%90/</url>
    <content><![CDATA[<blockquote>
<p>业务需求优化</p>
</blockquote>
<span id="more"></span>

<h3 id="1-如何求一段时间内每天的注册人数-并且实现每一天的统计人数都是之前的注册人数加上当天的注册人数"><a href="#1-如何求一段时间内每天的注册人数-并且实现每一天的统计人数都是之前的注册人数加上当天的注册人数" class="headerlink" title="1.如何求一段时间内每天的注册人数,并且实现每一天的统计人数都是之前的注册人数加上当天的注册人数"></a>1.如何求一段时间内每天的注册人数,并且实现每一天的统计人数都是之前的注册人数加上当天的注册人数</h3><blockquote>
<p>Demo数据如下表,时间范围为2019-01-01至2019-01-03,左边为原始数据,右边为聚合</p>
</blockquote>
<table>
<thead>
<tr>
<th>c_time</th>
<th>uid</th>
<th>-</th>
<th>c_time</th>
<th>sum</th>
</tr>
</thead>
<tbody><tr>
<td>2019-01-01</td>
<td>10001</td>
<td></td>
<td>2019-01-01</td>
<td>1</td>
</tr>
<tr>
<td>2019-01-02</td>
<td>10002</td>
<td></td>
<td>2019-01-02</td>
<td>2</td>
</tr>
<tr>
<td>2019-01-02</td>
<td>10003</td>
<td></td>
<td>2019-01-03</td>
<td>3</td>
</tr>
<tr>
<td>2019-01-03</td>
<td>10004</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019-01-03</td>
<td>10005</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2019-01-03</td>
<td>10006</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<blockquote>
<p>SQL简单实现</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="keyword">SUM</span>(<span class="keyword">IF</span>(c_time = <span class="string">&quot;2019-01-01&quot;</span>,<span class="keyword">sum</span>,<span class="number">0</span>)) <span class="keyword">AS</span> <span class="string">&quot;1&quot;</span>,</span><br><span class="line">    <span class="keyword">SUM</span>(<span class="keyword">IF</span>(c_time <span class="keyword">BETWEEN</span> <span class="string">&quot;2019-01-01&quot;</span> <span class="keyword">AND</span> <span class="string">&quot;2019-01-02&quot;</span>,<span class="keyword">sum</span>,<span class="number">0</span>)) <span class="keyword">AS</span> <span class="string">&quot;2&quot;</span>,</span><br><span class="line">    <span class="keyword">SUM</span>(<span class="keyword">IF</span>(c_time <span class="keyword">BETWEEN</span> <span class="string">&quot;2019-01-01&quot;</span> <span class="keyword">AND</span> <span class="string">&quot;2019-01-03&quot;</span>,<span class="keyword">sum</span>,<span class="number">0</span>)) <span class="keyword">AS</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"><span class="keyword">FROM</span> t</span><br><span class="line">然后将行转成列</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Spark SQL简单实现</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** </span></span><br><span class="line"><span class="comment">思路:</span></span><br><span class="line"><span class="comment">1.利用UDF,将c_time转成list形式</span></span><br><span class="line"><span class="comment">    例如:</span></span><br><span class="line"><span class="comment">        2019-01-01进入UDF,根据endDate转化成List[2019-01-01,2019-01-02,2019-01-03]</span></span><br><span class="line"><span class="comment">2.利用explode方法将List转成多行</span></span><br><span class="line"><span class="comment">3.最终进行groupBy</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">val</span> beginDate = <span class="string">&quot;2019-01-01&quot;</span></span><br><span class="line"><span class="keyword">val</span> endDate = <span class="string">&quot;2019-01-03&quot;</span></span><br><span class="line"><span class="keyword">val</span> result = tDF</span><br><span class="line">    .filter(<span class="string">s&quot;c_time &gt;=&#x27;<span class="subst">$beginDate</span>&#x27; and c_time &lt; &#x27;<span class="subst">$endDate</span>&#x27;&quot;</span>)</span><br><span class="line">    .selectExpr(<span class="string">&quot;uid&quot;</span>,<span class="string">&quot;c_time&quot;</span>)</span><br><span class="line">    .groupBy(<span class="string">&quot;c_time&quot;</span>)</span><br><span class="line">    .agg(count(<span class="string">&quot;uid&quot;</span>)).as(<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">    .selectExpr(<span class="string">&quot;sum&quot;</span>,<span class="string">&quot;dateList(c_time) date_list&quot;</span>)</span><br><span class="line">    .withColumn(<span class="string">&quot;c_time&quot;</span>,explode($<span class="string">&quot;date_list&quot;</span>))</span><br><span class="line">    .groupBy(<span class="string">&quot;c_time&quot;</span>)</span><br><span class="line">    .agg(sum(<span class="string">&quot;sum&quot;</span>)).as(<span class="string">&quot;sum&quot;</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows远程桌面多用户同时登录</title>
    <url>/2020/03/12/Windows%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2%E5%A4%9A%E7%94%A8%E6%88%B7%E5%90%8C%E6%97%B6%E7%99%BB%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>远程连接多用户同时操作</p>
</blockquote>
<span id="more"></span>

<h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p>Win 10系统设备一台,需要专业版<br>RDPWrap工具<a href="https://github.com/jxeditor/Software/blob/master/Win10%E5%A4%9A%E7%94%A8%E6%88%B7%E5%90%8C%E6%97%B6%E7%99%BB%E9%99%86.zip">下载链接</a></p>
<hr>
<h1 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.以管理员身份运行install.bat</span><br><span class="line">b.运行RDPconf.exe</span><br><span class="line">    ListenerState可能是不支持</span><br><span class="line">c.进入&#39;替换&#39;文件夹</span><br><span class="line">d.将termsrv.dll复制到C:\Windows\System32\下</span><br><span class="line">e.将termsrv.dll.mui复制到C:\Windows\System32\zh-CN\下</span><br><span class="line">f.重启电脑,运行RDPConf.exe</span><br><span class="line">    ListenerState为支持状态</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\Windows\System32\下文件提示没有权限</span><br><span class="line">右键要操作的文件-属性-&gt;安全-&gt;高级-&gt;更改&#39;所有者&#39;</span><br><span class="line">-&gt;高级-&gt;立即查找-&gt;Administrators-&gt;确定</span><br><span class="line">-&gt;确定-&gt;更改权限</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>tools</tag>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>二分查找算法</title>
    <url>/2016/06/29/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<blockquote>
<p>实现二分查找</p>
</blockquote>
<span id="more"></span>

<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.algorithm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BinarySearch</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 非递归的解决方式,确定最小下标,最大下标,中间下标</span></span><br><span class="line"><span class="comment">     * 当最小下标小于最大下标一直循环</span></span><br><span class="line"><span class="comment">     * 比较中间下标对应值与目标值的大小</span></span><br><span class="line"><span class="comment">     * 相等则找到</span></span><br><span class="line"><span class="comment">     * 比目标值小则最小下标置为中间下标+1</span></span><br><span class="line"><span class="comment">     * 比目标值大则最大下标置为中间下标-1</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> des</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> des)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> low = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> height = arr.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (low &lt;= height) &#123;</span><br><span class="line">            <span class="keyword">int</span> mid = (low + height) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (des == arr[mid]) &#123;</span><br><span class="line">                <span class="keyword">return</span> mid;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (des &lt; arr[mid]) &#123;</span><br><span class="line">                height = mid - <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                low = mid + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 递归的解决方式,计算中间下标,比较目标值是否在范围内,以及最小下标最大下标是否正确</span></span><br><span class="line"><span class="comment">     * 判断目标值与中间下标对应值,递归函数重新赋值</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> des</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> start</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> end</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> des, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> mid = (start + end) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (des &lt; arr[start] || des &gt; arr[end] || start &gt; end) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (des &lt; arr[mid]) &#123;</span><br><span class="line">            <span class="keyword">return</span> binarySearch(arr, des, start, mid - <span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (des &gt; arr[mid]) &#123;</span><br><span class="line">            <span class="keyword">return</span> binarySearch(arr, des, mid + <span class="number">1</span>, end);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> mid;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>&#125;;</span><br><span class="line">        System.out.println(binarySearch(arr, <span class="number">4</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>个人书单</title>
    <url>/2020/05/09/%E4%B8%AA%E4%BA%BA%E4%B9%A6%E5%8D%95/</url>
    <content><![CDATA[<blockquote>
<p>整理一波自己看过的或想看的书籍</p>
</blockquote>
<span id="more"></span>

<p><a href="https://pan.baidu.com/s/1q4FvQddAEiqi5XUa4CaHAg">Shell脚本学习指南</a> 提取码: o44x</p>
<p><a href="https://pan.baidu.com/s/1k_MlKMqtMs1nFccG6byqCw">数据结构与算法Java</a> 提取码: 1apa</p>
<p><a href="https://pan.baidu.com/s/1UwV5zec5G_Onep3XtcjOJg">鲜活的数据</a> 提取码: at5h</p>
<p><a href="https://pan.baidu.com/s/1AlJM2N3AH-qNNcjuyKuAYQ">大数据日知录</a> 提取码: p4uo </p>
<p><a href="https://pan.baidu.com/s/1NNYCbksVMq88_BTSEV76rA">Hadoop权威指南</a> 提取码: 24wb</p>
<p><a href="https://pan.baidu.com/s/1WdmpITcVtqXarK4hlPIFjw">Spark源码剖析</a> 提取码: 35z4</p>
<p><a href="https://pan.baidu.com/s/1J45m5y9rGFh9zIssNuEHuQ">Spark技术内幕</a> 提取码:80u9</p>
<p><a href="https://pan.baidu.com/s/11DSja7am_hW6SqeoxXL3jA">SparkSQL内核剖析</a> 提取码: 39i6</p>
<p><a href="https://pan.baidu.com/s/1HsQFkkpDNVOdcOBntx43Xw">Kafka技术内幕</a> 提取码: di5d</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>edit</tag>
      </tags>
  </entry>
  <entry>
    <title>优秀博客收集</title>
    <url>/2020/05/06/%E4%BC%98%E7%A7%80%E5%8D%9A%E5%AE%A2%E6%94%B6%E9%9B%86/</url>
    <content><![CDATA[<blockquote>
<p>学习他人的长处,弥补自己的不足</p>
</blockquote>
<span id="more"></span>
<h2 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h2><p><a href="https://halfrost.com/archives/">halfrost</a><br><a href="https://blog.bcmeng.com/">编程小梦</a><br><a href="https://matt33.com/">Matt</a><br><a href="http://jm.taobao.org/">阿里中间件团队博客</a><br><a href="https://tech.meituan.com/">美团技术团队</a><br><a href="http://wuchong.me/">阿里云邪</a><br><a href="https://www.luozhiyun.com/">luozhiyun</a><br><a href="http://www.54tianzhisheng.cn/">zhisheng</a><br><a href="https://www.yuque.com/herman-wwt/wwt">王文涛</a></p>
<h2 id="网站"><a href="#网站" class="headerlink" title="网站"></a>网站</h2><p><a href="http://apache-flink.147419.n8.nabble.com/">Flink中文用户邮件列表</a><br><a href="https://issues.apache.org/jira/projects/FLINK/issues">Flink Issues</a></p>
<h2 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h2><p><a href="https://github.com/yangyichao-mango/flink-protobuf">Flink-Protobuf连接器</a><br><a href="https://github.com/halfrost">Halfrost</a></p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>edit</tag>
      </tags>
  </entry>
  <entry>
    <title>优雅停止SparkStreaming</title>
    <url>/2018/07/03/%E4%BC%98%E9%9B%85%E5%81%9C%E6%AD%A2SparkStreaming/</url>
    <content><![CDATA[<blockquote>
<p>SparkStreaming从Kafka中读取数据，并使用Redis进行Offset保存，同时监听Redis中的Key来确定是否停止程序。</p>
</blockquote>
<span id="more"></span>

<h2 id="监听Redis中的Key"><a href="#监听Redis中的Key" class="headerlink" title="监听Redis中的Key"></a>监听Redis中的Key</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 优雅的停止Streaming程序</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param ssc</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stopByMarkKey</span></span>(ssc: <span class="type">StreamingContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> intervalMills = <span class="number">10</span> * <span class="number">1000</span> <span class="comment">// 每隔10秒扫描一次消息是否存在</span></span><br><span class="line">    <span class="keyword">var</span> isStop = <span class="literal">false</span></span><br><span class="line">    <span class="keyword">while</span> (!isStop) &#123;</span><br><span class="line">      isStop = ssc.awaitTerminationOrTimeout(intervalMills)</span><br><span class="line">        <span class="keyword">if</span> (!isStop &amp;&amp; isExists(<span class="type">STOP_FLAG</span>)) &#123;</span><br><span class="line">          <span class="type">LOG</span>.warn(<span class="string">&quot;2秒后开始关闭sparstreaming程序.....&quot;</span>)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">2000</span>)</span><br><span class="line">            ssc.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 判断Key是否存在</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param key</span></span><br><span class="line"><span class="comment">  * @return</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isExists</span></span>(key: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> jedis = <span class="type">InternalRedisClient</span>.getPool.getResource</span><br><span class="line">    <span class="keyword">val</span> flag = jedis.exists(key)</span><br><span class="line">    jedis.close()</span><br><span class="line">    flag</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="主程序"><a href="#主程序" class="headerlink" title="主程序"></a>主程序</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.dev.stream</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> com.dev.scala.<span class="type">ETLStreaming</span>.<span class="type">LOG</span></span><br><span class="line"><span class="keyword">import</span> com.dev.scala.util.<span class="type">InternalRedisClient</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.<span class="type">TopicPartition</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>, <span class="type">TaskContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaRedisStreaming</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">LOG</span> = <span class="type">LoggerFactory</span>.getLogger(<span class="string">&quot;KafkaRedisStreaming&quot;</span>)</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">STOP_FLAG</span> = <span class="string">&quot;TEST_STOP_FLAG&quot;</span></span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initRedisPool</span></span>() = &#123;</span><br><span class="line">    <span class="comment">// Redis configurations</span></span><br><span class="line">    <span class="keyword">val</span> maxTotal = <span class="number">20</span></span><br><span class="line">    <span class="keyword">val</span> maxIdle = <span class="number">10</span></span><br><span class="line">    <span class="keyword">val</span> minIdle = <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> redisHost = <span class="string">&quot;47.98.119.122&quot;</span></span><br><span class="line">    <span class="keyword">val</span> redisPort = <span class="number">6379</span></span><br><span class="line">    <span class="keyword">val</span> redisTimeout = <span class="number">30000</span></span><br><span class="line">    <span class="type">InternalRedisClient</span>.makePool(redisHost, redisPort, redisTimeout, maxTotal, maxIdle, minIdle)</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 从redis里获取Topic的offset值</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param topicName</span></span><br><span class="line"><span class="comment">    * @param partitions</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getLastCommittedOffsets</span></span>(topicName: <span class="type">String</span>, partitions: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">LOG</span>.isInfoEnabled())</span><br><span class="line">      <span class="type">LOG</span>.info(<span class="string">&quot;||--Topic:&#123;&#125;,getLastCommittedOffsets from Redis--||&quot;</span>, topicName)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//从Redis获取上一次存的Offset</span></span><br><span class="line">    <span class="keyword">val</span> jedis = <span class="type">InternalRedisClient</span>.getPool.getResource</span><br><span class="line">    <span class="keyword">val</span> fromOffsets = collection.mutable.<span class="type">HashMap</span>.empty[<span class="type">TopicPartition</span>, <span class="type">Long</span>]</span><br><span class="line">    <span class="keyword">for</span> (partition &lt;- <span class="number">0</span> to partitions - <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> topic_partition_key = topicName + <span class="string">&quot;_&quot;</span> + partition</span><br><span class="line">      <span class="keyword">val</span> lastSavedOffset = jedis.get(topic_partition_key)</span><br><span class="line">      <span class="keyword">val</span> lastOffset = <span class="keyword">if</span> (lastSavedOffset == <span class="literal">null</span>) <span class="number">0</span>L <span class="keyword">else</span> lastSavedOffset.toLong</span><br><span class="line">      fromOffsets += (<span class="keyword">new</span> <span class="type">TopicPartition</span>(topicName, partition) -&gt; lastOffset)</span><br><span class="line">    &#125;</span><br><span class="line">    jedis.close()</span><br><span class="line"> </span><br><span class="line">    fromOffsets.toMap</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//初始化Redis Pool</span></span><br><span class="line">    initRedisPool()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;ScalaKafkaStream&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">3</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> bootstrapServers = <span class="string">&quot;hadoop1:9092,hadoop2:9092,hadoop3:9092&quot;</span></span><br><span class="line">    <span class="keyword">val</span> groupId = <span class="string">&quot;kafka-test-group&quot;</span></span><br><span class="line">    <span class="keyword">val</span> topicName = <span class="string">&quot;Test&quot;</span></span><br><span class="line">    <span class="keyword">val</span> maxPoll = <span class="number">20000</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>(</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; bootstrapServers,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; groupId,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">MAX_POLL_RECORDS_CONFIG</span> -&gt; maxPoll.toString,</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">KEY_DESERIALIZER_CLASS_CONFIG</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">VALUE_DESERIALIZER_CLASS_CONFIG</span> -&gt; classOf[<span class="type">StringDeserializer</span>]</span><br><span class="line">    )</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 这里指定Topic的Partition的总数</span></span><br><span class="line">    <span class="keyword">val</span> fromOffsets = getLastCommittedOffsets(topicName, <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 初始化KafkaDS</span></span><br><span class="line">    <span class="keyword">val</span> kafkaTopicDS =</span><br><span class="line">      <span class="type">KafkaUtils</span>.createDirectStream(ssc, <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>, <span class="type">ConsumerStrategies</span>.<span class="type">Assign</span>[<span class="type">String</span>, <span class="type">String</span>](fromOffsets.keys.toList, kafkaParams, fromOffsets))</span><br><span class="line"> </span><br><span class="line">    kafkaTopicDS.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line"> </span><br><span class="line">      <span class="comment">// 如果rdd有数据</span></span><br><span class="line">      <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">val</span> jedis = <span class="type">InternalRedisClient</span>.getPool.getResource</span><br><span class="line">        <span class="keyword">val</span> p = jedis.pipelined()</span><br><span class="line">        p.multi() <span class="comment">//开启事务</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 处理数据</span></span><br><span class="line">        rdd</span><br><span class="line">          .map(_.value)</span><br><span class="line">          .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">          .map(x =&gt; (x, <span class="number">1</span>L))</span><br><span class="line">          .reduceByKey(_ + _)</span><br><span class="line">          .sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">          .foreach(println)</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//更新Offset</span></span><br><span class="line">        offsetRanges.foreach &#123; offsetRange =&gt;</span><br><span class="line">          println(<span class="string">&quot;partition : &quot;</span> + offsetRange.partition + <span class="string">&quot; fromOffset:  &quot;</span> + offsetRange.fromOffset + <span class="string">&quot; untilOffset: &quot;</span> + offsetRange.untilOffset)</span><br><span class="line">          <span class="keyword">val</span> topic_partition_key = offsetRange.topic + <span class="string">&quot;_&quot;</span> + offsetRange.partition</span><br><span class="line">          p.set(topic_partition_key, offsetRange.untilOffset + <span class="string">&quot;&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        p.exec() <span class="comment">//提交事务</span></span><br><span class="line">        p.sync <span class="comment">//关闭pipeline</span></span><br><span class="line">        jedis.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"> </span><br><span class="line">    ssc.start()</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 优雅停止</span></span><br><span class="line">    stopByMarkKey(ssc)</span><br><span class="line"> </span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Docker部署Web项目</title>
    <url>/2019/08/26/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Web%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<blockquote>
<p>docker进行项目部署</p>
</blockquote>
<span id="more"></span>

<h2 id="安装docker以及加载镜像"><a href="#安装docker以及加载镜像" class="headerlink" title="安装docker以及加载镜像"></a>安装docker以及加载镜像</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 本机使用的是CentOS6.5,docker存在很多bug</span></span><br><span class="line">yum install docker-engine</span><br><span class="line"><span class="comment"># 有网情况</span></span><br><span class="line">docker pull mysql</span><br><span class="line">docker pull tomcat</span><br><span class="line"><span class="comment"># 没网情况</span></span><br><span class="line">docker save -o ./images.tar images</span><br><span class="line">docker load -i images.tar 或者 docker load &lt; images.tar</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="制作镜像"><a href="#制作镜像" class="headerlink" title="制作镜像"></a>制作镜像</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi Dockerfile</span><br><span class="line">FROM tomcat:latest</span><br><span class="line">MAINTAINER xs</span><br><span class="line">COPY cas.war &#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;webapps&#x2F;</span><br><span class="line">COPY wiki.war &#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;webapps&#x2F;</span><br><span class="line"></span><br><span class="line">docker build -t demo:v1 .</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --name dmysql01 --hostname dmysql01 --ip 172.17.0.3 -e MYSQL_ROOT_PASSWORD=123456 -v /root/dmysql01/data:/var/lib/mysql -d -p 13306:3306 mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要知道mysql的IP地址</span></span><br><span class="line">docker inspect dmysql01</span><br><span class="line"><span class="comment"># docker exec -it 容器ID /bin/bash</span></span><br><span class="line">ALTER USER <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED WITH mysql_native_password BY <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"></span><br><span class="line">docker run --name myweb --add-host dmysql01:172.17.0.8 -v /root/docker/app/repository:/usr/<span class="built_in">local</span>/tomcat/work/Catalina/localhost/wiki/extension/repository -d -p 18080:8080 demo:v1</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Spark2.0的坑</title>
    <url>/2019/10/15/%E4%BD%BF%E7%94%A8Spark2.0%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<blockquote>
<p>从Spark1转换到Spark2,总体逻辑可能没有什么变化,但是小细节会有很多坑</p>
</blockquote>
<span id="more"></span>

<h2 id="连接MySQL报No-suitable-driver"><a href="#连接MySQL报No-suitable-driver" class="headerlink" title="连接MySQL报No suitable driver"></a>连接MySQL报No suitable driver</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注册临时表</span></span><br><span class="line">df.registerTempTable(<span class="string">&quot;demo&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> prop = <span class="keyword">new</span> java.util.<span class="type">Properties</span></span><br><span class="line">prop.setProperty(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>)</span><br><span class="line">prop.setProperty(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;123456&quot;</span>)</span><br><span class="line">prop.setProperty(<span class="string">&quot;driver&quot;</span>,<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">prop.setProperty(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://127.0.0.1:3306/test&quot;</span>)</span><br><span class="line">sqlContext.sql(<span class="string">&quot;select name,age,sex from demo&quot;</span>)</span><br><span class="line">.write</span><br><span class="line">.mode(org.apache.spark.sql.<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">.jdbc(prop.getProperty(<span class="string">&quot;url&quot;</span>),<span class="string">&quot;demo&quot;</span>,prop) </span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Spark生成一个空的DataFrame"><a href="#Spark生成一个空的DataFrame" class="headerlink" title="Spark生成一个空的DataFrame"></a>Spark生成一个空的DataFrame</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在1中可以直接用null,2中就不可以了</span><br><span class="line">var df:DataFrame &#x3D; null</span><br><span class="line"></span><br><span class="line"># 生成一个无列的空DataFrame</span><br><span class="line">var df &#x3D; spark.emptyDataFrame</span><br><span class="line"></span><br><span class="line"># 生成一个有列的空DataFrame</span><br><span class="line">val schema &#x3D; StructType(</span><br><span class="line">  Seq(</span><br><span class="line">    StructField(&quot;lie1&quot;, StringType, true),</span><br><span class="line">    StructField(&quot;lie2&quot;, StringType, true),</span><br><span class="line">    StructField(&quot;lie3&quot;, StringType, true)))</span><br><span class="line">val df &#x3D; spark.createDataFrame(sc.emptyRDD[Row], schema)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="写Hive速度极其慢"><a href="#写Hive速度极其慢" class="headerlink" title="写Hive速度极其慢"></a>写Hive速度极其慢</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用insertinto,字段顺序与hive表字段顺序不一致导致</span><br><span class="line">修改顺序后,速度正常</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>偷金问题</title>
    <url>/2017/10/11/%E5%81%B7%E9%87%91%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>一排n个房子,每个房子都有未知的金子,不能连续偷3个房子的金子,怎么才能偷到最多的金子</p>
</blockquote>
<span id="more"></span>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">stole</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">3</span>][array.length + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dp.length; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dp[<span class="number">0</span>].length; j++) &#123;</span><br><span class="line">                dp[i][j] = -<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        calculate(array, dp, <span class="number">0</span>, array.length);</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][array.length];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">calculate</span><span class="params">(<span class="keyword">int</span>[] array, <span class="keyword">int</span>[][] dp, <span class="keyword">int</span> front, <span class="keyword">int</span> remaining)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (remaining == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (dp[front][remaining] != -<span class="number">1</span>) <span class="keyword">return</span> dp[front][remaining];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (front &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            left = array[array.length - remaining] + calculate(array, dp, front + <span class="number">1</span>, remaining - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> right = calculate(array, dp, <span class="number">0</span>, remaining - <span class="number">1</span>);</span><br><span class="line">        dp[front][remaining] = Math.max(left, right);</span><br><span class="line">        <span class="keyword">return</span> dp[front][remaining];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">10</span>&#125;;</span><br><span class="line">        System.out.println(stole(arr));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>关于Flink-1.11中SqlClient的Bug</title>
    <url>/2020/06/03/%E5%85%B3%E4%BA%8EFlink-1.11%E4%B8%ADSqlClient%E7%9A%84Bug/</url>
    <content><![CDATA[<blockquote>
<p>在使用SqlClient时出现的一些问题整理</p>
</blockquote>
<span id="more"></span>

<h2 id="CatalogName或DBName获取不到"><a href="#CatalogName或DBName获取不到" class="headerlink" title="CatalogName或DBName获取不到"></a>CatalogName或DBName获取不到</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使用use catalog name或use name会报错</span><br><span class="line">Flink SQL&gt; use catalog hive;</span><br><span class="line">[ERROR] Could not execute SQL statement. Reason:</span><br><span class="line">org.apache.flink.table.catalog.exceptions.CatalogException: A catalog with name [&#96;hive&#96;] does not exist.</span><br></pre></td></tr></table></figure>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p><a href="https://issues.apache.org/jira/projects/FLINK/issues/FLINK-18055">FLINK-18055</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">出现这种问题的原因是SqlCommanParser类中</span><br><span class="line">name被&#96;&#96;包裹着,但是这种写法不识别</span><br><span class="line">operands &#x3D; new String[]&#123;String.format(&quot;&#96;%s&#96;&quot;, ((UseCatalogOperation) operation).getCatalogName())&#125;;</span><br><span class="line">operands &#x3D; new String[]&#123;String.format(&quot;&#96;%s&#96;.&#96;%s&#96;&quot;, op.getCatalogName(), op.getDatabaseName())&#125;;</span><br><span class="line"></span><br><span class="line">目前的解决方案去掉&#96;&#96;,个人感觉并没有很好的解决&#96;&#96;的问题</span><br><span class="line">operands &#x3D; new String[]&#123;((UseCatalogOperation) operation).getCatalogName()&#125;;</span><br><span class="line">operands &#x3D; new String[]&#123;((UseDatabaseOperation) operation).getDatabaseName()&#125;;</span><br><span class="line"></span><br><span class="line">看1.10版本能够知道使用的是正则</span><br><span class="line">USE_CATALOG(</span><br><span class="line">    &quot;USE\\s+CATALOG\\s+(.*)&quot;,</span><br><span class="line">    SINGLE_OPERAND),</span><br><span class="line">USE(</span><br><span class="line">    &quot;USE\\s+(?!CATALOG)(.*)&quot;,</span><br><span class="line">    SINGLE_OPERAND)</span><br><span class="line"></span><br><span class="line">SqlClient的一些语句使用并没有符合SQL的标准规范</span><br><span class="line">use catalog hive;可以使用,但是use catalog &#96;hive&#96;;就不能被识别</span><br><span class="line"></span><br><span class="line">优化为</span><br><span class="line">在现有的基础上使用StringUtils.strip()方法</span><br><span class="line">operands &#x3D; new String[]&#123;StringUtils.strip(((UseCatalogOperation) operation).getCatalogName(),&quot;&#96;&quot;)&#125;</span><br><span class="line"></span><br><span class="line">或者切回1.10版本模式修改正则,让其符合SQL的命名规范</span><br><span class="line">USE_CATALOG(</span><br><span class="line">    &quot;USE\\s+CATALOG\\s+&#96;?(\\w+)*&#96;?&quot;,</span><br><span class="line">    SINGLE_OPERAND),</span><br><span class="line">USE(</span><br><span class="line">    &quot;USE\\s+(?!CATALOG)&#96;?(\\w+)*&#96;&quot;,</span><br><span class="line">    SINGLE_OPERAND)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>关于Flink平台化设计的一些想法</title>
    <url>/2021/01/13/%E5%85%B3%E4%BA%8EFlink%E5%B9%B3%E5%8F%B0%E5%8C%96%E8%AE%BE%E8%AE%A1%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/</url>
    <content><![CDATA[<blockquote>
<p>基于阿里云的实时计算平台的一点想法,以最快的速度弄出一个架子</p>
</blockquote>
<span id="more"></span>

<h2 id="功能点"><a href="#功能点" class="headerlink" title="功能点"></a>功能点</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">任务提交</span><br><span class="line">任务运行</span><br><span class="line">任务监控</span><br><span class="line">任务调度</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="前端实现"><a href="#前端实现" class="headerlink" title="前端实现"></a>前端实现</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">目前可以考虑CodeMirror和ACE两款在线代码编辑器</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="任务提交"><a href="#任务提交" class="headerlink" title="任务提交"></a>任务提交</h2><h3 id="方案一-submitJar"><a href="#方案一-submitJar" class="headerlink" title="方案一(submitJar)"></a>方案一(submitJar)</h3><p>这方面可以考虑一下无邪的一个方案<a href="https://github.com/wuchong/flink-sql-submit">flink-sql-submit</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">思路</span><br><span class="line">    将web页面写好的FlinkSQL代码,配置参数等信息,存储起来</span><br><span class="line">    通过公共提交Jar(以下统称submitJar)去获取这些信息,并通过flink run命令进行提交</span><br><span class="line">    这样最基本的功能,任务能跑起来的点是实现了的</span><br><span class="line"></span><br><span class="line">优化</span><br><span class="line">    最主要是基于submitJar做优化</span><br><span class="line">    1.配置属性的使用</span><br><span class="line">        需要对Flink可配置项进行一系列梳理,最好自实现一个Configuration封装类</span><br><span class="line">    2.UDF加载功能</span><br><span class="line">        使用方式: create function test as &#39;com.test.flink.UpperUDF&#39; LANGUAGE SCALA</span><br><span class="line">        其中一个点,对于UDF加载应该在页面上有配置项,用于submitJar感知需不需要去加载UDF </span><br></pre></td></tr></table></figure>

<h3 id="方案二-Nest"><a href="#方案二-Nest" class="headerlink" title="方案二(Nest)"></a>方案二(Nest)</h3><p>主要是参考Hue,<a href="https://github.com/apache/zeppelin/blob/227fb9266d2ca232c774a72ec913ba4e97af3bf8/flink/interpreter/src/main/java/org/apache/zeppelin/flink/TableEnvFactory.java#L64">Zeppelin</a>和<a href="https://github.com/apache/flink/blob/56dbc24367979fdf9bd3f83ba115db1c5680effb/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/SqlClient.java#L62">SqlClient</a>的想法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">思路</span><br><span class="line">    对于SqlClient,是利用了Executor获取执行环境配置,然后使用TableEnvironment去执行任务</span><br><span class="line">    我们可以在Web项目中同样使用TableEnvironment,创建好环境,而在Flink1.12中jobName是可以通过pipeline.name设置的</span><br><span class="line">    最终的目的就是实现一个嵌套在网页上的编辑器实现在线运行</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="任务运行"><a href="#任务运行" class="headerlink" title="任务运行"></a>任务运行</h2><p>这一块目前感觉改动基于submitJar不太好改,只是在使用阿里平台时,可以获取任务的执行计划,并且可以自定义更改每个Oprator的配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">思路</span><br><span class="line">    通过flink info submitJar获取执行计划(submitJar需要先将SQL信息集成进去,是个问题)</span><br><span class="line">    SQL最终其实也是转换成DataStream去执行,针对DataStream对每一个Oprator进行参数配置(并行度等)</span><br><span class="line"></span><br><span class="line">主要的问题是如何将submitJar+SQL集成起来生成一个jar</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="任务监控"><a href="#任务监控" class="headerlink" title="任务监控"></a>任务监控</h2><p>可操作性很大,PushGateway+Prometheus+Grafana这3件套基本可以满足要求</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">思路</span><br><span class="line">    起初时,可以使用Flink自身的WebUI作为监控查看端</span><br><span class="line">    逐步的使用开源组件进行替换,最后自己实现监控UI</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="任务调度"><a href="#任务调度" class="headerlink" title="任务调度"></a>任务调度</h2><p>调度在实时任务方面,好像意义不是太大,一般实时任务启动之后基本不用做额外的操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从老到新,可以试试这几个组件</span><br><span class="line">Celery</span><br><span class="line">    纯Python命令行队列调用,配合crontab管理公司作业调度,未尝不可</span><br><span class="line">Azkaban</span><br><span class="line">    轻量级,批量工作流任务调度,简单上手快,目前大部分公司都是这种</span><br><span class="line">DolphinScheduler</span><br><span class="line">    应该是最新的一款调度组件,特色在于分布式,去中心化,可视化DAG,还没有尝过鲜</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>基于CentOS7.6搭建K8S-Dashboard篇</title>
    <url>/2019/09/06/%E5%9F%BA%E4%BA%8ECentOS7.6%E6%90%AD%E5%BB%BAK8S-Dashboard%E7%AF%87/</url>
    <content><![CDATA[<blockquote>
<p>在集群搭建篇中,已经有了一个测试K8S集群,现在为它装上Dashboard</p>
</blockquote>
<span id="more"></span>

<h2 id="部署仪表板UI-最新版-lt-坑很多-未完成-gt"><a href="#部署仪表板UI-最新版-lt-坑很多-未完成-gt" class="headerlink" title="部署仪表板UI,最新版&lt;坑很多,未完成&gt;"></a>部署仪表板UI,最新版&lt;坑很多,未完成&gt;</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 下述步骤会需要镜像(所有节点都进行镜像导入)</span><br><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;dashboard&#x2F;v2.0.0-beta4&#x2F;aio&#x2F;deploy&#x2F;recommended.yaml</span><br><span class="line">cat recommended.yaml | grep -i image</span><br><span class="line">docker pull kubernetesui&#x2F;dashboard:v2.0.0-beta4</span><br><span class="line">docker pull kubernetesui&#x2F;metrics-scraper:v1.0.1</span><br><span class="line"></span><br><span class="line"># 默认情况下不部署仪表板UI。要部署它，请运行以下命令：</span><br><span class="line"># 主节点</span><br><span class="line">kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;dashboard&#x2F;v2.0.0-beta4&#x2F;aio&#x2F;deploy&#x2F;recommended.yaml</span><br><span class="line"></span><br><span class="line"># 这时候访问WEB可能会出现问题,需要知道Dashboard的pod被部署到哪一个节点</span><br><span class="line">kubectl get pods --all-namespaces -o wide</span><br><span class="line"></span><br><span class="line"># 执行上述命令看到以下结果,则Dashboard部署完成,restart12次是因为部署时node02&#x2F;node03没有镜像导致的</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-fb986f88d-9zplq   1&#x2F;1     Running   0          51m   192.168.186.193   node03   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-6bb65fcc49-lm2qr       1&#x2F;1     Running     12         51m   192.168.140.65    node02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"># 查看日志</span><br><span class="line">kubectl logs kubernetes-dashboard-6bb65fcc49-rbghp --namespace&#x3D;kubernetes-dashboard</span><br><span class="line"></span><br><span class="line"># 有可能kubernetes-dashboard-*一直处于CrashLoopBackOff,进行删除</span><br><span class="line">kubectl delete -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;dashboard&#x2F;v2.0.0-beta4&#x2F;aio&#x2F;deploy&#x2F;recommended.yaml</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="部署仪表板UI-v1-10-1"><a href="#部署仪表板UI-v1-10-1" class="headerlink" title="部署仪表板UI,v1.10.1"></a>部署仪表板UI,v1.10.1</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;dashboard&#x2F;v1.10.1&#x2F;src&#x2F;deploy&#x2F;recommended&#x2F;kubernetes-dashboard.yaml</span><br><span class="line">cat kubernetes-dashboard.yaml |grep image</span><br><span class="line"># 去国内下载,然后修改tag</span><br><span class="line">docker pull mirrorgooglecontainers&#x2F;kubernetes-dashboard-amd64:v1.10.1</span><br><span class="line">docker tag mirrorgooglecontainers&#x2F;kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io&#x2F;kubernetes-dashboard-amd64:v1.10.1</span><br><span class="line">docker rmi mirrorgooglecontainers&#x2F;kubernetes-dashboard-amd64:v1.10.1</span><br><span class="line"></span><br><span class="line"># 修改kubernetes-dashboard.yaml(其实就是将dashbroad部署到master节点上)</span><br><span class="line"># 默认DashBroad部署到Worker节点,但是kube-apiserver在master节点上,Worker节点访问不到kube-apiserver</span><br><span class="line"># ------------------- Dashboard Deployment ------------------- #</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  # 省略</span><br><span class="line">spec:</span><br><span class="line">  # 不修改</span><br><span class="line">  template:</span><br><span class="line">    # 不修改</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        type: master # 新增</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubernetes-dashboard</span><br><span class="line">        image: k8s.gcr.io&#x2F;kubernetes-dashboard-amd64:v1.10.1</span><br><span class="line">        imagePullPolicy: IfNotPresent # 新增</span><br><span class="line">        # 不修改</span><br><span class="line">        </span><br><span class="line"># 部署</span><br><span class="line">kubectl label node node01 type&#x3D;master</span><br><span class="line">kubectl apply -f kubernetes-dashboard.yaml</span><br><span class="line">kubectl get pods --all-namespaces -o wide</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="访问仪表盘-lt-不要用谷歌-推荐用火狐-gt"><a href="#访问仪表盘-lt-不要用谷歌-推荐用火狐-gt" class="headerlink" title="访问仪表盘&lt;不要用谷歌,推荐用火狐&gt;"></a>访问仪表盘&lt;不要用谷歌,推荐用火狐&gt;</h2><h3 id="Proxy-lt-代理可以通过设置开启跳过进行DashBoard使用-gt"><a href="#Proxy-lt-代理可以通过设置开启跳过进行DashBoard使用-gt" class="headerlink" title="Proxy&lt;代理可以通过设置开启跳过进行DashBoard使用&gt;"></a>Proxy&lt;代理可以通过设置开启跳过进行DashBoard使用&gt;</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 只能从执行命令的机器访问UI</span><br><span class="line">kubectl proxy</span><br><span class="line"># 在其他机器上进行访问</span><br><span class="line">kubectl proxy --address&#x3D;192.168.17.129 --disable-filter&#x3D;true</span><br><span class="line"># UI地址</span><br><span class="line">http:&#x2F;&#x2F;192.168.17.129:8001&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kubernetes-dashboard&#x2F;services&#x2F;https:kubernetes-dashboard:&#x2F;proxy&#x2F;</span><br><span class="line"># 注意: 使用Proxy进行登陆很多坑</span><br></pre></td></tr></table></figure>
<h3 id="NodePort-lt-推荐-gt"><a href="#NodePort-lt-推荐-gt" class="headerlink" title="NodePort&lt;推荐&gt;"></a>NodePort&lt;推荐&gt;</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改NodePort</span><br><span class="line">kubectl patch svc -n kube-system kubernetes-dashboard -p &#39;&#123;&quot;spec&quot;:&#123;&quot;type&quot;:&quot;NodePort&quot;&#125;&#125;&#39;</span><br><span class="line">kubectl -n kube-system get service kubernetes-dashboard</span><br><span class="line">kubernetes-dashboard   NodePort   10.102.188.96   &lt;none&gt;        443:31031&#x2F;TCP   3h39m</span><br><span class="line"># UI地址(https)</span><br><span class="line">https:&#x2F;&#x2F;192.168.17.132:31031&#x2F;#!&#x2F;login</span><br></pre></td></tr></table></figure>
<h3 id="开启跳过"><a href="#开启跳过" class="headerlink" title="开启跳过"></a>开启跳过</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl edit deploy -n&#x3D;kube-system kubernetes-dashboard</span><br><span class="line"># 在containers下面的args输入</span><br><span class="line">- --enable-skip-login</span><br></pre></td></tr></table></figure>
<h3 id="kubeconfig和token登陆"><a href="#kubeconfig和token登陆" class="headerlink" title="kubeconfig和token登陆"></a>kubeconfig和token登陆</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建Dashboard管理用户</span><br><span class="line">kubectl create serviceaccount dashboard-admin -n kube-system</span><br><span class="line"># 绑定用户为集群管理用户</span><br><span class="line">kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole&#x3D;cluster-admin --serviceaccount&#x3D;kube-system:dashboard-admin</span><br><span class="line"># 获取tocken</span><br><span class="line">kubectl get secret -n kube-system</span><br><span class="line">kubectl describe secret -n kube-system dashboard-admin-token-l7kpn</span><br><span class="line"># 在dashboard后台使用tocken方式登录即可</span><br><span class="line"># 生成kubeconfig文件</span><br><span class="line">DASH_TOCKEN&#x3D;$(kubectl get secret -n kube-system dashboard-admin-token-l7kpn -o jsonpath&#x3D;&#123;.data.token&#125;|base64 -d)</span><br><span class="line">kubectl config set-cluster kubernetes --server&#x3D;192.168.17.129:6443 --kubeconfig&#x3D;&#x2F;root&#x2F;dashbord-admin.conf</span><br><span class="line">kubectl config set-credentials dashboard-admin --token&#x3D;$DASH_TOCKEN --kubeconfig&#x3D;&#x2F;root&#x2F;dashbord-admin.conf</span><br><span class="line">kubectl config set-context dashboard-admin@kubernetes --cluster&#x3D;kubernetes --user&#x3D;dashboard-admin --kubeconfig&#x3D;&#x2F;root&#x2F;dashbord-admin.conf</span><br><span class="line">kubectl config use-context dashboard-admin@kubernetes --kubeconfig&#x3D;&#x2F;root&#x2F;dashbord-admin.conf</span><br><span class="line"># 生成的dashbord-admin.conf即可用于登录dashboard</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>协同过滤和K-Mean算法原理记录</title>
    <url>/2020/08/05/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E5%92%8CK-Mean%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<blockquote>
<p>协同过滤的基本原理的简单介绍<br>k-mean算法的简单介绍</p>
</blockquote>
<span id="more"></span>


<h4 id="根据数据源的不同推荐引擎可以分为三类"><a href="#根据数据源的不同推荐引擎可以分为三类" class="headerlink" title="根据数据源的不同推荐引擎可以分为三类"></a>根据数据源的不同推荐引擎可以分为三类</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.基于人口的统计学推荐</span><br><span class="line">2.基于内容的推荐</span><br><span class="line">3.基于协同过滤的推荐</span><br></pre></td></tr></table></figure>

<h4 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.根据物品或内容的元数据，发现物品或内容的相关行，然后基于用户以前的喜好记录推荐给用户相似的物品</span><br></pre></td></tr></table></figure>

<h3 id="基于协同过滤推荐分为三类"><a href="#基于协同过滤推荐分为三类" class="headerlink" title="基于协同过滤推荐分为三类"></a>基于协同过滤推荐分为三类</h3><h4 id="基于用户的协同过滤推荐"><a href="#基于用户的协同过滤推荐" class="headerlink" title="基于用户的协同过滤推荐"></a>基于用户的协同过滤推荐</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">基于用户的协同过滤推荐算法，先使用统计寻找与目标用户有相同喜好的邻居，然后根据目标用户的邻居的喜好产生向目标用户的推荐。</span><br><span class="line">基本原理：利用用户访问行为的相似性来互相推荐用户可能感兴趣的资源</span><br><span class="line"></span><br><span class="line">例子：</span><br><span class="line">假设用户A喜欢物品A、物品C，用户B喜欢物品B，用户C喜欢物品A，物品C和物品D；从这些用户的历史喜好信息中，发现用户A 和用户C的口味和偏好都是比较类似的，同时用户C还喜欢物品D；那么我们可以推断用户A可能也喜欢物品D，因此可以把物品D推荐给用户A</span><br></pre></td></tr></table></figure>

<h4 id="基于项目的协同过滤推荐"><a href="#基于项目的协同过滤推荐" class="headerlink" title="基于项目的协同过滤推荐"></a>基于项目的协同过滤推荐</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">根据所有用户对物品或者信息的评价，发现物品和物品之间的相似度，然后根据用户的历史偏好信息将类似的物品推荐给该用户</span><br></pre></td></tr></table></figure>

<h4 id="协同过滤小结总结"><a href="#协同过滤小结总结" class="headerlink" title="协同过滤小结总结"></a>协同过滤小结总结</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.基于内容的推荐：只考虑了对象的本身性质，将对象按标签形成集合，如果你消费集合中的一个则想你推荐集合中的其他对象</span><br><span class="line"></span><br><span class="line">2.基于协同过滤的推荐算法：充分利用集体智慧，即在大量的人群行为和数据中收集答案，以帮助我们对整个人群得到统计意义上的结论，推荐的个性化程序高，基于以下两个出发点</span><br><span class="line">1.兴趣相近的用户可能会同样的东西感兴趣</span><br><span class="line">2.用户可能较偏爱与其已购买的东西相类似的商品；考虑进了用户的历史习惯，对象客观上不一定相似，但由于人的行为可能认为其主观上是相似的，就可以产生推荐</span><br></pre></td></tr></table></figure>

<h3 id="k-mean算法"><a href="#k-mean算法" class="headerlink" title="k-mean算法"></a>k-mean算法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k均值聚类算法是一种迭代求解的聚类分析算法，其步骤是：</span><br><span class="line">1.随机选取K个对象作为初始的聚类中心</span><br><span class="line">2.然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给据离它最近的聚类中心</span><br><span class="line">聚类中心以及分配给它们的对象就代表一个聚类</span><br><span class="line">每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算；这个过程将不断重复直到满足某个终止条件</span><br><span class="line">终止条件可以是以下其中一个：</span><br><span class="line">1.没有（或最小数目）对象被重新分配给不同的聚类</span><br><span class="line">2.没有（或最小数目）聚类中心再发生变化  （一般选用这个为终止条件）</span><br><span class="line">3.误差平方和局部最小</span><br><span class="line"></span><br><span class="line">原理：k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则他们的相似性越高，则它们越有可能在同一个类簇</span><br></pre></td></tr></table></figure>

<h4 id="摘取计算试题难度-能力值-回答正确概率的24个能力值"><a href="#摘取计算试题难度-能力值-回答正确概率的24个能力值" class="headerlink" title="摘取计算试题难度-能力值-回答正确概率的24个能力值"></a>摘取计算试题难度-能力值-回答正确概率的24个能力值</h4><p>一维聚类函数，points表示实数域的点集，K表示簇的个数，iterNum表示迭代次数<br>返回值为从小到大排列的中心点，List(中心点值，中心点顺序号)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def oneDimKmeans(points:Seq[Double], k:Int, iterNum:Int ) &#x3D;&#123;</span><br><span class="line">          def doKmeans(points:Array[Vector[Double]],initCenters:Array[Vector[Double]])&#x3D;&#123;</span><br><span class="line">            var centers &#x3D; initCenters</span><br><span class="line">            for(index &lt;- 0 until iterNum)&#123;</span><br><span class="line"></span><br><span class="line">              &#x2F;&#x2F;这里我们根据聚类中心利用groupBy()进行分组，最后得到的cluster是Map(Vector[Double],Array[Vector[Double]])类型</span><br><span class="line">              &#x2F;&#x2F;cluster共k个元素，Map中key值就是聚类中心，Value就是依赖于这个中心的点集</span><br><span class="line">              val cluster &#x3D; points.groupBy &#123; closestCenter(centers,_) &#125;</span><br><span class="line"></span><br><span class="line">              &#x2F;&#x2F;通过Map集合的get()方法取出每一个簇，然后采用匹配方法match()进行求取新的中心，这里再强调一遍，Vector类型是不可更改类型，即数据存入Vector以后就不能改变</span><br><span class="line">              &#x2F;&#x2F;所以需要你人为的定义Vector类型的加减乘除运算</span><br><span class="line">              centers &#x3D; centers.map &#123; oldCenter &#x3D;&gt;</span><br><span class="line">        cluster.get(oldCenter) match&#123;</span><br><span class="line">          case Some(pointsInCluster) &#x3D;&gt;</span><br><span class="line">            vectorDivide(pointsInCluster.reduceLeft(vectorAdd),pointsInCluster.length)</span><br><span class="line">          case None &#x3D;&gt; oldCenter</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    centers</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def vectorDis(v1:Vector[Double],v2:Vector[Double]):Double&#x3D;&#123;</span><br><span class="line">    var distance &#x3D; 0d</span><br><span class="line">    for(i &lt;- v1.indices)&#123;</span><br><span class="line">      distance +&#x3D; (v1(i)-v2(i))*(v1(i)-v2(i))</span><br><span class="line">    &#125;</span><br><span class="line">   math.sqrt(distance)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  def vectorAdd(v1:Vector[Double],v2:Vector[Double])&#x3D;&#123;</span><br><span class="line">    val len&#x3D;v1.length</span><br><span class="line">    val av1&#x3D;v1.toArray</span><br><span class="line">    val av2&#x3D;v2.toArray</span><br><span class="line">    val av3&#x3D;Array.fill(len)(0.0)</span><br><span class="line">    var vector &#x3D; Vector[Double]()</span><br><span class="line">    for(i&lt;-0 until len)&#123;</span><br><span class="line">      av3(i)&#x3D;av1(i)+av2(i)</span><br><span class="line">      vector ++&#x3D; Vector(av3(i))</span><br><span class="line">    &#125;</span><br><span class="line">    vector</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def vectorDivide(v1:Vector[Double],num:Int)&#x3D;&#123;</span><br><span class="line">    val av1&#x3D;v1.toArray</span><br><span class="line">    val len&#x3D;v1.size</span><br><span class="line">    val av2&#x3D;Array.fill(len)(0.0)</span><br><span class="line">    var vector &#x3D; Vector[Double]()</span><br><span class="line">    for(i&lt;-0 until len)&#123;</span><br><span class="line">      av2(i)&#x3D;av1(i)&#x2F;num</span><br><span class="line">      vector ++&#x3D; Vector(av2(i))</span><br><span class="line">    &#125;</span><br><span class="line">    vector</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def closestCenter(centers:Array[Vector[Double]],point:Vector[Double]):Vector[Double]&#x3D;&#123;</span><br><span class="line">    centers.reduceLeft((a, b) &#x3D;&gt;</span><br><span class="line">      if (vectorDis(a,point) &lt; vectorDis(b,point)) a else b</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  val pointArray &#x3D; points.sorted.map(List(_).toVector).toArray</span><br><span class="line">  val initCenters &#x3D;Array.range(1,k+1).toList.map(_*pointArray.length&#x2F;(k+1)).map(pointArray(_)).toArray</span><br><span class="line">  val finalCenter &#x3D;doKmeans(pointArray,initCenters)</span><br><span class="line">  finalCenter.toList.map(x&#x3D;&gt;x(0)).sorted.zipWithIndex</span><br><span class="line">&#125;</span><br><span class="line">sqlContext.udf.register(&quot;oneDimKmeans&quot;, oneDimKmeans _)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>基于CentOS7.6搭建K8S-问题</title>
    <url>/2019/09/23/%E5%9F%BA%E4%BA%8ECentOS7.6%E6%90%AD%E5%BB%BAK8S-%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>一系列的问题</p>
</blockquote>
<span id="more"></span>

<h2 id="CoreDNS解析不了域名"><a href="#CoreDNS解析不了域名" class="headerlink" title="CoreDNS解析不了域名"></a>CoreDNS解析不了域名</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 暂时未解决,官网提供的解决办法失效</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="master节点的Calico-pod启动不来"><a href="#master节点的Calico-pod启动不来" class="headerlink" title="master节点的Calico pod启动不来"></a>master节点的Calico pod启动不来</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 主要原因应该是系统环境问题,初始时可以启动</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>基于CentOS7.6搭建K8S-部署Redis集群篇</title>
    <url>/2019/09/10/%E5%9F%BA%E4%BA%8ECentOS7.6%E6%90%AD%E5%BB%BAK8S-%E9%83%A8%E7%BD%B2Redis%E9%9B%86%E7%BE%A4%E7%AF%87/</url>
    <content><![CDATA[<blockquote>
<p>搭建redis集群</p>
</blockquote>
<span id="more"></span>

<h2 id="安装NFS-共享存储"><a href="#安装NFS-共享存储" class="headerlink" title="安装NFS(共享存储)"></a>安装NFS(共享存储)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 所有节点安装nfs+rpcbind</span><br><span class="line">yum -y install nfs-utils rpcbind</span><br><span class="line"></span><br><span class="line"># node01设置需要共享的路径</span><br><span class="line"># rw表示读写权限</span><br><span class="line"># all_squash表示客户机上的任何用户访问该共享目录时都映射成服务器上的匿名用户(默认为nfsnobody)</span><br><span class="line"># *表示任意主机都可以访问该共享目录,也可以填写指定主机地址,同时支持正则</span><br><span class="line">vi &#x2F;etc&#x2F;exports</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv1 *(rw,all_squash)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv2 *(rw,all_squash)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv3 *(rw,all_squash)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv4 *(rw,all_squash)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv5 *(rw,all_squash)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv6 *(rw,all_squash)</span><br><span class="line"># 创建目录修改权限</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv&#123;1..6&#125;</span><br><span class="line">chmod 777 &#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv&#123;1..6&#125;</span><br><span class="line"># 启动服务</span><br><span class="line">systemctl enable nfs </span><br><span class="line">systemctl enable rpcbind</span><br><span class="line">systemctl start nfs</span><br><span class="line">systemctl start rpcbind</span><br><span class="line"></span><br><span class="line"># 测试一下,在node02上执行挂载</span><br><span class="line">mount -t nfs 192.168.17.129:&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv1 &#x2F;mnt</span><br><span class="line">cd &#x2F;mnt</span><br><span class="line">touch test</span><br><span class="line"># 可以在node01上看到这个文件则表示成功</span><br><span class="line">[root@node01 ~]# ll &#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv1&#x2F;</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r--. 1 nfsnobody nfsnobody 0 Sep 10 15:03 test</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="创建PV"><a href="#创建PV" class="headerlink" title="创建PV"></a>创建PV</h2><pre><code>供pvc挂载使用,每一个Redis Pod都需要一个独立的PV来存储自己的数据</code></pre>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi redis_pv.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pv1</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M      #磁盘大小200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany    #多客户可读写</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.17.129</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv1&quot;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-vp2</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.17.129</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv2&quot;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pv3</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.17.129</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv3&quot;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pv4</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.17.129</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv4&quot;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pv5</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.17.129</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv5&quot;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pv6</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.17.129</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv6&quot;</span><br><span class="line"></span><br><span class="line"># 字段说明</span><br><span class="line">apiversion: api版本</span><br><span class="line">kind: 这个yaml是生成pv的</span><br><span class="line">metadata: 元数据</span><br><span class="line">spec.capacity: 进行资源限制的</span><br><span class="line">spec.accessmodes: 访问模式(读写模式)</span><br><span class="line">spec.nfs: 这个pv卷名是通过nfs提供的</span><br><span class="line"></span><br><span class="line"># 创建</span><br><span class="line">kubectl apply -f redis_pv.yaml</span><br><span class="line">kubectl get pv # 查看PV</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="创建ConfigMap-redis-conf文件不要有注释"><a href="#创建ConfigMap-redis-conf文件不要有注释" class="headerlink" title="创建ConfigMap(redis.conf文件不要有注释)"></a>创建ConfigMap(redis.conf文件不要有注释)</h2><pre><code>用来存放redis的配置文件,方便后期修改</code></pre>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi redis.conf</span><br><span class="line">appendonly yes # 开启Redis的AOF持久化</span><br><span class="line">cluster-enabled yes # 集群模式打开</span><br><span class="line">cluster-config-file &#x2F;var&#x2F;lib&#x2F;redis&#x2F;nodes.conf # 保存节点配置文件的路径</span><br><span class="line">cluster-node-timeout 5000 # 节点超时时间</span><br><span class="line">dir &#x2F;var&#x2F;lib&#x2F;redis # AOF持久化文件存在的位置</span><br><span class="line">port 6379 # 开启的端口</span><br><span class="line"></span><br><span class="line"># 创建名为redis-conf的ConfigMap</span><br><span class="line">kubectl create configmap redis-conf --from-file&#x3D;redis.conf</span><br><span class="line">kubectl describe cm redis-conf # 查看</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="创建Headless-Service"><a href="#创建Headless-Service" class="headerlink" title="创建Headless Service"></a>创建Headless Service</h2><pre><code>Headless Service是StatefulSet实现稳定网络标识的基础</code></pre>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi headless-service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-service</span><br><span class="line">  labels:</span><br><span class="line">    app: redis</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: redis-port</span><br><span class="line">    port: 6379</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: redis</span><br><span class="line">    appCluster: redis-cluster</span><br><span class="line">    </span><br><span class="line"># 创建</span><br><span class="line">kubectl apply -f headless-service.yaml</span><br><span class="line">kubectl get service redis-service # 查看</span><br><span class="line"># CLUSTER-IP为None,无头服务</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="创建Redis集群节点"><a href="#创建Redis集群节点" class="headerlink" title="创建Redis集群节点"></a>创建Redis集群节点</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi redis.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1beta1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-app</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;redis-service&quot;</span><br><span class="line">  replicas: 6</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: redis</span><br><span class="line">        appCluster: redis-cluster</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 20</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">          - weight: 100</span><br><span class="line">            podAffinityTerm:</span><br><span class="line">              labelSelector:</span><br><span class="line">                matchExpressions:</span><br><span class="line">                - key: app</span><br><span class="line">                  operator: In</span><br><span class="line">                  values:</span><br><span class="line">                  - redis</span><br><span class="line">              topologyKey: kubernetes.io&#x2F;hostname</span><br><span class="line">      containers:</span><br><span class="line">      - name: redis</span><br><span class="line">        image: &quot;redis&quot;</span><br><span class="line">        command:</span><br><span class="line">          - &quot;redis-server&quot;                  #redis启动命令</span><br><span class="line">        args:</span><br><span class="line">          - &quot;&#x2F;etc&#x2F;redis&#x2F;redis.conf&quot;         #redis-server后面跟的参数,换行代表空格</span><br><span class="line">          - &quot;--protected-mode&quot;              #允许外网访问</span><br><span class="line">          - &quot;no&quot;</span><br><span class="line">        # command: redis-server &#x2F;etc&#x2F;redis&#x2F;redis.conf --protected-mode no</span><br><span class="line">        resources:                          #资源</span><br><span class="line">          requests:                         #请求的资源</span><br><span class="line">            cpu: &quot;100m&quot;                     #m代表千分之,相当于0.1 个cpu资源</span><br><span class="line">            memory: &quot;100Mi&quot;                 #内存100m大小</span><br><span class="line">        ports:</span><br><span class="line">            - name: redis</span><br><span class="line">              containerPort: 6379</span><br><span class="line">              protocol: &quot;TCP&quot;</span><br><span class="line">            - name: cluster</span><br><span class="line">              containerPort: 16379</span><br><span class="line">              protocol: &quot;TCP&quot;</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: &quot;redis-conf&quot;              #挂载configmap生成的文件</span><br><span class="line">            mountPath: &quot;&#x2F;etc&#x2F;redis&quot;         #挂载到哪个路径下</span><br><span class="line">          - name: &quot;redis-data&quot;              #挂载持久卷的路径</span><br><span class="line">            mountPath: &quot;&#x2F;var&#x2F;lib&#x2F;redis&quot;</span><br><span class="line">      volumes:</span><br><span class="line">      - name: &quot;redis-conf&quot;                  #引用configMap卷</span><br><span class="line">        configMap:</span><br><span class="line">          name: &quot;redis-conf&quot;</span><br><span class="line">          items:</span><br><span class="line">            - key: &quot;redis.conf&quot;             #创建configMap指定的名称</span><br><span class="line">              path: &quot;redis.conf&quot;            #里面的那个文件--from-file参数后面的文件</span><br><span class="line">  volumeClaimTemplates:                     #进行pvc持久卷声明,</span><br><span class="line">  - metadata:</span><br><span class="line">      name: redis-data</span><br><span class="line">    spec:</span><br><span class="line">      accessModes:</span><br><span class="line">      - ReadWriteMany</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 200M</span><br><span class="line"></span><br><span class="line"># 创建</span><br><span class="line">kubectl apply -f redis.yaml</span><br><span class="line">kubectl get pods -o wide # 查看</span><br><span class="line"># 可以看到这些Pods在部署时是以&#123;0..N-1&#125;的顺序依次创建的,当redis-app-0启动后达到Running状态后,才会创建redis-app-1</span><br><span class="line"># 同时每个Pod都会得到集群内的一个DNS域名,格式为$(podname).$(service name).$(namespace).svc.cluster.local</span><br><span class="line">redis-app-0.redis-service.default.svc.cluster.local</span><br><span class="line">redis-app-1.redis-service.default.svc.cluster.local</span><br><span class="line">...以此类推...</span><br><span class="line"></span><br><span class="line"># 宿主机的DNS解析</span><br><span class="line">vi &#x2F;etc&#x2F;resolv.conf</span><br><span class="line">search localdomain default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">nameserver 192.168.17.2</span><br><span class="line"></span><br><span class="line"># 在K8S集群内部,这些Pod就可以利用该域名互相通信,我们可以使用busybox镜像的nslookup检验这些域名</span><br><span class="line">kubectl run -i --tty --image busybox dns-test --restart&#x3D;Never --rm &#x2F;bin&#x2F;sh</span><br><span class="line">nslookup redis-app-1.redis-service.default.svc.cluster.local</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="初始化Redis集群"><a href="#初始化Redis集群" class="headerlink" title="初始化Redis集群"></a>初始化Redis集群</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建centos容器</span><br><span class="line">kubectl run -i --tty centos --image&#x3D;centos --restart&#x3D;Never &#x2F;bin&#x2F;bash</span><br><span class="line">cat &gt;&gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo&lt;&lt;&#39;EOF&#39;</span><br><span class="line">[epel]</span><br><span class="line">name&#x3D;Extra Packages for Enterprise Linux 7 - $basearch</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;epel&#x2F;7&#x2F;$basearch</span><br><span class="line">#mirrorlist&#x3D;https:&#x2F;&#x2F;mirrors.fedoraproject.org&#x2F;metalink?repo&#x3D;epel-7&amp;arch&#x3D;$basearch</span><br><span class="line">failovermethod&#x3D;priority</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">gpgkey&#x3D;file:&#x2F;&#x2F;&#x2F;etc&#x2F;pki&#x2F;rpm-gpg&#x2F;RPM-GPG-KEY-EPEL-7</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 初始化redis集群</span><br><span class="line">yum -y install redis-trib.noarch bind-utils</span><br><span class="line">redis-trib create --replicas 1 \</span><br><span class="line">&#96;dig +short redis-app-0.redis-service.default.svc.cluster.local&#96;:6379 \</span><br><span class="line">&#96;dig +short redis-app-1.redis-service.default.svc.cluster.local&#96;:6379 \</span><br><span class="line">&#96;dig +short redis-app-2.redis-service.default.svc.cluster.local&#96;:6379 \</span><br><span class="line">&#96;dig +short redis-app-3.redis-service.default.svc.cluster.local&#96;:6379 \</span><br><span class="line">&#96;dig +short redis-app-4.redis-service.default.svc.cluster.local&#96;:6379 \</span><br><span class="line">&#96;dig +short redis-app-5.redis-service.default.svc.cluster.local&#96;:6379</span><br><span class="line"></span><br><span class="line"># 进入redis pod检验</span><br><span class="line"> kubectl exec -it redis-app-0 &#x2F;bin&#x2F;bash</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;bin&#x2F;redis-cli -c</span><br><span class="line">cluster info</span><br><span class="line">cluster nodes</span><br><span class="line"></span><br><span class="line"># 也可以在NFS查看Redis</span><br><span class="line">tree &#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="创建Service"><a href="#创建Service" class="headerlink" title="创建Service"></a>创建Service</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 用于Redis集群访问与负载均衡</span><br><span class="line">piVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: redis-access-service</span><br><span class="line">  labels:</span><br><span class="line">    app: redis</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: redis-port</span><br><span class="line">    protocol: &quot;TCP&quot;</span><br><span class="line">    port: 6379</span><br><span class="line">    targetPort: 6379</span><br><span class="line">  selector:</span><br><span class="line">    app: redis</span><br><span class="line">    appCluster: redis-cluster</span><br><span class="line">    </span><br><span class="line">kubectl get svc redis-access-service -o wide</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="测试主从切换"><a href="#测试主从切换" class="headerlink" title="测试主从切换"></a>测试主从切换</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 找到一个master,查看他的slave</span><br><span class="line">kubectl exec -it redis-app-2 &#x2F;bin&#x2F;bash</span><br><span class="line">redis-cli</span><br><span class="line">role</span><br><span class="line"></span><br><span class="line"># 删除redis-app-2, IP会切换成salve的IP</span><br><span class="line">kubectl delete pods redis-app-2</span><br><span class="line">kubectl exec -it redis-app-2 &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="动态扩容"><a href="#动态扩容" class="headerlink" title="动态扩容"></a>动态扩容</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 添加NFS共享目录</span><br><span class="line">cat &gt;&gt; &#x2F;etc&#x2F;exports &lt;&lt;&#39;EOF&#39;</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv5 *(rw,all_squash)</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv6 *(rw,all_squash)</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">systemctl restart nfs rpcbind</span><br><span class="line">mkdir &#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv&#123;7..8&#125;</span><br><span class="line">chmod 777 &#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;*</span><br><span class="line"></span><br><span class="line"># 添加PV</span><br><span class="line">vi pv.yml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pv7</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.1.253</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv7&quot;</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-pv8</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 200M</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 192.168.1.253</span><br><span class="line">    path: &quot;&#x2F;usr&#x2F;local&#x2F;kubernetes&#x2F;redis&#x2F;pv8&quot;</span><br><span class="line">kubectl apply -f pv.yml</span><br><span class="line">kubectl get pv</span><br><span class="line"></span><br><span class="line"># 添加redis节点</span><br><span class="line"># 更改redis的yml文件里面的replicas:字段,把这个字段改为8,然后升级运行</span><br><span class="line">kubectl apply -f redis.yml</span><br><span class="line">kubectl get  pods</span><br><span class="line"></span><br><span class="line"># 添加集群节点</span><br><span class="line">kubectl exec -it centos &#x2F;bin&#x2F;bash</span><br><span class="line">redis-trib add-node \</span><br><span class="line">&#96;dig +short redis-app-6.redis-service.default.svc.cluster.local&#96;:6379 \</span><br><span class="line">&#96;dig +short redis-app-0.redis-service.default.svc.cluster.local&#96;:6379</span><br><span class="line"></span><br><span class="line">redis-trib add-node \</span><br><span class="line">&#96;dig +short redis-app-7.redis-service.default.svc.cluster.local&#96;:6379 \</span><br><span class="line">&#96;dig +short redis-app-0.redis-service.default.svc.cluster.local&#96;:6379</span><br><span class="line"></span><br><span class="line"># 重新分配哈希槽</span><br><span class="line">redis-trib.rb reshard &#96;dig +short redis-app-0.redis-service.default.svc.cluster.local&#96;:6379</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>基于CentOS7.6搭建K8S-集群搭建篇</title>
    <url>/2019/09/04/%E5%9F%BA%E4%BA%8ECentOS7.6%E6%90%AD%E5%BB%BAK8S-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E7%AF%87/</url>
    <content><![CDATA[<blockquote>
<p>针对内网机器搭建K8S集群,先联网机器将依赖之类的进行downloadonly,然后将依赖打包拷贝到不能联网机器<br>离线环境需要准备CentOS7.6的DVD版的ISO文件</p>
</blockquote>
<span id="more"></span>

<h2 id="节点内容"><a href="#节点内容" class="headerlink" title="节点内容"></a>节点内容</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 每个节点都有docker+k8s,并且k8s所需镜像都得有</span><br><span class="line">k8s.gcr.io&#x2F;kube-proxy</span><br><span class="line">k8s.gcr.io&#x2F;kube-apiserver</span><br><span class="line">k8s.gcr.io&#x2F;kube-scheduler</span><br><span class="line">k8s.gcr.io&#x2F;kube-controller-manager</span><br><span class="line">k8s.gcr.io&#x2F;coredns</span><br><span class="line">k8s.gcr.io&#x2F;etcd</span><br><span class="line">k8s.gcr.io&#x2F;pause</span><br><span class="line"># 每个节点都需要网络镜像</span><br><span class="line">calico&#x2F;node</span><br><span class="line">calico&#x2F;cni</span><br><span class="line">calico&#x2F;pod2daemon-flexvol</span><br><span class="line">calico&#x2F;kube-controllers </span><br></pre></td></tr></table></figure>

<hr>
<h2 id="节点准备"><a href="#节点准备" class="headerlink" title="节点准备"></a>节点准备</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># CentOS7.6三台</span><br><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33</span><br><span class="line">onboot&#x3D;yes</span><br><span class="line">service network restart</span><br><span class="line">ip addr</span><br><span class="line"></span><br><span class="line">192.168.17.129</span><br><span class="line">192.168.17.130</span><br><span class="line">192.168.17.131</span><br><span class="line"></span><br><span class="line"># 配置阿里yum源--联网操作</span><br><span class="line">[base]</span><br><span class="line">name&#x3D;CentOS-$releasever - Base - mirrors.aliyun.com</span><br><span class="line">failovermethod&#x3D;priority</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;$releasever&#x2F;os&#x2F;$basearch&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;RPM-GPG-KEY-CentOS-7</span><br><span class="line"> </span><br><span class="line">#released updates </span><br><span class="line">[updates]</span><br><span class="line">name&#x3D;CentOS-$releasever - Updates - mirrors.aliyun.com</span><br><span class="line">failovermethod&#x3D;priority</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;$releasever&#x2F;updates&#x2F;$basearch&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;RPM-GPG-KEY-CentOS-7</span><br><span class="line"> </span><br><span class="line">#additional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name&#x3D;CentOS-$releasever - Extras - mirrors.aliyun.com</span><br><span class="line">failovermethod&#x3D;priority</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;$releasever&#x2F;extras&#x2F;$basearch&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;RPM-GPG-KEY-CentOS-7</span><br><span class="line"> </span><br><span class="line">#additional packages that extend functionality of existing packages</span><br><span class="line">[centosplus]</span><br><span class="line">name&#x3D;CentOS-$releasever - Plus - mirrors.aliyun.com</span><br><span class="line">failovermethod&#x3D;priority</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;$releasever&#x2F;centosplus&#x2F;$basearch&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">enabled&#x3D;0</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;RPM-GPG-KEY-CentOS-7</span><br><span class="line"> </span><br><span class="line">#contrib - packages by Centos Users</span><br><span class="line">[contrib]</span><br><span class="line">name&#x3D;CentOS-$releasever - Contrib - mirrors.aliyun.com</span><br><span class="line">failovermethod&#x3D;priority</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;$releasever&#x2F;contrib&#x2F;$basearch&#x2F;</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">enabled&#x3D;0</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;centos&#x2F;RPM-GPG-KEY-CentOS-7</span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line"></span><br><span class="line"># 关闭防火墙</span><br><span class="line">systemctl stop firewalld &amp; systemctl disable firewalld</span><br><span class="line"></span><br><span class="line"># 关闭Swap</span><br><span class="line">swapoff -a #临时关闭</span><br><span class="line">sed -i &#39;&#x2F; swap &#x2F; s&#x2F;^&#x2F;#&#x2F;&#39; &#x2F;etc&#x2F;fstab #永久关闭</span><br><span class="line"></span><br><span class="line"># 关闭Selinux</span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;root&#x2F;yum</span><br><span class="line">yum install --downloadonly --downloaddir&#x3D;&#x2F;root&#x2F;yum&#x2F; yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum install yum-utils device-mapper-persistent-data lvm2</span><br><span class="line"># 添加仓库--联网操作</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line"># 安装</span><br><span class="line">yum install --downloadonly --downloaddir&#x3D;&#x2F;root&#x2F;yum&#x2F; docker-ce</span><br><span class="line">yum install  docker-ce</span><br><span class="line"># 启动docker服务并开机启动</span><br><span class="line">systemctl start docker &amp; systemctl enable docker</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure>

<h2 id="安装Kubernetes"><a href="#安装Kubernetes" class="headerlink" title="安装Kubernetes"></a>安装Kubernetes</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 配置yum源</span><br><span class="line">vi &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">repo_gpgcheck&#x3D;0</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg</span><br><span class="line">        http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line"></span><br><span class="line">yum install --downloadonly --downloaddir&#x3D;&#x2F;root&#x2F;yum&#x2F; kubelet kubeadm kubectl</span><br><span class="line">yum install kubelet kubeadm kubectl</span><br><span class="line"></span><br><span class="line"># 启动kubelet</span><br><span class="line">systemctl enable kubelet</span><br><span class="line"></span><br><span class="line"># 配置kubelet的cgroup drive</span><br><span class="line">docker info | grep -i Cgroup</span><br><span class="line">vi &#x2F;etc&#x2F;docker&#x2F;daemon.json #手动创建</span><br><span class="line">&#123;</span><br><span class="line">    &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;]</span><br><span class="line">&#125;</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 初始化Master(需要翻墙拉取镜像)</span><br><span class="line">kubeadm init</span><br><span class="line"></span><br><span class="line"># kubeadm的配置文件</span><br><span class="line">&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kubelet.service.d&#x2F;10-kubeadm.conf</span><br><span class="line"></span><br><span class="line"># 修改配置文件需要重启kubelet</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br><span class="line"></span><br><span class="line"># 查看需要用到的镜像&lt;不需要翻墙的做法&gt;</span><br><span class="line">kubeadm config images list</span><br><span class="line"># 拉取镜像</span><br><span class="line">kubeadm config images list |sed -e &#39;s&#x2F;^&#x2F;docker pull &#x2F;g&#39; -e &#39;s#k8s.gcr.io#mirrorgooglecontainers#g&#39; |sh -x</span><br><span class="line"># kube-apiserver拉取失败</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;kube-apiserver:v1.15.3</span><br><span class="line"># coredns需要从其他仓库下载</span><br><span class="line">docker pull coredns&#x2F;coredns:1.3.1</span><br><span class="line"># 修改tag,将镜像标记为k8s.gcr.io</span><br><span class="line">docker images |grep mirrorgooglecontainers |awk &#39;&#123;print &quot;docker tag &quot;,$1&quot;:&quot;$2,$1&quot;:&quot;$2&#125;&#39; |sed -e &#39;s#mirrorgooglecontainers#k8s.gcr.io#2&#39; |sh -x</span><br><span class="line">docker tag coredns&#x2F;coredns:1.3.1 k8s.gcr.io&#x2F;coredns:1.3.1</span><br><span class="line">docker tag registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;kube-apiserver:v1.15.3 k8s.gcr.io&#x2F;kube-apiserver:v1.15.3</span><br><span class="line"># 删除没用的镜像</span><br><span class="line">docker images | grep mirrorgooglecontainers | awk &#39;&#123;print &quot;docker rmi &quot;  $1&quot;:&quot;$2&#125;&#39; | sh -x</span><br><span class="line">docker rmi registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;kube-apiserver:v1.15.3 </span><br><span class="line">docker rmi coredns&#x2F;coredns:1.3.1</span><br><span class="line"># 查看已有镜像</span><br><span class="line">docker images</span><br><span class="line"># 初始化指定k8s版本</span><br><span class="line">kubeadm init --kubernetes-version&#x3D;1.15.3</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="集群启动"><a href="#集群启动" class="headerlink" title="集群启动"></a>集群启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 需要创建另一个网卡,VMware直接手动添加一个网络适配器</span><br><span class="line">ip a</span><br><span class="line">cd &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;</span><br><span class="line">cp .&#x2F;ifcfg-ens33 .&#x2F;ifcfg-ens37</span><br><span class="line">vi ifcfg-ens37</span><br><span class="line"># Master(node01)启动</span><br><span class="line">kubeadm init --pod-network-cidr&#x3D;192.168.0.0&#x2F;16 --kubernetes-version&#x3D;v1.15.3 --apiserver-advertise-address&#x3D;192.168.17.132</span><br><span class="line"></span><br><span class="line"># 执行完上述命令后,会有提示,根据提示执行命令,同时记住join命令</span><br><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line"></span><br><span class="line"># 创建网络需要引入docker镜像,如果需要在内网部署,则提前save好镜像文件,版本与calico.yaml内版本一致</span><br><span class="line">calico&#x2F;node</span><br><span class="line">calico&#x2F;cni</span><br><span class="line">calico&#x2F;kube-controllers</span><br><span class="line">calico&#x2F;pod2daemon-flexvol</span><br><span class="line"></span><br><span class="line"># 创建网络(calico.yaml可以下载下来,链接可能更新,可以在官网自行找到)</span><br><span class="line">kubectl apply -f https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.8&#x2F;manifests&#x2F;calico.yaml</span><br><span class="line"></span><br><span class="line"># 加入集群</span><br><span class="line">kubeadm join 192.168.17.129:6443 --token jt6t8v.p7btvogl1l0wivga \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:a2ee7cf619eb081474fcf295a4da7491afd75b3d913763888f5f01226ca80e90 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 重启&#x2F;删除集群(删除.kube很重要)</span><br><span class="line">kubeadm reset</span><br><span class="line">rm -rf $HOME&#x2F;.kube</span><br><span class="line"># 其余节点删除</span><br><span class="line">rm -rf &#x2F;etc&#x2F;cni&#x2F;*</span><br><span class="line">rm -rf &#x2F;etc&#x2F;kubernetes&#x2F;*</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;*</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;*</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;cni&#x2F;*</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;calico&#x2F;*</span><br><span class="line">kubeadm init --pod-network-cidr&#x3D;192.168.0.0&#x2F;16 --kubernetes-version&#x3D;v1.15.3 --apiserver-advertise-address&#x3D;192.168.17.129</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="设置集群"><a href="#设置集群" class="headerlink" title="设置集群"></a>设置集群</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将Master作为工作节点</span><br><span class="line">kubectl taint nodes --all node-role.kubernetes.io&#x2F;master-</span><br><span class="line"></span><br><span class="line"># 将其他节点加入集群(其他节点执行join命令即可)</span><br><span class="line">kubeadm join 192.168.17.129:6443 --token jt6t8v.p7btvogl1l0wivga \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:a2ee7cf619eb081474fcf295a4da7491afd75b3d913763888f5f01226ca80e90 </span><br><span class="line"></span><br><span class="line"># 查看节点</span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line"># 查看pod状态</span><br><span class="line">kubectl get pod -n kube-system</span><br><span class="line"></span><br><span class="line"># 查看所有pod状态</span><br><span class="line">kubectl get pods -n kube-system</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="kubeadm-init参数"><a href="#kubeadm-init参数" class="headerlink" title="kubeadm init参数"></a>kubeadm init参数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--apiserver-advertise-address string</span><br><span class="line">API Server将要广播的监听地址。如指定为 &#96;0.0.0.0&#96; 将使用缺省的网卡地址。</span><br><span class="line"></span><br><span class="line">--apiserver-bind-port int32     缺省值: 6443</span><br><span class="line">API Server绑定的端口</span><br><span class="line"></span><br><span class="line">--apiserver-cert-extra-sans stringSlice</span><br><span class="line">可选的额外提供的证书主题别名（SANs）用于指定API Server的服务器证书。可以是IP地址也可以是DNS名称。</span><br><span class="line"></span><br><span class="line">--cert-dir string     缺省值: &quot;&#x2F;etc&#x2F;kubernetes&#x2F;pki&quot;</span><br><span class="line">证书的存储路径。</span><br><span class="line"></span><br><span class="line">--config string</span><br><span class="line">kubeadm配置文件的路径。警告：配置文件的功能是实验性的。</span><br><span class="line"></span><br><span class="line">--cri-socket string     缺省值: &quot;&#x2F;var&#x2F;run&#x2F;dockershim.sock&quot;</span><br><span class="line">指明要连接的CRI socket文件</span><br><span class="line"></span><br><span class="line">--dry-run</span><br><span class="line">不会应用任何改变；只会输出将要执行的操作。</span><br><span class="line"></span><br><span class="line">--feature-gates string</span><br><span class="line">键值对的集合，用来控制各种功能的开关。可选项有:</span><br><span class="line">Auditing&#x3D;true|false (当前为ALPHA状态 - 缺省值&#x3D;false)</span><br><span class="line">CoreDNS&#x3D;true|false (缺省值&#x3D;true)</span><br><span class="line">DynamicKubeletConfig&#x3D;true|false (当前为BETA状态 - 缺省值&#x3D;false)</span><br><span class="line"></span><br><span class="line">-h, --help</span><br><span class="line">获取init命令的帮助信息</span><br><span class="line"></span><br><span class="line">--ignore-preflight-errors stringSlice</span><br><span class="line">忽视检查项错误列表，列表中的每一个检查项如发生错误将被展示输出为警告，而非错误。 例如: &#39;IsPrivilegedUser,Swap&#39;. 如填写为 &#39;all&#39; 则将忽视所有的检查项错误。</span><br><span class="line"></span><br><span class="line">--kubernetes-version string     缺省值: &quot;stable-1&quot;</span><br><span class="line">为control plane选择一个特定的Kubernetes版本。</span><br><span class="line"></span><br><span class="line">--node-name string</span><br><span class="line">指定节点的名称。</span><br><span class="line"></span><br><span class="line">--pod-network-cidr string</span><br><span class="line">指明pod网络可以使用的IP地址段。 如果设置了这个参数，control plane将会为每一个节点自动分配CIDRs。</span><br><span class="line"></span><br><span class="line">--service-cidr string     缺省值: &quot;10.96.0.0&#x2F;12&quot;</span><br><span class="line">为service的虚拟IP地址另外指定IP地址段</span><br><span class="line"></span><br><span class="line">--service-dns-domain string     缺省值: &quot;cluster.local&quot;</span><br><span class="line">为services另外指定域名, 例如： &quot;myorg.internal&quot;.</span><br><span class="line"></span><br><span class="line">--skip-token-print</span><br><span class="line">不打印出由 &#96;kubeadm init&#96; 命令生成的默认令牌。</span><br><span class="line"></span><br><span class="line">--token string</span><br><span class="line">这个令牌用于建立主从节点间的双向受信链接。格式为 [a-z0-9]&#123;6&#125;\.[a-z0-9]&#123;16&#125; - 示例： abcdef.0123456789abcdef</span><br><span class="line"></span><br><span class="line">--token-ttl duration     缺省值: 24h0m0s</span><br><span class="line">令牌被自动删除前的可用时长 (示例： 1s, 2m, 3h). 如果设置为 &#39;0&#39;, 令牌将永不过期。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="镜像源"><a href="#镜像源" class="headerlink" title="镜像源"></a>镜像源</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 微软google gcr镜像源</span><br><span class="line">#以gcr镜像为例，以下镜像无法直接拉取</span><br><span class="line">docker pull gcr.io&#x2F;google-containers&#x2F;kube-apiserver:v1.15.2</span><br><span class="line">#改为以下方式即可成功拉取：</span><br><span class="line">docker pull gcr.azk8s.cn&#x2F;google-containers&#x2F;kube-apiserver:v1.15.2</span><br><span class="line"></span><br><span class="line"># 微软coreos quay镜像源</span><br><span class="line">#以coreos镜像为例，以下镜像无法直接拉取</span><br><span class="line">docker pull quay.io&#x2F;coreos&#x2F;kube-state-metrics:v1.7.2</span><br><span class="line">#改为以下方式即可成功拉取：</span><br><span class="line">docker pull quay.azk8s.cn&#x2F;coreos&#x2F;kube-state-metrics:v1.7.2</span><br><span class="line"></span><br><span class="line"># 微软dockerhub镜像源</span><br><span class="line">#以下方式拉取镜像较慢</span><br><span class="line">docker pull centos</span><br><span class="line">#改为以下方式使用微软镜像源：</span><br><span class="line">docker pull dockerhub.azk8s.cn&#x2F;library&#x2F;centos</span><br><span class="line">docker pull dockerhub.azk8s.cn&#x2F;willdockerhub&#x2F;centos</span><br><span class="line"></span><br><span class="line"># dockerhub google镜像源</span><br><span class="line">#以gcr镜像为例，以下镜像无法直接拉取</span><br><span class="line">docker pull gcr.io&#x2F;google-containers&#x2F;kube-apiserver:v1.15.2</span><br><span class="line">#改为以下方式即可成功拉取：</span><br><span class="line">docker pull mirrorgooglecontainers&#x2F;google-containers&#x2F;kube-apiserver:v1.15.2</span><br><span class="line"></span><br><span class="line"># 阿里云google镜像源</span><br><span class="line">#以gcr镜像为例，以下镜像无法直接拉取</span><br><span class="line">docker pull gcr.io&#x2F;google-containers&#x2F;kube-apiserver:v1.15.2</span><br><span class="line">#改为以下方式即可成功拉取：</span><br><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;kube-apiserver:v1.15.2</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="离线包制作"><a href="#离线包制作" class="headerlink" title="离线包制作"></a>离线包制作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 将上述操作产生的&#x2F;root&#x2F;yum文件打包,在内网机器进行createrepo</span><br><span class="line">tar -zcvf yum.tgz &#x2F;root&#x2F;yum</span><br><span class="line"># 内网机器操作</span><br><span class="line">tar -zxvf yum.tgz</span><br><span class="line">createrepo &#x2F;root&#x2F;yum</span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line"># 进行安装操作就行</span><br><span class="line"></span><br><span class="line"># kubeadm所需要的镜像直接进行save,load</span><br><span class="line">docker save -o kube-apiserver.tar kube-apiserver</span><br><span class="line">docker load -i kube-apiserver.tar</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>基于FastDFS实现类OSS以及扩展</title>
    <url>/2019/06/13/%E5%9F%BA%E4%BA%8EFastDFS%E5%AE%9E%E7%8E%B0%E7%B1%BBOSS%E4%BB%A5%E5%8F%8A%E6%89%A9%E5%B1%95/</url>
    <content><![CDATA[<blockquote>
<p>首先，FastDFS肯定没有OSS功能强大以及齐全，碍于成本问题，只能寻找一个类似OSS功能的替代品，所以需要考虑到以后的一些问题。</p>
</blockquote>
<span id="more"></span>

<h2 id="一、未来考虑"><a href="#一、未来考虑" class="headerlink" title="一、未来考虑"></a>一、未来考虑</h2><h3 id="1-FastDFS能否实现OSS的部分功能"><a href="#1-FastDFS能否实现OSS的部分功能" class="headerlink" title="1.FastDFS能否实现OSS的部分功能"></a>1.FastDFS能否实现OSS的部分功能</h3><p><strong>基本能实现,以下是FastDFS,hdfs,OSS的区别</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FastDFS:</span><br><span class="line">是一个开源的分布式文件系统，它对文件进行管理</span><br><span class="line">功能包括:文件存储、文件同步、文件访问(文件上传、文件下载)等，解决了大容量存储和负载均衡的问题</span><br><span class="line">特别适合以文件为载体的在线服务， 如相册网站，视频网站等等</span><br><span class="line"></span><br><span class="line">HDFS: </span><br><span class="line">默认的最基本的存储单位是64M的数据块。</span><br><span class="line">和普通的块文件系统相同的是，HDFS中的文件是被分成64M一块的数据块存储的。</span><br><span class="line">不同于普通文件系统的是，HDFS中，如果一个文件小于一个数据块的大小，会占用整个存储块的空间。</span><br><span class="line"></span><br><span class="line">OSS:</span><br><span class="line">阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务;</span><br><span class="line">可以使用阿里云提供的 API、SDK 接口或者 OSS 迁移工具轻松地将海量数据移入或移出阿里云 OSS。</span><br><span class="line">数据存储到阿里云 OSS 以后，您可以选择标准类型（Standard）的阿里云 OSS 服务作为移动应用、大型网站、图片分享或热点音视频的主要存储方式</span><br><span class="line">也可以选择成本更低、存储期限更长的低频访问类型（Infrequent Access）和归档类型（Archive）的阿里云 OSS 服务作为不经常访问数据的备份和归档。</span><br></pre></td></tr></table></figure>

<p>HDFS和FastDFS的上传,下载效率对比测试结果:<br><a href="https://blog.csdn.net/a519781181/article/details/79125379/">https://blog.csdn.net/a519781181/article/details/79125379/</a></p>
<p>使用OSS有什么优势，又有什么缺点，什么情况下我们会使用OSS来代替HDFS，到底使用什么存储方案:<br><a href="https://yq.aliyun.com/articles/371984">https://yq.aliyun.com/articles/371984</a></p>
<p>oss上传方式,以及对应的大小限制:<br><a href="http://www.360doc.com/content/18/0823/22/49604565_780716594.shtml">http://www.360doc.com/content/18/0823/22/49604565_780716594.shtml</a></p>
<hr>
<h3 id="2-FastDFS文件上传后文件名如何保存"><a href="#2-FastDFS文件上传后文件名如何保存" class="headerlink" title="2.FastDFS文件上传后文件名如何保存"></a>2.FastDFS文件上传后文件名如何保存</h3><p><strong>上传文件的流程</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">客户端上传文件后存储服务器将文件ID返回给客户端，次文件ID用于以后访问改文件的索引信息。文件索引信息包括：</span><br><span class="line">    组名，虚拟磁盘路径，数据两级目录，文件名。</span><br><span class="line">    比如：group&#x2F;M00&#x2F;02&#x2F;44&#x2F;wKqDrE348waaaaaaaaGkEIYJK42378.sh</span><br><span class="line">组名：文件上传后所在的storage组名称，在文件上传成功后有storage服务器返回，需要客户端自行保存。</span><br><span class="line">虚拟磁盘路径：storage配置的虚拟路径，与磁盘选项store_path对应。如果配置了store_path0则是M00，如果配置了store_path1则是M01，以此类推。</span><br><span class="line">数据两级目录：storage服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据文件。</span><br><span class="line">文件名：与文件上传时不同。是由存储服务器根据特定信息生成，文件名包含：源存储服务器IP地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。</span><br></pre></td></tr></table></figure>
<p><strong>上传步骤</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">加载配置文件，配置文件中的内容就是tracker服务的地址。</span><br><span class="line">配置文件内容：tracker_server&#x3D;192.168.25.121:22122</span><br><span class="line">创建一个TrackerClient对象。直接new一个。</span><br><span class="line">使用TrackerClient对象创建连接，获得一个TrackerServer对象。</span><br><span class="line">创建一个StorageServer的引用，值为null</span><br><span class="line">创建一个StorageClient对象，需要两个参数TrackerServer对象、StorageServer的引用</span><br><span class="line">使用StorageClient对象上传图片。</span><br><span class="line">返回数组。包含组名和图片的路径。</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="3-Hadoop生态体系能否像使用OSS一样使用FastDFS"><a href="#3-Hadoop生态体系能否像使用OSS一样使用FastDFS" class="headerlink" title="3.Hadoop生态体系能否像使用OSS一样使用FastDFS"></a>3.Hadoop生态体系能否像使用OSS一样使用FastDFS</h3><p><strong>可能有点悬,原因如下</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">oss是阿里收费的服务,而且是专门提供云对象的服务,设计之初就是提供给hadoop生态体系的;而fastdfs是开源的分布式文件系统;fastdfs存储方式等本身就跟hdfs有所区别;</span><br><span class="line"></span><br><span class="line">fastDFS和oss以个体上面来看,都API、SDK对其存储的文件进行使用,看业务情况,如果需要牵涉很多大数据组件的,建议使用oss,毕竟花了钱,对应的服务也很方便;如果只是简单存储,访问,涉及到的其他数据分析比较少,就可以使用fastDFS,毕竟是个开源项目</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="4-FastDFS数据能否迁移到OSS上"><a href="#4-FastDFS数据能否迁移到OSS上" class="headerlink" title="4.FastDFS数据能否迁移到OSS上"></a>4.FastDFS数据能否迁移到OSS上</h3><p><strong>如何迁移</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">找一台中转服务器,最好是带宽高的阿里云ECS,然后将fastDFS映射到一个目录,OSS也可以挂载一个目录,接下来通过cp命令进行拷贝即可</span><br><span class="line">如果这两个都用的不同的挂载服务器,也可以通过RSync工具来进行目录同步,支持端点续传多线程</span><br></pre></td></tr></table></figure>
<hr>
]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>oss</tag>
      </tags>
  </entry>
  <entry>
    <title>基于CentOS7.6搭建K8S-Ingress篇</title>
    <url>/2019/09/09/%E5%9F%BA%E4%BA%8ECentOS7.6%E6%90%AD%E5%BB%BAK8S-Ingress%E7%AF%87/</url>
    <content><![CDATA[<blockquote>
<p>在Dashboard篇中,UI界面已经存在,现在加上Ingress</p>
</blockquote>
<span id="more"></span>

<h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><pre><code>Ingress的主要作用是可以利用nginx，haproxy，envoy,traefik等负载均衡器来暴露集群内部服务。</code></pre>
<p>包含两个组件</p>
<ul>
<li>Ingress<br>将Nginx的配置抽象成一个Ingress对象，每添加一个新的服务只需写一个新的Ingress的yaml文件即可</li>
<li>Ingress Controller<br>将新加入的Ingress转化成Nginx的配置文件并使之生效</li>
</ul>
<hr>
<h2 id="导入镜像"><a href="#导入镜像" class="headerlink" title="导入镜像"></a>导入镜像</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;ingress-nginx&#x2F;master&#x2F;deploy&#x2F;static&#x2F;mandatory.yaml</span><br><span class="line">cat mandatory.yaml|grep image</span><br><span class="line">docker pull image: quay.io&#x2F;kubernetes-ingress-controller&#x2F;nginx-ingress-controller:0.25.1</span><br><span class="line"># 分发给其他机器</span><br><span class="line">docker sava -o ingress.tar 0439eb3e11f1</span><br><span class="line">docker load -i ingress.tar</span><br><span class="line">docker tag 0439eb3e11f1 quay.io&#x2F;kubernetes-ingress-controller&#x2F;nginx-ingress-controller:0.25.1</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 强制命令</span><br><span class="line">kubectl apply -f mandatory.yaml</span><br><span class="line"># 编辑mandatory.yaml,在containers上添加</span><br><span class="line">vi mandatory.yaml</span><br><span class="line">spec:</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      containers:</span><br><span class="line"># 基于Bare-metal安装</span><br><span class="line">wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;ingress-nginx&#x2F;master&#x2F;deploy&#x2F;static&#x2F;provider&#x2F;baremetal&#x2F;service-nodeport.yaml</span><br><span class="line">kubectl apply -f service-nodeport.yaml</span><br><span class="line"># 验证安装</span><br><span class="line">kubectl get pods --all-namespaces -l app.kubernetes.io&#x2F;name&#x3D;ingress-nginx --watch</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># mandatory.yaml装的是Ingress Controller</span><br><span class="line"># service-nodeport装的是服务</span><br><span class="line"># 自己编写一个Ingress的yaml文件,如下</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: test-ingress</span><br><span class="line">  namespace: ingress-nginx # 与service的命名空间一致</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: node01 # host可以自定义</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F; # 自定义</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: ingress-nginx # 服务名</span><br><span class="line">          servicePort: 10254 # service的内部端口(内部端口:宿主机端口)</span><br><span class="line"># 测试一下,IP地址是Ingress Controller所在节点IP</span><br><span class="line">curl -v http:&#x2F;&#x2F;192.168.17.131 -H &#39;host: node01&#39;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="附加测试-ingress-tomcat"><a href="#附加测试-ingress-tomcat" class="headerlink" title="附加测试-ingress-tomcat"></a>附加测试-ingress-tomcat</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 首先创建Service和Deployment(默认default命名空间)</span><br><span class="line">vi tomcat-deploy.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat</span><br><span class="line">    release: canary</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    targetPort: 8080</span><br><span class="line">    port: 8080</span><br><span class="line">  - name: ajp</span><br><span class="line">    targetPort: 8009</span><br><span class="line">    port: 8009</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-deploy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat</span><br><span class="line">      release: canary</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat</span><br><span class="line">        release: canary</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat</span><br><span class="line">        image: tomcat</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line"></span><br><span class="line">kubectl apply -f tomcat-deploy.yaml</span><br><span class="line"></span><br><span class="line"># 创建Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-tomcat</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io&#x2F;ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: www.tomcat.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path:</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat</span><br><span class="line">          servicePort: 8080</span><br><span class="line"></span><br><span class="line">kubectl apply -f ingress-tomcat.yaml </span><br><span class="line"></span><br><span class="line"># 修改hosts</span><br><span class="line">IP(Ingress Controller所在IP)    www.tomcat.com</span><br><span class="line"># 访问</span><br><span class="line">http:&#x2F;&#x2F;www.tomcat.com&#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="启用Https"><a href="#启用Https" class="headerlink" title="启用Https"></a>启用Https</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 生成私钥 tls.key, 密钥位数是 2048</span><br><span class="line">openssl genrsa -out tls.key 2048</span><br><span class="line"># 使用 tls.key 生成自签证书</span><br><span class="line">openssl req -new -x509 -key tls.key -out tls.crt -subj &#x2F;C&#x3D;CN&#x2F;ST&#x3D;GuangDong&#x2F;L&#x3D;Guangzhou&#x2F;O&#x3D;DevOps&#x2F;CN&#x3D;www.tomcat.com</span><br><span class="line">kubectl create secret tls tomcat-ingress-secret --cert&#x3D;tls.crt --key&#x3D;tls.key </span><br><span class="line">kubectl get secret</span><br><span class="line">kubectl describe secret tomcat-ingress-secret</span><br><span class="line"></span><br><span class="line"># 创建Ingress</span><br><span class="line">vi ingress-tomcat-tls.yaml </span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-tomcat-tls</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io&#x2F;ingress.class: &quot;nginx&quot;</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts: </span><br><span class="line">    - www.tomcat.com</span><br><span class="line">    secretName: tomcat-ingress-secret</span><br><span class="line">  rules:</span><br><span class="line">  - host: www.tomcat.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path:</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat</span><br><span class="line">          servicePort: 8080</span><br><span class="line">          </span><br><span class="line">kubectl apply -f ingress-tomcat-tls.yaml </span><br><span class="line">kubectl get ingress</span><br><span class="line">kubectl describe ingress ingress-tomcat-tls</span><br><span class="line"></span><br><span class="line"># 访问</span><br><span class="line">https:&#x2F;&#x2F;www.tomcat.com&#x2F;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>多维函数的使用</title>
    <url>/2020/05/03/%E5%A4%9A%E7%BB%B4%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>多维分析函数的使用</p>
</blockquote>
<span id="more"></span>

<h2 id="多维分析"><a href="#多维分析" class="headerlink" title="多维分析"></a>多维分析</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在业务需要上,我们时长需要对数据进行分组,假如一个学校的学生数据</span><br><span class="line">想查看他们每个年级的人数,需要对年级进行分组</span><br><span class="line">想查看他们每个班级的人数,需要对班级进行分组</span><br><span class="line">想查看他们每个小组的人数,需要对小组进行分组</span><br><span class="line"></span><br><span class="line">可以看到年级-&gt;班级-&gt;人数,这就是多个维度,也可以说是一个维度的不同深度</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Grouping-Sets与GroupBy的优缺点"><a href="#Grouping-Sets与GroupBy的优缺点" class="headerlink" title="Grouping Sets与GroupBy的优缺点"></a>Grouping Sets与GroupBy的优缺点</h2><h3 id="GroupBy"><a href="#GroupBy" class="headerlink" title="GroupBy"></a>GroupBy</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">分组函数(常用)</span><br><span class="line"># 所有学生+各年级的学生+各班级的学生</span><br><span class="line">SELECT stuId,&#39;all&#39; key,&#39;all&#39; value,dt</span><br><span class="line">FROM school</span><br><span class="line">GROUP BY stuId,dt</span><br><span class="line">UNION ALL</span><br><span class="line">SELECT stuId,&#39;grade&#39; key, grade value,dt</span><br><span class="line">FROM school</span><br><span class="line">GROUP BY grade,stuId,dt</span><br><span class="line">UNION ALL</span><br><span class="line">SELECT stuId,&#39;class&#39; key, class value,dt</span><br><span class="line">FROM school</span><br><span class="line">GROUP BY class,stuId,dt</span><br><span class="line"></span><br><span class="line"># Job数(Spark&#x2F;Hive)</span><br><span class="line">9&#x2F;3</span><br><span class="line"></span><br><span class="line"># 优点</span><br><span class="line">可以灵活指定分组字段并直接添加额外字段</span><br><span class="line"></span><br><span class="line"># 缺点</span><br><span class="line">对于需要不同分组的业务,SQL过于冗余</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="Grouping-Sets"><a href="#Grouping-Sets" class="headerlink" title="Grouping Sets"></a>Grouping Sets</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">多维分析函数(不常用)</span><br><span class="line">SELECT stuId,</span><br><span class="line">CASE WHEN grade IS NULL AND class IS NULL THEN &#39;all&#39;</span><br><span class="line">WHEN grade IS NOT NULL THEN &#39;grade&#39;</span><br><span class="line">ELSE &#39;class&#39; END key,</span><br><span class="line">CASE WHEN grade IS NULL AND class IS NULL THEN &#39;all&#39;</span><br><span class="line">WHEN grade IS NOT NULL THEN grade</span><br><span class="line">ELSE class END value,dt</span><br><span class="line">FROM (</span><br><span class="line">	SELECT stuId,grade,class,dt</span><br><span class="line">	FROM school</span><br><span class="line">	GROUP BY grade,class,stuId,dt</span><br><span class="line">	grouping sets ((stuId,dt),(grade,stuId,dt),(class,stuId,dt))</span><br><span class="line">) a</span><br><span class="line"></span><br><span class="line"># Job数(Spark&#x2F;Hive)</span><br><span class="line">3&#x2F;1</span><br><span class="line"></span><br><span class="line"># 优点</span><br><span class="line">在Hive&#x2F;Spark执行中,相较于GroupBy执行计划更优</span><br><span class="line"></span><br><span class="line"># 缺点</span><br><span class="line">最大的缺点就是,需要进行NULL值填充</span><br><span class="line">尤其是不能确定原始数据中是否有值为NULL的字段</span><br><span class="line"></span><br><span class="line"># 补充点</span><br><span class="line">对于其缺点有grouping__id函数进行完善</span><br><span class="line">grouping__id为位向量,计算逻辑如下</span><br><span class="line">grade,class,stuId,dt四个分组字段</span><br><span class="line">0000</span><br><span class="line">在分组集合内的置为1,否则置为0</span><br><span class="line">坑:</span><br><span class="line">    最右边的表示第1列,依次类推</span><br><span class="line">    然后进行反转,计算出十进制值</span><br><span class="line">(stuId,dt)-&gt;0011-&gt;1100-&gt;12</span><br><span class="line">(grade,stuId,dt)-&gt;1011-&gt;1101-&gt;13</span><br><span class="line">(class,stuId,dt)-&gt;0111-&gt;1110-&gt;14</span><br><span class="line">假如有字段为空,那么其grouping__id字段值将发生改变</span><br><span class="line">e.g:</span><br><span class="line">    其他分组字段有为空的情况</span><br><span class="line">    10086,2020-05-13,null,null,12-&gt;(选择stuId,dt)</span><br><span class="line">    10086,2020-05-13,null,null,13-&gt;(选择grade,stuId,dt,但是grade有空值)</span><br><span class="line">这样就可以很好的辨认分组字段是什么</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Cube"><a href="#Cube" class="headerlink" title="Cube"></a>Cube</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> gradeID, classID, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gradeID, classID </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> gradeID, classID, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gradeID, classID </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> gradeID, <span class="literal">NULL</span>, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gradeID, <span class="literal">NULL</span> </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>, classID, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="literal">NULL</span>, classID </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="literal">NULL</span>, <span class="literal">NULL</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Rollup"><a href="#Rollup" class="headerlink" title="Rollup"></a>Rollup</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> gradeID, classID, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gradeID, classID </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> gradeID, classID, <span class="keyword">MAX</span>(score)</span><br><span class="line"><span class="keyword">FROM</span> exam </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gradeID, classID </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> gradeID, <span class="literal">NULL</span>, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> gradeID, <span class="literal">NULL</span> </span><br><span class="line"><span class="keyword">UNION</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="keyword">MAX</span>(score) </span><br><span class="line"><span class="keyword">FROM</span> exam </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="literal">NULL</span>, <span class="literal">NULL</span>;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink上的使用"><a href="#Flink上的使用" class="headerlink" title="Flink上的使用"></a>Flink上的使用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE VIEW temp AS SELECT GROUPING_ID(supplier_id,rating) grouping_id,supplier_id, rating, COUNT(*) AS total</span><br><span class="line">FROM (VALUES</span><br><span class="line">    (&#39;supplier1&#39;, &#39;product1&#39;, 4),</span><br><span class="line">    (&#39;supplier1&#39;, &#39;product2&#39;, 3),</span><br><span class="line">    (&#39;supplier2&#39;, &#39;product3&#39;, 3),</span><br><span class="line">    (&#39;supplier2&#39;, &#39;product4&#39;, 4))</span><br><span class="line">AS Products(supplier_id, product_id, rating)</span><br><span class="line">GROUP BY GROUPING SETS ((supplier_id, rating), (supplier_id), ())</span><br><span class="line"></span><br><span class="line">DROP TABLE IF EXISTS s_r;</span><br><span class="line">DROP TABLE IF EXISTS s;</span><br><span class="line"></span><br><span class="line">CREATE TABLE IF NOT EXISTS s_r (</span><br><span class="line">   supplier_id VARCHAR,</span><br><span class="line">   rating VARCHAR,</span><br><span class="line">   total BIGINT</span><br><span class="line"> ) WITH (</span><br><span class="line">   &#39;connector&#39; &#x3D; &#39;kafka&#39;,</span><br><span class="line">   &#39;topic&#39; &#x3D; &#39;s_r&#39;,</span><br><span class="line">   &#39;properties.bootstrap.servers&#39; &#x3D; &#39;localhost:9092&#39;,</span><br><span class="line">   &#39;scan.startup.mode&#39; &#x3D; &#39;earliest-offset&#39;,</span><br><span class="line">   &#39;format&#39; &#x3D; &#39;json&#39;</span><br><span class="line">);</span><br><span class="line">CREATE TABLE IF NOT EXISTS s (</span><br><span class="line">   supplier_id VARCHAR,</span><br><span class="line">   rating VARCHAR,</span><br><span class="line">   total BIGINT</span><br><span class="line"> ) WITH (</span><br><span class="line">   &#39;connector&#39; &#x3D; &#39;kafka&#39;,</span><br><span class="line">   &#39;topic&#39; &#x3D; &#39;s&#39;,</span><br><span class="line">   &#39;properties.bootstrap.servers&#39; &#x3D; &#39;localhost:9092&#39;,</span><br><span class="line">   &#39;scan.startup.mode&#39; &#x3D; &#39;earliest-offset&#39;,</span><br><span class="line">   &#39;format&#39; &#x3D; &#39;json&#39;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">-- 在Zeppelin上使用时,可以使用runAsOne&#x3D;true,运行在同一个Job上</span><br><span class="line">insert into s_r </span><br><span class="line">select cast(supplier_id as STRING) supplier_id,cast(rating as STRING) rating,total</span><br><span class="line">from temp</span><br><span class="line">where grouping_id &#x3D; 0;</span><br><span class="line"></span><br><span class="line">insert into s</span><br><span class="line">select cast(supplier_id as STRING) supplier_id,cast(rating as STRING) rating,total</span><br><span class="line">from temp</span><br><span class="line">where grouping_id &#x3D; 1;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
        <tag>spark</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据协议知识</title>
    <url>/2021/02/18/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%8D%8F%E8%AE%AE%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<blockquote>
<p>空闲整理下协议相关知识</p>
</blockquote>
<span id="more"></span>

<h2 id="协议总览"><a href="#协议总览" class="headerlink" title="协议总览"></a>协议总览</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ZAB</span><br><span class="line">Paxos</span><br><span class="line">Raft</span><br><span class="line">2PC</span><br><span class="line">Quorum</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ZAB协议"><a href="#ZAB协议" class="headerlink" title="ZAB协议"></a>ZAB协议</h2><h3 id="介绍-ZK选举"><a href="#介绍-ZK选举" class="headerlink" title="介绍_ZK选举"></a>介绍_<a href="https://jxeditor.github.io/2017/08/28/Zookeeper%E7%9A%84%E9%A2%86%E5%AF%BC%E8%80%85%E9%80%89%E4%B8%BE%E5%92%8C%E5%8E%9F%E5%AD%90%E5%B9%BF%E6%92%AD/">ZK选举</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">全程Zookeeper Atomic Broadcast(ZK原子广播)</span><br><span class="line">使用场景,Zookeeper</span><br><span class="line">过程:</span><br><span class="line">    实现了主备模型的系统架构来保证集群中各个副本之间数据的一致性</span><br><span class="line">    Leader将客户端的操作转化为Proposal,Leader在操作完数据后向所有Follower发送广播请求,等待反馈</span><br><span class="line">    ZAB协议中,超过半数Follower节点反馈成功,Leader向所有Follower发送Commit消息</span><br><span class="line">    Follower进行数据操作.</span><br><span class="line"></span><br><span class="line">看上面的过程,Leader先是将操作广播给Follower,等待反馈,然后再将Commit消息发送给Follower.</span><br><span class="line">这个过程和2PC很相似,只不过2PC要么全部反馈成功,要么全部反馈失败(精确一次).</span><br><span class="line">而ZAB只需要半数反馈成功即可.</span><br></pre></td></tr></table></figure>
<h3 id="疑问点"><a href="#疑问点" class="headerlink" title="疑问点"></a>疑问点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.为什么ZAB协议不会同步阻塞?</span><br><span class="line">    因为Leader和每一个Follower之间收发消息都有单独的队列,做到了异步.</span><br><span class="line">2.ZAB只要半数反馈成功,怎么保证的数据一致性?</span><br><span class="line">    首先ZAB中只能是Leader接收写请求,其次如果Leader宕机,ZAB要求集群进行崩溃恢复和Leader选举</span><br><span class="line">    其中崩溃恢复必须确保:</span><br><span class="line">        1.已经被Leader提交的Proposal必须被所有Follower提交</span><br><span class="line">        2.丢弃Leader没有提交Proposal(保证Follower没有未提交Proposal,一定比宕机Leader的Proposal小)</span><br><span class="line">    Leader宕机时存在未提交的Proposal在崩溃恢复后,新选举的Leader肯定是换了节点</span><br><span class="line">    新Leader会把自身最大Proposal的ZXID发送给其他Follower</span><br><span class="line">    Follower对比自身Proposal的ZXID,自身大则回退,自身小则进行数据同步.</span><br><span class="line">3.ZXID是什么?</span><br><span class="line">    64位的数字,低32位按照数字递增,客服端发送Proposal,低32位简单加1</span><br><span class="line">    高32位是Leader周期的epoch编号,每选举出新的Leader,Leader都从本地Proposal获取ZXID,解析出高32位epoch编号,进行加1</span><br><span class="line">    低32位全部设置为0,保证ZXID的唯一性,并且递增.</span><br><span class="line">4.Leader崩溃,已经提出但是未提交的Proposal被丢弃,是不是意味着ZK丢失了一次请求?</span><br><span class="line">    我个人认为是丢失了,没有重试机制的话,对应就应该是ZK请求失败.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Paxos"><a href="#Paxos" class="headerlink" title="Paxos"></a>Paxos</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决问题:</span><br><span class="line">    每个人都有提出建议,同意建议,接受建议的权利</span><br><span class="line">    最终的采纳哪个建议,采取少数服从多数的方式</span><br><span class="line">采纳方式:</span><br><span class="line">    只有被提出的建议才能被大家同意</span><br><span class="line">    只能采纳一个建议</span><br></pre></td></tr></table></figure>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A,B,C三个人提议吃东西(对吃什么各持己见)</span><br><span class="line">A--&gt;日料</span><br><span class="line">B--&gt;法餐</span><br><span class="line">C--&gt;中餐</span><br><span class="line"></span><br><span class="line">提议顺序 A-&gt;B-&gt;C</span><br><span class="line"></span><br><span class="line">Prepare阶段</span><br><span class="line">    A将提议(1)发送给BC</span><br><span class="line">        BC没有接受小于编号为1的提议,BC接受,并不再接受小于1的提议</span><br><span class="line">    B将提议(2)发送给AC</span><br><span class="line">        A没有接受小于编号为2的提议,A接受,并不再接受小于2的提议</span><br><span class="line">        C接受过编号为1的提议,但是2&gt;1,C接受,并不再接受小于2的提议</span><br><span class="line">    C将提议(3)发送给AB</span><br><span class="line">        A接受过编号为2的提议,但是3&gt;2,A接受,并不再接受小于3的提议</span><br><span class="line">        B接受过编号为2的提议,但是3&gt;2,B接受,并不再接受小于3的提议</span><br><span class="line"></span><br><span class="line">Accept阶段</span><br><span class="line">    A将提议(1,日料)发送给BC</span><br><span class="line">        B不接受小于3的提议,拒绝</span><br><span class="line">        C不接受小于2的提议,拒绝</span><br><span class="line">        A的票数0,再次进入Prepare阶段</span><br><span class="line">    B将提议(2,法餐)发送给AC</span><br><span class="line">        A不接受小于3的提议,拒绝</span><br><span class="line">        C不接受小于2的提议,接受</span><br><span class="line">        B的票数为1</span><br><span class="line">    C将提议(3,中餐)发送给AB</span><br><span class="line">        A不接受小于3的提议,接受</span><br><span class="line">        B不接受小于3的提议,接受</span><br><span class="line">        C的票数为2</span><br><span class="line"></span><br><span class="line">C(3,中餐)获得多数人同意</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h2><h3 id="介绍-动画"><a href="#介绍-动画" class="headerlink" title="介绍_动画"></a>介绍_<a href="http://thesecretlivesofdata.com/raft/">动画</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">个人看Raft和ZAB很是相似,也是超过半数选举成功即可</span><br><span class="line">角色:</span><br><span class="line">    Leader</span><br><span class="line">    Follower</span><br><span class="line">    Candidate(候选)</span><br></pre></td></tr></table></figure>
<h3 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">多Candidate选主</span><br><span class="line">    4个节点都是Follower,有两个Follower同时Timeout,变成Candidate</span><br><span class="line">    Candidate: A B</span><br><span class="line">    Follower: C D</span><br><span class="line">    分别给另外的Follower发送投票请求(自身给自身投一票),此处遵循先后的关系,分为下列情况:</span><br><span class="line">        A-&gt;C,B-&gt;D,A-&gt;D,B-&gt;C,投过票的Follower会拒绝请求,即AB各2票,进入下一轮投票</span><br><span class="line">        A-&gt;C,A-&gt;D,B-&gt;C,B-&gt;D,A率先获得3票,会转化为Leader,期间B可以发起投票,但CD不会同意</span><br><span class="line">    Leader进行心跳,Candidate接收到会转化为Follower</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2PC"><a href="#2PC" class="headerlink" title="2PC"></a>2PC</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两阶段提交,强一致性,中心化的原子提交协议</span><br><span class="line">Coordinator(协调者)</span><br><span class="line">Partcipant(参与者)</span><br><span class="line">顾名思义,两阶段提交,分为两个阶段:投票阶段,提交阶段</span><br></pre></td></tr></table></figure>
<h3 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一阶段</span><br><span class="line">    Coordinator向所有的Partcipant发送Prepare,并等待反馈</span><br><span class="line">    Partcipant接收到了Prepare,先执行本地事务操作(不会真正的执行),向Coordinator发送反馈</span><br><span class="line">    结果产生:1.所有Partcipant都反馈成功,2.存在反馈不成功的情况,只有1才会进入第二阶段</span><br><span class="line">第二阶段</span><br><span class="line">    Coordinator向所有的Partcipant发送Commit</span><br><span class="line">    所有Partcipant执行Commit操作</span><br><span class="line"></span><br><span class="line">如果有出现反馈不成功情况</span><br><span class="line">    Coordinator向所有的Partcipant发送Rollback</span><br><span class="line">    Partcipant执行回滚操作</span><br><span class="line"></span><br><span class="line">2PC是有很多坑的</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Quorum"><a href="#Quorum" class="headerlink" title="Quorum"></a>Quorum</h2><h3 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">准确来讲是一种机制,确保一次写操作在多个副本中有半数副本操作成功,则说明这次写操作成功</span><br><span class="line">读操作也确保读超过半数的副本,则总会读取到最新的数据</span><br><span class="line"></span><br><span class="line">使用场景:Hadoop HA的QJM(Quorum Journal Manager)</span><br><span class="line">当Active Namenode向QJM写入Editlog时,要求半数以上的QJM写入成功,才算操作成功</span><br><span class="line">Standby Namenode定期从QJM上获取最新的Editlog更新自身的数据</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据日知录阅读</title>
    <url>/2020/06/19/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%97%A5%E7%9F%A5%E5%BD%95%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<blockquote>
<p>架构与算法</p>
</blockquote>
<span id="more"></span>

<h2 id="数据分片与路由"><a href="#数据分片与路由" class="headerlink" title="数据分片与路由"></a>数据分片与路由</h2><h3 id="哈希分片"><a href="#哈希分片" class="headerlink" title="哈希分片"></a>哈希分片</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 一致性哈希算法</span><br><span class="line">给定一个由0-31组成的Hash值闭环,由5个机器存储对应的分片数据</span><br><span class="line">顺时针方向最近的节点存储分片数据</span><br><span class="line">N5-&gt;N14-&gt;N20-&gt;N25-&gt;N29-&gt;N5</span><br><span class="line">N5(30,31,0,1,2,3,4,5)</span><br><span class="line">N14(6,7,8,9,10,11,12,13,14)</span><br><span class="line">N20(15,16,17,18,19,20)</span><br><span class="line">N25(21,22,23,24,25)</span><br><span class="line">N29(26,27,28,29)</span><br><span class="line"></span><br><span class="line">举例:</span><br><span class="line">N5发起存储[test]数据请求</span><br><span class="line">先将[test]进行hash取值[0-31]得到18</span><br><span class="line">逐级寻找对应的存储节点,N5&lt;N14&lt;18&lt;N20</span><br><span class="line">最终存储入N20节点</span><br><span class="line"></span><br><span class="line">为了加快访问速度,引用了路由表</span><br><span class="line">在每一个节点上记录了距离其他节点的距离信息</span><br><span class="line">N14的路由表信息(距离-&gt;节点)</span><br><span class="line">1(2^0)-&gt;N20</span><br><span class="line">2(2^1)-&gt;N20</span><br><span class="line">4(2^2)-&gt;N20</span><br><span class="line">8(2^3)-&gt;N25</span><br><span class="line">16(2^4)-&gt;N5</span><br><span class="line">N14接收到查询Hash值为4的请求</span><br><span class="line">先确定4是否在N20节点上,不在则寻找路由表,找到小于4的最大编号节点</span><br><span class="line">发现找不到,取倒数第2项路由信息N25,请求N25取查询4的信息</span><br></pre></td></tr></table></figure>
<h3 id="范围分片"><a href="#范围分片" class="headerlink" title="范围分片"></a>范围分片</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">将所有记录的主键进行排序,在排好序的主键空间内将记录划分成数据分片</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="集群资源管理与调度"><a href="#集群资源管理与调度" class="headerlink" title="集群资源管理与调度"></a>集群资源管理与调度</h2><h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">集中式调度器</span><br><span class="line">    整个系统中只运行一个全局的中央调度器,所有框架或计算任务又中央调度器满足</span><br><span class="line">    单路径调度器(无论计算任务的类型,统一的调度策略进行资源管理与调度)</span><br><span class="line">    多路径调度器(支持多种调度策略)</span><br><span class="line">    </span><br><span class="line">两级调度器</span><br><span class="line">    将整个系统分为两个级别,中央调度器和框架调度器</span><br><span class="line">    中央调度器可以将集群中所有机器的可用资源分配给计算框架</span><br><span class="line">    计算框架接收到所需要的资源后,根据自身任务是用调度策略进一步分配资源(Mesos,Yarn,Hadoop On Demand)</span><br><span class="line">    Mesos在具体实现中央调度器调度策略时,倾向于资源分配的公平性,如果各任务所需资源类似,是用Mesos比较合适</span><br><span class="line">    </span><br><span class="line">状态共享调度器</span><br><span class="line">    每个计算框架都可以看到整个集群的资源,采用竞争的方式获取资源,增加了系统的并发性能</span><br><span class="line">    但是系统存在大量资源竞争冲突时,失败者可能需要重新执行任务</span><br><span class="line">    或者优先级高的任务始终能获得资源,而优先级低的任务可能一直获取不到资源</span><br></pre></td></tr></table></figure>
<h3 id="资源调度策略"><a href="#资源调度策略" class="headerlink" title="资源调度策略"></a>资源调度策略</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FIFO调度策略</span><br><span class="line">    根据提交的时间先后顺序&#x2F;优先级次序放入线性队列中,先进先出的进行调度与资源分配</span><br><span class="line">    新加入的作业容易长时间等待调度</span><br><span class="line"></span><br><span class="line">公平调度器</span><br><span class="line">    根据每个资源池的最小资源保障量,将系统中部分资源分配给各个资源池</span><br><span class="line">    根据资源池的指定优先级将剩余资源按照比例分配给各个资源池</span><br><span class="line">    在各个资源池中,按照作业优先级或者根据公平策略将资源分配给各个作业</span><br><span class="line">    公平策略是最大最小公平算法的具体实现</span><br><span class="line">    </span><br><span class="line">能力调度器</span><br><span class="line">    将用户与任务组织成多个队列,每个队列都有资源最低保障和使用上限</span><br><span class="line">    一个队列资源有剩余,可以将剩余资源共享给其他队列</span><br><span class="line">    调度时,优先将资源分配给资源使用率最低的队列</span><br><span class="line">    队列内部按照作业优先级顺序使用FIFO策略调度</span><br><span class="line">    资源使用率&#x3D;已使用资源量&#x2F;分配的资源量</span><br><span class="line"></span><br><span class="line">延迟调度策略</span><br><span class="line">    一般作为其他调度策略的辅助措施</span><br><span class="line">    如果当前资源不满足,可以暂时放弃分配公平性,等待后续资源分配,当前资源跳过该任务分配给其他任务</span><br><span class="line">    如果该任务跳过N次后仍然等不到满足的资源,则接受当前资源启动任务执行</span><br><span class="line"></span><br><span class="line">主资源公平调度策略DRF</span><br><span class="line">    最大最小公平算法基本思想:最大化目前分配到最少资源量的用户或任务的资源量</span><br><span class="line">    DRF则将扩展到了多个资源的公平分配</span><br><span class="line">    例如:</span><br><span class="line">        系统共有9个CPU和18GB内存,A用户主资源CPU&lt;3CPU,1GB&gt;,B用户主资源内存&lt;1CPU,4GB&gt;</span><br><span class="line">        A用户3&#x2F;9总CPU量,1&#x2F;18总内存量</span><br><span class="line">        B用户1&#x2F;9总CPU量,4&#x2F;18总内存量</span><br><span class="line">        经过DRF算法,A启动2个任务,B启动3个任务</span><br><span class="line">        每个用户获得同样的主资源量,A占用2&#x2F;3CPU,B占用2&#x2F;3内存 </span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用JLine仿Shell终端</title>
    <url>/2021/01/15/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JLine%E4%BB%BFShell%E7%BB%88%E7%AB%AF/</url>
    <content><![CDATA[<blockquote>
<p>JLine是用于处理控制台输入的Java库</p>
</blockquote>
<span id="more"></span>

<h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.jline<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jline<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.18.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="简单实现"><a href="#简单实现" class="headerlink" title="简单实现"></a>简单实现</h2><p>以Flink的SqlClient为例</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.jline.reader.EndOfFileException;</span><br><span class="line"><span class="keyword">import</span> org.jline.reader.LineReader;</span><br><span class="line"><span class="keyword">import</span> org.jline.reader.LineReaderBuilder;</span><br><span class="line"><span class="keyword">import</span> org.jline.reader.UserInterruptException;</span><br><span class="line"><span class="keyword">import</span> org.jline.terminal.Terminal;</span><br><span class="line"><span class="keyword">import</span> org.jline.terminal.TerminalBuilder;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> jxeditor by 2021/1/18</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JlineDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">static</span> <span class="keyword">final</span> String MESSAGE_WELCOME = <span class="string">&quot;                                   \u2592\u2593\u2588\u2588\u2593\u2588\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                               \u2593\u2588\u2588\u2588\u2588\u2592\u2592\u2588\u2593\u2592\u2593\u2588\u2588\u2588\u2593\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                            \u2593\u2588\u2588\u2588\u2593\u2591\u2591        \u2592\u2592\u2592\u2593\u2588\u2588\u2592  \u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                          \u2591\u2588\u2588\u2592   \u2592\u2592\u2593\u2593\u2588\u2593\u2593\u2592\u2591      \u2592\u2588\u2588\u2588\u2588\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                          \u2588\u2588\u2592         \u2591\u2592\u2593\u2588\u2588\u2588\u2592    \u2592\u2588\u2592\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                            \u2591\u2593\u2588            \u2588\u2588\u2588   \u2593\u2591\u2592\u2588\u2588\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                              \u2593\u2588       \u2592\u2592\u2592\u2592\u2592\u2593\u2588\u2588\u2593\u2591\u2592\u2591\u2593\u2593\u2588\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                            \u2588\u2591 \u2588   \u2592\u2592\u2591       \u2588\u2588\u2588\u2593\u2593\u2588 \u2592\u2588\u2592\u2592\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                            \u2588\u2588\u2588\u2588\u2591   \u2592\u2593\u2588\u2593      \u2588\u2588\u2592\u2592\u2592 \u2593\u2588\u2588\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                         \u2591\u2592\u2588\u2593\u2593\u2588\u2588       \u2593\u2588\u2592    \u2593\u2588\u2592\u2593\u2588\u2588\u2593 \u2591\u2588\u2591\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                   \u2593\u2591\u2592\u2593\u2588\u2588\u2588\u2588\u2592 \u2588\u2588         \u2592\u2588    \u2588\u2593\u2591\u2592\u2588\u2592\u2591\u2592\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                  \u2588\u2588\u2588\u2593\u2591\u2588\u2588\u2593  \u2593\u2588           \u2588   \u2588\u2593 \u2592\u2593\u2588\u2593\u2593\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                \u2591\u2588\u2588\u2593  \u2591\u2588\u2591            \u2588  \u2588\u2592 \u2592\u2588\u2588\u2588\u2588\u2588\u2593\u2592 \u2588\u2588\u2593\u2591\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;               \u2588\u2588\u2588\u2591 \u2591 \u2588\u2591          \u2593 \u2591\u2588 \u2588\u2588\u2588\u2588\u2588\u2592\u2591\u2591    \u2591\u2588\u2591\u2593  \u2593\u2591\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;              \u2588\u2588\u2593\u2588 \u2592\u2592\u2593\u2592          \u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2591       \u2592\u2588\u2592 \u2592\u2593 \u2593\u2588\u2588\u2593\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;           \u2592\u2588\u2588\u2593 \u2593\u2588 \u2588\u2593\u2588       \u2591\u2592\u2588\u2588\u2588\u2588\u2588\u2593\u2593\u2592\u2591         \u2588\u2588\u2592\u2592  \u2588 \u2592  \u2593\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;           \u2593\u2588\u2593  \u2593\u2588 \u2588\u2588\u2593 \u2591\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2592              \u2592\u2588\u2588\u2593           \u2591\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;           \u2593\u2588    \u2588 \u2593\u2588\u2588\u2588\u2593\u2592\u2591              \u2591\u2593\u2593\u2593\u2588\u2588\u2588\u2593          \u2591\u2592\u2591 \u2593\u2588\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;           \u2588\u2588\u2593    \u2588\u2588\u2592    \u2591\u2592\u2593\u2593\u2588\u2588\u2588\u2593\u2593\u2593\u2593\u2593\u2588\u2588\u2588\u2588\u2588\u2588\u2593\u2592            \u2593\u2588\u2588\u2588  \u2588\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;          \u2593\u2588\u2588\u2588\u2592 \u2588\u2588\u2588   \u2591\u2593\u2593\u2592\u2591\u2591   \u2591\u2593\u2588\u2588\u2588\u2588\u2593\u2591                  \u2591\u2592\u2593\u2592  \u2588\u2593\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;          \u2588\u2593\u2592\u2592\u2593\u2593\u2588\u2588  \u2591\u2592\u2592\u2591\u2591\u2591\u2592\u2592\u2592\u2592\u2593\u2588\u2588\u2593\u2591                            \u2588\u2593\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;          \u2588\u2588 \u2593\u2591\u2592\u2588   \u2593\u2593\u2593\u2593\u2592\u2591\u2591  \u2592\u2588\u2593       \u2592\u2593\u2593\u2588\u2588\u2593    \u2593\u2592          \u2592\u2592\u2593\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;          \u2593\u2588\u2593 \u2593\u2592\u2588  \u2588\u2593\u2591  \u2591\u2592\u2593\u2593\u2588\u2588\u2592            \u2591\u2593\u2588\u2592   \u2592\u2592\u2592\u2591\u2592\u2592\u2593\u2588\u2588\u2588\u2588\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;           \u2588\u2588\u2591 \u2593\u2588\u2592\u2588\u2592  \u2592\u2593\u2593\u2592  \u2593\u2588                \u2588\u2591      \u2591\u2591\u2591\u2591   \u2591\u2588\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;           \u2593\u2588   \u2592\u2588\u2593   \u2591     \u2588\u2591                \u2592\u2588              \u2588\u2593\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;            \u2588\u2593   \u2588\u2588         \u2588\u2591                 \u2593\u2593        \u2592\u2588\u2593\u2593\u2593\u2592\u2588\u2591\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;             \u2588\u2593 \u2591\u2593\u2588\u2588\u2591       \u2593\u2592                  \u2593\u2588\u2593\u2592\u2591\u2591\u2591\u2592\u2593\u2588\u2591    \u2592\u2588\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;              \u2588\u2588   \u2593\u2588\u2593\u2591      \u2592                    \u2591\u2592\u2588\u2592\u2588\u2588\u2592      \u2593\u2593\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;               \u2593\u2588\u2592   \u2592\u2588\u2593\u2592\u2591                         \u2592\u2592 \u2588\u2592\u2588\u2593\u2592\u2592\u2591\u2591\u2592\u2588\u2588\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                \u2591\u2588\u2588\u2592    \u2592\u2593\u2593\u2592                     \u2593\u2588\u2588\u2593\u2592\u2588\u2592 \u2591\u2593\u2593\u2593\u2593\u2592\u2588\u2593\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                  \u2591\u2593\u2588\u2588\u2592                          \u2593\u2591  \u2592\u2588\u2593\u2588  \u2591\u2591\u2592\u2592\u2592\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;                      \u2592\u2593\u2593\u2593\u2593\u2593\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2592\u2591\u2591\u2593\u2593  \u2593\u2591\u2592\u2588\u2591\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;          \n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;    ______ _ _       _       _____  ____  _         _____ _ _            _  BETA   \n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   |  ____| (_)     | |     / ____|/ __ \\| |       / ____| (_)          | |  \n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   | |__  | |_ _ __ | | __ | (___ | |  | | |      | |    | |_  ___ _ __ | |_ \n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   |  __| | | | &#x27;_ \\| |/ /  \\___ \\| |  | | |      | |    | | |/ _ \\ &#x27;_ \\| __|\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   | |    | | | | | |   &lt;   ____) | |__| | |____  | |____| | |  __/ | | | |_ \n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;   |_|    |_|_|_| |_|_|\\_\\ |_____/ \\___\\_\\______|  \\_____|_|_|\\___|_| |_|\\__|\n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;          \n&quot;</span> +</span><br><span class="line">            <span class="string">&quot;        Welcome! Enter &#x27;HELP;&#x27; to list all available commands. &#x27;QUIT;&#x27; to exit.\n\n&quot;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建终端</span></span><br><span class="line">        Terminal terminal = TerminalBuilder.builder()</span><br><span class="line">                .system(<span class="keyword">true</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取终端输入</span></span><br><span class="line">        LineReader lineReader = LineReaderBuilder.builder()</span><br><span class="line">                .terminal(terminal)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出欢迎语</span></span><br><span class="line">        terminal.writer().append(JlineDemo.MESSAGE_WELCOME);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提示符</span></span><br><span class="line">        String prompt = <span class="string">&quot;Flink SQL&gt; &quot;</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            terminal.writer().append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">            terminal.flush();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">final</span> String line;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                line = lineReader.readLine(prompt);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (UserInterruptException e) &#123;</span><br><span class="line">                <span class="comment">// user cancelled line with Ctrl+C</span></span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (EndOfFileException e) &#123;</span><br><span class="line">                <span class="comment">// user cancelled application with Ctrl+D or kill</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">&quot;Could not read from command line.&quot;</span>, t);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (line == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 获取输入,根据输入做对应的操作,CommandCall</span></span><br><span class="line">            System.out.println(line);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="更多操作"><a href="#更多操作" class="headerlink" title="更多操作"></a>更多操作</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自动补全,在Windows下测试发现TAB建补全有些问题</span></span><br><span class="line">Completer commandCompleter = <span class="keyword">new</span> StringsCompleter(<span class="string">&quot;CREATE&quot;</span>, <span class="string">&quot;SELECT&quot;</span>, <span class="string">&quot;INSERT&quot;</span>, <span class="string">&quot;SHOW&quot;</span>);</span><br><span class="line">LineReader lineReader = LineReaderBuilder.builder()</span><br><span class="line">    .terminal(terminal)</span><br><span class="line">    .completer(commandCompleter)</span><br><span class="line">    .build();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 历史命令,存储以及加载历史命令的位置</span></span><br><span class="line">lineReader.setVariable(LineReader.HISTORY_FILE, <span class="keyword">new</span> File(<span class="string">&quot;E:/history.log&quot;</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更多可以看下Jline源码</span></span><br><span class="line"><span class="comment">// https://github.com/jline/jline3</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>系统</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>常见排序算法</title>
    <url>/2016/06/29/%E5%B8%B8%E8%A7%81%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<blockquote>
<p>整理常见的排序方式,尽量挑几个达到手写</p>
</blockquote>
<span id="more"></span>

<h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 冒泡排序: 两层for循环,外层i让每一个元素都能遍历到,内层j控制相邻的元素进行大小对比</span></span><br><span class="line"><span class="comment"> * 注意if判断,对比相邻的两个元素,将大的放到右边,那么右边在第一次遍历后会产生一个最大元素</span></span><br><span class="line"><span class="comment"> * i值的逐渐变大,那么依次右边的的元素也确定了</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span>[] bubbleSort(<span class="keyword">int</span>[] arr) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; arr.length - i - <span class="number">1</span>; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[j] &gt; arr[j + <span class="number">1</span>]) &#123;</span><br><span class="line">                <span class="keyword">int</span> temp = arr[j];</span><br><span class="line">                arr[j] = arr[j + <span class="number">1</span>];</span><br><span class="line">                arr[j + <span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 选择排序: i控制的for循环遍历当前元素,j控制的for循环每次都去找当前元素右边的最小元素,和当前元素互换位置</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span>[] selectSort(<span class="keyword">int</span>[] arr) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> index = i;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; arr.length; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[j] &lt; arr[index]) &#123;</span><br><span class="line">                index = j;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (index != i) &#123;</span><br><span class="line">            <span class="keyword">int</span> temp = arr[index];</span><br><span class="line">            arr[index] = arr[i];</span><br><span class="line">            arr[i] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 插入排序:从第二个元素开始,j控制的for循环内只进行替换,当替换条件不成立后,跳出j循环进行交换</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span>[] insertSort(<span class="keyword">int</span>[] arr) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> j;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] &lt; arr[i - <span class="number">1</span>]) &#123;</span><br><span class="line">            <span class="keyword">int</span> temp = arr[i];</span><br><span class="line">            <span class="keyword">for</span> (j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span> &amp;&amp; temp &lt; arr[j]; j--) &#123;</span><br><span class="line">                arr[j + <span class="number">1</span>] = arr[j];</span><br><span class="line">            &#125;</span><br><span class="line">            arr[j + <span class="number">1</span>] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种实现</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertSort</span><span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &gt; <span class="number">0</span>; j--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[j - <span class="number">1</span>] &gt; arr[j]) &#123;</span><br><span class="line">                temp = arr[j - <span class="number">1</span>];</span><br><span class="line">                arr[j - <span class="number">1</span>] = arr[j];</span><br><span class="line">                arr[j] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 希尔排序</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] shellSort(<span class="keyword">int</span>[] arr) &#123;</span><br><span class="line">    <span class="keyword">int</span> increasement = arr.length;</span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        <span class="comment">// 确定分组的增量</span></span><br><span class="line">        increasement = increasement / <span class="number">3</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; increasement; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (j = i + increasement; j &lt; arr.length; j += increasement) &#123;</span><br><span class="line">                <span class="keyword">if</span> (arr[j] &lt; arr[j - increasement]) &#123;</span><br><span class="line">                    <span class="keyword">int</span> temp = arr[j];</span><br><span class="line">                    <span class="keyword">for</span> (k = j - increasement; k &gt;= <span class="number">0</span> &amp;&amp; temp &lt; arr[k]; k -= increasement) &#123;</span><br><span class="line">                        arr[k + increasement] = arr[k];</span><br><span class="line">                    &#125;</span><br><span class="line">                    arr[k + increasement] = temp;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">while</span> (increasement &gt; <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 快速排序</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> start</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> end</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (start &gt;= end)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> i = start;</span><br><span class="line">    <span class="keyword">int</span> j = end;</span><br><span class="line">    <span class="comment">// 基准数</span></span><br><span class="line">    <span class="keyword">int</span> baseval = arr[start];</span><br><span class="line">    <span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line">        <span class="comment">// 从右向左找比基准数小的数</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt; j &amp;&amp; arr[j] &gt;= baseval) &#123;</span><br><span class="line">            j--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; j) &#123;</span><br><span class="line">            arr[i] = arr[j];</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 从左向右找比基准数大的数</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt; j &amp;&amp; arr[i] &lt; baseval) &#123;</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; j) &#123;</span><br><span class="line">            arr[j] = arr[i];</span><br><span class="line">            j--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 把基准数放到i的位置</span></span><br><span class="line">    arr[i] = baseval;</span><br><span class="line">    <span class="comment">// 递归</span></span><br><span class="line">    quickSort(arr, start, i - <span class="number">1</span>);</span><br><span class="line">    quickSort(arr, i + <span class="number">1</span>, end);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 归并排序</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> arr</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> start</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> end</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> temp</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> start, <span class="keyword">int</span> end, <span class="keyword">int</span>... temp)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (start &gt;= end)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> mid = (start + end) / <span class="number">2</span>;</span><br><span class="line">    mergeSort(arr, start, mid, temp);</span><br><span class="line">    mergeSort(arr, mid + <span class="number">1</span>, end, temp);</span><br><span class="line">    <span class="comment">// 合并两个有序序列</span></span><br><span class="line">    <span class="keyword">int</span> length = <span class="number">0</span>; <span class="comment">// 表示辅助空间有多少个元素</span></span><br><span class="line">    <span class="keyword">int</span> i_start = start;</span><br><span class="line">    <span class="keyword">int</span> i_end = mid;</span><br><span class="line">    <span class="keyword">int</span> j_start = mid + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> j_end = end;</span><br><span class="line">    <span class="keyword">while</span> (i_start &lt;= i_end &amp;&amp; j_start &lt;= j_end) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i_start] &lt; arr[j_start]) &#123;</span><br><span class="line">            temp[length] = arr[i_start];</span><br><span class="line">            length++;</span><br><span class="line">            i_start++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            temp[length] = arr[j_start];</span><br><span class="line">            length++;</span><br><span class="line">            j_start++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (i_start &lt;= i_end) &#123;</span><br><span class="line">        temp[length] = arr[i_start];</span><br><span class="line">        i_start++;</span><br><span class="line">        length++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (j_start &lt;= j_end) &#123;</span><br><span class="line">        temp[length] = arr[j_start];</span><br><span class="line">        length++;</span><br><span class="line">        j_start++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 把辅助空间的数据放到原空间</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">        arr[start + i] = temp[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">heapAdjust</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> i, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 调整i位置的结点</span></span><br><span class="line">    <span class="comment">// 先保存当前结点的下标</span></span><br><span class="line">    <span class="keyword">int</span> max = i;</span><br><span class="line">    <span class="comment">// 当前结点左右孩子结点的下标</span></span><br><span class="line">    <span class="keyword">int</span> lchild = i * <span class="number">2</span> + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> rchild = i * <span class="number">2</span> + <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span> (lchild &lt; length &amp;&amp; arr[lchild] &gt; arr[max]) &#123;</span><br><span class="line">        max = lchild;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (rchild &lt; length &amp;&amp; arr[rchild] &gt; arr[max]) &#123;</span><br><span class="line">        max = rchild;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 若i处的值比其左右孩子结点的值小，就将其和最大值进行交换</span></span><br><span class="line">    <span class="keyword">if</span> (max != i) &#123;</span><br><span class="line">        <span class="keyword">int</span> temp;</span><br><span class="line">        temp = arr[i];</span><br><span class="line">        arr[i] = arr[max];</span><br><span class="line">        arr[max] = temp;</span><br><span class="line">        <span class="comment">// 递归</span></span><br><span class="line">        heapAdjust(arr, max, length);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 堆排序</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">heapSort</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 初始化堆</span></span><br><span class="line">    <span class="comment">// length / 2 - 1是二叉树中最后一个非叶子结点的序号</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = length / <span class="number">2</span> - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        heapAdjust(arr, i, length);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 交换堆顶元素和最后一个元素</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">int</span> temp;</span><br><span class="line">        temp = arr[i];</span><br><span class="line">        arr[i] = arr[<span class="number">0</span>];</span><br><span class="line">        arr[<span class="number">0</span>] = temp;</span><br><span class="line">        heapAdjust(arr, <span class="number">0</span>, i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>将SBT的依赖转换为Maven目录结构的依赖</title>
    <url>/2019/12/06/%E5%B0%86SBT%E7%9A%84%E4%BE%9D%E8%B5%96%E8%BD%AC%E6%8D%A2%E4%B8%BAMaven%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E7%9A%84%E4%BE%9D%E8%B5%96/</url>
    <content><![CDATA[<blockquote>
<p>如果不联网的话,需要写程序对ivy仓库目录进行刷新</p>
</blockquote>
<span id="more"></span>

<h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 主要是使用了mvn命令,ivy仓库中的original结尾文件与maven仓库中pom文件内容一致</span><br><span class="line"># 遍历ivy仓库中以original结尾的文件,执行命令</span><br><span class="line">mvn -f 文件.original dependency:list</span><br><span class="line"></span><br><span class="line"># windows系统下需要加dependency参数</span><br><span class="line"># Linux系统下不需要</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dependency:tree # 显示依赖树</span><br><span class="line">dependency:list # 显示依赖</span><br><span class="line">dependency:copy-dependencies # 赋值依赖jar到目标文件 一般与-DoutputDirectory&#x3D;&#x2F;libs一起使用</span><br><span class="line">dependency:resolve 打印出已解决依赖的列表 </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>编译</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>sbt</tag>
      </tags>
  </entry>
  <entry>
    <title>常用小工具收集</title>
    <url>/2019/08/28/%E5%B8%B8%E7%94%A8%E5%B0%8F%E5%B7%A5%E5%85%B7%E6%94%B6%E9%9B%86/</url>
    <content><![CDATA[<blockquote>
<p>工作上比较好用的一些辅助工具</p>
</blockquote>
<span id="more"></span>

<h2 id="Vitrite"><a href="#Vitrite" class="headerlink" title="Vitrite"></a>Vitrite</h2><blockquote>
<p>一款轻量级的窗口透明化工具</p>
</blockquote>
<hr>
<h2 id="HiJson"><a href="#HiJson" class="headerlink" title="HiJson"></a>HiJson</h2><blockquote>
<p>离线Json解析器</p>
</blockquote>
<hr>
<h2 id="小白盘"><a href="#小白盘" class="headerlink" title="小白盘"></a>小白盘</h2><blockquote>
<p>盘搜搜凉了</p>
</blockquote>
<hr>
<h2 id="EveryThing"><a href="#EveryThing" class="headerlink" title="EveryThing"></a>EveryThing</h2><blockquote>
<p>全局文件搜索工具</p>
</blockquote>
<hr>
<h2 id="向日葵"><a href="#向日葵" class="headerlink" title="向日葵"></a>向日葵</h2><blockquote>
<p>远程连接工具</p>
</blockquote>
<hr>
<h2 id="HexoEditor"><a href="#HexoEditor" class="headerlink" title="HexoEditor"></a>HexoEditor</h2><blockquote>
<p>MD编写工具</p>
</blockquote>
]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库基础配置</title>
    <url>/2018/07/23/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<blockquote>
<p>数据库设置</p>
</blockquote>
<span id="more"></span>

<h1 id="字符编码配置"><a href="#字符编码配置" class="headerlink" title="字符编码配置"></a>字符编码配置</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">default-character-set=utf8mb4 </span><br><span class="line">[mysql]</span><br><span class="line">default-character-set=utf8mb4</span><br><span class="line">[mysqld]</span><br><span class="line">character-set-server=utf8mb4</span><br><span class="line">collation-server=utf8mb4_unicode_ci</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><blockquote>
<p>如果是Windows版本,在修改my.ini时重启不了,可以通过杀死对应进程之后,对my.ini文件进行另存为,选择ASCII编码,即可解决问题.<br>在mysqld中使用default-character-set设置,mysql启动会报错而无法启动.</p>
</blockquote>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>实现单链表反转</title>
    <url>/2016/06/25/%E5%AE%9E%E7%8E%B0%E5%8D%95%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/</url>
    <content><![CDATA[<blockquote>
<p>链表实现以及反转链表</p>
</blockquote>
<span id="more"></span>

<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.algorithm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingleListReverse</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ListNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        ListNode next;</span><br><span class="line"></span><br><span class="line">        ListNode(<span class="keyword">int</span> x) &#123;</span><br><span class="line">            val = x;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * temp = b     a.next = null    prev = a    head = b</span></span><br><span class="line"><span class="comment">     * temp = c     b.next = a       prev = b    head = c</span></span><br><span class="line"><span class="comment">     * temp = d     c.next = b       prev = c    head = d</span></span><br><span class="line"><span class="comment">     * temp = null  d.next = c       prev = d    head = null</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> head</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> ListNode <span class="title">reverseList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        ListNode prev = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">while</span> (head != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ListNode temp = head.next;</span><br><span class="line">            head.next = prev;</span><br><span class="line">            prev = head;</span><br><span class="line">            head = temp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> prev;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ListNode a = <span class="keyword">new</span> ListNode(<span class="number">1</span>);</span><br><span class="line">        ListNode b = <span class="keyword">new</span> ListNode(<span class="number">2</span>);</span><br><span class="line">        ListNode c = <span class="keyword">new</span> ListNode(<span class="number">3</span>);</span><br><span class="line">        ListNode d = <span class="keyword">new</span> ListNode(<span class="number">4</span>);</span><br><span class="line">        a.next = b;</span><br><span class="line">        b.next = c;</span><br><span class="line">        c.next = d;</span><br><span class="line"></span><br><span class="line">        ListNode result = reverseList(a);</span><br><span class="line">        System.out.println(result.next.val);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>常用的脚本整合</title>
    <url>/2017/08/20/%E5%B8%B8%E7%94%A8%E7%9A%84%E8%84%9A%E6%9C%AC%E6%95%B4%E5%90%88/</url>
    <content><![CDATA[<blockquote>
<p>有一部分命令在&lt;**运维-系统常用的命令**&gt;</p>
</blockquote>
<span id="more"></span>

<h2 id="监控Yarn任务运行情况"><a href="#监控Yarn任务运行情况" class="headerlink" title="监控Yarn任务运行情况"></a>监控Yarn任务运行情况</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#### vi exec.sh</span></span><br><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"></span><br><span class="line">title=<span class="string">&quot;SparkStreaming任务监控&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取正在运行的任务</span></span><br><span class="line">name=(`sshpass -p <span class="string">&quot;password&quot;</span> ssh root@ip <span class="string">&quot;yarn application -list -appStates RUNNING|sed -n &#x27;3~1p&#x27;&quot;</span>|awk -F <span class="string">&#x27;\t&#x27;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>|sed -e <span class="string">&#x27;s/[[:space:]]//g&#x27;</span>`)</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> `dirname <span class="variable">$0</span>`</span><br><span class="line"></span><br><span class="line">base=`<span class="built_in">pwd</span>`</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取name文件中的内容，去除空格</span></span><br><span class="line">arr=(`cat ./name|sed -e <span class="string">&#x27;s/[[:space:]]//g&#x27;</span>`)</span><br><span class="line">dt=`date <span class="string">&quot;+%Y-%m-%d_%H%M%S&quot;</span>`</span><br><span class="line"><span class="built_in">log</span>=<span class="string">&quot;./&quot;</span><span class="variable">$dt</span><span class="string">&quot;.stop&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$&#123;arr[@]&#125;</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">     <span class="keyword">for</span> j <span class="keyword">in</span> <span class="variable">$&#123;name[@]&#125;</span></span><br><span class="line">     <span class="keyword">do</span></span><br><span class="line">           [ <span class="string">&quot;<span class="variable">$i</span>&quot;</span> = <span class="string">&quot;<span class="variable">$j</span>&quot;</span> ] &amp;&amp; flag=<span class="string">&quot;yes&quot;</span></span><br><span class="line">     <span class="keyword">done</span></span><br><span class="line">     [ <span class="string">&quot;<span class="variable">$flag</span>&quot;</span> != <span class="string">&quot;yes&quot;</span> ] &amp;&amp; <span class="built_in">echo</span> <span class="variable">$i</span> &gt;&gt; <span class="variable">$log</span></span><br><span class="line">     flag=<span class="string">&quot;false&quot;</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-e 判断文件是否存在</span></span><br><span class="line"><span class="comment">#cat $log|xargs echo 输出到一行</span></span><br><span class="line"><span class="comment">#&#x27;s/ /,/g&#x27; 以逗号分开</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -e <span class="variable">$log</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">   z=`cat <span class="variable">$log</span>|xargs <span class="built_in">echo</span>|sed -e <span class="string">&#x27;s/ /,/g&#x27;</span>`</span><br><span class="line">   <span class="built_in">cd</span> <span class="variable">$base</span> &amp;&amp; python <span class="variable">$base</span>/send_mail.py <span class="variable">$title</span> <span class="variable">$z</span></span><br><span class="line">   rm -rf <span class="variable">$log</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### vi name</span></span><br><span class="line">jobname1</span><br><span class="line">jobname2</span><br><span class="line">jobname3</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="删除日志文件脚本"><a href="#删除日志文件脚本" class="headerlink" title="删除日志文件脚本"></a>删除日志文件脚本</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用find</span></span><br><span class="line">find /root/<span class="built_in">test</span>/ -<span class="built_in">type</span> f -newermt <span class="string">&quot;Aug 19&quot;</span> -delete <span class="comment"># 删除8月19日的文件</span></span><br><span class="line">find /root/ -<span class="built_in">type</span> f -mtime +1 -delete <span class="comment"># 删除一天前的文件</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="增删后缀"><a href="#增删后缀" class="headerlink" title="增删后缀"></a>增删后缀</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 增加后缀</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> *</span><br><span class="line">   <span class="keyword">do</span> mv <span class="variable">$i</span> <span class="variable">$i</span><span class="string">&quot;.bak&quot;</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除后缀</span></span><br><span class="line">rename <span class="string">&#x27;s/\.xml$//&#x27;</span> *.xml</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="crontab定时脚本"><a href="#crontab定时脚本" class="headerlink" title="crontab定时脚本"></a>crontab定时脚本</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crontab -l <span class="comment"># 列出定时任务</span></span><br><span class="line">crontab -e <span class="comment"># 编辑定时任务</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定时任务格式</span></span><br><span class="line">* * * * * sh /root/os.sh &gt; /root/os.log</span><br><span class="line">第一个*为分(0-59)</span><br><span class="line">第二个*为时(0-23)</span><br><span class="line">第三个*为日(1-31)</span><br><span class="line">第四个*为月(1-12)</span><br><span class="line">第五个*为周(1-7)或(0-6),周日=0或7</span><br><span class="line"></span><br><span class="line">n-m表示到n到m之间</span><br><span class="line">*/n表示每n段时间</span><br><span class="line">n,m表示第n和第m</span><br><span class="line"></span><br><span class="line">&gt; /dev/null 2&gt;&amp;1</span><br><span class="line">&gt;代表重定向</span><br><span class="line">/dev/null代表空设备文件</span><br><span class="line">2&gt;&amp;1表示标准错误输出重定向等同于标准输出</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="发送邮件"><a href="#发送邮件" class="headerlink" title="发送邮件"></a>发送邮件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#### vi send_mail.py</span></span><br><span class="line"><span class="comment"># 使用python来进行发送邮件</span></span><br><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> sys </span><br><span class="line"><span class="keyword">import</span> smtplib</span><br><span class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</span><br><span class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</span><br><span class="line"><span class="keyword">from</span> email.mime.multipart <span class="keyword">import</span> MIMEMultipart</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 第三方 SMTP 服务</span></span><br><span class="line">mail_host=<span class="string">&quot;smtp.sina.com&quot;</span>  <span class="comment">#设置服务器</span></span><br><span class="line">mail_user=<span class="string">&quot;username&quot;</span>    <span class="comment">#用户名</span></span><br><span class="line">mail_pass=<span class="string">&quot;password&quot;</span>   <span class="comment">#口令 </span></span><br><span class="line"> </span><br><span class="line">sender = <span class="string">&#x27;发送人&#x27;</span></span><br><span class="line">receivers = [<span class="string">&#x27;接收邮箱1&#x27;</span>,<span class="string">&#x27;接收邮箱2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_file</span>(<span class="params">title,file</span>):</span></span><br><span class="line">    message = MIMEMultipart()</span><br><span class="line">    message[<span class="string">&#x27;From&#x27;</span>] = Header(<span class="string">&quot;发送人&quot;</span>)</span><br><span class="line">    subject = title</span><br><span class="line">    <span class="comment"># 标题</span></span><br><span class="line">    message[<span class="string">&#x27;Subject&#x27;</span>] = Header(subject, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 信息</span></span><br><span class="line">    message.attach(MIMEText(title, <span class="string">&#x27;plain&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 附件</span></span><br><span class="line">    att1 = MIMEText(<span class="built_in">open</span>(file, <span class="string">&#x27;rb&#x27;</span>).read(), <span class="string">&#x27;base64&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    att1[<span class="string">&quot;Content-Type&quot;</span>] = <span class="string">&#x27;application/octet-stream&#x27;</span></span><br><span class="line">    att1[<span class="string">&quot;Content-Disposition&quot;</span>] = <span class="string">&#x27;attachment; filename=&quot;att.log&quot;&#x27;</span></span><br><span class="line">    message.attach(att1)</span><br><span class="line">    </span><br><span class="line">    smtpObj = smtplib.SMTP() </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        smtpObj.connect(mail_host, <span class="number">25</span>)    <span class="comment"># 25 为 SMTP 端口号</span></span><br><span class="line">        smtpObj.login(mail_user,mail_pass)  </span><br><span class="line">        smtpObj.sendmail(sender, receivers, message.as_string())</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;邮件发送成功&quot;</span></span><br><span class="line">    <span class="keyword">except</span> smtplib.SMTPException:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Error: 无法发送邮件&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_html</span>(<span class="params">title,msg</span>):</span></span><br><span class="line">    message = MIMEMultipart()</span><br><span class="line">    message[<span class="string">&#x27;From&#x27;</span>] = Header(<span class="string">&quot;发送人&quot;</span>)</span><br><span class="line">    subject = title</span><br><span class="line">    message[<span class="string">&#x27;Subject&#x27;</span>] = Header(subject, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># HTML信息</span></span><br><span class="line">    message.attach(MIMEText(<span class="string">&quot;&lt;font color=&#x27;red&#x27;&gt;&quot;</span>+msg+<span class="string">&quot;&lt;/font&gt;&quot;</span>, <span class="string">&#x27;html&#x27;</span>, <span class="string">&#x27;utf-8&#x27;</span>)) </span><br><span class="line">    </span><br><span class="line">    smtpObj = smtplib.SMTP() </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        smtpObj.connect(mail_host, <span class="number">25</span>)    <span class="comment"># 25 为 SMTP 端口号</span></span><br><span class="line">        smtpObj.login(mail_user,mail_pass)  </span><br><span class="line">        smtpObj.sendmail(sender, receivers, message.as_string())</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;邮件发送成功&quot;</span></span><br><span class="line">    <span class="keyword">except</span> smtplib.SMTPException:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Error: 无法发送邮件&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sys.argv) &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;usage: python send_mail.py title msg&quot;</span></span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    title = sys.argv[<span class="number">1</span>]</span><br><span class="line">    msg = sys.argv[<span class="number">2</span>]</span><br><span class="line">    send_html(title,msg)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Flink任务重启脚本"><a href="#Flink任务重启脚本" class="headerlink" title="Flink任务重启脚本"></a>Flink任务重启脚本</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#### vi getConfigChk.sh</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># 获取配置文件中的ck目录</span></span><br><span class="line"><span class="comment"># fs.backend.dir=/job/flink/ck/job1</span></span><br><span class="line">dir=`cat <span class="variable">$1</span> | grep <span class="string">&quot;fs.backend.dir&quot;</span> | awk -F <span class="string">&quot;=&quot;</span> <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>`</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取HDFS上最新的CK文件</span></span><br><span class="line">path=`hdfs dfs -ls -R <span class="variable">$&#123;dir&#125;</span> | grep _metadata | sort -k 6,7 -r | sed -n 1p | awk <span class="string">&#x27;&#123;print $8&#125;&#x27;</span>`</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ ! <span class="string">&quot;<span class="variable">$path</span>&quot;</span> =~ ^hdfs* ]]; <span class="keyword">then</span></span><br><span class="line">  path=<span class="string">&quot;hdfs://<span class="variable">$path</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$path</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### vi exec.sh</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">BASE=<span class="string">&quot;/root/xiashuai/real_job/data_sync&quot;</span></span><br><span class="line">config=<span class="string">&quot;conf.properties&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># flink 作业样例</span></span><br><span class="line">chk=`getConfigChk <span class="variable">$BASE</span>/<span class="variable">$config</span>`</span><br><span class="line">jarName=<span class="string">&#x27;real_data_clean-full.jar&#x27;</span></span><br><span class="line">className=<span class="string">&#x27;com.example.SyncToHiveMain&#x27;</span></span><br><span class="line">appName=<span class="string">&#x27;DataSync&#x27;</span></span><br><span class="line">parallelism=1</span><br><span class="line">slot=1</span><br><span class="line">noRecover=<span class="literal">false</span></span><br><span class="line"><span class="keyword">for</span> arg <span class="keyword">in</span> <span class="variable">$@</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$arg</span>&quot;</span> = <span class="string">&quot;-init&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">   noRecover=<span class="literal">true</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$chk</span>&quot;</span> = <span class="string">&quot;hdfs://&quot;</span> -o <span class="variable">$noRecover</span> = <span class="literal">true</span> ]; <span class="keyword">then</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot;##########################################&quot;</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot;#              初始化运行                #&quot;</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot;##########################################&quot;</span></span><br><span class="line"> flink run -m yarn-cluster \</span><br><span class="line"> -ynm <span class="variable">$appName</span> \</span><br><span class="line"> -p <span class="variable">$parallelism</span> -ys <span class="variable">$slot</span> -ytm 2048 \</span><br><span class="line"> -c <span class="variable">$className</span> \</span><br><span class="line"> <span class="variable">$BASE</span>/../<span class="variable">$jarName</span> \</span><br><span class="line"> <span class="variable">$BASE</span>/<span class="variable">$config</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot;##########################################&quot;</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot;# 加载 checkpoint =&gt; <span class="variable">$chk</span>                 &quot;</span></span><br><span class="line"> <span class="built_in">echo</span> <span class="string">&quot;##########################################&quot;</span></span><br><span class="line"> flink run -m yarn-cluster \</span><br><span class="line"> -ynm <span class="variable">$appName</span><span class="string">&quot;-recover&quot;</span> \</span><br><span class="line"> -p <span class="variable">$parallelism</span> -ys <span class="variable">$slot</span> -ytm 2048 \</span><br><span class="line"> -c <span class="variable">$className</span> \</span><br><span class="line"> -s <span class="variable">$chk</span> \</span><br><span class="line"> <span class="variable">$BASE</span>/../<span class="variable">$jarName</span> \</span><br><span class="line"> <span class="variable">$BASE</span>/<span class="variable">$config</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构图的遍历</title>
    <url>/2021/04/06/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%9B%BE%E7%9A%84%E9%81%8D%E5%8E%86/</url>
    <content><![CDATA[<blockquote>
<p>对于不同的跳转都有其对应出度或入度,遍历所有路径</p>
</blockquote>
<span id="more"></span>

<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GraphDemo</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 边</span></span><br><span class="line">  <span class="keyword">var</span> edgeSet: <span class="type">Set</span>[<span class="type">String</span>] = <span class="type">Set</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="comment">// 节点对应出度</span></span><br><span class="line">  <span class="keyword">var</span> nodeMap: scala.collection.mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">String</span>]] = scala.collection.mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">String</span>]]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 生成节点对应出度信息</span></span><br><span class="line"><span class="comment">   * @param list</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">generate</span></span>(list: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    list.map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> kv = x.split(<span class="string">&quot;-&gt;&quot;</span>)</span><br><span class="line">      <span class="keyword">var</span> value: <span class="type">Seq</span>[<span class="type">String</span>] = nodeMap.getOrElse(kv(<span class="number">0</span>), <span class="type">Seq</span>[<span class="type">String</span>]())</span><br><span class="line">      value = value :+ kv(<span class="number">1</span>)</span><br><span class="line">      nodeMap.put(kv(<span class="number">0</span>),value)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 输出指定节点到叶子节点的所有路径</span></span><br><span class="line"><span class="comment">   * @param root</span></span><br><span class="line"><span class="comment">   * @param path</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dis</span></span>(root: <span class="type">String</span>, path: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> nodes = nodeMap.getOrElse(root, <span class="type">Seq</span>[<span class="type">String</span>]())</span><br><span class="line">    nodes.foreach(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> temp = path + x</span><br><span class="line">      dis(x, temp)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">if</span>(nodes.isEmpty)&#123;</span><br><span class="line">      println(path)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">&quot;a-&gt;b&quot;</span>, <span class="string">&quot;c-&gt;d&quot;</span>, <span class="string">&quot;c-&gt;e&quot;</span>, <span class="string">&quot;b-&gt;c&quot;</span>, <span class="string">&quot;e-&gt;f&quot;</span>)</span><br><span class="line">    generate(list)</span><br><span class="line">    println(nodeMap)</span><br><span class="line">    <span class="comment">// Map(e -&gt; List(f), b -&gt; List(c), a -&gt; List(b), c -&gt; List(d, e))</span></span><br><span class="line">    dis(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;a&quot;</span>)</span><br><span class="line">    <span class="comment">// abcd</span></span><br><span class="line">    <span class="comment">// abcef</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="处理逻辑"><a href="#处理逻辑" class="headerlink" title="处理逻辑"></a>处理逻辑</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.将所有边进行拆解,可以得到上游与下游的关系</span><br><span class="line">2.对指定节点开始进行递归处理,直到节点没有下游节点为之</span><br><span class="line">3.在没有下游节点时输出</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">只适合DAG,如果出现环的情况下,该逻辑会有问题</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构-树</title>
    <url>/2021/02/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%91/</url>
    <content><![CDATA[<blockquote>
<p>学习数据结构算法题</p>
</blockquote>
<span id="more"></span>

<h2 id="基本树遍历"><a href="#基本树遍历" class="headerlink" title="基本树遍历"></a>基本树遍历</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNodeDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val;</span><br><span class="line">        TreeNode left;</span><br><span class="line">        TreeNode right;</span><br><span class="line"></span><br><span class="line">        TreeNode(<span class="keyword">int</span> val) &#123;</span><br><span class="line">            <span class="keyword">this</span>.val = val;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        TreeNode node1 = <span class="keyword">new</span> TreeNode(<span class="number">1</span>);</span><br><span class="line">        TreeNode node2 = <span class="keyword">new</span> TreeNode(<span class="number">2</span>);</span><br><span class="line">        TreeNode node3 = <span class="keyword">new</span> TreeNode(<span class="number">3</span>);</span><br><span class="line">        TreeNode node4 = <span class="keyword">new</span> TreeNode(<span class="number">4</span>);</span><br><span class="line">        TreeNode node5 = <span class="keyword">new</span> TreeNode(<span class="number">5</span>);</span><br><span class="line">        node1.left = node2;</span><br><span class="line">        node1.right = node3;</span><br><span class="line">        node2.left = node4;</span><br><span class="line">        node2.right = node5;</span><br><span class="line"></span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; ret = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        afterOrderTreeStack(node1);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先序遍历</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">beforeOrderTree</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        System.out.println(node.val);</span><br><span class="line">        beforeOrderTree(node.left);</span><br><span class="line">        beforeOrderTree(node.right);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">beforeOrderTreeStack</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        Stack&lt;TreeNode&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        stack.push(node);</span><br><span class="line">        <span class="keyword">while</span> (!stack.isEmpty()) &#123;</span><br><span class="line">            TreeNode pop = stack.pop();</span><br><span class="line">            System.out.println(pop.val);</span><br><span class="line">            <span class="keyword">if</span> (pop.right != <span class="keyword">null</span>) stack.push(pop.right);</span><br><span class="line">            <span class="keyword">if</span> (pop.left != <span class="keyword">null</span>) stack.push(pop.left);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 中序遍历</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mobileOrderTree</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        beforeOrderTree(node.left);</span><br><span class="line">        System.out.println(node.val);</span><br><span class="line">        beforeOrderTree(node.right);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mobileOrderTreeStack</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        Stack&lt;TreeNode&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        <span class="keyword">while</span> (node != <span class="keyword">null</span> || !stack.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">while</span> (node != <span class="keyword">null</span>) &#123;</span><br><span class="line">                stack.push(node);</span><br><span class="line">                node = node.left;</span><br><span class="line">            &#125;</span><br><span class="line">            TreeNode pop = stack.pop();</span><br><span class="line">            System.out.println(pop.val);</span><br><span class="line">            node = pop.right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 后序遍历</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">afterOrderTree</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        beforeOrderTree(node.left);</span><br><span class="line">        System.out.println(node.val);</span><br><span class="line">        beforeOrderTree(node.right);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">afterOrderTreeStack</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        Stack&lt;TreeNode&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">        stack.push(node);</span><br><span class="line">        <span class="keyword">while</span> (!stack.isEmpty()) &#123;</span><br><span class="line">            TreeNode pop = stack.pop();</span><br><span class="line">            System.out.println(pop.val);</span><br><span class="line">            <span class="keyword">if</span> (pop.left != <span class="keyword">null</span>) stack.push(pop.left);</span><br><span class="line">            <span class="keyword">if</span> (pop.right != <span class="keyword">null</span>) stack.push(pop.right);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输出需要进行反转</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 层级遍历-递归</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">levelOrderTreeRecursion</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; ret, TreeNode node, <span class="keyword">int</span> deep)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">if</span> (ret.size() == deep) ret.add(<span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line">        ret.get(deep).add(node.val);</span><br><span class="line">        levelOrderTreeRecursion(ret, node.left, deep + <span class="number">1</span>);</span><br><span class="line">        levelOrderTreeRecursion(ret, node.right, deep + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 层级遍历-队列</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">levelOrderTreeQueue</span><span class="params">(TreeNode node)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Queue&lt;TreeNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        queue.add(node);</span><br><span class="line">        <span class="keyword">while</span> (!queue.isEmpty()) &#123;</span><br><span class="line">            TreeNode poll = queue.poll();</span><br><span class="line">            System.out.println(poll.val);</span><br><span class="line">            <span class="keyword">if</span> (poll.left != <span class="keyword">null</span>) queue.add(poll.left);</span><br><span class="line">            <span class="keyword">if</span> (poll.right != <span class="keyword">null</span>) queue.add(poll.right);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>整理概念性知识</title>
    <url>/2021/02/02/%E6%95%B4%E7%90%86%E6%A6%82%E5%BF%B5%E6%80%A7%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<blockquote>
<p>整理最近被问到的一些概念性知识,虽然有很多都是整理过的,但是被问到的时候还是会回答得不流畅</p>
</blockquote>
<span id="more"></span>

<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="大数据理论知识合集"><a href="#大数据理论知识合集" class="headerlink" title="大数据理论知识合集"></a><a href="https://jxeditor.github.io/2019/11/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%90%88%E9%9B%86/">大数据理论知识合集</a></h3><hr>
<h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><h3 id="Flink面试整理"><a href="#Flink面试整理" class="headerlink" title="Flink面试整理"></a><a href="https://jxeditor.github.io/2020/02/27/Flink%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86/">Flink面试整理</a></h3><h3 id="Flink系列问题"><a href="#Flink系列问题" class="headerlink" title="Flink系列问题"></a><a href="https://jxeditor.github.io/2019/09/23/Flink%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98/">Flink系列问题</a></h3><h3 id="Flink反压原理"><a href="#Flink反压原理" class="headerlink" title="Flink反压原理"></a><a href="https://jxeditor.github.io/2019/06/14/Ververica&Flink%E8%BF%9B%E9%98%B6%E4%B9%8B%E4%B8%83%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%8F%8A%E5%8F%8D%E5%8E%8B(%E7%B2%BE)/">Flink反压原理</a></h3><hr>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="Spark面试整理"><a href="#Spark面试整理" class="headerlink" title="Spark面试整理"></a><a href="https://jxeditor.github.io/2020/03/06/Spark%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86/">Spark面试整理</a></h3><h3 id="Spark任务生成和提交过程之OnStandAlone"><a href="#Spark任务生成和提交过程之OnStandAlone" class="headerlink" title="Spark任务生成和提交过程之OnStandAlone"></a><a href="https://jxeditor.github.io/2017/11/21/Spark%E4%BB%BB%E5%8A%A1%E7%94%9F%E6%88%90%E5%92%8C%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BOnStandAlone/">Spark任务生成和提交过程之OnStandAlone</a></h3><h3 id="Spark任务生成和提交过程之OnYarn"><a href="#Spark任务生成和提交过程之OnYarn" class="headerlink" title="Spark任务生成和提交过程之OnYarn"></a><a href="https://jxeditor.github.io/2017/11/21/Spark%E4%BB%BB%E5%8A%A1%E7%94%9F%E6%88%90%E5%92%8C%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%E4%B9%8BOnYarn/">Spark任务生成和提交过程之OnYarn</a></h3><hr>
<h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><h3 id="组件面试整理"><a href="#组件面试整理" class="headerlink" title="组件面试整理"></a><a href="https://jxeditor.github.io/2020/03/16/%E7%BB%84%E4%BB%B6%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86/">组件面试整理</a></h3><h3 id="HBase知识点整理"><a href="#HBase知识点整理" class="headerlink" title="HBase知识点整理"></a><a href="https://jxeditor.github.io/2018/01/31/HBase%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/">HBase知识点整理</a></h3><h3 id="HBase的基本命令"><a href="#HBase的基本命令" class="headerlink" title="HBase的基本命令"></a><a href="https://jxeditor.github.io/2017/11/23/HBase%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/">HBase的基本命令</a></h3><h3 id="HBase的工作原理"><a href="#HBase的工作原理" class="headerlink" title="HBase的工作原理"></a><a href="https://jxeditor.github.io/2017/11/23/HBase%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/">HBase的工作原理</a></h3><h3 id="Kafka的概念性知识整合"><a href="#Kafka的概念性知识整合" class="headerlink" title="Kafka的概念性知识整合"></a><a href="https://jxeditor.github.io/2018/01/25/Kafka%E7%9A%84%E6%A6%82%E5%BF%B5%E6%80%A7%E7%9F%A5%E8%AF%86%E6%95%B4%E5%90%88/">Kafka的概念性知识整合</a></h3><h3 id="Logstash与Flume监控之后做了什么操作"><a href="#Logstash与Flume监控之后做了什么操作" class="headerlink" title="Logstash与Flume监控之后做了什么操作"></a><a href="https://jxeditor.github.io/2017/10/29/Logstash%E4%B8%8EFlume%E7%9B%91%E6%8E%A7%E4%B9%8B%E5%90%8E%E5%81%9A%E4%BA%86%E4%BB%80%E4%B9%88%E6%93%8D%E4%BD%9C/">Logstash与Flume监控之后做了什么操作</a></h3><h3 id="Zookeeper的领导者选举和原子广播"><a href="#Zookeeper的领导者选举和原子广播" class="headerlink" title="Zookeeper的领导者选举和原子广播"></a><a href="https://jxeditor.github.io/2017/08/28/Zookeeper%E7%9A%84%E9%A2%86%E5%AF%BC%E8%80%85%E9%80%89%E4%B8%BE%E5%92%8C%E5%8E%9F%E5%AD%90%E5%B9%BF%E6%92%AD/">Zookeeper的领导者选举和原子广播</a></h3>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>数据仓库工具箱阅读学习之一</title>
    <url>/2020/07/26/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7%E7%AE%B1%E9%98%85%E8%AF%BB%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%80/</url>
    <content><![CDATA[<blockquote>
<p>根据目前工作内容,专注且系统化的学习下数仓方面知识</p>
</blockquote>
<span id="more"></span>

<h2 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NF:</span><br><span class="line">    符合某一级别的关系模式的集合,表示一个关系内部各属性之间的联系的合理化程度</span><br><span class="line">1NF:</span><br><span class="line">    冗余情况最严重</span><br><span class="line">    符合1NF的关系中的每个属性都不可再分</span><br><span class="line">    像进货字段,还可以分为进货数量,进货单价,那么就不符合1NF的要求</span><br><span class="line">2NF:</span><br><span class="line">    对1NF表进行拆分,对重复冗余的数据进行分表</span><br><span class="line">    例如将学生院系科目成绩表,拆分为学生表+成绩表,通过学号进行关联</span><br><span class="line">    但,如果涉及到删除操作,院系信息会丢失</span><br><span class="line">3NF:</span><br><span class="line">    数据库中强调的3NF主要是为了消除冗余</span><br><span class="line">    针对2NF,进行进一步拆分,学生院系科目成绩表,拆分为学生表+成绩表+院系表</span><br><span class="line">    这样,对于删除学生操作,对于院系是没有影响的</span><br><span class="line"></span><br><span class="line">BCNF(巴斯范式)&amp;4NF&amp;5NF:</span><br><span class="line">    这一块具体还不太理解多值依赖具体意义</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="星型模式-amp-雪花模式"><a href="#星型模式-amp-雪花模式" class="headerlink" title="星型模式&amp;雪花模式"></a>星型模式&amp;雪花模式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">星型模式:</span><br><span class="line">    在关系数据库管理系统中实现的维度模型</span><br><span class="line">雪花模式:</span><br><span class="line">    多重维度表层次,建立的多级层次结构被称为雪花模式</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="维度表"><a href="#维度表" class="headerlink" title="维度表"></a>维度表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">维度表是事实表不可或缺的组成部分</span><br><span class="line">退化维度:</span><br><span class="line">    维度除了主键外没有其他内容,但是数量仍然是合法维度键,常见于交易和累计快照事实表</span><br><span class="line">多层次维度:</span><br><span class="line">    多数维度包含不止一个自然层次,天,周,月,年之类</span><br><span class="line">扮演角色的维度:</span><br><span class="line">    单个物理维度可以被事实表多次引用,例如日期维度,可以有不同的角色(下单,支付等)</span><br><span class="line">处理缓慢变化维度属性:</span><br><span class="line">    类型0-原样保留:</span><br><span class="line">        维度属性值不会发生变化,适用于日期维度的大多数属性</span><br><span class="line">    类型1-重写:</span><br><span class="line">        维度行中原来的属性值被新值覆盖,会破坏历史情况</span><br><span class="line">    类型2-增加新行:</span><br><span class="line">        在维度表中增加新行,新行中采用修改的属性值,像拉链表</span><br><span class="line">    类型3-增加新属性:</span><br><span class="line">        类似保留原列,新开一列,不常用</span><br><span class="line">    类型4-增加微型维度:</span><br><span class="line">        用于维度中一组属性快速变化并划分为微型维度时采用,也叫快速变换魔鬼维度</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="事实表"><a href="#事实表" class="headerlink" title="事实表"></a>事实表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">事实这一术语表示某个业务度量</span><br><span class="line">事实表的粒度可划分为三类:</span><br><span class="line">    事务,周期性快照,累计快照</span><br><span class="line">        事务事实表的一行对应空间或时间上某点的度量事件</span><br><span class="line">        周期性快照事实表的每行汇总了发生在某一标准周期,某天,某周,某月的多个度量事件</span><br><span class="line">        累计快照事实表的行汇总了发生在过程开始和结束之间可预测步骤内的度量事件</span><br><span class="line">事实表数字度量划分为三类:</span><br><span class="line">    可加,半可加,不可加</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="不常用的事实表模式"><a href="#不常用的事实表模式" class="headerlink" title="不常用的事实表模式"></a>不常用的事实表模式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">事实表代理键</span><br><span class="line">    作为事实表唯一主键列,事实表行的直接标识符,允许更新操作分解为插入和删除操作</span><br><span class="line">蜈蚣事实表</span><br><span class="line">    表示包含与维度相关的多个维度,常见于日期,月份,季度,年度维度</span><br><span class="line">属性或事实的数字值</span><br><span class="line">    类似产品的标准价格,如果数字值用于计算目的,则属于事实表;如果数字值用于确定分组或过滤,应定义为维度属性</span><br><span class="line">日志&#x2F;持续时间事实</span><br><span class="line">    累计快照事实表获取多个过程里程牌,每个都包含日期外键并可能包含日期&#x2F;时间戳</span><br><span class="line">头&#x2F;行事实表</span><br><span class="line">    操作型交易系统通常包括事务头指针行,头指针行与多个事务行关联</span><br><span class="line">分配的事实</span><br><span class="line">    分配头指针事实,使其基于业务所提供的规则划分行级别</span><br><span class="line">利用分配建立利润与损失事实表</span><br><span class="line">    理想的实现利润方程的事实表应为收入事务粒度并包含许多开销项</span><br><span class="line">    收入-开销&#x3D;利润</span><br><span class="line">多种货币事实</span><br><span class="line">    以多种货币单位记录财务事实的事实表行应该包含一对列</span><br><span class="line">    其中一列包含以真实币种表示的事实,另一列包含同样的,但以整个事实表统一的单一标准币种表示的事实</span><br><span class="line">多种度量事实单位</span><br><span class="line">    需要事实同时以多种度量单位表示,可以统一标准度量,然后存储其他度量的转换系数</span><br><span class="line">年-日事实</span><br><span class="line">    商业用户在事实表中需要年-日(YTD)值</span><br><span class="line">多遍SQL以避免事实表间的连接</span><br><span class="line">    不应该跨事实表的外键处理两个事实表的连接操作</span><br><span class="line">针对事实表的时间跟踪</span><br><span class="line">    类似缓慢变化维度处理,增加行有效时期,行截止日期以及当前行标识</span><br><span class="line">迟到的事实</span><br><span class="line">    新事实行的多数当前维度内容无法匹配输入行的情况</span><br><span class="line">    通常发生在事实行延迟产生的时候</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="数仓总线"><a href="#数仓总线" class="headerlink" title="数仓总线"></a>数仓总线</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">总线矩阵</span><br><span class="line">    行表示业务过程,列表示维度,点表示维度与给定的业务过程是否存在关联关系</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="维度建模涉及过程"><a href="#维度建模涉及过程" class="headerlink" title="维度建模涉及过程"></a>维度建模涉及过程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.选择业务过程</span><br><span class="line">b.声明粒度</span><br><span class="line">c.确定维度</span><br><span class="line">d.确定事实</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>画像篇之试题画像</title>
    <url>/2020/08/10/%E7%94%BB%E5%83%8F%E7%AF%87%E4%B9%8B%E8%AF%95%E9%A2%98%E7%94%BB%E5%83%8F/</url>
    <content><![CDATA[<blockquote>
<p>试题画像是根据用户做题信息,老师发布试题情况以及收藏点赞等信息抽象出的一个标签化的模型</p>
</blockquote>
<span id="more"></span>

<h3 id="从静态信息数据和动态数据进行剖析"><a href="#从静态信息数据和动态数据进行剖析" class="headerlink" title="从静态信息数据和动态数据进行剖析"></a>从静态信息数据和动态数据进行剖析</h3><h4 id="静态信息数据"><a href="#静态信息数据" class="headerlink" title="静态信息数据"></a>静态信息数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">试题相对稳定的信息,更多的是数据清洗工作</span><br><span class="line">paper_type:试题类型(填空,单选,多选,试题组,单选组试题,多选组试题)</span><br><span class="line">difficult:老师标记的试题难度</span><br><span class="line">enable:是否有效(逻辑删除)</span><br><span class="line">concat_ws(&#39;,&#39;,body,resolve):将试题题干,解析,答案以及组试题的内容都合并在一个字段中;用于试题搜索</span><br><span class="line">学段信息:小学,初中,高中</span><br><span class="line">省份,城市,试卷类型,学校,年份:试题根据成卷的信息走,通过成卷和试题的关系;(单元测试如果计算出来的年份小于去年,修正成去年)</span><br><span class="line">年份权重:最近两年的算2,最近四年有小于两年的算1,其他为0</span><br><span class="line">区域权重:有城市的算1,没有城市有省份的为0.5,都没有为0</span><br><span class="line">中高考标记:主要标记出中高考真题和中高考模拟</span><br><span class="line">学科:存储最大的学科id</span><br><span class="line">以list的方式存在:</span><br><span class="line">教材id;教材name,版本id,年级id</span><br><span class="line">share_type:是否校内共享</span><br></pre></td></tr></table></figure>
<h4 id="动态信息数据"><a href="#动态信息数据" class="headerlink" title="动态信息数据"></a>动态信息数据</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">相关信息是随着用户的做题等情况进行动态变化的;根据用户的行为,可以看做试题动态信息的唯一数据来源;对用户行为数据构建数据模型,分析出试题标签</span><br><span class="line">ctt_diff:大数据预测的试题难度</span><br><span class="line">right_rate:正确率</span><br><span class="line">standard_time:试题的标准时长</span><br><span class="line">hot_option: 热门选项</span><br><span class="line">collect_count:试题收藏量</span><br><span class="line">use_count:试题使用量</span><br><span class="line">use_value:试题使用量权重(为null的为0;小于3的为0.5,大于3的为1)</span><br></pre></td></tr></table></figure>

<h4 id="主要用于网校-数校平台的试题搜索功能"><a href="#主要用于网校-数校平台的试题搜索功能" class="headerlink" title="主要用于网校,数校平台的试题搜索功能"></a>主要用于网校,数校平台的试题搜索功能</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">打通数校网校的数据后,进行试题的标记(有标记的为网校),可以直接使用在两个平台上;保证两平台的数据一致性</span><br><span class="line"></span><br><span class="line">对返回的数据进行重新打分,并根据省份,城市,年份和使用量进行权限的设置</span><br><span class="line"></span><br><span class="line">主要对body和point_name使用text类型,使用multi_match进行多字段匹配,type使用best_field,还可以通过tie_breaker来控制其他field的得分</span><br><span class="line">score&#x3D;best_field.score*boost.score+other_field*boost.score*tie_breaker</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="搜索系统之multi-match多字段匹配"><a href="#搜索系统之multi-match多字段匹配" class="headerlink" title="搜索系统之multi_match多字段匹配"></a>搜索系统之multi_match多字段匹配</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">multi_match多字段匹配的三种类型:best_fields(最佳字段),most_fields(多数字段),cross_fields(跨字段)</span><br><span class="line"></span><br><span class="line">1.best_fields:multi_match默认的查询类型,返回某一个字段匹配到最多关键字的文档;一般搭配tie_breaker,将其他匹配的查询子句考虑进来;通过指定tie_breaker参数将其他每个匹配的子句的分值乘以tie_breaker,以达到取得最佳匹配查询子句</span><br><span class="line"></span><br><span class="line">2.most_fields:多数字段匹配成功的得分之和,字段匹配越多,得分越高</span><br><span class="line"></span><br><span class="line">3.cross_fields:一个唯一标识,跨域多个字段;主要用于名字区域[first_name,last_name]或者[province,city](浙江省杭州市)</span><br></pre></td></tr></table></figure>

<h4 id="kibana相关代码摘取"><a href="#kibana相关代码摘取" class="headerlink" title="kibana相关代码摘取"></a>kibana相关代码摘取</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET new_ques_all_profile&#x2F;_search</span><br><span class="line">  &#123;</span><br><span class="line">  &quot;size&quot; : 20,</span><br><span class="line">  &quot;timeout&quot; : &quot;5s&quot;,</span><br><span class="line">  &quot;query&quot; : &#123;</span><br><span class="line">    &quot;bool&quot; : &#123;</span><br><span class="line">      &quot;must&quot; : [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;function_score&quot; : &#123;</span><br><span class="line">            &quot;query&quot; : &#123;</span><br><span class="line">              &quot;bool&quot; : &#123;</span><br><span class="line">                &quot;must&quot; : [</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;multi_match&quot; : &#123;</span><br><span class="line">                      &quot;query&quot; : &quot;用电场线能很直观&quot;,</span><br><span class="line">                      &quot;fields&quot; : [</span><br><span class="line">                        &quot;bodys^1.0&quot;,</span><br><span class="line">                        &quot;point_names^1.0&quot;</span><br><span class="line">                      ],</span><br><span class="line">                      &quot;type&quot; : &quot;best_fields&quot;,</span><br><span class="line">                      &quot;max_expansions&quot; : 50,</span><br><span class="line">                      &quot;minimum_should_match&quot; : &quot;50%&quot;,</span><br><span class="line">                      &quot;tie_breaker&quot; : 0.9</span><br><span class="line">                    &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">                ]</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;functions&quot; : [</span><br><span class="line">              &#123;</span><br><span class="line">                &quot;filter&quot; : &#123;</span><br><span class="line">                  &quot;match_all&quot; : &#123;</span><br><span class="line">                    &quot;boost&quot; : 1.0</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;script_score&quot; : &#123;</span><br><span class="line">                  &quot;script&quot; : &#123;</span><br><span class="line">                    &quot;inline&quot; : &quot;double area &#x3D; 0.0; if (params.c_id &#x3D;&#x3D; doc[&#39;city_id&#39;].value)&#123;area &#x3D; 1;&#125; if (params.c_id !&#x3D; doc[&#39;city_id&#39;].value &amp;&amp; params.p_id &#x3D;&#x3D; doc[&#39;province_id&#39;].value)&#123;area &#x3D; 0.5;&#125; Math.log(2+(doc[&#39;year_value&#39;].value+ area +doc[&#39;use_value&#39;].value))&#x2F;Math.log(10)&quot;,</span><br><span class="line">                    &quot;lang&quot; : &quot;painless&quot;,</span><br><span class="line">                    &quot;params&quot; : &#123;</span><br><span class="line">                      &quot;c_id&quot; : null,</span><br><span class="line">                      &quot;p_id&quot; : null</span><br><span class="line">                    &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            ]</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;filter&quot; : [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;range&quot; : &#123;</span><br><span class="line">            &quot;type&quot; : &#123;</span><br><span class="line">              &quot;from&quot; : null,</span><br><span class="line">              &quot;to&quot; : 7,</span><br><span class="line">              &quot;include_lower&quot; : true,</span><br><span class="line">              &quot;include_upper&quot; : false,</span><br><span class="line">              &quot;boost&quot; : 1.0</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;_source&quot; : &#123;</span><br><span class="line">    &quot;includes&quot; : [</span><br><span class="line">      &quot;question_id&quot;,</span><br><span class="line">      &quot;right_rate&quot;,</span><br><span class="line">      &quot;difficult&quot;,</span><br><span class="line">      &quot;cttdiff&quot;,</span><br><span class="line">      &quot;use_count&quot;,</span><br><span class="line">      &quot;collect_count&quot;,</span><br><span class="line">      &quot;paper_type_id&quot;,</span><br><span class="line">      &quot;exam_type&quot;,</span><br><span class="line">      &quot;year&quot;,</span><br><span class="line">      &quot;sizhong_type&quot;,</span><br><span class="line">      &quot;type&quot;,</span><br><span class="line">      &quot;province_names&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;excludes&quot; : [ ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>profile</tag>
      </tags>
  </entry>
  <entry>
    <title>简单解决接口调用跨域问题</title>
    <url>/2018/11/27/%E7%AE%80%E5%8D%95%E8%A7%A3%E5%86%B3%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8%E8%B7%A8%E5%9F%9F%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>简单的从接口端解决前端Ajax调用跨域问题</p>
</blockquote>
<span id="more"></span>

<h2 id="给Response添加Header"><a href="#给Response添加Header" class="headerlink" title="给Response添加Header"></a>给Response添加Header</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">response.addHeader(&quot;Access-Control-Allow-Origin&quot;,&quot;*&quot;);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>最大最小堆的实现TopN</title>
    <url>/2021/02/20/%E6%9C%80%E5%A4%A7%E6%9C%80%E5%B0%8F%E5%A0%86%E7%9A%84%E5%AE%9E%E7%8E%B0TopN/</url>
    <content><![CDATA[<blockquote>
<p>需要手撸的一道面试题</p>
</blockquote>
<span id="more"></span>

<h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">堆-&gt;完全二叉树</span><br><span class="line">对于完全二叉树</span><br><span class="line">    有子节点的节点遵循下标小于等于一个定式值(length &#x2F; 2 - 1)</span><br><span class="line">左叶子节点 &#x3D; (i + 1) &lt;&lt; 1 - 1</span><br><span class="line">右叶子节点 &#x3D; (i + 1) &lt;&lt; 1</span><br><span class="line"></span><br><span class="line">最小堆(父节点一定小于等于子节点)</span><br><span class="line">最大堆(父节点一定大于等于子节点)</span><br><span class="line"></span><br><span class="line">遍历有叶子节点的节点</span><br><span class="line">最小堆比较顺序:</span><br><span class="line">    左子节点&lt;父节点,进行交换</span><br><span class="line">    右子节点&lt;父节点,进行交换</span><br><span class="line">最大堆比较顺序:</span><br><span class="line">    左子节点&gt;父节点,进行交换</span><br><span class="line">    右子节点&gt;父节点,进行交换</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">    添加新节点时需要重新构建堆</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Heap</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 堆的存储结构 - 数组</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[] data;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将一个数组传入构造方法，并转换成一个堆</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Heap</span><span class="params">(<span class="keyword">int</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.data = data;</span><br><span class="line">        buildHeap();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将数组转换成堆</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">buildHeap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = (data.length) / <span class="number">2</span> - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            <span class="comment">// 对有子结点的元素heapify</span></span><br><span class="line">            heapify(i, data.length);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">heapify</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取左右结点的数组下标</span></span><br><span class="line">        <span class="keyword">int</span> l = left(i);</span><br><span class="line">        <span class="keyword">int</span> r = right(i);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这是一个临时变量，表示 跟结点、左结点、右结点中最小的值的结点的下标</span></span><br><span class="line">        <span class="keyword">int</span> smallest = i;</span><br><span class="line">        <span class="comment">// 存在左结点，且左结点的值小于根结点的值</span></span><br><span class="line">        <span class="keyword">if</span> (l &lt; length &amp;&amp; data[l] &lt; data[smallest])</span><br><span class="line">            smallest = l;</span><br><span class="line">        <span class="comment">// 存在右结点，且右结点的值小于以上比较的较小值</span></span><br><span class="line">        <span class="keyword">if</span> (r &lt; length &amp;&amp; data[r] &lt; data[smallest])</span><br><span class="line">            smallest = r;</span><br><span class="line">        <span class="comment">// 左右结点的值都大于根节点，直接return，不做任何操作</span></span><br><span class="line">        <span class="keyword">if</span> (i == smallest)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="comment">// 交换根节点和左右结点中最小的那个值，把根节点的值替换下去</span></span><br><span class="line">        swap(i, smallest);</span><br><span class="line">        <span class="comment">// 由于替换后左右子树会被影响，所以要对受影响的子树再进行heapify</span></span><br><span class="line">        heapify(smallest, length);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 交换堆顶元素和最后一个元素,再重新heapify可以排序,堆排序</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = data.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            swap(i, <span class="number">0</span>);</span><br><span class="line">            heapify(<span class="number">0</span>, i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取右结点的数组下标</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">right</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (i + <span class="number">1</span>) &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取左结点的数组下标</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">left</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ((i + <span class="number">1</span>) &lt;&lt; <span class="number">1</span>) - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 交换元素位置</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> tmp = data[i];</span><br><span class="line">        data[i] = data[j];</span><br><span class="line">        data[j] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取堆中的最小的元素，根元素</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getRoot</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> data[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 替换根元素，并重新heapify</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setRoot</span><span class="params">(<span class="keyword">int</span> root)</span> </span>&#123;</span><br><span class="line">        data[<span class="number">0</span>] = root;</span><br><span class="line">        heapify(<span class="number">0</span>, data.length);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 测试,此时堆中是没有顺序的</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HeapDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 源数据</span></span><br><span class="line">        <span class="keyword">int</span>[] data = &#123;<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">10</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取Top5</span></span><br><span class="line">        <span class="keyword">int</span>[] top5 = topK(data, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; top5.length; i++) &#123;</span><br><span class="line">            System.out.println(top5[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从data数组中获取最大的k个数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span>[] topK(<span class="keyword">int</span>[] data, <span class="keyword">int</span> k) &#123;</span><br><span class="line">        <span class="comment">// 先取K个元素放入一个数组topk中</span></span><br><span class="line">        <span class="keyword">int</span>[] topk = <span class="keyword">new</span> <span class="keyword">int</span>[k];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">            topk[i] = data[i];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Heap heap = <span class="keyword">new</span> Heap(topk);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从k开始，遍历data</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = k; i &lt; data.length; i++) &#123;</span><br><span class="line">            heap.sort();</span><br><span class="line">            <span class="keyword">int</span> root = heap.getRoot();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 当数据大于堆中最小的数（根节点）时，替换堆中的根节点，再转换成堆</span></span><br><span class="line">            <span class="keyword">if</span> (data[i] &gt; root) &#123;</span><br><span class="line">                heap.setRoot(data[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> topk;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>系统常用的命令</title>
    <url>/2017/06/18/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<blockquote>
<p>常用小命令</p>
</blockquote>
<span id="more"></span>

<h2 id="Linux关闭在线用户"><a href="#Linux关闭在线用户" class="headerlink" title="Linux关闭在线用户"></a>Linux关闭在线用户</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">who</span><br><span class="line"># 通知关闭该用户</span><br><span class="line">echo &quot;I will close your connection&quot; &gt; &#x2F;dev&#x2F;pts&#x2F;2</span><br><span class="line"># 关闭用户</span><br><span class="line">fuser -k &#x2F;dev&#x2F;pts&#x2F;2</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="查看端口占用并释放"><a href="#查看端口占用并释放" class="headerlink" title="查看端口占用并释放"></a>查看端口占用并释放</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># windows</span></span><br><span class="line">netstat -ano|findstr 端口号</span><br><span class="line">taskkill /pid 进程号 -f</span><br><span class="line"><span class="comment"># linux</span></span><br><span class="line">netstat -anop |grep 端口号</span><br><span class="line"><span class="built_in">kill</span> -9 进程号</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="UNICODE编码文件转换"><a href="#UNICODE编码文件转换" class="headerlink" title="UNICODE编码文件转换"></a>UNICODE编码文件转换</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 需要jdk</span><br><span class="line"># unicode转本地</span><br><span class="line">native2ascii -reverse ApplicationResources_zh.properties ApplicationResources_zh1.properties</span><br><span class="line"></span><br><span class="line"># 本地转unicode</span><br><span class="line">native2ascii ApplicationResources_zh1.properties ApplicationResources_zh.properties</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Linux后台进程管理"><a href="#Linux后台进程管理" class="headerlink" title="Linux后台进程管理"></a>Linux后台进程管理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Ctrl+Z: 可以将一个正在前台执行的命令放到后台，并且处于暂停状态，不可执行</span><br><span class="line"></span><br><span class="line">Ctrl+D: 表示结束当前输入（即用户不再给当前程序发出指令），那么Linux通常将结束当前程序</span><br><span class="line"></span><br><span class="line">jobs: 查看当前有多少在后台运行的命令</span><br><span class="line"></span><br><span class="line">fg: 将后台中的命令调至前台继续运行</span><br><span class="line"></span><br><span class="line">bg: 将一个在后台暂停的命令，变成继续执行 （在后台执行）</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="切换CMD字符编码"><a href="#切换CMD字符编码" class="headerlink" title="切换CMD字符编码"></a>切换CMD字符编码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看现在的编码</span><br><span class="line">chcp</span><br><span class="line"># 切换到utf-8</span><br><span class="line">chcp 65001</span><br><span class="line"># 设置cmd属性</span><br><span class="line">字体&#x3D;Lucida Console</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="限制服务器上传下载速度"><a href="#限制服务器上传下载速度" class="headerlink" title="限制服务器上传下载速度"></a>限制服务器上传下载速度</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install wondershaper</span><br><span class="line">pip install speedtest-cli</span><br><span class="line"></span><br><span class="line"># 限制上传速度2M,下载速度1M</span><br><span class="line">wondershaper eth0 2048 1024</span><br><span class="line"># 测试网速</span><br><span class="line">speedtest-cli</span><br><span class="line"># 解开限制</span><br><span class="line">wondershaper clean eth0</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="查看文件大小"><a href="#查看文件大小" class="headerlink" title="查看文件大小"></a>查看文件大小</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df -hT # 查看磁盘分区大小使用情况</span><br><span class="line">du -h --max-depth&#x3D;1 &#x2F;root # 查看深度为1的文件大小</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="清除缓存"><a href="#清除缓存" class="headerlink" title="清除缓存"></a>清除缓存</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/sh</span></span><br><span class="line">sync</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解压缩"><a href="#解压缩" class="headerlink" title="解压缩"></a>解压缩</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 解压</span><br><span class="line">tar -zxvf</span><br><span class="line"># 压缩</span><br><span class="line">tar -zcvf</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="隐藏文件到图片"><a href="#隐藏文件到图片" class="headerlink" title="隐藏文件到图片"></a>隐藏文件到图片</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 准备需要隐藏的文件与图片文件</span><br><span class="line">copy &#x2F;b 图片文件.jpg+隐藏文件 目标图片文件.jpg</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="sed-amp-awk-amp-xargs"><a href="#sed-amp-awk-amp-xargs" class="headerlink" title="sed&amp;awk&amp;xargs"></a>sed&amp;awk&amp;xargs</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将前命令的输出当做本命令的输入</span></span><br><span class="line">lsof -P|grep <span class="string">&#x27;:5000&#x27;</span>|awk <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>|xargs <span class="built_in">kill</span> -9</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找到指定字符串的行,并将其下一行更改为需要的字符串</span></span><br><span class="line">sed -i <span class="string">&#x27;/你找的字符串/ &#123; N; s/\n.*$/\n你要写的字符串/&#125;&#x27;</span> 你的文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换命令</span></span><br><span class="line">sed <span class="string">&#x27;s/目标字符串/替换字符串/&#x27;</span> 文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某行</span></span><br><span class="line">awk <span class="string">&#x27;NR==7&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增一列</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;a=&quot;TIME&quot;&#125;&#123;printf(&quot;%s\t&quot;,a);for(i=1;i&lt;=NF;i++)&#123;printf($i);printf(&quot;\t&quot;)&#125;printf(&quot;%s&quot;,&quot;\n&quot;)&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增时间列</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;a=strftime(&quot;%Y-%m-%d %H:%M:%S&quot;,systime())&#125;&#123;printf(&quot;%s\t&quot;,a);for(i=1;i&lt;=NF;i++)&#123;printf($i);printf(&quot;\t&quot;)&#125;printf(&quot;%s&quot;,&quot;\n&quot;)&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看第一列</span></span><br><span class="line">awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看匹配数据</span></span><br><span class="line">awk <span class="string">&#x27;/匹配字符串/&#x27;</span> file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 联合使用</span></span><br><span class="line">awk <span class="string">&#x27;/匹配字符串/ &#123;print $1&#125;&#x27;</span> file</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>运维</category>
        <category>命令</category>
      </categories>
      <tags>
        <tag>os</tag>
      </tags>
  </entry>
  <entry>
    <title>组件面试整理</title>
    <url>/2020/03/16/%E7%BB%84%E4%BB%B6%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>记录关于大数据组件的常见面试题目</p>
</blockquote>
<span id="more"></span>

<h1 id="组件面试整理"><a href="#组件面试整理" class="headerlink" title="组件面试整理"></a>组件面试整理</h1><h1 id="Redis持久化机制"><a href="#Redis持久化机制" class="headerlink" title="Redis持久化机制"></a>Redis持久化机制</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两种方式,快照(RDB),只追加文件(AOF)</span><br><span class="line"></span><br><span class="line">快照(RDB)--默认</span><br><span class="line">    Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本</span><br><span class="line">    </span><br><span class="line">只追加文件(AOF)</span><br><span class="line">    开启AOF持久化后每执行一条会更改Redis中的数据的命令</span><br><span class="line">    Redis就会将该命令写入硬盘中的AOF文件</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Hive的执行原理"><a href="#Hive的执行原理" class="headerlink" title="Hive的执行原理"></a>Hive的执行原理</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Driver组件</span><br><span class="line">    该组件包括Compiler,Optimizer,Executor</span><br><span class="line">    它可以将Hive的编译,解析,优化转化为MapReduce任务</span><br><span class="line">    提交给Hadoop1中的JobTracker或者是Hadoop2中ResourceManager</span><br><span class="line">    进行实际的执行相应的任务</span><br><span class="line">MetaStore组件</span><br><span class="line">    存储着Hive的元数据信息,将自己的元数据存储到了关系型数据库当中</span><br><span class="line">    支持的数据库主要有MySQL,Derby</span><br><span class="line">    支持把MetaStore独立出来放在远程的集群上面</span><br><span class="line">    使得Hive更加健壮</span><br><span class="line">    元数据主要包括了表的名称,表的列,分区和属性</span><br><span class="line">    表的属性(是否是外部表等等),表数据所在目录</span><br><span class="line">用户接口</span><br><span class="line">    CLI(Command Line Interface,常用的接口:命令行模式)</span><br><span class="line">    Client(Hive的客户端连接至Hive Server,在启动Client的时候,需要制定Hive Server所在的节点,并且在该节点上启动Hive Server)</span><br><span class="line">    WUI(通过浏览器的方式访问Hive)</span><br><span class="line"></span><br><span class="line">工作过程</span><br><span class="line">    用户提交查询等任务给Driver</span><br><span class="line">    编译器获得该用户的任务Plan</span><br><span class="line">    编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息</span><br><span class="line">    编译器Compiler得到元数据信息,对任务进行编译</span><br><span class="line">        先将HQL转换为抽象语法树</span><br><span class="line">        然后将抽象语法树转换成查询块</span><br><span class="line">        将查询快转化为逻辑的查询计划,重写逻辑查询计划</span><br><span class="line">        将逻辑计划转化为物理计划(MapReduce)</span><br><span class="line">        最后选择最优策略</span><br><span class="line">    将最终的计划提交给Driver</span><br><span class="line">    Driver将计划Plan转交给ExecutorEngine去执行</span><br><span class="line">        获取元数据信息</span><br><span class="line">        提交给JobTracker或者ResourceManager执行该任务</span><br><span class="line">        任务会直接读取HDFS中文件进行相应的操作</span><br><span class="line">    获取执行的结果</span><br><span class="line">    取得并返回执行结果</span><br><span class="line">    </span><br><span class="line">优化器的主要功能</span><br><span class="line">    将多Multiple Join合并为一个Muti-Way Join</span><br><span class="line">    对Join,Group By和自定义的MapReduce操作重新进行划分</span><br><span class="line">    消减不必要的列</span><br><span class="line">    在表的扫描操作中推行使用断言</span><br><span class="line">    对于已分区的表,消减不必要的分区</span><br><span class="line">    在抽样查询中,消减不必要的桶</span><br><span class="line">    优化器还增加了局部聚合操作用于处理大分组聚合和增加再分区操作用于处理不对称的分组聚合</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>hive</tag>
        <tag>redis</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>规则引擎与敏捷开发语言入门Demo</title>
    <url>/2021/05/28/%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8E%E4%B8%8E%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8Demo/</url>
    <content><![CDATA[<blockquote>
<p>查漏补缺,扩展自己的宽度</p>
</blockquote>
<span id="more"></span>

<h2 id="规则引擎介绍"><a href="#规则引擎介绍" class="headerlink" title="规则引擎介绍"></a>规则引擎介绍</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">Drools</span><br><span class="line">    业务代码和业务规则分离引擎</span><br><span class="line"></span><br><span class="line">传送门: http://www.drools.org.cn/category/use</span><br><span class="line"></span><br><span class="line">Aviator</span><br><span class="line">    高性能,轻量级的Java语言实现的表达式求值引擎</span><br><span class="line"></span><br><span class="line">MVEL</span><br><span class="line">    基于Java应用程序的表达式语言</span><br><span class="line">    </span><br><span class="line">EasyRules</span><br><span class="line">    Java规则引擎,提供Rule抽象以创建具有条件和动作的规则</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="入门使用"><a href="#入门使用" class="headerlink" title="入门使用"></a>入门使用</h2><h3 id="Drools"><a href="#Drools" class="headerlink" title="Drools"></a>Drools</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 引用依赖</span><br><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.kie&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;kie-api&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;6.5.0.Final&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.drools&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;drools-compiler&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;6.5.0.Final&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;scope&gt;runtime&lt;&#x2F;scope&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">        &lt;version&gt;4.12&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;dependency&gt;</span><br><span class="line">&lt;&#x2F;dependencies&gt;</span><br><span class="line"></span><br><span class="line">resources&#x2F;helloworld.drl文件</span><br><span class="line">package helloworld;</span><br><span class="line"></span><br><span class="line">rule &quot;HelloWorld&quot;</span><br><span class="line">    when</span><br><span class="line">        eval(true)</span><br><span class="line">    then</span><br><span class="line">        System.out.println(&quot;HelloWorld&quot;);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">resources&#x2F;META-INF&#x2F;kmodule.xml文件</span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;</span><br><span class="line">&lt;kmodule xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.drools.org&#x2F;xsd&#x2F;kmodule&quot;&gt;</span><br><span class="line">    &lt;kbase name&#x3D;&quot;helloWorldBase&quot;&gt;</span><br><span class="line">        &lt;ksession name&#x3D;&quot;helloWorldSession&quot;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;kbase&gt;</span><br><span class="line">&lt;&#x2F;kmodule&gt;</span><br><span class="line"></span><br><span class="line">public class HelloWorldTest &#123;</span><br><span class="line">    @Test</span><br><span class="line">    public void testHelloWorld() &#123;</span><br><span class="line">        KieServices kieServices &#x3D; KieServices.Factory.get();</span><br><span class="line">        KieContainer kieContainer &#x3D; kieServices.newKieClasspathContainer();</span><br><span class="line">        KieSession kieSession &#x3D; kieContainer.newKieSession(&quot;helloWorldSession&quot;);</span><br><span class="line">        kieSession.fireAllRules();</span><br><span class="line">        kieSession.dispose();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Aviator"><a href="#Aviator" class="headerlink" title="Aviator"></a>Aviator</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 引用依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.googlecode.aviator&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;aviator&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.3.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.Map;</span><br><span class="line"> </span><br><span class="line">import com.googlecode.aviator.AviatorEvaluator;</span><br><span class="line"> </span><br><span class="line">public class AviatorDemo &#123;</span><br><span class="line"> </span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        String expression &#x3D; &quot;a + b + c&quot;;</span><br><span class="line"> </span><br><span class="line">        Map&lt;String, Object&gt; params &#x3D; new HashMap&lt;&gt;();</span><br><span class="line">        params.put(&quot;a&quot;, 1);</span><br><span class="line">        params.put(&quot;b&quot;, 2);</span><br><span class="line">        params.put(&quot;c&quot;, 3);</span><br><span class="line"> </span><br><span class="line">        long result &#x3D; (long) AviatorEvaluator.execute(expression, params);</span><br><span class="line"> </span><br><span class="line">        System.out.printf(&quot;result : &quot; + result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="MVEL"><a href="#MVEL" class="headerlink" title="MVEL"></a>MVEL</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 引用依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.mvel&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mvel2&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.4.12.Final&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public class MVELTest &#123;</span><br><span class="line">	public static void main(String[] args) &#123;</span><br><span class="line">		String expression &#x3D; &quot;foobar &gt; 99&quot;;</span><br><span class="line"> </span><br><span class="line">		Map vars &#x3D; new HashMap();</span><br><span class="line">		vars.put(&quot;foobar&quot;, new Integer(100));</span><br><span class="line"> </span><br><span class="line">		&#x2F;&#x2F; We know this expression should return a boolean.</span><br><span class="line">		Boolean result &#x3D; (Boolean) MVEL.eval(expression, vars);</span><br><span class="line"> </span><br><span class="line">		if (result.booleanValue()) &#123;</span><br><span class="line">			System.out.println(&quot;It works!&quot;);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Easy-Rules"><a href="#Easy-Rules" class="headerlink" title="Easy Rules"></a>Easy Rules</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">引用依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.jeasy&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;easy-rules-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.0.0&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">import org.jeasy.rules.annotation.Action;</span><br><span class="line">import org.jeasy.rules.annotation.Condition;</span><br><span class="line">import org.jeasy.rules.annotation.Rule;</span><br><span class="line"></span><br><span class="line">@Rule(name &#x3D; &quot;Hello World rule&quot;, description &#x3D; &quot;Always say hello world&quot;)</span><br><span class="line">public class HelloWorldRule &#123;</span><br><span class="line"></span><br><span class="line">    @Condition</span><br><span class="line">    public boolean when() &#123;</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Action</span><br><span class="line">    public void then() throws Exception &#123;</span><br><span class="line">        System.out.println(&quot;hello world&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="JavaScript"><a href="#JavaScript" class="headerlink" title="JavaScript"></a>JavaScript</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import javax.script.ScriptEngine;</span><br><span class="line">import javax.script.ScriptEngineManager;</span><br><span class="line">import javax.script.ScriptException;</span><br><span class="line"> </span><br><span class="line">public class ExpressionCalculate &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        ScriptEngineManager scriptEngineManager &#x3D; new ScriptEngineManager();</span><br><span class="line">        ScriptEngine scriptEngine &#x3D; scriptEngineManager.getEngineByName(&quot;nashorn&quot;);</span><br><span class="line">        String expression &#x3D; &quot;10 * 2 + 6 &#x2F; (3 - 1)&quot;;</span><br><span class="line"> </span><br><span class="line">        try &#123;</span><br><span class="line">            String result &#x3D; String.valueOf(scriptEngine.eval(expression));</span><br><span class="line">            System.out.println(result);</span><br><span class="line">        &#125; catch (ScriptException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ScriptEvaluator"><a href="#ScriptEvaluator" class="headerlink" title="ScriptEvaluator"></a>ScriptEvaluator</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">引用依赖</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.codehaus.janino&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;janino&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.0.7&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">import org.codehaus.commons.compiler.CompileException;</span><br><span class="line">import org.codehaus.janino.ScriptEvaluator;</span><br><span class="line">import java.lang.reflect.InvocationTargetException;</span><br><span class="line"></span><br><span class="line">public class ScriptEvaluatorTest &#123;</span><br><span class="line">    public static void main(String[] args) throws CompileException, InvocationTargetException &#123;</span><br><span class="line">        String value &#x3D; &quot;\&quot;&#123;&#39;data&#39;: [-1, 2358513858109449, 1, &#39;ffffffff-d066-4f4b-ffff-ffffc64e4518&#39;, &#39;1db8248e-2ca9-4337-9c8e-9765d3b21a63&#39;, null, &#39;337956e4408101f716aefab6b0b7b0c4&#39;, &#39;f8ffc8c37ce4&#39;, &#39;QC_Reference_Phone,Xiaomi,armeabi-v7a,santoni,Xiaomi,Redmi 4X,santoni&#39;, 1576759226316], &#39;createTime&#39;: 1576759226316&#125;\&quot;&quot;;</span><br><span class="line">        ScriptEvaluator se &#x3D; new ScriptEvaluator();</span><br><span class="line">        se.setReturnType(String.class);</span><br><span class="line">        se.cook(&quot;import com.alibaba.fastjson.JSON;\n&quot; +</span><br><span class="line">                &quot;        import com.alibaba.fastjson.JSONArray;\n&quot; +</span><br><span class="line">                &quot;        import com.alibaba.fastjson.JSONObject;\n&quot; +</span><br><span class="line">                &quot;        System.out.println(&quot; + value + &quot;);&quot; +</span><br><span class="line">                &quot;        String valueData &#x3D; &quot; + value + &quot;;\n&quot; +</span><br><span class="line">                &quot;        JSONObject jsonObject &#x3D; JSON.parseObject(valueData);\n&quot; +</span><br><span class="line">                &quot;        JSONArray data &#x3D; jsonObject.getJSONArray(\&quot;data\&quot;);\n&quot; +</span><br><span class="line">                &quot;        String adjustId &#x3D; (String) data.get(3);\n&quot; +</span><br><span class="line">                &quot;        return adjustId;&quot;);</span><br><span class="line">        Object evaluate &#x3D; se.evaluate(new Object[]&#123;&#125;);</span><br><span class="line">        System.out.println(evaluate.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>learn</tag>
      </tags>
  </entry>
  <entry>
    <title>记录一次Flink写Kafka的In-Flight问题</title>
    <url>/2021/02/07/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1Flink%E5%86%99Kafka%E7%9A%84In-Flight%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>涉及到Kafka的消息发送的问题</p>
</blockquote>
<span id="more"></span>

<h2 id="问题点"><a href="#问题点" class="headerlink" title="问题点"></a>问题点</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">场景: 简单的Flink消费数据写入kafka,无其他任何复杂操作,在晚上9点附近有峰值情况</span><br><span class="line">问题: 写入Kafka数据失败,并带有CK失败</span><br><span class="line">堆栈:</span><br><span class="line">    throw new IllegalStateException(&quot;There are no in-flight requests for node &quot; + node);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">简单说下往Kafka写数据的操作,会将一批量请求放入InFlightRequests中</span><br><span class="line">包括已处理的请求以及未处理的请求,每个请求都会有他的响应结果(response)</span><br><span class="line">在Kafka底层做Socket通信的时候,会将指定节点的request队列进行队尾拉出处理</span><br><span class="line"></span><br><span class="line">现在就发现在InFlightRequests中不存在指定节点的队列</span><br><span class="line">NetworkClient</span><br><span class="line">    poll()</span><br><span class="line">        ---&gt;handleCompletedSends()</span><br><span class="line">        ---&gt;handleCompletedReceives()</span><br><span class="line">InFlightRequest</span><br><span class="line">    lastSent()</span><br><span class="line">    completeNext()</span><br><span class="line">        ---&gt;requestQueue()</span><br><span class="line">        </span><br><span class="line">指定节点的NodeId按理应该是固定不变的broker.id</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Kafka方向(调参)</span><br><span class="line">queued.max.requests</span><br><span class="line">socket.receive.buffer.bytes</span><br><span class="line">socket.send.buffer.bytes</span><br><span class="line"></span><br><span class="line"># Flink方向</span><br><span class="line">1.问题首先是由峰值引起的,所以进行slot调节,设为Kafka分区数*2</span><br><span class="line">2.增大TM内存,用于提高网络缓冲区的大小,或者直接调节taskmanager.memory.network.fraction,加大网络缓冲区内存大小</span><br><span class="line">3.加大CK的间隔,避免频繁CK</span><br><span class="line"></span><br><span class="line">为什么会产生一个未知的Node,是本次问题的关键</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>编辑器的那些问题</title>
    <url>/2016/07/24/%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E9%82%A3%E4%BA%9B%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>修改编辑器</p>
</blockquote>
<span id="more"></span>

<h2 id="IDEA启动Tomcat中文乱码"><a href="#IDEA启动Tomcat中文乱码" class="headerlink" title="IDEA启动Tomcat中文乱码"></a>IDEA启动Tomcat中文乱码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 排除Tomcat本身乱码问题</span><br><span class="line"># 修改D:\JetBrains\IntelliJ IDEA\bin下的</span><br><span class="line">idea.exe.vmoptions</span><br><span class="line">idea64.exe.vmoptions</span><br><span class="line"></span><br><span class="line"># 添加</span><br><span class="line">-Dfile.encoding&#x3D;UTF-8</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="IDEA重复代码"><a href="#IDEA重复代码" class="headerlink" title="IDEA重复代码"></a>IDEA重复代码</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">File &gt;&gt; Settings &gt;&gt; Editor &gt;&gt; Inspections &gt;&gt; General &gt;&gt; Duplicated Code</span><br><span class="line"># 取消打勾</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>编辑器</category>
      </categories>
      <tags>
        <tag>edit</tag>
      </tags>
  </entry>
  <entry>
    <title>记录一次GP数据更新慢问题</title>
    <url>/2021/03/29/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1GP%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0%E6%85%A2%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>使用GP进行ETL过程中发现插入速度正常,更新查询速度过慢,<a href="https://blog.csdn.net/xfg0218/article/details/83031550">传送门</a></p>
</blockquote>
<span id="more"></span>

<h2 id="问题产生原因"><a href="#问题产生原因" class="headerlink" title="问题产生原因"></a>问题产生原因</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GP支持行存和列存,对于列存,虽然是AppendOnly,但也是可以进行删除更新操作</span><br><span class="line">进行删除更新操作,并没有直接进行物理删除,而是通过BItMap进行标记</span><br><span class="line">当一张列存存在大量删除更新操作,对于查询扫描的成本是很浪费的</span><br><span class="line">PG可以通过Hot技术和AutoVacuum来避免和减少垃圾空间产生</span><br><span class="line">但是GP没有自动回收功能,需要手动进行触发</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一.可删表情况下</span><br><span class="line">根源上解决,删表重建,不使用列存方式</span><br><span class="line"></span><br><span class="line">二.不可删表情况下</span><br><span class="line">执行vacuum table,共享锁(但是需要膨胀率大于gp_append_only_compaction_threshold)</span><br><span class="line">执行vacuum full table,DDL锁(很吃CPU和IO)</span><br><span class="line">执行重分布,DDL</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="查看存储类型"><a href="#查看存储类型" class="headerlink" title="查看存储类型"></a>查看存储类型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># \timing  打开SQL执行时间</span><br><span class="line"># select distinct relstorage from pg_class;</span><br><span class="line">h &#x3D; 堆表(heap),索引</span><br><span class="line">a &#x3D; append only row存储表</span><br><span class="line">c &#x3D; append only column存储表</span><br><span class="line">x &#x3D; 外部表(external table)</span><br><span class="line">v &#x3D; 视图</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="查看当前有哪些AO表"><a href="#查看当前有哪些AO表" class="headerlink" title="查看当前有哪些AO表"></a>查看当前有哪些AO表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># select t2.nspname,t1.relname from pg_class t1,pg_namespace t2 where t1.relnamespace &#x3D; t2.oid and relstorage in (&#39;c&#39;,&#39;a&#39;);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="查看AO表的膨胀率"><a href="#查看AO表的膨胀率" class="headerlink" title="查看AO表的膨胀率"></a>查看AO表的膨胀率</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># select * from gp_toolkit.__gp_aovisimap_compaction_info(&#39;tablename&#39;::regclass);</span><br><span class="line">gp_appendonly_compaction_threshold:AO压缩进程</span><br><span class="line">content:对应gp_configuration.content,表示GP每个节点的唯一编号</span><br><span class="line">datafile:数据文件编号</span><br><span class="line">hidden_tupcount:已更新或删除记录数(不可见)</span><br><span class="line">total_tupcount:总共记录数(包含更新删除记录)</span><br><span class="line">percent_hidden:不可见记录占比,如果占比大于gp_appendonly_compaction_threshold,执行vacuum会收缩这个数据文件</span><br><span class="line">compaction_possible:数据文件是否可以被收缩</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="检查系统中膨胀率超过N的AO表"><a href="#检查系统中膨胀率超过N的AO表" class="headerlink" title="检查系统中膨胀率超过N的AO表"></a>检查系统中膨胀率超过N的AO表</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># select * from (select t2.nspname, t1.relname, (gp_toolkit.__gp_aovisimap_compaction_info(t1.oid)).* from pg_class t1, pg_namespace t2 where t1.relnamespace&#x3D;t2.oid and relstorage in (&#39;c&#39;, &#39;a&#39;)) t where t.percent_hidden &gt; 0.2;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="查看膨胀数据的占用大小"><a href="#查看膨胀数据的占用大小" class="headerlink" title="查看膨胀数据的占用大小"></a>查看膨胀数据的占用大小</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># select pg_size_pretty(pg_relation_size(&#39;tablename&#39;));</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>greenplum</tag>
      </tags>
  </entry>
  <entry>
    <title>记录有关语言类的面试问题</title>
    <url>/2020/03/25/%E8%AE%B0%E5%BD%95%E6%9C%89%E5%85%B3%E8%AF%AD%E8%A8%80%E7%B1%BB%E7%9A%84%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>有时不仅有大数据技术栈的面试问题,也会对像Java/Scala里面的一些使用产生一些问题,记录一下</p>
</blockquote>
<span id="more"></span>

<h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h2 id="1-可变和不可变集合有什么作用"><a href="#1-可变和不可变集合有什么作用" class="headerlink" title="1.可变和不可变集合有什么作用?"></a>1.可变和不可变集合有什么作用?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">从官网的Doc中可以得出</span><br><span class="line">Scala支同时持可变集合和不可变集合,不可变集合可以安全的并发访问</span><br><span class="line">不可变集合是线程安全的</span><br><span class="line">不可变集合仍然可以做一些类似的增加,删除,或者更新</span><br><span class="line">但是实际上他返回了一个新的对象,这里面就是指返回了一个新的集合</span><br><span class="line">而老的集合没有改变</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h1><h2 id="1-HashMap和TreeMap的区别"><a href="#1-HashMap和TreeMap的区别" class="headerlink" title="1.HashMap和TreeMap的区别"></a>1.HashMap和TreeMap的区别</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">HashMap通过hashcode对其内容进行快速查找</span><br><span class="line">HashMap中元素的排列顺序是不固定的</span><br><span class="line">适用于在Map中插入,删除和定位元素</span><br><span class="line">基于hash表实现</span><br><span class="line"></span><br><span class="line">TreeMap中所有的元素都保持着某种固定的顺序</span><br><span class="line">如果你需要得到一个有序的结果你就应该使TreeMap</span><br><span class="line">适用于按自然顺序或自定义顺序遍历键</span><br><span class="line">基于红黑树实现</span><br></pre></td></tr></table></figure>
<h2 id="2-ConcurrentHashMap是怎么实现的？"><a href="#2-ConcurrentHashMap是怎么实现的？" class="headerlink" title="2.ConcurrentHashMap是怎么实现的？"></a>2.ConcurrentHashMap是怎么实现的？</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">concurrent包中线程安全的哈希表，采用分段锁;</span><br><span class="line">可以理解为把一个大的Map拆分成N个小的HashTable;</span><br><span class="line">根据key.hashCode()来决定把key放到哪个HashTable中。</span><br><span class="line">在ConcurrentHashMap中，就是把Map分成了N个Segment;</span><br><span class="line">put和get的时候，都是现根据key.hashCode()算出放到哪个Segment中。</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">    和HashMap的区别在于,ConcurrentHashMap的KV不允许为空。</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>访问者模式的使用</title>
    <url>/2020/06/28/%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>访问者模式贯穿着Spark的底层源码</p>
</blockquote>
<span id="more"></span>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">表示一个作用于某对象结构中的各元素的操作。</span><br><span class="line">它使你可以在不改变各元素类的前提下定义作用于这些元素的新操作。</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 抽象访问者角色,为该对象结构中具体角色声明一个访问操作接口</span></span><br><span class="line"><span class="comment">// 该操作接口的名字和参数标识了发送访问请求给具体访问者的具体元素角色</span></span><br><span class="line"><span class="comment">// 这样访问者就可以通过该元素角色的特定接口直接访问它</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Visitor</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Beijing beijing)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Shanghai shanghai)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Shenzhen shenzhen)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 抽象元素,定义一个接受访问操作,以一个访问者作为参数</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">City</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Visitor visitor)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 具体元素,实现了抽象元素定义的接受操作接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Beijing</span> <span class="keyword">implements</span> <span class="title">City</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Visitor visitor)</span> </span>&#123;</span><br><span class="line">        visitor.visit(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Shanghai</span> <span class="keyword">implements</span> <span class="title">City</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Visitor visitor)</span> </span>&#123;</span><br><span class="line">        visitor.visit(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Shenzhen</span> <span class="keyword">implements</span> <span class="title">City</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Visitor visitor)</span> </span>&#123;</span><br><span class="line">        visitor.visit(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 结构对象角色,使用访问者必备的角色</span></span><br><span class="line"><span class="comment">// 能枚举它的元素;可以提供一个高层接口以允许访问者访问他的元素;</span></span><br><span class="line"><span class="comment">// 如有需要,可以设计成一个符合对象或者一个聚集(列表或无序集合)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TravelCities</span> <span class="keyword">implements</span> <span class="title">City</span> </span>&#123;</span><br><span class="line">    City[] cities;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TravelCities</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        cities = <span class="keyword">new</span> City[]&#123;<span class="keyword">new</span> Beijing(), <span class="keyword">new</span> Shanghai(), <span class="keyword">new</span> Shenzhen()&#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accept</span><span class="params">(Visitor visitor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; cities.length; i++) &#123;</span><br><span class="line">            cities[i].accept(visitor);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 具体访问者角色,实现Visitor声明的接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SingleVisitor</span> <span class="keyword">implements</span> <span class="title">Visitor</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Beijing beijing)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;bj&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Shanghai shanghai)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;sh&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">visit</span><span class="params">(Shenzhen shenzhen)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;sz&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Demo</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        TravelCities travelCities = <span class="keyword">new</span> TravelCities();</span><br><span class="line">        travelCities.accept(<span class="keyword">new</span> SingleVisitor());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
        <category>教程</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>输出两个日期之间的所有日期</title>
    <url>/2019/12/02/%E8%BE%93%E5%87%BA%E4%B8%A4%E4%B8%AA%E6%97%A5%E6%9C%9F%E4%B9%8B%E9%97%B4%E7%9A%84%E6%89%80%E6%9C%89%E6%97%A5%E6%9C%9F/</url>
    <content><![CDATA[<blockquote>
<p>两个日期相隔的所有日期</p>
</blockquote>
<span id="more"></span>

<h2 id="使用Scala实现"><a href="#使用Scala实现" class="headerlink" title="使用Scala实现"></a>使用Scala实现</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getDateList</span></span>(cDate: <span class="type">String</span>, endDate: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> formatter = <span class="type">DateTimeFormatter</span>.ofPattern(<span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> date1 = <span class="type">LocalDate</span>.parse(cDate, formatter)</span><br><span class="line">    <span class="keyword">val</span> date2 = <span class="type">LocalDate</span>.parse(endDate, formatter)</span><br><span class="line">    <span class="keyword">val</span> duration = <span class="type">ChronoUnit</span>.<span class="type">DAYS</span>.between(date1, date2)</span><br><span class="line">    <span class="type">List</span>.range(<span class="number">0</span>, duration).map(date1.plusDays(_).toString)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>阿里BlinkSQL使用</title>
    <url>/2020/10/14/%E9%98%BF%E9%87%8CBlinkSQL%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<blockquote>
<p>现公司使用的都是阿里云组件,并无自建大数据环境,与开源还是有一定的区别,DataStream兼容Flink1.5版本<br><a href="https://help.aliyun.com/document_detail/62515.html?spm=a2c4g.11186623.6.703.1dbd4706FtJCkK">传送门</a></p>
</blockquote>
<span id="more"></span>

<h2 id="源表"><a href="#源表" class="headerlink" title="源表"></a>源表</h2><h3 id="源表支持"><a href="#源表支持" class="headerlink" title="源表支持"></a>源表支持</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DataHub源表</span><br><span class="line">Oracle数据库源表</span><br><span class="line">日志服务SLS源表</span><br><span class="line">Hologres源表</span><br><span class="line">MQ源表</span><br><span class="line">Kafka源表</span><br><span class="line">TableStore源表</span><br><span class="line">全量MaxCompute源表</span><br><span class="line">增量MaxCompute源表</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="结果表"><a href="#结果表" class="headerlink" title="结果表"></a>结果表</h2><h3 id="结果表支持"><a href="#结果表支持" class="headerlink" title="结果表支持"></a>结果表支持</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MySQL结果表</span><br><span class="line">Hologres结果表</span><br><span class="line">Oracle结果表</span><br><span class="line">DataHub结果表</span><br><span class="line">日志服务SLS结果表</span><br><span class="line">MQ结果表</span><br><span class="line">TableStore结果表</span><br><span class="line">RDS结果表</span><br><span class="line">MaxCompute结果表</span><br><span class="line">HBase结果表</span><br><span class="line">ElasticSearch结果表</span><br><span class="line">时间序列数据库结果表</span><br><span class="line">Kafka结果表</span><br><span class="line">HybridDB For MySQL结果表</span><br><span class="line">RDS SQL Server结果表</span><br><span class="line">Redis结果表</span><br><span class="line">MongoDB结果表</span><br><span class="line">PostgreSQL结果表</span><br><span class="line">自定义结果表</span><br><span class="line">InfluxDB结果表</span><br><span class="line">Phoenix5结果表</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="维表"><a href="#维表" class="headerlink" title="维表"></a>维表</h2><h3 id="维表支持"><a href="#维表支持" class="headerlink" title="维表支持"></a>维表支持</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Hologres维表</span><br><span class="line">TableStore维表</span><br><span class="line">HBase维表</span><br><span class="line">RDS维表</span><br><span class="line">MaxCompute维表</span><br><span class="line">Redis维表</span><br><span class="line">Phoenix5维表</span><br><span class="line">ElasticSearch维表</span><br><span class="line">MySQL维表</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>FlinkMeetup会议观后整理之OPPO</title>
    <url>/2019/06/04/FlinkMeetup%E4%BC%9A%E8%AE%AE%E8%A7%82%E5%90%8E%E6%95%B4%E7%90%86%E4%B9%8BOPPO/</url>
    <content><![CDATA[<blockquote>
<p>对于4月份在深圳举行的FlinkMeetup峰会,做一些知识性总结,提升一下自己,OPPO篇其实张俊张老师已经在过往记忆上做过总结整理了-<a href="https://mp.weixin.qq.com/s/ZZaaN0ubQgLqFwTiySi8UQ">传送门</a>,本文更多的是了解自己的不足</p>
</blockquote>
<span id="more"></span>

<hr>
<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><ul>
<li>OPPO实时数仓的演进思路</li>
<li>基于Flink SQL的扩展条件</li>
<li>构建实时数仓的应用案例</li>
<li>未来的思考和展望</li>
</ul>
<hr>
<h2 id="一、OPPO实时数仓的演进思路"><a href="#一、OPPO实时数仓的演进思路" class="headerlink" title="一、OPPO实时数仓的演进思路"></a>一、OPPO实时数仓的演进思路</h2><h3 id="1-OPPO业务与数据规模"><a href="#1-OPPO业务与数据规模" class="headerlink" title="1.OPPO业务与数据规模"></a>1.OPPO业务与数据规模</h3><p>OPPO基于Android定制了自己的ColorOS系统,日活跃用户超过2亿.围绕着ColorOS,OPPO构建了很多互联网应用,比如应用商店,浏览器,信息流等.用户在使用这些互联网应用的同时,也给OPPO积累了大量的数据.目前OPPO的总数据量超过100PB,日增数据量超过200TB—<strong>数据</strong>.</p>
<p>要支撑这么大的一个数据量,OPPO研发出一整套的数据系统与服务,并逐渐形成了自己的数据中台体系—<strong>数据中台</strong>.</p>
<h3 id="2-OPPO数据中台"><a href="#2-OPPO数据中台" class="headerlink" title="2.OPPO数据中台"></a>2.OPPO数据中台</h3><p>数据中台是什么?数据中台是指通过数据技术,对海量数据进行采集,计算,存储,加工,同时统一标准和口径.如果将数据看做原材料,那么数据中台就是加工坊.把它分成4个层次:</p>
<blockquote>
<p><strong>业务支撑</strong>: 应用商店,浏览器,广告等;生产,品质,销售等;手软,ColorOs,影像等;IOT厂商</p>
<p><strong>数据产品与服务</strong>: BI报表,用户洞察,内容标签,精准营销,舆情监测</p>
<p><strong>全域数据体系</strong>: ID-Mapping,用户标签,内容标签</p>
<p><strong>数据仓库</strong>: 原始层,明细层,汇总层,应用层</p>
<p><strong>统一工具体系</strong>: 数据接入,数据治理,数据开发,数据消费</p>
<p><strong>基础设施</strong>: 存储,计算</p>
</blockquote>
<ul>
<li>最下层是统一工具体系,涵盖了”接入-治理-开发-消费”全数据链路;</li>
<li>基于工具体系之上构建了数据仓库,划分成”原始层-明细层-汇总层-应用层”,这也是经典的数仓架构;</li>
<li>再往上就是全域的数据体系,什么是全域呢?就是把公司所有的业务数据全部打通,形成统一的数据资产,比如ID-Mapping,用户标签等;</li>
<li>最终,数据要能被业务用起来,需要场景驱动的数据产品与服务.</li>
</ul>
<p>以上就是OPPO数据中台的整个体系,而数据仓库在其中处于非常基础与核心的位置.</p>
<h3 id="3-构建OPPO离线数仓"><a href="#3-构建OPPO离线数仓" class="headerlink" title="3.构建OPPO离线数仓"></a>3.构建OPPO离线数仓</h3><p>构建过程: 首先,数据来源基本是手机,日志文件以及DB数据库,基于Apache NiFI打造高可用,高吞吐的接入系统,将数据统一落入HDFS,形成原始层;紧接着,基于Hive的小时级ETL与天级汇总Hive任务,分别负责计算生成明细层与汇总层;最后,应用层是基于OPPO内部研发的数据产品,主要是报表分析,用户画像以及接口服务.此外,中间的明细层还支持Presto的<strong>即席查询</strong>与<strong>自助取数</strong>.</p>
<h3 id="4-数仓实时化的诉求"><a href="#4-数仓实时化的诉求" class="headerlink" title="4.数仓实时化的诉求"></a>4.数仓实时化的诉求</h3><blockquote>
<p><strong>业务侧</strong></p>
</blockquote>
<ul>
<li>实时报表: 人群投放的到达率/曝光率/点击率</li>
<li>实时标签: 用户当前所在的商圈</li>
<li>实时接口: 用户最近下载某APP的时间</li>
</ul>
<blockquote>
<p><strong>平台侧</strong></p>
</blockquote>
<ul>
<li>调度任务: 凌晨0点大批量启动</li>
<li>标签导入: 全量导入耗费数小时</li>
<li>质量监控: 及时发现数据问题</li>
</ul>
<p>对于数仓实时化的诉求,大家通常都是从业务视角来看,但其实站在平台的角度,实时化也能带来切实的好处.首先,从业务侧来看,报表,标签,接口等都会有实时的应用场景;其次,对平台侧来说:第一,大量的批量任务都是从0点开始启动,都是通过T+1的方式去做数据处理,这会导致计算负载集中爆发,对集群的压力很大;第二,标签导入也属于一种T+1批量任务,每次全量导入都会耗费很长的时间;第三,数据质量的监控也必须是T+1的,导致没办法及时发现数据的一些问题.</p>
<h3 id="5-离线到实现的平滑迁移"><a href="#5-离线到实现的平滑迁移" class="headerlink" title="5.离线到实现的平滑迁移"></a>5.离线到实现的平滑迁移</h3><table>
    <tr>
        <th rowspan="4">小时/天级</th>
        <th rowspan="2">API</th>
        <th>编程接口</th>
        <th>SQL+UDF</th>
    </tr>
    <tr>
        <th>数仓抽象</th>
        <th>Table</th>
    </tr>
    <tr>
        <th rowspan="2">RunTime</th>
        <th>批量计算</th>
        <th>Hive</th>
    </tr>
    <tr>
        <th>离线数据</th>
        <th>HDFS</th>
    </tr>
</table>
<table>
    <tr>
        <th rowspan="4">秒级/分级</th>
        <th rowspan="2">API</th>
        <th>编程接口</th>
        <th>SQL+UDF</th>
    </tr>
    <tr>
        <th>数仓抽象</th>
        <th>Table</th>
    </tr>
    <tr>
        <th rowspan="2">RunTime</th>
        <th>流式计算</th>
        <th>Flink</th>
    </tr>
    <tr>
        <th>实时数据</th>
        <th>Kafka</th>
    </tr>
</table>
无论是一个平台还是一个系统,都离不开上下两个层次的构成: 上层是API,是面向用户的编程抽象与接口;下层是RunTime,是面向内核的执行引擎.从离线到实时的迁移是平滑的,是什么意思?从API这层来看,数仓的抽象是Table,编程接口是SQL+UDF,离线数仓时代用户已经习惯了这样的API,迁移到实时数仓最好也能保持一致.而从RunTime层来看,计算引擎从Hive演进到了Flink,存储引擎从HDFS演进到了Kakfa.

<h3 id="6-构建OPPO实时数仓"><a href="#6-构建OPPO实时数仓" class="headerlink" title="6.构建OPPO实时数仓"></a>6.构建OPPO实时数仓</h3><p>构建过程: 与离线数仓基本相似,只是把Hive替换成Flink,把HDFS替换为Kafka.总体流程来看,基本模型是不变的,还是由原始层,明细层,汇总层,应用层的级联计算来构成.</p>
<p>核心问题: 如何基于Flink构建实时数仓.</p>
<hr>
<h2 id="二、基于Flink-SQL的扩展工作"><a href="#二、基于Flink-SQL的扩展工作" class="headerlink" title="二、基于Flink SQL的扩展工作"></a>二、基于Flink SQL的扩展工作</h2><h3 id="1-Why-Flink-SQL"><a href="#1-Why-Flink-SQL" class="headerlink" title="1.Why Flink SQL"></a>1.Why Flink SQL</h3><blockquote>
<p><strong>SQL</strong>: High-level Language</p>
</blockquote>
<ul>
<li>ANSI SQL + UDF</li>
<li>数据类型 + 内置函数</li>
<li>自定义Source/Sink</li>
<li>批流统一</li>
</ul>
<blockquote>
<p><strong>Table API</strong>: Declarative DSL</p>
</blockquote>
<blockquote>
<p><strong>DataStream/DataSet API</strong>: Core APIs</p>
</blockquote>
<blockquote>
<p><strong>Stateful Stream Processing</strong>: Low-level building block(streams,state,[event] time)</p>
</blockquote>
<ul>
<li>低延迟,高吞吐</li>
<li>高容错的状态管理</li>
<li>端到端exactly-once</li>
<li>Windows &amp; Event Time</li>
</ul>
<p>首先,为什么要用Flink SQL?上面展示了Flink框架的基本结构,最下面是Runtime,这个执行引擎我们认为最核心的优势是四个: 第一,低延迟,高吞吐;第二,端到端的Exactly-once;第三,可容错的状态管理;第四,Window &amp; Event time的支持.基于Runtime抽象出3个层次的API,SQL处于最上层.</p>
<p>Flink SQL API有哪些优势?也可以从四个方面去看: 第一,支持ANSI SQL的标准;第二, 支持丰富的数据类型与内置函数,包括常见的算术运算与统计聚合;第三,可自定义Source/Sink,基于此可以灵活地扩展上下游;第四,批流统一,同样的SQL,既可以跑离线也可以跑实时.</p>
<p><strong>ps:想了解下scala怎么实现获取Kafka消息,FlinkKafkaConsumer010?</strong></p>
<p>Flink SQL编程示例</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"><span class="keyword">final</span> StreamTableEnvironment tblEnv = TableEnvironment.getTableEnvronment(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义与注册输入表</span></span><br><span class="line">tblEnv.connect(<span class="keyword">new</span> Kafka().version(<span class="string">&quot;0.10&quot;</span>)</span><br><span class="line">    .topic(<span class="string">&quot;input&quot;</span>).properties(kafkaParops).startFromGroupOffsets())</span><br><span class="line">    .withFormat(<span class="keyword">new</span> Avro().recordClass(SdkLog.class))</span><br><span class="line">    .withSchema(<span class="keyword">new</span> Schema().schema(TableSchema.fromTypeInfo(</span><br><span class="line">        AvroSchemaConverter.convertToTypeInfo(SdkLog.class))))</span><br><span class="line">    .inAppenddMode()</span><br><span class="line">    .registerTableSource(<span class="string">&quot;srcTable&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义与注册输出表</span></span><br><span class="line">tblEnv.connect(<span class="keyword">new</span> Kafka().version(<span class="string">&quot;0.10&quot;</span>)</span><br><span class="line">    .topic(<span class="string">&quot;output&quot;</span>).properties(kafkaParops).startFromGroupOffsets())</span><br><span class="line">    .withFormat(<span class="keyword">new</span> Avro().recordClass(SdkLog.class))</span><br><span class="line">    .withSchema(<span class="keyword">new</span> Schema().schema(TableSchema.fromTypeInfo(</span><br><span class="line">        AvroSchemaConverter.convertToTypeInfo(SdkLog.class))))</span><br><span class="line">    .inAppenddMode()</span><br><span class="line">    .registerTableSource(<span class="string">&quot;dstTable&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册UDF</span></span><br><span class="line">tblEnv.registerFunction(<span class="string">&quot;doubleFunc&quot;</span>, <span class="keyword">new</span> DoubleInt());</span><br><span class="line"><span class="comment">// 执行SQL</span></span><br><span class="line">tblEnv.sqlUpdate(<span class="string">&quot;INSERT INTO dstTable SELECT id,name,doubleFunc(age) FROM srcTable WHERE event[&#x27;eventTag&#x27;] = &#x27;10004&#x27;&quot;</span>);</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">&quot;Flink meetup demo&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>首先定义与注册输入/输出表,创建了2张Kafka的表,指定了Kafka版本,对应哪个topic;接下来注册UDF;最后才是执行真正的SQL.可以看到,为了执行SQL,需要做这么多编码工作,这不是我们希望暴露给用户的接口.</p>
<h3 id="2-基于WEB的开发IDE"><a href="#2-基于WEB的开发IDE" class="headerlink" title="2.基于WEB的开发IDE"></a>2.基于WEB的开发IDE</h3><p>前面提到过,数仓的抽象是Table,编程接口是SQL+UDF.对于用户来说,平台提供的编程界面应该是类似HUE的那种,有用过HUE做交互查询的应该很熟悉.左边是Table列表,右边是SQL编辑器,可以在上面直接写SQL,然后提交执行.要实现这样一种交互方式,Flink SQL默认是无法实现的,中间存在缺口,总结下来就两点:第一,元数据的管理,怎么去创建库表,怎么去上传UDF,使得之后在SQL中可直接引用;第二,SQL作业的管理,怎么去编译SQL,怎么去提交作业.</p>
<h3 id="3-AthenaX-基于REST的SQL管理器"><a href="#3-AthenaX-基于REST的SQL管理器" class="headerlink" title="3.AthenaX: 基于REST的SQL管理器"></a>3.AthenaX: 基于REST的SQL管理器</h3><p>AthenaX可以看作一个基于REST的SQL管理器,它是怎么实现SQL作业与元数据管理的呢?</p>
<ul>
<li>对于SQL作业提交,AthenaX中有一个Job的抽象,封装了要执行的SQL以及作业资源等信息.所有的Job由一个JobStore来托管,它定期跟YARN当中处于Running的App做一个匹配.如果不一致,就会向YARN提交对应的Job.</li>
<li>对于元数据管理,核心的问题是如何将外部创建的库表注入Flink,使得SQL中可以识别到.实际上,Flink本身就预留了与外部元数据对接的能力,分别提供了ExternalCatalog和ExternalCatalogTable这两个抽象.AthenaX在此基础上再封装出一个TableCatalog,在接口层面做了一定的扩展.在提交SQL作业的阶段,AthenaX会自动将TableCatalog注册到Flink,再调用Flink SQL的接口将SQL编译为Flink的可执行单元JobGraph,并最终提交到YARN生成新的App.</li>
</ul>
<p>AthenaX虽然定义好了TableCatalog接口,但并没有提供可直接使用的实现.那么,我们怎么来实现,以便对接到我们已有的元数据系统呢?</p>
<h3 id="4-Flink-SQL注册库表的过程"><a href="#4-Flink-SQL注册库表的过程" class="headerlink" title="4.Flink SQL注册库表的过程"></a>4.Flink SQL注册库表的过程</h3><p>首先,我们的搞清楚Flink SQL内部是如何注册库表的.整个过程涉及到三个基本的抽象:TableDescriptor,TableFactory以及TableEnvironment.</p>
<p>TableDescriptor顾名思义,是对表的描述,它由三个子描述符构成:第一是Connector,描述数据的来源,比如Kafka,ES等;第二是Format,描述数据的格式,比如csv,json,avro等;第三是Schema,描述每个字段的名称与类型.</p>
<p>TableDescriptor有两个基本的实现</p>
<ul>
<li>ConnectTableDescriptor用于描述内部表,也就是编程方式创建的表.</li>
<li>ExternalCatalogTable用于描述外部表.</li>
</ul>
<p>有了TableDescriptor,接下来需要TableFactory根据描述信息来实例化Table.不同的描述信息需要不同的TableFactory来处理,Flink如何找到匹配的TableFactory实现呢?实际上,为了保证框架的可扩展性,Flink采用了JavaSPI机制来加载所有声明过的TableFactory,通过遍历的方式去寻找哪个TableFactory是匹配该TableDescriptor的.TableDescriptor在传递给TableFactory前,被转换成一个map,所有的描述信息都用key-value的形式来表达.TableFactory定义了两个用于过滤匹配的方法,一个是requiredContext(),用于检测某些特定的key的value是否匹配,比如connector.type是否为kafka;另一个是supportedProperties(),用于检测key是否能识别,如果出现不识别的key,说明无法匹配.</p>
<p>匹配到了正确的TableFactory,接下来就是创建真正的Table,然后将其通过TableEnvironment注册.最终注册成功的Table,才能在SQL中引用.</p>
<h3 id="5-Flink-SQL对接外部数据源"><a href="#5-Flink-SQL对接外部数据源" class="headerlink" title="5.Flink SQL对接外部数据源"></a>5.Flink SQL对接外部数据源</h3><p>搞清楚了Flink SQL注册库表的过程,给我们带来这样一个思路:如果外部元数据创建的表也能被转换成TableFactory可识别的map,那么就能被无缝地注册到TableEnvironment.基于这个思路,我们实现了Flink SQL与已有元数据中心的对接.</p>
<p>通过元数据中心创建的表,都会将元数据信息存储到MySQL,我们用一张表来记录Table的基本信息,然后另外三张表分别记录Connector,Format,Schema转换成key-value后的描述信息.之所以拆开成三张表,是为了能够能独立的更新这三种描述信息.接下来是定制实现的ExternalCatalog,能够读取MySQL这四张表,并转换成map结构.</p>
<h3 id="6-实时表-维表关联"><a href="#6-实时表-维表关联" class="headerlink" title="6.实时表-维表关联"></a>6.实时表-维表关联</h3><p>到目前为止,我们的平台已经具备了元数据管理与SQL作业管理的能力,但是要真正开放给用户使用,还有一点基本特性存在缺失.通过我们去构建数仓,星型模型是无法避免的.这里有一个比较简单的案例:中间的事实表记录了广告点击流,周边是关于用户,广告,产品,渠道的维度表.</p>
<p>假定我们有一个SQL分析,需要将点击流表与用户维表进行关联,这个目前在Flink SQL中应该怎么实现?我们有两种实现方式,一个基于UDF,一个基于SQL转换.</p>
<h3 id="7-基于UDF的维表关联"><a href="#7-基于UDF的维表关联" class="headerlink" title="7.基于UDF的维表关联"></a>7.基于UDF的维表关联</h3><p>首先是基于UDF的实现,需要用户将原始SQL改写成带UDF调用的SQL,这里是userDimFunc,UserDimFunc继承了Flink SQL抽象的TableFunction,它是其中一种UDF类型,可以将任意一行数据转换成一行或多行数据.为了实现维表关联,在UDF初始化时需要从MySQL全量加载维表的数据,缓存在内存cache中.后续对每行数据的处理,TableFunction会调用eval()方法,在eval()中根据user_id去查找cache,从而实现关联.当然,这里是假定维表数据比较小,如果数据量很大,不适合全量的加载与缓存,这里不做展开了.</p>
<p>基于UDF的实现,对用户与平台来说都不太友好:用户需要写奇怪的SQL语句;平台需要为每个关联场景定制特定的UDF,维护成本太高.</p>
<h3 id="8-基于SQL转换的维表关联"><a href="#8-基于SQL转换的维表关联" class="headerlink" title="8.基于SQL转换的维表关联"></a>8.基于SQL转换的维表关联</h3><p>我们希望解决基于UDF实现所带来的问题,用户不需要改写原始SQL,平台不需要开发很多UDF.有一种思路是,是否可以在SQL交给Flink编译之前,加一层SQL的解析与改写,自动实现维表的关联?经过一定的技术调研与POC,我们发现是行得通的,所以称之为基于SQL转换的实现.</p>
<p>首先,增加的SQL解析是为了识别SQL中是否存在预先定义的维度表.一旦识别到维表,将触发SQL改写的流程,将红框标注的join语句改写成新的Table,这个Table怎么得到?我们知道,流计算领域近年来发展出”流表二象性”的理念,Flink也是该理念的践行者.这意味着,在Flink中Stream与Table之间是可以相互转换的.flatmap怎么实现维表关联?</p>
<p>Flink中对于Stream的flatmap操作,实际上是执行一个RichFlatmapFunction,每来一行数据就调用其flatmap()方法做转换.那么,我们可以定制一个RichFlatmapFunction,来实现维表数据的加载,缓存,查找以及关联,功能与基于UDF的TableFunction实现类似.</p>
<p>既然RichFlatmapFunction的实现逻辑与TableFunction相似,那为什么相比基于UDF的方式,这种实现能更加通用呢?核心的点在于多了一层SQL解析,可以将维表的信息获取出来(比如维表名,关联字段,select字段等),再封装成JoinContext传递给RichFlatmapFunction,使得它的表达能力就具备通用性了.</p>
<hr>
<h2 id="三、构建实时数仓的应用案例"><a href="#三、构建实时数仓的应用案例" class="headerlink" title="三、构建实时数仓的应用案例"></a>三、构建实时数仓的应用案例</h2><h3 id="1-实时ETL拆分"><a href="#1-实时ETL拆分" class="headerlink" title="1.实时ETL拆分"></a>1.实时ETL拆分</h3><p>这里是一个典型的实时ETL链路,从大表中拆分各业务对应的小表: 手机-&gt;NIFI-&gt;Kafka-&gt;ETL-&gt;Kafka/HDFS</p>
<p>OPPO的最大数据来源是手机端埋点,从手机APP过来的数据有一个特点,所有的数据是通过统一的几个通道上报过来.因为不可能每一次业务有新的埋点,都要去升级客户端,去增加新的通道.比如我们有个sdk_log通道,所有APP应用的埋点都往这个通道上报数据,导致这个通道对应的原始层表巨大,一天几十个TB.但实际上,每个业务只关心它自身的那部分数据,这就要求我们在原始层进行ETL拆分.</p>
<p>这个SQL逻辑比较简单,无非是根据某些业务字段做筛选,插入到不同的业务表中去.它的特点是,多行SQL最终合并成一个SQL提交给Flink执行.大家担心的是,包含了4个SQL,会不会对同一份数据重复读取4次?其实,在Flink编译SQL的阶段是会做一些优化的,因为最终指向的是同一个Kakfa topic,所以只会读取1次数据.</p>
<p>另外,同样的Flink SQL,我们同时用于离线与实时数仓的ETL拆分,分别落入HDFS与Kafka.Flink中本身支持写入HDFS的Sink,比如RollingFileSink.</p>
<h3 id="2-实时指标统计"><a href="#2-实时指标统计" class="headerlink" title="2.实时指标统计"></a>2.实时指标统计</h3><p>这里是一个典型的计算信息流CTR的这个案例,分别计算一定时间段内的曝光与点击次数,相除得到点击率导入MySQL,然后通过我们内部的报表系统来可视化.这个SQL的特点是它用到了窗口(Tumbling Window)以及子查询.</p>
<p><strong>窗口函数:</strong> TUMBLE函数</p>
<h3 id="3-实时标签导入"><a href="#3-实时标签导入" class="headerlink" title="3.实时标签导入"></a>3.实时标签导入</h3><p>这个SQL的特点是用了AggregateFunction,在5分钟的窗口内,我们只关心用户最新一次上报的经纬度.AggregateFunction是一种UDF类型,通常是用于聚合指标的统计,比如计算sum或者average.在这个示例中,由于我们只关心最新的经纬度,所以每次都替换老的数据即可.</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>FlinkSQL在使用创建表语句时的源码解析</title>
    <url>/2020/06/11/FlinkSQL%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%88%9B%E5%BB%BA%E8%A1%A8%E8%AF%AD%E5%8F%A5%E6%97%B6%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<blockquote>
<p>这一篇只是对于FlinkSQL创建表语句的解析，有涉及FlinkSQL源码部分可参考前面文章-<a href="https://jxeditor.github.io/2020/05/05/FlinkSQL%E6%BA%90%E7%A0%81%E6%A6%82%E8%A7%88/">FlinkSQL源码概览</a></p>
</blockquote>
<span id="more"></span>

<h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="使用SQL"><a href="#使用SQL" class="headerlink" title="使用SQL"></a>使用SQL</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">tEnv.sqlUpdate(</span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  |CREATE TABLE demo1 (</span></span><br><span class="line"><span class="string">  |    uid VARCHAR COMMENT &#x27;uid&#x27;,</span></span><br><span class="line"><span class="string">  |    rid VARCHAR COMMENT &#x27;rid&#x27;</span></span><br><span class="line"><span class="string">  |)</span></span><br><span class="line"><span class="string">  |WITH (</span></span><br><span class="line"><span class="string">  |    &#x27;connector.type&#x27; = &#x27;kafka&#x27;, -- 使用 kafka connector</span></span><br><span class="line"><span class="string">  |    &#x27;connector.version&#x27; = &#x27;universal&#x27;,  -- kafka 版本</span></span><br><span class="line"><span class="string">  |    &#x27;connector.topic&#x27; = &#x27;test&#x27;,  -- kafka topic</span></span><br><span class="line"><span class="string">  |    &#x27;connector.properties.0.key&#x27; = &#x27;zookeeper.connect&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">  |    &#x27;connector.properties.0.value&#x27; = &#x27;hosts:2181&#x27;,  -- zk连接信息</span></span><br><span class="line"><span class="string">  |    &#x27;connector.properties.1.key&#x27; = &#x27;bootstrap.servers&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">  |    &#x27;connector.properties.1.value&#x27; = &#x27;hosts:9092&#x27;,  -- broker连接信息</span></span><br><span class="line"><span class="string">  |    &#x27;connector.sink-partitioner&#x27; = &#x27;fixed&#x27;,</span></span><br><span class="line"><span class="string">  |    &#x27;update-mode&#x27; = &#x27;append&#x27;,</span></span><br><span class="line"><span class="string">  |    &#x27;format.type&#x27; = &#x27;json&#x27;,  -- 数据源格式为 json</span></span><br><span class="line"><span class="string">  |    &#x27;format.derive-schema&#x27; = &#x27;true&#x27; -- 从 DDL schema 确定 json 解析规则</span></span><br><span class="line"><span class="string">  |)</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br></pre></td></tr></table></figure>
<h3 id="如何解析配置"><a href="#如何解析配置" class="headerlink" title="如何解析配置"></a>如何解析配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这一部分得深入了解之前文章,有一部分其实已经在前面说过</span><br><span class="line">SqlNode其实就包括了表的配置信息</span><br><span class="line">然后会在转换为Operatoin时获取这些配置信息</span><br><span class="line">&#x2F;&#x2F; SqlToOperationConverter.convertCreateTable()-&gt;转换</span><br><span class="line">private Operation convertCreateTable(SqlCreateTable sqlCreateTable) &#123;</span><br><span class="line">    &#x2F;&#x2F; primary key and unique keys are not supported</span><br><span class="line">    if ((sqlCreateTable.getPrimaryKeyList().size() &gt; 0)</span><br><span class="line">        || (sqlCreateTable.getUniqueKeysList().size() &gt; 0)) &#123;</span><br><span class="line">        throw new SqlConversionException(&quot;Primary key and unique key are not supported yet.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (sqlCreateTable.getWatermark().isPresent()) &#123;</span><br><span class="line">        throw new SqlConversionException(</span><br><span class="line">            &quot;Watermark statement is not supported in Old Planner, please use Blink Planner instead.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; set with properties</span><br><span class="line">    Map&lt;String, String&gt; properties &#x3D; new HashMap&lt;&gt;();</span><br><span class="line">    &#x2F;&#x2F; 设置配置</span><br><span class="line">    sqlCreateTable.getPropertyList().getList().forEach(p -&gt;</span><br><span class="line">        properties.put(((SqlTableOption) p).getKeyString(), ((SqlTableOption) p).getValueString()));</span><br><span class="line"></span><br><span class="line">    TableSchema tableSchema &#x3D; createTableSchema(sqlCreateTable);</span><br><span class="line">    String tableComment &#x3D; sqlCreateTable.getComment().map(comment -&gt;</span><br><span class="line">        comment.getNlsString().getValue()).orElse(null);</span><br><span class="line">    &#x2F;&#x2F; set partition key</span><br><span class="line">    List&lt;String&gt; partitionKeys &#x3D; sqlCreateTable.getPartitionKeyList()</span><br><span class="line">        .getList()</span><br><span class="line">        .stream()</span><br><span class="line">        .map(p -&gt; ((SqlIdentifier) p).getSimple())</span><br><span class="line">        .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">    CatalogTable catalogTable &#x3D; new CatalogTableImpl(tableSchema,</span><br><span class="line">        partitionKeys,</span><br><span class="line">        properties,</span><br><span class="line">        tableComment);</span><br><span class="line"></span><br><span class="line">    UnresolvedIdentifier unresolvedIdentifier &#x3D; UnresolvedIdentifier.of(sqlCreateTable.fullTableName());</span><br><span class="line">    ObjectIdentifier identifier &#x3D; catalogManager.qualifyIdentifier(unresolvedIdentifier);</span><br><span class="line"></span><br><span class="line">    return new CreateTableOperation(</span><br><span class="line">        identifier,</span><br><span class="line">        catalogTable,</span><br><span class="line">        sqlCreateTable.isIfNotExists());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="加载进入Catalog"><a href="#加载进入Catalog" class="headerlink" title="加载进入Catalog"></a>加载进入Catalog</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">sqlUpdate</span><br><span class="line">    -&gt;parser.parse()-&gt;operations</span><br><span class="line">        -&gt;CreateTableOperation</span><br><span class="line">            -&gt;createTable()</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sqlUpdate</span><span class="params">(String stmt)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 解析SQL语句为Operation</span></span><br><span class="line">    List&lt;Operation&gt; operations = parser.parse(stmt);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (operations.size() != <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> TableException(UNSUPPORTED_QUERY_IN_SQL_UPDATE_MSG);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Operation operation = operations.get(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> ModifyOperation) &#123;</span><br><span class="line">        List&lt;ModifyOperation&gt; modifyOperations = Collections.singletonList((ModifyOperation) operation);</span><br><span class="line">        <span class="keyword">if</span> (isEagerOperationTranslation()) &#123;</span><br><span class="line">            translate(modifyOperations);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            buffer(modifyOperations);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> CreateTableOperation) &#123;</span><br><span class="line">        CreateTableOperation createTableOperation = (CreateTableOperation) operation;</span><br><span class="line">        <span class="comment">// 加载进Catalog</span></span><br><span class="line">        catalogManager.createTable(</span><br><span class="line">            createTableOperation.getCatalogTable(),</span><br><span class="line">            createTableOperation.getTableIdentifier(),</span><br><span class="line">            createTableOperation.isIgnoreIfExists());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> CreateDatabaseOperation) &#123;</span><br><span class="line">        CreateDatabaseOperation createDatabaseOperation = (CreateDatabaseOperation) operation;</span><br><span class="line">        Catalog catalog = getCatalogOrThrowException(createDatabaseOperation.getCatalogName());</span><br><span class="line">        String exMsg = getDDLOpExecuteErrorMsg(createDatabaseOperation.asSummaryString());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            catalog.createDatabase(</span><br><span class="line">                    createDatabaseOperation.getDatabaseName(),</span><br><span class="line">                    createDatabaseOperation.getCatalogDatabase(),</span><br><span class="line">                    createDatabaseOperation.isIgnoreIfExists());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (DatabaseAlreadyExistException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ValidationException(exMsg, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(exMsg, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> DropTableOperation) &#123;</span><br><span class="line">        DropTableOperation dropTableOperation = (DropTableOperation) operation;</span><br><span class="line">        catalogManager.dropTable(</span><br><span class="line">            dropTableOperation.getTableIdentifier(),</span><br><span class="line">            dropTableOperation.isIfExists());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> AlterTableOperation) &#123;</span><br><span class="line">        AlterTableOperation alterTableOperation = (AlterTableOperation) operation;</span><br><span class="line">        Catalog catalog = getCatalogOrThrowException(alterTableOperation.getTableIdentifier().getCatalogName());</span><br><span class="line">        String exMsg = getDDLOpExecuteErrorMsg(alterTableOperation.asSummaryString());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (alterTableOperation <span class="keyword">instanceof</span> AlterTableRenameOperation) &#123;</span><br><span class="line">                AlterTableRenameOperation alterTableRenameOp = (AlterTableRenameOperation) operation;</span><br><span class="line">                catalog.renameTable(</span><br><span class="line">                        alterTableRenameOp.getTableIdentifier().toObjectPath(),</span><br><span class="line">                        alterTableRenameOp.getNewTableIdentifier().getObjectName(),</span><br><span class="line">                        <span class="keyword">false</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (alterTableOperation <span class="keyword">instanceof</span> AlterTablePropertiesOperation)&#123;</span><br><span class="line">                AlterTablePropertiesOperation alterTablePropertiesOp = (AlterTablePropertiesOperation) operation;</span><br><span class="line">                catalog.alterTable(</span><br><span class="line">                        alterTablePropertiesOp.getTableIdentifier().toObjectPath(),</span><br><span class="line">                        alterTablePropertiesOp.getCatalogTable(),</span><br><span class="line">                        <span class="keyword">false</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (TableAlreadyExistException | TableNotExistException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ValidationException(exMsg, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(exMsg, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> DropDatabaseOperation) &#123;</span><br><span class="line">        DropDatabaseOperation dropDatabaseOperation = (DropDatabaseOperation) operation;</span><br><span class="line">        Catalog catalog = getCatalogOrThrowException(dropDatabaseOperation.getCatalogName());</span><br><span class="line">        String exMsg = getDDLOpExecuteErrorMsg(dropDatabaseOperation.asSummaryString());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            catalog.dropDatabase(</span><br><span class="line">                    dropDatabaseOperation.getDatabaseName(),</span><br><span class="line">                    dropDatabaseOperation.isIfExists(),</span><br><span class="line">                    dropDatabaseOperation.isCascade());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (DatabaseNotExistException | DatabaseNotEmptyException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ValidationException(exMsg, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(exMsg, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> AlterDatabaseOperation) &#123;</span><br><span class="line">        AlterDatabaseOperation alterDatabaseOperation = (AlterDatabaseOperation) operation;</span><br><span class="line">        Catalog catalog = getCatalogOrThrowException(alterDatabaseOperation.getCatalogName());</span><br><span class="line">        String exMsg = getDDLOpExecuteErrorMsg(alterDatabaseOperation.asSummaryString());</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            catalog.alterDatabase(</span><br><span class="line">                    alterDatabaseOperation.getDatabaseName(),</span><br><span class="line">                    alterDatabaseOperation.getCatalogDatabase(),</span><br><span class="line">                    <span class="keyword">false</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (DatabaseNotExistException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> ValidationException(exMsg, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(exMsg, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> CreateFunctionOperation) &#123;</span><br><span class="line">        CreateFunctionOperation createFunctionOperation = (CreateFunctionOperation) operation;</span><br><span class="line">        createCatalogFunction(createFunctionOperation);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> CreateTempSystemFunctionOperation) &#123;</span><br><span class="line">        CreateTempSystemFunctionOperation createtempSystemFunctionOperation =</span><br><span class="line">            (CreateTempSystemFunctionOperation) operation;</span><br><span class="line">        createSystemFunction(createtempSystemFunctionOperation);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> AlterFunctionOperation) &#123;</span><br><span class="line">        AlterFunctionOperation alterFunctionOperation = (AlterFunctionOperation) operation;</span><br><span class="line">        alterCatalogFunction(alterFunctionOperation);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> DropFunctionOperation) &#123;</span><br><span class="line">        DropFunctionOperation dropFunctionOperation = (DropFunctionOperation) operation;</span><br><span class="line">        dropCatalogFunction(dropFunctionOperation);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> DropTempSystemFunctionOperation) &#123;</span><br><span class="line">        DropTempSystemFunctionOperation dropTempSystemFunctionOperation =</span><br><span class="line">            (DropTempSystemFunctionOperation) operation;</span><br><span class="line">        dropSystemFunction(dropTempSystemFunctionOperation);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> UseCatalogOperation) &#123;</span><br><span class="line">        UseCatalogOperation useCatalogOperation = (UseCatalogOperation) operation;</span><br><span class="line">        catalogManager.setCurrentCatalog(useCatalogOperation.getCatalogName());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (operation <span class="keyword">instanceof</span> UseDatabaseOperation) &#123;</span><br><span class="line">        UseDatabaseOperation useDatabaseOperation = (UseDatabaseOperation) operation;</span><br><span class="line">        catalogManager.setCurrentCatalog(useDatabaseOperation.getCatalogName());</span><br><span class="line">        catalogManager.setCurrentDatabase(useDatabaseOperation.getDatabaseName());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> TableException(UNSUPPORTED_QUERY_IN_SQL_UPDATE_MSG);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CatalogManager</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(CatalogBaseTable table, ObjectIdentifier objectIdentifier, <span class="keyword">boolean</span> ignoreIfExists)</span> </span>&#123;</span><br><span class="line">	execute(</span><br><span class="line">        <span class="comment">// 交由具体的Catalog去创建</span></span><br><span class="line">        <span class="comment">// GenericInMemoryCatalog(默认)以及HiveCatalog</span></span><br><span class="line">		(catalog, path) -&gt; catalog.createTable(path, table, ignoreIfExists),</span><br><span class="line">		objectIdentifier,</span><br><span class="line">		<span class="keyword">false</span>,</span><br><span class="line">		<span class="string">&quot;CreateTable&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="实际查询使用"><a href="#实际查询使用" class="headerlink" title="实际查询使用"></a>实际查询使用</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 其实就是QueryOperation的逻辑了,流程一致</span></span><br><span class="line">ParserImpl.parse()-&gt;解析SQL为SqlNode</span><br><span class="line">SqlToOperationConverter.convert()-&gt;转换为Operation</span><br><span class="line">SqlToOperationConverter.convertSqlQuery()-&gt;匹配为QueryOperation</span><br><span class="line">SqlToOperationConverter.toQueryOperation()-&gt;转换为QueryOperation</span><br><span class="line">FlinkPlannerImpl.rel()-&gt;开始转换为RelNode</span><br><span class="line">SqlToRelConverter.convertQuery()-&gt;转换</span><br><span class="line">SqlToRelConverter.convertQueryRecursive()-&gt;转换查询</span><br><span class="line">SqlToRelConverter.convertSelect()</span><br><span class="line">SqlToRelConverter.convertSelectImpl()</span><br><span class="line">SqlToRelConverter.convertFrom()</span><br><span class="line">SqlToRelConverter.convertIdentifier()</span><br><span class="line">SqlToRelConverter.toRel()-&gt;开始初始化源表</span><br><span class="line">CatalogSourceTable.toRel()</span><br><span class="line">CatalogSourceTable.tableSource()</span><br><span class="line">CatalogSourceTable.findAndCreateTableSource()-&gt;寻找并创建数据源表</span><br><span class="line">TableFactoryUtil.findAndCreateTableSource()</span><br><span class="line">TableFactoryUtil.findAndCreateTableSource()</span><br><span class="line">TableFactoryService.find()-&gt;获取对应的TableSourceFactory</span><br><span class="line">TableSourceFactory.createTableSource()-&gt;创建源表</span><br><span class="line"></span><br><span class="line"><span class="comment">// TableFactoryUtil,此处去加载TableSourceFactory</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">TableSource&lt;T&gt; <span class="title">findAndCreateTableSource</span><span class="params">(Map&lt;String, String&gt; properties)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> TableFactoryService</span><br><span class="line">            <span class="comment">// 根据配置去遍历寻找TableSourceFactory</span></span><br><span class="line">            .find(TableSourceFactory.class, properties)</span><br><span class="line">            <span class="comment">// 使用配置参数创建TableSource</span></span><br><span class="line">            .createTableSource(properties);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> TableException(<span class="string">&quot;findAndCreateTableSource failed.&quot;</span>, t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaTableSourceSinkFactoryBase.createStreamTableSource()</span></span><br><span class="line"><span class="comment">// 此处是流处理Kafka源表创建工厂类,继承的是TableSourceFactory</span></span><br><span class="line"><span class="comment">// 需要注意,TableSourceFactory对应connector.type</span></span><br><span class="line"><span class="comment">// 这里不细讲</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamTableSource&lt;Row&gt; <span class="title">createStreamTableSource</span><span class="params">(Map&lt;String, String&gt; properties)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> DescriptorProperties descriptorProperties = getValidatedProperties(properties);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据配置获取topic</span></span><br><span class="line">    <span class="keyword">final</span> String topic = descriptorProperties.getString(CONNECTOR_TOPIC);</span><br><span class="line">    <span class="comment">// 获取DeserializationSchema</span></span><br><span class="line">    <span class="keyword">final</span> DeserializationSchema&lt;Row&gt; deserializationSchema = getDeserializationSchema(properties);</span><br><span class="line">    <span class="keyword">final</span> StartupOptions startupOptions = getStartupOptions(descriptorProperties, topic);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> createKafkaTableSource(</span><br><span class="line">        TableSchemaUtils.getPhysicalSchema(descriptorProperties.getTableSchema(SCHEMA)),</span><br><span class="line">        SchemaValidator.deriveProctimeAttribute(descriptorProperties),</span><br><span class="line">        SchemaValidator.deriveRowtimeAttributes(descriptorProperties),</span><br><span class="line">        SchemaValidator.deriveFieldMapping(</span><br><span class="line">            descriptorProperties,</span><br><span class="line">            Optional.of(deserializationSchema.getProducedType())),</span><br><span class="line">        topic,</span><br><span class="line">        getKafkaProperties(descriptorProperties),</span><br><span class="line">        deserializationSchema,</span><br><span class="line">        startupOptions.startupMode,</span><br><span class="line">        startupOptions.specificOffsets);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> DeserializationSchema&lt;Row&gt; <span class="title">getDeserializationSchema</span><span class="params">(Map&lt;String, String&gt; properties)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 逻辑与上面相似,通过TableFactoryService去找工厂类</span></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">    <span class="keyword">final</span> DeserializationSchemaFactory&lt;Row&gt; formatFactory = TableFactoryService.find(</span><br><span class="line">        DeserializationSchemaFactory.class,</span><br><span class="line">        properties,</span><br><span class="line">        <span class="keyword">this</span>.getClass().getClassLoader());</span><br><span class="line">    <span class="keyword">return</span> formatFactory.createDeserializationSchema(properties);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TableFactoryService"><a href="#TableFactoryService" class="headerlink" title="TableFactoryService"></a>TableFactoryService</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 个人感觉,这是贯穿整个流程的重点,也是用户可以自定义的一个关键</span><br><span class="line">&#x2F;&#x2F; 需要理解TableFactory继承关系树,TableFactory是顶级节点</span><br><span class="line">public static &lt;T extends TableFactory&gt; T find(Class&lt;T&gt; factoryClass, Map&lt;String, String&gt; propertyMap, ClassLoader classLoader) &#123;</span><br><span class="line">    Preconditions.checkNotNull(classLoader);</span><br><span class="line">    return findSingleInternal(factoryClass, propertyMap, Optional.of(classLoader));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static &lt;T extends TableFactory&gt; T findSingleInternal(Class&lt;T&gt; factoryClass, Map&lt;String, String&gt; properties, Optional&lt;ClassLoader&gt; classLoader) &#123;</span><br><span class="line">    &#x2F;&#x2F; 去寻找所有工厂类</span><br><span class="line">    List&lt;TableFactory&gt; tableFactories &#x3D; discoverFactories(classLoader);</span><br><span class="line">    &#x2F;&#x2F; 进行根据配置文件进行过滤</span><br><span class="line">    List&lt;T&gt; filtered &#x3D; filter(tableFactories, factoryClass, properties);</span><br><span class="line">    if (filtered.size() &gt; 1) &#123;</span><br><span class="line">        throw new AmbiguousTableFactoryException(filtered, factoryClass, tableFactories, properties);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return (TableFactory)filtered.get(0);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static List&lt;TableFactory&gt; discoverFactories(Optional&lt;ClassLoader&gt; classLoader) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        List&lt;TableFactory&gt; result &#x3D; new LinkedList();</span><br><span class="line">        ClassLoader cl &#x3D; (ClassLoader)classLoader.orElse(Thread.currentThread().getContextClassLoader());</span><br><span class="line">        ServiceLoader.load(TableFactory.class, cl).iterator().forEachRemaining(result::add);</span><br><span class="line">        return result;</span><br><span class="line">    &#125; catch (ServiceConfigurationError var3) &#123;</span><br><span class="line">        LOG.error(&quot;Could not load service provider for table factories.&quot;, var3);</span><br><span class="line">        throw new TableException(&quot;Could not load service provider for table factories.&quot;, var3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flink.table.api.TableException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.table.factories.TableFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.ServiceConfigurationError;</span><br><span class="line"><span class="keyword">import</span> java.util.ServiceLoader;</span><br><span class="line"><span class="keyword">import</span> java.util.Optional;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> XiaShuai on 2020/6/11.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        List&lt;TableFactory&gt; tableFactories = discoverFactories(Optional.empty());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; tableFactories.size(); i++) &#123;</span><br><span class="line">            System.out.println(tableFactories.get(i).toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;TableFactory&gt; <span class="title">discoverFactories</span><span class="params">(Optional&lt;ClassLoader&gt; classLoader)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            List&lt;TableFactory&gt; result = <span class="keyword">new</span> LinkedList();</span><br><span class="line">            ClassLoader cl = (ClassLoader) classLoader.orElse(Thread.currentThread().getContextClassLoader());</span><br><span class="line">            ServiceLoader.load(TableFactory.class, cl).iterator().forEachRemaining(result::add);</span><br><span class="line">            <span class="keyword">return</span> result;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ServiceConfigurationError var3) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(<span class="string">&quot;Could not load service provider for table factories.&quot;</span>, var3);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>FlinkSQL源码概览</title>
    <url>/2020/05/05/FlinkSQL%E6%BA%90%E7%A0%81%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<blockquote>
<p>基于Flink的Demo,从代码层深入源码,逐层逐层剖析</p>
</blockquote>
<span id="more"></span>

<h2 id="FlinkSQL引擎-Calcite"><a href="#FlinkSQL引擎-Calcite" class="headerlink" title="FlinkSQL引擎: Calcite"></a>FlinkSQL引擎: Calcite</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解析SQL</span><br><span class="line">SqlNode(SQL语句转换而来的语法树)</span><br><span class="line">    SqlToOperationConverter(SqlNode转换为Operation)</span><br><span class="line"></span><br><span class="line">准备执行SQL</span><br><span class="line">RelNode(可以看做对整体数据处理的一个语法树)</span><br><span class="line">    Converter(RelNode之间转换)</span><br><span class="line">    FlinkRelNode(Flink的运算树)</span><br><span class="line">        DataSetRel</span><br><span class="line">        DataStreamRel</span><br><span class="line"></span><br><span class="line">RexNode(行表达式,对一行数据处理的语法树)</span><br><span class="line"></span><br><span class="line">RelOptCluster(查询优化过程中相关关系表达式的环境)</span><br><span class="line">    FlinkRelOptCluster</span><br><span class="line">    </span><br><span class="line">RelOptPlanner(查询优化器,根据给定的规则集和成本模型,将关系表达式转换为语义等价的关系表达式)</span><br><span class="line">    AbstractRelOptPlanner</span><br><span class="line">        HepPlanner</span><br><span class="line">        VolcanoPlanner</span><br><span class="line">            HiveVolcanoPlanner</span><br><span class="line"></span><br><span class="line">RelOptCost(优化器成本模型会依赖)</span><br><span class="line"></span><br><span class="line">RelOptRule(规则匹配使用)</span><br><span class="line">    ConverterRule(规则之间的转换)</span><br><span class="line"></span><br><span class="line">RelTrait(表示特性定义中关系表达式特性的表现形式)</span><br><span class="line">    Convention(代表一个单一的数据源)</span><br><span class="line">    RelMultipleTrait</span><br><span class="line">        RelCollation</span><br><span class="line">        RelDistribution</span><br><span class="line"></span><br><span class="line">RelTraitDef</span><br><span class="line">    ConventionTraitDef(代表数据源)</span><br><span class="line">    RelCollationTraitDef(定义参与排序的字段)</span><br><span class="line">    RelDistributionTraitDef(定义数据在物理存储上的分布方式)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="FlinkSQL解析阶段"><a href="#FlinkSQL解析阶段" class="headerlink" title="FlinkSQL解析阶段"></a>FlinkSQL解析阶段</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Calcite使用JavaCC做SQL,JavaCC根据Parser.jj&#x2F;Parser.tdd文件生成一系列java代码</span><br><span class="line">生成的代码会将SQL转换为AST的数据结构(SqlNode,未经过验证)</span><br><span class="line"></span><br><span class="line">调用SqlToOperationConverter的convert函数将SqlNode转换为Operator</span><br><span class="line"></span><br><span class="line">    这期间SqlNode经过语法检查validate函数,生成经过验证的SqlNode</span><br><span class="line">    调用SqlToOperationConverter的convertSqlQuery函数,将SqlNode转换为RelRoot</span><br><span class="line">    RelRoot里面包含RelNode信息,RelNode可以看做是初始逻辑计划</span><br><span class="line"></span><br><span class="line">进行Optimizer优化,查看源码可以知道,在执行writeToAppendSink时才进行优化操作</span><br><span class="line">生成OptimizerPlan</span><br><span class="line">    这个过程中包含规则的匹配:(从逻辑计划转换为物理计划)</span><br><span class="line">        先基于Calcite Rules去优化</span><br><span class="line">        后基于Flink定制Rules去优化</span><br><span class="line">        optimizeConvertSubQueries</span><br><span class="line">        optimizeExpandPlan</span><br><span class="line">        decorrelateQuery</span><br><span class="line">        optimizeNormalizeLogicalPlan</span><br><span class="line">        optimizeLogicalPlan</span><br><span class="line">        optimizeLogicalRewritePlan</span><br><span class="line">        optimizePhysicalPlan</span><br><span class="line">        </span><br><span class="line">最后将OptimizerPlan转换为DataStream进行输出</span><br><span class="line">RelNode-&gt;DataStreamNode-&gt;translateToPlan-&gt;DataStream</span><br><span class="line"></span><br><span class="line">生成DataStream时会使用到CodeGen</span><br><span class="line">e.g:</span><br><span class="line">    node.translateToPlan之后调用DataStreamScan的translateToPlan函数</span><br><span class="line">    DataStreamScan调用接口StreamScan的convertToInternalRow函数</span><br><span class="line">    generateConversionProcessFunction</span><br><span class="line">        generateFunction(生成Function)</span><br><span class="line">    GeneratedFunction</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="SqlNode的产生"><a href="#SqlNode的产生" class="headerlink" title="SqlNode的产生"></a>SqlNode的产生</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Parser.parser()-&gt;解析sql的接口类</span></span><br><span class="line"><span class="function">List&lt;Operation&gt; <span class="title">parse</span><span class="params">(String statement)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ParserImpl.parser()-&gt;实现类</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Operation&gt; <span class="title">parse</span><span class="params">(String statement)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取CalciteParser解析器</span></span><br><span class="line">    CalciteParser parser = calciteParserSupplier.get();</span><br><span class="line">    FlinkPlannerImpl planner = validatorSupplier.get();</span><br><span class="line">    <span class="comment">// parse the sql query</span></span><br><span class="line">    SqlNode parsed = parser.parse(statement);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// SqlNode转换为Operation</span></span><br><span class="line">    Operation operation = SqlToOperationConverter.convert(planner, catalogManager, parsed)</span><br><span class="line">        .orElseThrow(() -&gt; <span class="keyword">new</span> TableException(</span><br><span class="line">            <span class="string">&quot;Unsupported SQL query! parse() only accepts SQL queries of type &quot;</span> +</span><br><span class="line">                <span class="string">&quot;SELECT, UNION, INTERSECT, EXCEPT, VALUES, ORDER_BY or INSERT;&quot;</span> +</span><br><span class="line">                <span class="string">&quot;and SQL DDLs of type &quot;</span> +</span><br><span class="line">                <span class="string">&quot;CREATE TABLE&quot;</span>));</span><br><span class="line">    <span class="comment">// 返回</span></span><br><span class="line">    <span class="keyword">return</span> Collections.singletonList(operation);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CalciteParser.parser()-&gt;Calcite引擎解析器</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parse</span><span class="params">(String sql)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 根据config创建SqlParser</span></span><br><span class="line">		SqlParser parser = SqlParser.create(sql, config);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 获取SqlNode</span></span><br><span class="line">		<span class="keyword">return</span> parser.parseStmt();</span><br><span class="line">	&#125; <span class="keyword">catch</span> (SqlParseException e) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> SqlParserException(<span class="string">&quot;SQL parse failed. &quot;</span> + e.getMessage());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// SqlParser.create()-&gt;根据config去创建SqlParser</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SqlParser <span class="title">create</span><span class="params">(String sql, SqlParser.Config config)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> create((Reader)(<span class="keyword">new</span> SourceStringReader(sql)), config);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SqlParser <span class="title">create</span><span class="params">(Reader reader, SqlParser.Config config)</span> </span>&#123;</span><br><span class="line">    SqlAbstractParserImpl parser = config.parserFactory().getParser(reader);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> SqlParser(parser, config);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// SqlParser.parseStmt()-&gt;获取SqlNode</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseStmt</span><span class="params">()</span> <span class="keyword">throws</span> SqlParseException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.parseQuery();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseQuery</span><span class="params">()</span> <span class="keyword">throws</span> SqlParseException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 切换到SQLAbstractParserImpl的实现类FlinkSqlParserImpl</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.parser.parseSqlStmtEof();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable var2) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">this</span>.handleException(var2);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// FlinkSqlParserImpl.parseSqlStmtEof()-&gt;获取SqlNode</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SqlNode <span class="title">parseSqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.SqlStmtEof();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> SqlNode <span class="title">SqlStmtEof</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">    <span class="comment">// 在SqlStmt方法中有着各种可能性</span></span><br><span class="line">    SqlNode stmt = <span class="keyword">this</span>.SqlStmt();</span><br><span class="line">    <span class="keyword">this</span>.jj_consume_token(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> stmt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// FlinkSqlParserImpl.SqlStmt(),有几百行,这里挑一部分</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> SqlNode <span class="title">SqlStmt</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">    Object stmt;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">case</span> <span class="number">109</span>:</span><br><span class="line">        stmt = <span class="keyword">this</span>.SqlCreate();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> (SqlNode)stmt;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> SqlCreate <span class="title">SqlCreate</span><span class="params">()</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> replace = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">this</span>.jj_consume_token(<span class="number">109</span>);</span><br><span class="line">    Span s = <span class="keyword">this</span>.span();</span><br><span class="line">    <span class="keyword">switch</span>(<span class="keyword">this</span>.jj_ntk == -<span class="number">1</span> ? <span class="keyword">this</span>.jj_ntk() : <span class="keyword">this</span>.jj_ntk) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">366</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">366</span>);</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">441</span>);</span><br><span class="line">        replace = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_la1[<span class="number">224</span>] = <span class="keyword">this</span>.jj_gen;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SqlCreate create;</span><br><span class="line">    <span class="keyword">switch</span>(<span class="keyword">this</span>.jj_ntk == -<span class="number">1</span> ? <span class="keyword">this</span>.jj_ntk() : <span class="keyword">this</span>.jj_ntk) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">129</span>:</span><br><span class="line">        create = <span class="keyword">this</span>.SqlCreateDatabase(s, replace);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">215</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">585</span>:</span><br><span class="line">        create = <span class="keyword">this</span>.SqlCreateFunction(s, replace);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">582</span>:</span><br><span class="line">        create = <span class="keyword">this</span>.SqlCreateTable(s, replace);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">650</span>:</span><br><span class="line">        create = <span class="keyword">this</span>.SqlCreateView(s, replace);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_la1[<span class="number">225</span>] = <span class="keyword">this</span>.jj_gen;</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(-<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> ParseException();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> create;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> SqlCreate <span class="title">SqlCreateTable</span><span class="params">(Span s, <span class="keyword">boolean</span> replace)</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line">    SqlParserPos startPos;</span><br><span class="line">    SqlIdentifier tableName;</span><br><span class="line">    SqlNodeList primaryKeyList;</span><br><span class="line">    Object uniqueKeysList;</span><br><span class="line">    SqlWatermark watermark;</span><br><span class="line">    SqlNodeList columnList;</span><br><span class="line">    SqlCharStringLiteral comment;</span><br><span class="line">    SqlNodeList propertyList;</span><br><span class="line">    SqlNodeList partitionColumns;</span><br><span class="line">    startPos = s.pos();</span><br><span class="line">    primaryKeyList = SqlNodeList.EMPTY;</span><br><span class="line">    uniqueKeysList = <span class="keyword">new</span> ArrayList();</span><br><span class="line">    watermark = <span class="keyword">null</span>;</span><br><span class="line">    columnList = SqlNodeList.EMPTY;</span><br><span class="line">    comment = <span class="keyword">null</span>;</span><br><span class="line">    propertyList = SqlNodeList.EMPTY;</span><br><span class="line">    partitionColumns = SqlNodeList.EMPTY;</span><br><span class="line">    <span class="keyword">this</span>.jj_consume_token(<span class="number">582</span>);</span><br><span class="line">    tableName = <span class="keyword">this</span>.CompoundIdentifier();</span><br><span class="line">    label61:</span><br><span class="line">    <span class="keyword">switch</span>(<span class="keyword">this</span>.jj_ntk == -<span class="number">1</span> ? <span class="keyword">this</span>.jj_ntk() : <span class="keyword">this</span>.jj_ntk) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">694</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">694</span>);</span><br><span class="line">        SqlParserPos pos = <span class="keyword">this</span>.getPos();</span><br><span class="line">        TableCreationContext ctx = <span class="keyword">new</span> TableCreationContext();</span><br><span class="line">        <span class="keyword">this</span>.TableColumn(ctx);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">switch</span>(<span class="keyword">this</span>.jj_ntk == -<span class="number">1</span> ? <span class="keyword">this</span>.jj_ntk() : <span class="keyword">this</span>.jj_ntk) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">706</span>:</span><br><span class="line">                <span class="keyword">this</span>.jj_consume_token(<span class="number">706</span>);</span><br><span class="line">                <span class="keyword">this</span>.TableColumn(ctx);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">this</span>.jj_la1[<span class="number">51</span>] = <span class="keyword">this</span>.jj_gen;</span><br><span class="line">                pos = pos.plus(<span class="keyword">this</span>.getPos());</span><br><span class="line">                columnList = <span class="keyword">new</span> SqlNodeList(ctx.columnList, pos);</span><br><span class="line">                primaryKeyList = ctx.primaryKeyList;</span><br><span class="line">                uniqueKeysList = ctx.uniqueKeysList;</span><br><span class="line">                watermark = ctx.watermark;</span><br><span class="line">                <span class="keyword">this</span>.jj_consume_token(<span class="number">695</span>);</span><br><span class="line">                <span class="keyword">break</span> label61;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_la1[<span class="number">52</span>] = <span class="keyword">this</span>.jj_gen;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span>(<span class="keyword">this</span>.jj_ntk == -<span class="number">1</span> ? <span class="keyword">this</span>.jj_ntk() : <span class="keyword">this</span>.jj_ntk) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">666</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">666</span>);</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">689</span>);</span><br><span class="line">        String p = SqlParserUtil.parseString(<span class="keyword">this</span>.token.image);</span><br><span class="line">        comment = SqlLiteral.createCharString(p, <span class="keyword">this</span>.getPos());</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_la1[<span class="number">53</span>] = <span class="keyword">this</span>.jj_gen;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span>(<span class="keyword">this</span>.jj_ntk == -<span class="number">1</span> ? <span class="keyword">this</span>.jj_ntk() : <span class="keyword">this</span>.jj_ntk) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">667</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">667</span>);</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">46</span>);</span><br><span class="line">        partitionColumns = <span class="keyword">this</span>.ParenthesizedSimpleIdentifierList();</span><br><span class="line">        <span class="keyword">if</span> (!((FlinkSqlConformance)<span class="keyword">this</span>.conformance).allowCreatePartitionedTable()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> SqlUtil.newContextException(<span class="keyword">this</span>.getPos(), ParserResource.RESOURCE.createPartitionedTableIsOnlyAllowedForHive());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_la1[<span class="number">54</span>] = <span class="keyword">this</span>.jj_gen;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span>(<span class="keyword">this</span>.jj_ntk == -<span class="number">1</span> ? <span class="keyword">this</span>.jj_ntk() : <span class="keyword">this</span>.jj_ntk) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">657</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_consume_token(<span class="number">657</span>);</span><br><span class="line">        <span class="comment">// 获取表属性</span></span><br><span class="line">        propertyList = <span class="keyword">this</span>.TableProperties();</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">this</span>.jj_la1[<span class="number">55</span>] = <span class="keyword">this</span>.jj_gen;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> SqlCreateTable(startPos.plus(<span class="keyword">this</span>.getPos()), tableName, columnList, primaryKeyList, (List)uniqueKeysList, propertyList, partitionColumns, watermark, comment);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong> FlinkParserImpl是由代码生成的类,并不是一个文件<br><em>flink-table/flink-sql-parser/src/main/codegen/data/Parser.tdd</em></p>
<hr>
<h2 id="如何匹配SQL是什么类型"><a href="#如何匹配SQL是什么类型" class="headerlink" title="如何匹配SQL是什么类型"></a>如何匹配SQL是什么类型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在FlinkSqlParserImpl.SqlStmt()方法中,有着switch语句进行匹配</span><br><span class="line">其实在第一个方法调用中就已经完成了匹配的必须信息的获取</span><br><span class="line">简单的讲就是在创建SqlParser时,将SQL语句转换成流的形式</span><br><span class="line">现在就是对流中的一个字符一个字符去获取解析,然后生成一个Token</span><br><span class="line">这个Token就是匹配类型的关键</span><br><span class="line"></span><br><span class="line">public final SqlNode SqlStmt() throws ParseException &#123;</span><br><span class="line">    Object stmt;</span><br><span class="line">    &#x2F;&#x2F; jj_2_4就是去调用jj_3_4方法然后最终调用jj_scan_token方法</span><br><span class="line">    if (this.jj_2_4(2)) &#123;</span><br><span class="line">        stmt &#x3D; this.RichSqlInsert();</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">private final boolean jj_scan_token(int kind) &#123;</span><br><span class="line">    if (this.jj_scanpos &#x3D;&#x3D; this.jj_lastpos) &#123;</span><br><span class="line">        --this.jj_la;</span><br><span class="line">        if (this.jj_scanpos.next &#x3D;&#x3D; null) &#123;</span><br><span class="line">            &#x2F;&#x2F; 利用FlinkSqlParserImplTokenManager去获取Token</span><br><span class="line">            &#x2F;&#x2F; Manager在CalciteParser的parse方法中创建</span><br><span class="line">            this.jj_lastpos &#x3D; this.jj_scanpos &#x3D; this.jj_scanpos.next &#x3D; this.token_source.getNextToken();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            this.jj_lastpos &#x3D; this.jj_scanpos &#x3D; this.jj_scanpos.next;</span><br><span class="line">        &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; getNextToken去调用SimpleCharStream的BeginToken方法循环获取字符</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Operation的产生"><a href="#Operation的产生" class="headerlink" title="Operation的产生"></a>Operation的产生</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; SqlToOperationConverter.convert()-&gt;将SqlNode转换为Operation</span><br><span class="line">public static Optional&lt;Operation&gt; convert(</span><br><span class="line">			FlinkPlannerImpl flinkPlanner,</span><br><span class="line">			CatalogManager catalogManager,</span><br><span class="line">			SqlNode sqlNode) &#123;</span><br><span class="line">    &#x2F;&#x2F; validate the query</span><br><span class="line">    final SqlNode validated &#x3D; flinkPlanner.validate(sqlNode);</span><br><span class="line">    SqlToOperationConverter converter &#x3D; new SqlToOperationConverter(flinkPlanner, catalogManager);</span><br><span class="line">    if (validated instanceof SqlCreateTable) &#123;</span><br><span class="line">        return Optional.of(converter.convertCreateTable((SqlCreateTable) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlDropTable) &#123;</span><br><span class="line">        return Optional.of(converter.convertDropTable((SqlDropTable) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlAlterTable) &#123;</span><br><span class="line">        return Optional.of(converter.convertAlterTable((SqlAlterTable) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlCreateFunction) &#123;</span><br><span class="line">        return Optional.of(converter.convertCreateFunction((SqlCreateFunction) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlAlterFunction) &#123;</span><br><span class="line">        return Optional.of(converter.convertAlterFunction((SqlAlterFunction) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlDropFunction) &#123;</span><br><span class="line">        return Optional.of(converter.convertDropFunction((SqlDropFunction) validated));</span><br><span class="line">    &#125; else if (validated instanceof RichSqlInsert) &#123;</span><br><span class="line">        SqlNodeList targetColumnList &#x3D; ((RichSqlInsert) validated).getTargetColumnList();</span><br><span class="line">        if (targetColumnList !&#x3D; null &amp;&amp; targetColumnList.size() !&#x3D; 0) &#123;</span><br><span class="line">            throw new ValidationException(&quot;Partial inserts are not supported&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        return Optional.of(converter.convertSqlInsert((RichSqlInsert) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlUseCatalog) &#123;</span><br><span class="line">        return Optional.of(converter.convertUseCatalog((SqlUseCatalog) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlUseDatabase) &#123;</span><br><span class="line">        return Optional.of(converter.convertUseDatabase((SqlUseDatabase) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlCreateDatabase) &#123;</span><br><span class="line">        return Optional.of(converter.convertCreateDatabase((SqlCreateDatabase) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlDropDatabase) &#123;</span><br><span class="line">        return Optional.of(converter.convertDropDatabase((SqlDropDatabase) validated));</span><br><span class="line">    &#125; else if (validated instanceof SqlAlterDatabase) &#123;</span><br><span class="line">        return Optional.of(converter.convertAlterDatabase((SqlAlterDatabase) validated));</span><br><span class="line">    &#125; else if (validated.getKind().belongsTo(SqlKind.QUERY)) &#123;</span><br><span class="line">        return Optional.of(converter.convertSqlQuery(validated));</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return Optional.empty();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="结合Demo测试"><a href="#结合Demo测试" class="headerlink" title="结合Demo测试"></a>结合Demo测试</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 打印出SQL的AST语法树,优化好的逻辑计划以及物理计划</span><br><span class="line">tEnv.explain(result)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="补充SQL转换到DataStream操作"><a href="#补充SQL转换到DataStream操作" class="headerlink" title="补充SQL转换到DataStream操作"></a>补充SQL转换到DataStream操作</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">SQL</span>是如何转换为<span class="type">DataStream</span>操作的</span><br><span class="line">当直接使用<span class="type">Table</span>时,会发现<span class="type">Table</span>的<span class="type">API</span>并没有类似打印,输出数据的功能</span><br><span class="line">只能使用toAppendStream/toRetractStream将<span class="type">Table</span>转换为<span class="type">DataStream</span>进行输出</span><br><span class="line"></span><br><span class="line">这里我使用的剖析入口是insertInto</span><br><span class="line"><span class="comment">// TableImpl(实体类,也就是Table)</span></span><br><span class="line">public void insertInto(<span class="type">String</span> tablePath) &#123;</span><br><span class="line">	tableEnvironment.insertInto(tablePath, <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TableEnvironment接口</span></span><br><span class="line">void insertInto(<span class="type">String</span> targetPath, <span class="type">Table</span> table);</span><br><span class="line"><span class="comment">// TableEnvironmentImpl实现类</span></span><br><span class="line">public void insertInto(<span class="type">String</span> targetPath, <span class="type">Table</span> table) &#123;</span><br><span class="line">    <span class="comment">// 获取要插入的表信息,这里不进行分析</span></span><br><span class="line">	<span class="type">UnresolvedIdentifier</span> unresolvedIdentifier = parser.parseIdentifier(targetPath);</span><br><span class="line">    <span class="comment">// 插入,看看做了什么</span></span><br><span class="line">	insertIntoInternal(unresolvedIdentifier, table);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> void insertIntoInternal(<span class="type">UnresolvedIdentifier</span> unresolvedIdentifier, <span class="type">Table</span> table) &#123;</span><br><span class="line">	<span class="type">ObjectIdentifier</span> objectIdentifier = catalogManager.qualifyIdentifier(unresolvedIdentifier);</span><br><span class="line">    <span class="comment">// 获取了table的QueryOperation,转换成ModifyOperation列表</span></span><br><span class="line">	<span class="type">List</span>&lt;<span class="type">ModifyOperation</span>列表&gt; modifyOperations = <span class="type">Collections</span>.singletonList(</span><br><span class="line">		<span class="keyword">new</span> <span class="type">CatalogSinkModifyOperation</span>(</span><br><span class="line">			objectIdentifier,</span><br><span class="line">			table.getQueryOperation()));</span><br><span class="line">    <span class="comment">// 重点:将Operation转换为Translation</span></span><br><span class="line">	<span class="keyword">if</span> (isEagerOperationTranslation()) &#123;</span><br><span class="line">        <span class="comment">// 如果是立即执行的,后面我们就直接剖析的是立即执行</span></span><br><span class="line">		translate(modifyOperations);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 否则将会把这写Operation放入bufferedModifyOperations中,等待tEnv.execute的操作</span></span><br><span class="line">		buffer(modifyOperations);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> void translate(<span class="type">List</span>&lt;<span class="type">ModifyOperation</span>&gt; modifyOperations) &#123;</span><br><span class="line">    <span class="comment">// 调用Planner.translate进行转换,这里我们使用StreamPlanner</span></span><br><span class="line">	<span class="type">List</span>&lt;<span class="type">Transformation</span>&lt;?&gt;&gt; transformations = planner.translate(modifyOperations);</span><br><span class="line">	execEnv.apply(transformations);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// StreamPlanner</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">translate</span></span>(tableOperations: util.<span class="type">List</span>[<span class="type">ModifyOperation</span>])</span><br><span class="line">  : util.<span class="type">List</span>[<span class="type">Transformation</span>[_]] = &#123;</span><br><span class="line">  <span class="comment">// 转换</span></span><br><span class="line">  tableOperations.asScala.map(translate).filter(<span class="type">Objects</span>.nonNull).asJava</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将ModifyOperation转换成对应的Transformation</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">translate</span></span>(tableOperation: <span class="type">ModifyOperation</span>)</span><br><span class="line">  : <span class="type">Transformation</span>[_] = &#123;</span><br><span class="line">  tableOperation <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> s : <span class="type">UnregisteredSinkModifyOperation</span>[_] =&gt;</span><br><span class="line">      <span class="comment">// Sink</span></span><br><span class="line">      writeToSink(s.getChild, s.getSink, unwrapQueryConfig)</span><br><span class="line">    <span class="keyword">case</span> catalogSink: <span class="type">CatalogSinkModifyOperation</span> =&gt;</span><br><span class="line">      getTableSink(catalogSink.getTableIdentifier)</span><br><span class="line">        .map(sink =&gt; &#123;</span><br><span class="line">          <span class="type">TableSinkUtils</span>.validateSink(</span><br><span class="line">            catalogSink.getStaticPartitions,</span><br><span class="line">            catalogSink.getChild,</span><br><span class="line">            catalogSink.getTableIdentifier,</span><br><span class="line">            sink)</span><br><span class="line">          <span class="comment">// set static partitions if it is a partitioned sink</span></span><br><span class="line">          sink <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> partitionableSink: <span class="type">PartitionableTableSink</span> =&gt;</span><br><span class="line">              partitionableSink.setStaticPartition(catalogSink.getStaticPartitions)</span><br><span class="line">            <span class="keyword">case</span> _ =&gt;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// set whether to overwrite if it&#x27;s an OverwritableTableSink</span></span><br><span class="line">          sink <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> overwritableTableSink: <span class="type">OverwritableTableSink</span> =&gt;</span><br><span class="line">              overwritableTableSink.setOverwrite(catalogSink.isOverwrite)</span><br><span class="line">            <span class="keyword">case</span> _ =&gt;</span><br><span class="line">              assert(!catalogSink.isOverwrite, <span class="string">&quot;INSERT OVERWRITE requires &quot;</span> +</span><br><span class="line">                <span class="string">s&quot;<span class="subst">$&#123;classOf[OverwritableTableSink].getSimpleName&#125;</span> but actually got &quot;</span> +</span><br><span class="line">                sink.getClass.getName)</span><br><span class="line">          &#125;</span><br><span class="line">          writeToSink(catalogSink.getChild, sink, unwrapQueryConfig)</span><br><span class="line">        &#125;) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(t) =&gt; t</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TableException</span>(<span class="string">s&quot;Sink <span class="subst">$&#123;catalogSink.getTableIdentifier&#125;</span> does not exists&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> outputConversion: <span class="type">OutputConversionModifyOperation</span> =&gt;</span><br><span class="line">      <span class="keyword">val</span> (isRetract, withChangeFlag) = outputConversion.getUpdateMode <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">UpdateMode</span>.<span class="type">RETRACT</span> =&gt; (<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">UpdateMode</span>.<span class="type">APPEND</span> =&gt; (<span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">UpdateMode</span>.<span class="type">UPSERT</span> =&gt; (<span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      translateToType(</span><br><span class="line">        tableOperation.getChild,</span><br><span class="line">        unwrapQueryConfig,</span><br><span class="line">        isRetract,</span><br><span class="line">        withChangeFlag,</span><br><span class="line">        <span class="type">TypeConversions</span>.fromDataTypeToLegacyInfo(outputConversion.getType)).getTransformation</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TableException</span>(<span class="string">s&quot;Unsupported ModifyOperation: <span class="subst">$tableOperation</span>&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以看出上面的只是进行一个匹配操作,真正的转换在translateToType中,writeToSink同样是一个匹配操作</span></span><br><span class="line"><span class="comment">// 其中writeToSink中会根据sink的不同类型,转换成不同的Sink</span></span><br><span class="line"><span class="comment">// AppendSink,UpsertSink,RetractSink</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeToSink</span></span>[<span class="type">T</span>](</span><br><span class="line">    tableOperation: <span class="type">QueryOperation</span>,</span><br><span class="line">    sink: <span class="type">TableSink</span>[<span class="type">T</span>],</span><br><span class="line">    queryConfig: <span class="type">StreamQueryConfig</span>)</span><br><span class="line">  : <span class="type">Transformation</span>[_] = &#123;</span><br><span class="line">  <span class="keyword">val</span> resultSink = sink <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> retractSink: <span class="type">RetractStreamTableSink</span>[<span class="type">T</span>] =&gt;</span><br><span class="line">      retractSink <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">PartitionableTableSink</span> =&gt;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TableException</span>(<span class="string">&quot;Partitionable sink in retract stream mode &quot;</span> +</span><br><span class="line">            <span class="string">&quot;is not supported yet!&quot;</span>)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      &#125;</span><br><span class="line">      writeToRetractSink(retractSink, tableOperation, queryConfig)</span><br><span class="line">    <span class="keyword">case</span> upsertSink: <span class="type">UpsertStreamTableSink</span>[<span class="type">T</span>] =&gt;</span><br><span class="line">      upsertSink <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">PartitionableTableSink</span> =&gt;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TableException</span>(<span class="string">&quot;Partitionable sink in upsert stream mode &quot;</span> +</span><br><span class="line">            <span class="string">&quot;is not supported yet!&quot;</span>)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      &#125;</span><br><span class="line">      writeToUpsertSink(upsertSink, tableOperation, queryConfig)</span><br><span class="line">    <span class="keyword">case</span> appendSink: <span class="type">AppendStreamTableSink</span>[<span class="type">T</span>] =&gt;</span><br><span class="line">      writeToAppendSink(appendSink, tableOperation, queryConfig)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ValidationException</span>(<span class="string">&quot;Stream Tables can only be emitted by AppendStreamTableSink, &quot;</span></span><br><span class="line">        + <span class="string">&quot;RetractStreamTableSink, or UpsertStreamTableSink.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (resultSink != <span class="literal">null</span>) &#123;</span><br><span class="line">    resultSink.getTransformation</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// translateToType,将类型Schema获取出来</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">translateToType</span></span>[<span class="type">A</span>](</span><br><span class="line">    table: <span class="type">QueryOperation</span>,</span><br><span class="line">    queryConfig: <span class="type">StreamQueryConfig</span>,</span><br><span class="line">    updatesAsRetraction: <span class="type">Boolean</span>,</span><br><span class="line">    withChangeFlag: <span class="type">Boolean</span>,</span><br><span class="line">    tpe: <span class="type">TypeInformation</span>[<span class="type">A</span>])</span><br><span class="line">  : <span class="type">DataStream</span>[<span class="type">A</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> relNode = getRelBuilder.tableOperation(table).build()</span><br><span class="line">  <span class="comment">// 重点,对relNode进行优化</span></span><br><span class="line">  <span class="keyword">val</span> dataStreamPlan = optimizer.optimize(relNode, updatesAsRetraction, getRelBuilder)</span><br><span class="line">  <span class="comment">// 数据类型</span></span><br><span class="line">  <span class="keyword">val</span> rowType = getTableSchema(table.getTableSchema.getFieldNames, dataStreamPlan)</span><br><span class="line">  <span class="comment">// if no change flags are requested, verify table is an insert-only (append-only) table.</span></span><br><span class="line">  <span class="keyword">if</span> (!withChangeFlag &amp;&amp; !<span class="type">UpdatingPlanChecker</span>.isAppendOnly(dataStreamPlan)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ValidationException</span>(</span><br><span class="line">      <span class="string">&quot;Table is not an append-only table. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Use the toRetractStream() in order to handle add and retract messages.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// get CRow plan</span></span><br><span class="line">  <span class="comment">// 将OptimizerPlan转化成CRow</span></span><br><span class="line">  <span class="comment">// 个人理解:就是将执行计划交由对应的DataStreamRel进行代码生成DataStream[CRow]</span></span><br><span class="line">  translateOptimized(dataStreamPlan, rowType, tpe, queryConfig, withChangeFlag)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">translateOptimized</span></span>[<span class="type">A</span>](</span><br><span class="line">    optimizedPlan: <span class="type">RelNode</span>,</span><br><span class="line">    logicalSchema: <span class="type">TableSchema</span>,</span><br><span class="line">    tpe: <span class="type">TypeInformation</span>[<span class="type">A</span>],</span><br><span class="line">    queryConfig: <span class="type">StreamQueryConfig</span>,</span><br><span class="line">    withChangeFlag: <span class="type">Boolean</span>)</span><br><span class="line">  : <span class="type">DataStream</span>[<span class="type">A</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> dataStream = translateToCRow(optimizedPlan, queryConfig)</span><br><span class="line">  <span class="comment">// 将生成的DataStream[CRow]根据TypeInformation转换为对应的DataStream[A]</span></span><br><span class="line">  <span class="type">DataStreamConversions</span>.convert(dataStream, logicalSchema, withChangeFlag, tpe, config)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 逻辑计划转换为实际的运算代码</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">translateToCRow</span></span>(</span><br><span class="line">  logicalPlan: <span class="type">RelNode</span>,</span><br><span class="line">  queryConfig: <span class="type">StreamQueryConfig</span>): <span class="type">DataStream</span>[<span class="type">CRow</span>] = &#123;</span><br><span class="line">  logicalPlan <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> node: <span class="type">DataStreamRel</span> =&gt;</span><br><span class="line">      getExecutionEnvironment.configure(</span><br><span class="line">        config.getConfiguration,</span><br><span class="line">        <span class="type">Thread</span>.currentThread().getContextClassLoader)</span><br><span class="line">      <span class="comment">// 这里调用的是DataStreamRel接口方法</span></span><br><span class="line">      node.translateToPlan(<span class="keyword">this</span>, queryConfig)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TableException</span>(<span class="string">&quot;Cannot generate DataStream due to an invalid logical plan. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;This is a bug and should not happen. Please file an issue.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里DataStreamRel的实现类有很多,对应的是具体的操作</span></span><br><span class="line"><span class="comment">// 看下DataStreamCalc中的实现</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">translateToPlan</span></span>(</span><br><span class="line">    planner: <span class="type">StreamPlanner</span>,</span><br><span class="line">    queryConfig: <span class="type">StreamQueryConfig</span>): <span class="type">DataStream</span>[<span class="type">CRow</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> config = planner.getConfig <span class="comment">// 配置信息</span></span><br><span class="line">  <span class="comment">// 获取input输入数据,其实也是进行调用translateToPlan</span></span><br><span class="line">  <span class="keyword">val</span> inputDataStream =</span><br><span class="line">    getInput.asInstanceOf[<span class="type">DataStreamRel</span>].translateToPlan(planner, queryConfig)</span><br><span class="line">  <span class="comment">// materialize time attributes in condition</span></span><br><span class="line">  <span class="keyword">val</span> condition = <span class="keyword">if</span> (calcProgram.getCondition != <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> materializedCondition = <span class="type">RelTimeIndicatorConverter</span>.convertExpression(</span><br><span class="line">      calcProgram.expandLocalRef(calcProgram.getCondition),</span><br><span class="line">      inputSchema.relDataType,</span><br><span class="line">      cluster.getRexBuilder)</span><br><span class="line">    <span class="type">Some</span>(materializedCondition)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">None</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// filter out time attributes</span></span><br><span class="line">  <span class="keyword">val</span> projection = calcProgram.getProjectList.asScala</span><br><span class="line">    .map(calcProgram.expandLocalRef)</span><br><span class="line">  <span class="comment">// 获取CodeGenerator代码生成器 </span></span><br><span class="line">  <span class="keyword">val</span> generator = <span class="keyword">new</span> <span class="type">FunctionCodeGenerator</span>(config, <span class="literal">false</span>, inputSchema.typeInfo)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 生成Function</span></span><br><span class="line">  <span class="keyword">val</span> genFunction = generateFunction(</span><br><span class="line">    generator,</span><br><span class="line">    ruleDescription,</span><br><span class="line">    schema,</span><br><span class="line">    projection,</span><br><span class="line">    condition,</span><br><span class="line">    config,</span><br><span class="line">    classOf[<span class="type">ProcessFunction</span>[<span class="type">CRow</span>, <span class="type">CRow</span>]])</span><br><span class="line">  <span class="keyword">val</span> inputParallelism = inputDataStream.getParallelism</span><br><span class="line">  <span class="comment">// 创建CRowProcessRunner</span></span><br><span class="line">  <span class="keyword">val</span> processFunc = <span class="keyword">new</span> <span class="type">CRowProcessRunner</span>(</span><br><span class="line">    genFunction.name,</span><br><span class="line">    genFunction.code,</span><br><span class="line">    <span class="type">CRowTypeInfo</span>(schema.typeInfo))</span><br><span class="line">  <span class="comment">// 对输入数据流进行计算</span></span><br><span class="line">  inputDataStream</span><br><span class="line">    .process(processFunc)</span><br><span class="line">    .name(calcOpName(calcProgram, getExpressionString))</span><br><span class="line">    <span class="comment">// keep parallelism to ensure order of accumulate and retract messages</span></span><br><span class="line">    .setParallelism(inputParallelism)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">同理,sqlUpdate中的转换也是一样的操作</span><br><span class="line">都是通过translate进行转换为DataStream</span><br><span class="line">其实这方面可以分为两部分进行剖析,sqlQuery和sqlUpdate</span><br><span class="line">sqlQuery对应createTable创建source,对应TableImpl</span><br><span class="line">sqlUpdate对应sink</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink使用Hudi的问题</title>
    <url>/2021/05/14/Flink%E4%BD%BF%E7%94%A8Hudi%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>真的是坑居多…</p>
</blockquote>
<span id="more"></span>

<blockquote>
<p>主要还是状态的存储以及合并删除操作对连接不友好</p>
</blockquote>
<h2 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 这个问题,真的是莫名其妙,看了源码,逻辑是没有问题的,不知道为什么会报越界</span><br><span class="line">java.lang.ArrayIndexOutOfBoundsException: -1</span><br><span class="line">    at java.util.ArrayList.elementData(ArrayList.java:424) ~[?:1.8.0_281]</span><br><span class="line">    at java.util.ArrayList.get(ArrayList.java:437) ~[?:1.8.0_281]</span><br><span class="line">    at org.apache.hudi.table.format.mor.MergeOnReadInputFormat.lambda$getReader$0(MergeOnReadInputFormat.java:280) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at java.util.LinkedHashMap.forEach(LinkedHashMap.java:684) ~[?:1.8.0_281]</span><br><span class="line">    at org.apache.hudi.table.format.mor.MergeOnReadInputFormat.getReader(MergeOnReadInputFormat.java:278) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.format.mor.MergeOnReadInputFormat.open(MergeOnReadInputFormat.java:173) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.source.StreamReadOperator.processSplits(StreamReadOperator.java:159) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:297) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:189) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:617) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:581) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_281]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 这个,使用rocksdb作为状态后端时会出现该问题,并且频繁出现</span><br><span class="line"># 个人推测与commit生成或clean动作有关</span><br><span class="line">java.io.IOException: Cannot connect to the client to send back the stream</span><br><span class="line">    at org.apache.flink.streaming.experimental.CollectSink.open(CollectSink.java:86) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:428) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$2(StreamTask.java:543) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:533) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:573) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_281]</span><br><span class="line">    Caused by: java.net.ConnectException: Connection refused (Connection refused)</span><br><span class="line">    at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:476) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:218) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:200) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:394) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.Socket.connect(Socket.java:606) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.Socket.connect(Socket.java:555) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.Socket.&lt;init&gt;(Socket.java:451) ~[?:1.8.0_281]</span><br><span class="line">    at java.net.Socket.&lt;init&gt;(Socket.java:261) ~[?:1.8.0_281]</span><br><span class="line">    at org.apache.flink.streaming.experimental.CollectSink.open(CollectSink.java:82) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    ... 11 more</span><br><span class="line">    </span><br><span class="line"># 改为filesystem状态后端则会出现这种情况(这个应该是因为我内存不足导致的)</span><br><span class="line">java.io.IOException: Could not perform checkpoint 22 for operator hoodie_stream_write (1&#x2F;1)#2.</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:963) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:115) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.io.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:156) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:180) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:157) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:179) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:396) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:191) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:617) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:581) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:755) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:570) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_281]</span><br><span class="line">Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 22 for operator hoodie_stream_write (1&#x2F;1)#2. Failure reason: Checkpoint was declined.</span><br><span class="line">    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:241) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:162) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:371) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:686) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:607) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:572) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:1004) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:988) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:947) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    ... 13 more</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: Error upserting bucketType UPDATE for partition :</span><br><span class="line">    at org.apache.hudi.table.action.commit.BaseFlinkCommitActionExecutor.handleUpsertPartition(BaseFlinkCommitActionExecutor.java:201) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.BaseFlinkCommitActionExecutor.execute(BaseFlinkCommitActionExecutor.java:110) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.BaseFlinkCommitActionExecutor.execute(BaseFlinkCommitActionExecutor.java:72) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.FlinkWriteHelper.write(FlinkWriteHelper.java:70) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.delta.FlinkUpsertDeltaCommitActionExecutor.execute(FlinkUpsertDeltaCommitActionExecutor.java:49) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.HoodieFlinkMergeOnReadTable.upsert(HoodieFlinkMergeOnReadTable.java:60) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.client.HoodieFlinkWriteClient.upsert(HoodieFlinkWriteClient.java:146) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.lambda$initWriteFunction$1(StreamWriteFunction.java:260) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.lambda$flushRemaining$6(StreamWriteFunction.java:459) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608) ~[?:1.8.0_281]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.flushRemaining(StreamWriteFunction.java:453) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.snapshotState(StreamWriteFunction.java:192) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:205) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:162) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:371) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointStreamOperator(SubtaskCheckpointCoordinatorImpl.java:686) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.buildOperatorSnapshotFutures(SubtaskCheckpointCoordinatorImpl.java:607) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:572) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:298) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$9(StreamTask.java:1004) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:988) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:947) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    ... 13 more</span><br><span class="line">Caused by: org.apache.flink.util.SerializedThrowable: Java heap space</span><br><span class="line">    at java.util.Arrays.copyOf(Arrays.java:3236) ~[?:1.8.0_281]</span><br><span class="line">    at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118) ~[?:1.8.0_281]</span><br><span class="line">    at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93) ~[?:1.8.0_281]</span><br><span class="line">    at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153) ~[?:1.8.0_281]</span><br><span class="line">    at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_281]</span><br><span class="line">    at java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_281]</span><br><span class="line">    at org.apache.hudi.common.table.log.block.HoodieAvroDataBlock.serializeRecords(HoodieAvroDataBlock.java:114) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.common.table.log.block.HoodieDataBlock.getContentBytes(HoodieDataBlock.java:97) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.common.table.log.HoodieLogFormatWriter.appendBlocks(HoodieLogFormatWriter.java:164) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.io.HoodieAppendHandle.appendDataAndDeleteBlocks(HoodieAppendHandle.java:349) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:332) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.delta.BaseFlinkDeltaCommitActionExecutor.handleUpdate(BaseFlinkDeltaCommitActionExecutor.java:55) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.BaseFlinkCommitActionExecutor.handleUpsertPartition(BaseFlinkCommitActionExecutor.java:194) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.BaseFlinkCommitActionExecutor.execute(BaseFlinkCommitActionExecutor.java:110) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.BaseFlinkCommitActionExecutor.execute(BaseFlinkCommitActionExecutor.java:72) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.FlinkWriteHelper.write(FlinkWriteHelper.java:70) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.action.commit.delta.FlinkUpsertDeltaCommitActionExecutor.execute(FlinkUpsertDeltaCommitActionExecutor.java:49) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.table.HoodieFlinkMergeOnReadTable.upsert(HoodieFlinkMergeOnReadTable.java:60) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.client.HoodieFlinkWriteClient.upsert(HoodieFlinkWriteClient.java:146) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.lambda$initWriteFunction$1(StreamWriteFunction.java:260) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction$$Lambda$499&#x2F;1784817951.apply(Unknown Source) ~[?:?]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.lambda$flushRemaining$6(StreamWriteFunction.java:459) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction$$Lambda$659&#x2F;1589969316.accept(Unknown Source) ~[?:?]</span><br><span class="line">    at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608) ~[?:1.8.0_281]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.flushRemaining(StreamWriteFunction.java:453) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.hudi.sink.StreamWriteFunction.snapshotState(StreamWriteFunction.java:192) ~[hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar:0.9.0-SNAPSHOT]</span><br><span class="line">    at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:89) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:205) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:162) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br><span class="line">    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:371) ~[flink-dist_2.11-1.12.2.jar:1.12.2]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hudi</tag>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink动态CEP实例</title>
    <url>/2021/06/02/Flink%E5%8A%A8%E6%80%81CEP%E5%AE%9E%E4%BE%8B/</url>
    <content><![CDATA[<blockquote>
<p>在使用CEP时,需要动态改变规则,且不停用程序</p>
</blockquote>
<span id="more"></span>

<h2 id="整体思路"><a href="#整体思路" class="headerlink" title="整体思路"></a>整体思路</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.参考刘博大佬给出的方案进行复现</span><br><span class="line">    https:&#x2F;&#x2F;developer.aliyun.com&#x2F;article&#x2F;738454</span><br><span class="line">    https:&#x2F;&#x2F;blog.csdn.net&#x2F;u013516966&#x2F;article&#x2F;details&#x2F;110412808</span><br><span class="line">2.对外开放一个接口,方便自定义外部规则以及对比规则是否变化</span><br><span class="line">    org.apache.flink.cep.functions.InjectionPatternFunction</span><br><span class="line">3.需要修改CEP涉及的底层对象(我这边使用的是scala)</span><br><span class="line">    将DataStream转换为PatternStream以执行CEP的实用方法</span><br><span class="line">        scala&#x2F;org.apache.flink.cep.scala.CEP --&gt; CEP1</span><br><span class="line">        java&#x2F;org.apache.flink.cep.CEP --&gt; CEP1</span><br><span class="line">    用于CEP模式检测的流抽象,模式流是将检测到的模式序列作为与其名称相关联的事件的映射而发出的流</span><br><span class="line">    使用Nfa检测该模式.为了处理检测到的序列,用户必须指定PatternSelectFunction或PatternsFlatSelectFunction</span><br><span class="line">        java&#x2F;org.apache.flink.cep.PatternStream</span><br><span class="line">    创建模式流的方式</span><br><span class="line">        java&#x2F;org.apache.flink.cep.PatternStreamBuilder</span><br><span class="line">    键控输入流的CEP模式运算符.对于每个键,操作符创建一个NFA和一个优先级队列来缓冲无序元素</span><br><span class="line">    这两种数据结构都使用托管键控状态存储</span><br><span class="line">        java&#x2F;org.apache.flink.cep.operator.CepOperator</span><br><span class="line"></span><br><span class="line">4.修改底层构造方法,将自定义接口传入到CepOperator中</span><br><span class="line">5.在CepOperator中实现ProcessingTimeCallback回调接口</span><br><span class="line">6.启动监听器按固定频次触发规则监测</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="具体实现-按步骤进行"><a href="#具体实现-按步骤进行" class="headerlink" title="具体实现(按步骤进行)"></a>具体实现(按步骤进行)</h2><h3 id="InjectionPatternFunction"><a href="#InjectionPatternFunction" class="headerlink" title="InjectionPatternFunction"></a>InjectionPatternFunction</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.flink.cep.functions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.functions.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.pattern.Pattern;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">InjectionPatternFunction</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Function</span>, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化外部连接</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 动态规则注入</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Pattern&lt;T, T&gt; <span class="title">inject</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 轮询周期(监听不需要)</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">getPeriod</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 规则是否发生变更</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isChanged</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="CEP1-scala"><a href="#CEP1-scala" class="headerlink" title="CEP1(scala)"></a>CEP1(scala)</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.flink.cep.scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.functions.<span class="type">InjectionPatternFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.&#123;<span class="type">EventComparator</span>, <span class="type">CEP1</span> =&gt; <span class="type">JCEP</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.pattern.<span class="type">Pattern</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">DataStream</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CEP1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pattern</span></span>[<span class="type">T</span>](input: <span class="type">DataStream</span>[<span class="type">T</span>], pattern: <span class="type">Pattern</span>[<span class="type">T</span>, _ &lt;: <span class="type">T</span>]): <span class="type">PatternStream</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    wrapPatternStream(<span class="type">JCEP</span>.pattern(input.javaStream, pattern.wrappedPattern))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">pattern</span></span>[<span class="type">T</span>](</span><br><span class="line">                  input: <span class="type">DataStream</span>[<span class="type">T</span>],</span><br><span class="line">                  pattern: <span class="type">Pattern</span>[<span class="type">T</span>, _ &lt;: <span class="type">T</span>],</span><br><span class="line">                  comparator: <span class="type">EventComparator</span>[<span class="type">T</span>]): <span class="type">PatternStream</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    wrapPatternStream(<span class="type">JCEP</span>.pattern(input.javaStream, pattern.wrappedPattern, comparator))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 新增传入外部接口的调用方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">injectionPattern</span></span>[<span class="type">T</span>](</span><br><span class="line">    input: <span class="type">DataStream</span>[<span class="type">T</span>],</span><br><span class="line">    injectionPatternFunction: <span class="type">InjectionPatternFunction</span>[<span class="type">T</span>]): <span class="type">PatternStream</span>[<span class="type">T</span>]= &#123;</span><br><span class="line">    <span class="comment">// 调用java中的CEP1类</span></span><br><span class="line">    wrapPatternStream(<span class="type">JCEP</span>.injectionPattern(input.javaStream,injectionPatternFunction))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="CEP1-java"><a href="#CEP1-java" class="headerlink" title="CEP1(java)"></a>CEP1(java)</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.flink.cep;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.functions.InjectionPatternFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.pattern.Pattern;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CEP1</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> PatternStream&lt;&gt;(input, pattern);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">pattern</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            DataStream&lt;T&gt; input, Pattern&lt;T, ?&gt; pattern, EventComparator&lt;T&gt; comparator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> PatternStream&lt;T&gt; stream = <span class="keyword">new</span> PatternStream&lt;&gt;(input, pattern);</span><br><span class="line">        <span class="keyword">return</span> stream.withComparator(comparator);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 新增传入外部接口的调用方法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">PatternStream&lt;T&gt; <span class="title">injectionPattern</span><span class="params">(DataStream&lt;T&gt; input, InjectionPatternFunction&lt;T&gt; injectionPatternFunction)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> PatternStream&lt;&gt;(input, injectionPatternFunction);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="PatternStream"><a href="#PatternStream" class="headerlink" title="PatternStream"></a>PatternStream</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PatternStream</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> PatternStreamBuilder&lt;T&gt; builder;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 新增构造方法</span></span><br><span class="line">    PatternStream(<span class="keyword">final</span> DataStream&lt;T&gt; inputStream, <span class="keyword">final</span> InjectionPatternFunction&lt;T&gt; injectionPatternFunction) &#123;</span><br><span class="line">        <span class="keyword">this</span>(PatternStreamBuilder.forStreamAndPattern(inputStream, injectionPatternFunction));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="PatternStreamBuilder"><a href="#PatternStreamBuilder" class="headerlink" title="PatternStreamBuilder"></a>PatternStreamBuilder</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">PatternStreamBuilder</span>&lt;<span class="title">IN</span>&gt; </span>&#123;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> InjectionPatternFunction&lt;IN&gt; injectionPatternFunction;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 添加构造函数</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">PatternStreamBuilder</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> DataStream&lt;IN&gt; inputStream,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> InjectionPatternFunction&lt;IN&gt; injectionPatternFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> TimeBehaviour timeBehaviour,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="meta">@Nullable</span> <span class="keyword">final</span> EventComparator&lt;IN&gt; comparator,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="meta">@Nullable</span> <span class="keyword">final</span> OutputTag&lt;IN&gt; lateDataOutputTag)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.inputStream = checkNotNull(inputStream);</span><br><span class="line">        <span class="keyword">this</span>.injectionPatternFunction = injectionPatternFunction;</span><br><span class="line">        <span class="keyword">this</span>.timeBehaviour = checkNotNull(timeBehaviour);</span><br><span class="line">        <span class="keyword">this</span>.comparator = comparator;</span><br><span class="line">        <span class="keyword">this</span>.lateDataOutputTag = lateDataOutputTag;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    &lt;OUT, K&gt; <span class="function">SingleOutputStreamOperator&lt;OUT&gt; <span class="title">build</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">final</span> TypeInformation&lt;OUT&gt; outTypeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">final</span> PatternProcessFunction&lt;IN, OUT&gt; processFunction)</span>  </span>&#123;</span><br><span class="line"></span><br><span class="line">        checkNotNull(outTypeInfo);</span><br><span class="line">        checkNotNull(processFunction);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> TypeSerializer&lt;IN&gt; inputSerializer =</span><br><span class="line">                inputStream.getType().createSerializer(inputStream.getExecutionConfig());</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">boolean</span> isProcessingTime = timeBehaviour == TimeBehaviour.ProcessingTime;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">boolean</span> timeoutHandling = processFunction <span class="keyword">instanceof</span> TimedOutPartialMatchHandler;</span><br><span class="line">        <span class="keyword">final</span> NFACompiler.NFAFactory&lt;IN&gt; nfaFactory =</span><br><span class="line">                NFACompiler.compileFactory(pattern, timeoutHandling);</span><br><span class="line"></span><br><span class="line">        CepOperator&lt;IN, K, OUT&gt; operator = <span class="keyword">null</span>;</span><br><span class="line">        <span class="comment">// 当外部接口方法不为空时,构造自定义的CepOperator</span></span><br><span class="line">        <span class="keyword">if</span> (injectionPatternFunction == <span class="keyword">null</span>) &#123;</span><br><span class="line">            operator = <span class="keyword">new</span> CepOperator&lt;&gt;(</span><br><span class="line">                    inputSerializer,</span><br><span class="line">                    isProcessingTime,</span><br><span class="line">                    nfaFactory,</span><br><span class="line">                    comparator,</span><br><span class="line">                    pattern.getAfterMatchSkipStrategy(),</span><br><span class="line">                    processFunction,</span><br><span class="line">                    lateDataOutputTag);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                operator = <span class="keyword">new</span> CepOperator&lt;&gt;(</span><br><span class="line">                        inputSerializer,</span><br><span class="line">                        isProcessingTime,</span><br><span class="line">                        injectionPatternFunction,</span><br><span class="line">                        comparator,</span><br><span class="line">                        <span class="keyword">null</span>,</span><br><span class="line">                        processFunction,</span><br><span class="line">                        lateDataOutputTag);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">final</span> SingleOutputStreamOperator&lt;OUT&gt; patternStream;</span><br><span class="line">        <span class="keyword">if</span> (inputStream <span class="keyword">instanceof</span> KeyedStream) &#123;</span><br><span class="line">            KeyedStream&lt;IN, K&gt; keyedStream = (KeyedStream&lt;IN, K&gt;) inputStream;</span><br><span class="line"></span><br><span class="line">            patternStream = keyedStream.transform(<span class="string">&quot;CepOperator&quot;</span>, outTypeInfo, operator);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            KeySelector&lt;IN, Byte&gt; keySelector = <span class="keyword">new</span> NullByteKeySelector&lt;&gt;();</span><br><span class="line"></span><br><span class="line">            patternStream =</span><br><span class="line">                    inputStream</span><br><span class="line">                            .keyBy(keySelector)</span><br><span class="line">                            .transform(<span class="string">&quot;GlobalCepOperator&quot;</span>, outTypeInfo, operator)</span><br><span class="line">                            .forceNonParallel();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> patternStream;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="CepOperator"><a href="#CepOperator" class="headerlink" title="CepOperator"></a>CepOperator</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Internal</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CepOperator</span>&lt;<span class="title">IN</span>, <span class="title">KEY</span>, <span class="title">OUT</span>&gt;</span></span><br><span class="line"><span class="class">        <span class="keyword">extends</span> <span class="title">AbstractUdfStreamOperator</span>&lt;<span class="title">OUT</span>, <span class="title">PatternProcessFunction</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>&gt;&gt;</span></span><br><span class="line"><span class="class">        <span class="keyword">implements</span> <span class="title">OneInputStreamOperator</span>&lt;<span class="title">IN</span>, <span class="title">OUT</span>&gt;, <span class="title">Triggerable</span>&lt;<span class="title">KEY</span>, <span class="title">VoidNamespace</span>&gt;, <span class="title">ProcessingTimeCallback</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> InjectionPatternFunction&lt;IN&gt; injectionPatternFunction;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 添加构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CepOperator</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">final</span> TypeSerializer&lt;IN&gt; inputSerializer,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">final</span> <span class="keyword">boolean</span> isProcessingTime,</span></span></span><br><span class="line"><span class="function"><span class="params">            InjectionPatternFunction&lt;IN&gt; injectionPatternFunction,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="meta">@Nullable</span> <span class="keyword">final</span> EventComparator&lt;IN&gt; comparator,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="meta">@Nullable</span> <span class="keyword">final</span> AfterMatchSkipStrategy afterMatchSkipStrategy,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">final</span> PatternProcessFunction&lt;IN, OUT&gt; function,</span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="meta">@Nullable</span> <span class="keyword">final</span> OutputTag&lt;IN&gt; lateDataOutputTag)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(function);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.inputSerializer = Preconditions.checkNotNull(inputSerializer);</span><br><span class="line">        <span class="keyword">this</span>.injectionPatternFunction = injectionPatternFunction;</span><br><span class="line">        <span class="keyword">this</span>.isProcessingTime = isProcessingTime;</span><br><span class="line">        <span class="keyword">this</span>.comparator = comparator;</span><br><span class="line">        <span class="keyword">this</span>.lateDataOutputTag = lateDataOutputTag;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (afterMatchSkipStrategy == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.afterMatchSkipStrategy = AfterMatchSkipStrategy.noSkip();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.afterMatchSkipStrategy = afterMatchSkipStrategy;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open();</span><br><span class="line">        timerService =</span><br><span class="line">                getInternalTimerService(</span><br><span class="line">                        <span class="string">&quot;watermark-callbacks&quot;</span>, VoidNamespaceSerializer.INSTANCE, <span class="keyword">this</span>);</span><br><span class="line">        <span class="comment">// 初始化NFA</span></span><br><span class="line">        <span class="keyword">if</span> (injectionPatternFunction != <span class="keyword">null</span>) &#123;</span><br><span class="line">            injectionPatternFunction.init();</span><br><span class="line">            Pattern pattern = injectionPatternFunction.inject();</span><br><span class="line">            <span class="keyword">boolean</span> timeoutHandling = getUserFunction() <span class="keyword">instanceof</span> TimedOutPartialMatchHandler;</span><br><span class="line">            nfaFactory = NFACompiler.compileFactory(pattern, timeoutHandling);</span><br><span class="line">            <span class="keyword">long</span> period = injectionPatternFunction.getPeriod();</span><br><span class="line">            <span class="comment">// 注册了一个定时检测规则是否变更的定时器</span></span><br><span class="line">            <span class="keyword">if</span> (period &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                getProcessingTimeService().registerTimer(timerService.currentProcessingTime() + period, <span class="keyword">this</span>::onProcessingTime);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        nfa = nfaFactory.createNFA();</span><br><span class="line">        nfa.open(cepRuntimeContext, <span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        context = <span class="keyword">new</span> ContextFunctionImpl();</span><br><span class="line">        collector = <span class="keyword">new</span> TimestampedCollector&lt;&gt;(output);</span><br><span class="line">        cepTimerService = <span class="keyword">new</span> TimerServiceImpl();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// metrics</span></span><br><span class="line">        <span class="keyword">this</span>.numLateRecordsDropped = metrics.counter(LATE_ELEMENTS_DROPPED_METRIC_NAME);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 监听回调方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (injectionPatternFunction.isChanged()) &#123;</span><br><span class="line">            System.err.println(<span class="string">&quot;状态改变&quot;</span>);</span><br><span class="line">            <span class="comment">//重新注入</span></span><br><span class="line">            Pattern pattern = injectionPatternFunction.inject();</span><br><span class="line">            <span class="keyword">boolean</span> timeoutHandling = getUserFunction() <span class="keyword">instanceof</span> TimedOutPartialMatchHandler;</span><br><span class="line">            <span class="comment">// 重新生成NFA</span></span><br><span class="line">            nfaFactory = NFACompiler.compileFactory(pattern, timeoutHandling);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nfa = nfaFactory.createNFA();</span><br><span class="line">            nfa.open(cepRuntimeContext, <span class="keyword">new</span> Configuration());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//重新注册监听器</span></span><br><span class="line">        <span class="keyword">if</span> (injectionPatternFunction.getPeriod() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            System.err.println(timerService.currentProcessingTime());</span><br><span class="line">            getProcessingTimeService().registerTimer(timerService.currentProcessingTime() + injectionPatternFunction.getPeriod(), <span class="keyword">this</span>::onProcessingTime);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实现案例"><a href="#实现案例" class="headerlink" title="实现案例"></a>实现案例</h2><h3 id="案例描述"><a href="#案例描述" class="headerlink" title="案例描述"></a>案例描述</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">初始化规则为,同一userId连续出现两次状态为fail的情况,产生告警消息</span><br><span class="line">希望转换的规则为同一userId连续出现两次状态为success的情况,产生告警</span><br><span class="line">规则改变由本地文件控制</span><br></pre></td></tr></table></figure>
<h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.example.cep</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.&#123;<span class="type">PatternSelectFunction</span>, pattern&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.functions.<span class="type">InjectionPatternFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.scala.&#123;<span class="type">CEP1</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.pattern.<span class="type">Pattern</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.cep.pattern.conditions.&#123;<span class="type">IterativeCondition</span>, <span class="type">RichIterativeCondition</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.<span class="type">TimeCharacteristic</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.source.<span class="type">SourceFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.<span class="type">Time</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">DataStream</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.io.&#123;<span class="type">BufferedSource</span>, <span class="type">Source</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LoginFailWithCustomCEP</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(<span class="type">TimeCharacteristic</span>.<span class="type">EventTime</span>)</span><br><span class="line">    env.setParallelism(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//自定义测试数据</span></span><br><span class="line">    <span class="keyword">val</span> loginStream = env.addSource(<span class="keyword">new</span> <span class="type">CustomGenerator</span>).assignAscendingTimestamps(_.eventTime)</span><br><span class="line"></span><br><span class="line">    loginStream.print()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//在输入流的基础上应用pattern,得到匹配的pattern stream</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> patternStream = <span class="type">CEP1</span>.injectionPattern(loginStream.keyBy(_.userId), <span class="keyword">new</span> <span class="type">InjectionPatternFunction</span>[<span class="type">LoginEvent</span>] &#123;</span><br><span class="line">      <span class="keyword">var</span> current: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 初始化外部连接</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> source = <span class="type">Source</span>.fromFile(<span class="string">&quot;/Users/xz/Local/Projects/LearnDemo/Flink_1_12/src/main/resources/rulestatus.txt&quot;</span>, <span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> lines = source.getLines().toArray</span><br><span class="line">        source.close()</span><br><span class="line">        current = lines(<span class="number">0</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 动态规则注入</span></span><br><span class="line"><span class="comment">       *</span></span><br><span class="line"><span class="comment">       * @return</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inject</span></span>(): pattern.<span class="type">Pattern</span>[<span class="type">LoginEvent</span>, <span class="type">LoginEvent</span>] = &#123;</span><br><span class="line">        println(current)</span><br><span class="line">        <span class="keyword">if</span> (current == <span class="string">&quot;a&quot;</span>) &#123;</span><br><span class="line">          <span class="type">Pattern</span>.begin[<span class="type">LoginEvent</span>](<span class="string">&quot;begin&quot;</span>)</span><br><span class="line">            .where(<span class="keyword">new</span> <span class="type">RichIterativeCondition</span>[<span class="type">LoginEvent</span>]() &#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">LoginEvent</span>, ctx: <span class="type">IterativeCondition</span>.<span class="type">Context</span>[<span class="type">LoginEvent</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">                value.eventTpye.equals(<span class="string">&quot;fail&quot;</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .next(<span class="string">&quot;next&quot;</span>)</span><br><span class="line">            .where(<span class="keyword">new</span> <span class="type">RichIterativeCondition</span>[<span class="type">LoginEvent</span>]() &#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">LoginEvent</span>, ctx: <span class="type">IterativeCondition</span>.<span class="type">Context</span>[<span class="type">LoginEvent</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">                value.eventTpye.equals(<span class="string">&quot;fail&quot;</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">Pattern</span>.begin[<span class="type">LoginEvent</span>](<span class="string">&quot;begin&quot;</span>)</span><br><span class="line">            .where(<span class="keyword">new</span> <span class="type">RichIterativeCondition</span>[<span class="type">LoginEvent</span>]() &#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">LoginEvent</span>, ctx: <span class="type">IterativeCondition</span>.<span class="type">Context</span>[<span class="type">LoginEvent</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">                value.eventTpye.equals(<span class="string">&quot;success&quot;</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .next(<span class="string">&quot;next&quot;</span>)</span><br><span class="line">            .where(<span class="keyword">new</span> <span class="type">RichIterativeCondition</span>[<span class="type">LoginEvent</span>]() &#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(value: <span class="type">LoginEvent</span>, ctx: <span class="type">IterativeCondition</span>.<span class="type">Context</span>[<span class="type">LoginEvent</span>]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">                value.eventTpye.equals(<span class="string">&quot;success&quot;</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 轮询周期(监听不需要)</span></span><br><span class="line"><span class="comment">       *</span></span><br><span class="line"><span class="comment">       * @return</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPeriod</span></span>: <span class="type">Long</span> = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">/**</span></span><br><span class="line"><span class="comment">       * 规则是否发生变更</span></span><br><span class="line"><span class="comment">       *</span></span><br><span class="line"><span class="comment">       * @return</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isChanged</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> source = <span class="type">Source</span>.fromFile(<span class="string">&quot;/Users/xz/Local/Projects/LearnDemo/Flink_1_12/src/main/resources/rulestatus.txt&quot;</span>, <span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> lines = source.getLines().toArray</span><br><span class="line">        source.close()</span><br><span class="line">        <span class="keyword">val</span> tempStatus = current</span><br><span class="line">        current = lines(<span class="number">0</span>)</span><br><span class="line">        !lines(<span class="number">0</span>).equals(tempStatus)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loginFailDataStream = patternStream.select(<span class="keyword">new</span> <span class="type">MySelectFunction</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将得到的警告信息流输出sink</span></span><br><span class="line">    loginFailDataStream.print(<span class="string">&quot;warning&quot;</span>)</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">&quot;Login Fail Detect with CEP&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//登录样例类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginEvent</span>(<span class="params">userId: <span class="type">Long</span>, ip: <span class="type">String</span>, eventTpye: <span class="type">String</span>, eventTime: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">//输出报警信息样例类</span></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Warning</span>(<span class="params">userId: <span class="type">Long</span>, firstFailTime: <span class="type">Long</span>, lastFailTime: <span class="type">Long</span>, warningMSG: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">MySelectFunction</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">PatternSelectFunction</span>[<span class="type">LoginEvent</span>, <span class="type">Warning</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(patternEvents: util.<span class="type">Map</span>[<span class="type">String</span>, util.<span class="type">List</span>[<span class="type">LoginEvent</span>]]): <span class="type">Warning</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> firstFailEvent = patternEvents.getOrDefault(<span class="string">&quot;begin&quot;</span>, <span class="literal">null</span>).iterator().next()</span><br><span class="line">    <span class="keyword">val</span> secondEvent = patternEvents.getOrDefault(<span class="string">&quot;next&quot;</span>, <span class="literal">null</span>).iterator().next()</span><br><span class="line">    <span class="type">Warning</span>(firstFailEvent.userId, firstFailEvent.eventTime, secondEvent.eventTime, <span class="string">&quot;login fail warning&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomGenerator</span> <span class="keyword">extends</span> <span class="title">SourceFunction</span>[<span class="type">LoginEvent</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> running = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(ctx: <span class="type">SourceFunction</span>.<span class="type">SourceContext</span>[<span class="type">LoginEvent</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 随机数生成器</span></span><br><span class="line">    <span class="keyword">val</span> state = <span class="type">Array</span>(<span class="string">&quot;fail&quot;</span>, <span class="string">&quot;success&quot;</span>)</span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line">      <span class="comment">// 利用ctx上下文将数据返回</span></span><br><span class="line">      ctx.collect(<span class="type">LoginEvent</span>(scala.util.<span class="type">Random</span>.nextInt(<span class="number">10</span>), <span class="string">&quot;192.168.0.1&quot;</span>, state(scala.util.<span class="type">Random</span>.nextInt(<span class="number">2</span>)), <span class="type">System</span>.currentTimeMillis()))</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">500</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">cancel</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    running = <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="改进方向"><a href="#改进方向" class="headerlink" title="改进方向"></a>改进方向</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.状态存储可以加上</span><br><span class="line">2.规则定义可以放在外部数据存储中,然后使用ScriptEvaluator进行Pattern类生成(也可以选用其他的方式生成,有很多种方式)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink技术选型相关</title>
    <url>/2020/08/10/Flink%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<blockquote>
<p>公司的相关需求越来越重视对毫秒级数据的处理,flink刚好在这方面暂有不可替代的优势;使得在技术选型上有着重要的地位</p>
</blockquote>
<span id="more"></span>
<h4 id="数据源处理"><a href="#数据源处理" class="headerlink" title="数据源处理"></a>数据源处理</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">根据业务的不同,实时数据源选取kafka,但是数据内容分为两块内容:</span><br><span class="line">1.研发将实时日志数据打点至kafka上,数据格式json形式</span><br><span class="line">2.使用debezium监控mysql的binlog,实时将mysql变更的数据捕获至kafka;数据格式json形式,并且使用before和after的形式来区分数据更变之前和更变之后的数据内容</span><br></pre></td></tr></table></figure>
<p>方式2进行简单代码公共方法提取:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">方式2进行简单区分:</span><br><span class="line">import com.etiantian.bigdata.flink.graph.MapPoint</span><br><span class="line">import org.json.JSONObject</span><br><span class="line"></span><br><span class="line">import scala.util.Try</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line">  * 对数据分成多种形式处理</span><br><span class="line">  * 0 表示只有key的形式</span><br><span class="line">  * 1 表示有value，但是只有before    删除</span><br><span class="line">  * 2 表示有value，但是只有after     添加</span><br><span class="line">  * 3 表示有value，并且before和after都存在   更改</span><br><span class="line">  *&#x2F;</span><br><span class="line">class OptTypeMap extends MapPoint[String,(String,Int)] &#123;</span><br><span class="line">  override def process(stream: String): (String, Int) &#x3D; &#123;</span><br><span class="line">    val x &#x3D; new JSONObject(stream)</span><br><span class="line">    var value &#x3D; new JSONObject()</span><br><span class="line">    var flag &#x3D; 0</span><br><span class="line">    if (Try(new JSONObject(x.get(&quot;value&quot;).toString.toLowerCase)).isSuccess) &#123;</span><br><span class="line">      value &#x3D; new JSONObject(x.get(&quot;value&quot;).toString.toLowerCase)</span><br><span class="line">      if (Try(value.getJSONObject(&quot;before&quot;)).isSuccess)</span><br><span class="line">        if (Try(value.getJSONObject(&quot;after&quot;)).isSuccess)</span><br><span class="line">          flag &#x3D; 3</span><br><span class="line">        else</span><br><span class="line">          flag &#x3D; 1</span><br><span class="line">      else</span><br><span class="line">        flag &#x3D; 2</span><br><span class="line">    &#125;</span><br><span class="line">    (stream, flag)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="选取语义使用exactly-once"><a href="#选取语义使用exactly-once" class="headerlink" title="选取语义使用exactly once"></a>选取语义使用exactly once</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个sender发送一条message到recevier;根据receiver出现fail时sender如何处理fail,可以降message delivery分为三种语义:</span><br><span class="line">1.at most once: 对于一条message,receiver最多收到一次</span><br><span class="line">sender把massage发送给receiver;无论receiver是否收到message,sender都不再重发message</span><br><span class="line">2.at least once : 对于一条message,receiver最少收到一次</span><br><span class="line">sender把message发送给receiver;当receiver在规定时间内没有回复ack或回复了error信息,那么sender重发这条message给receiver,知道sender收到receiver的ack</span><br><span class="line">3.exactly once:对于一条message,receiver确保只收到一次</span><br><span class="line">选取原因:</span><br><span class="line">根据公司业务需求对准确的数据量的要求性比较高,最后选用了exactly once;其他语义也进行了测试,确定了确实有数据量变多或者变少的情况</span><br><span class="line"></span><br><span class="line">exactly once模式:</span><br><span class="line">flink会持续对整个系统做snapslot,然后把global state存储到master node或HDFS;当系统出现failure,flink会停止数据处理,然后把系统恢复到最近的一次checkpoint</span><br><span class="line"></span><br><span class="line">flink的snapslot算法:</span><br><span class="line">为了消去记录channel state,process在接收到第一个barrier后不会马上做snapslot</span><br><span class="line">而是等待接受其他上游channel的barrier</span><br><span class="line">在等待期间,process会把barrier已到的channel的record放入input buffer</span><br><span class="line">当所有上游channel的barrier到齐后,process才记录自身state,之后向所有下游channel发送barrier</span><br><span class="line">因为先到的barrier会等待后到的barrier,所有barrier相当于同时到达process;因此,该process的上游channel的state都是空集,避免了去记录channel的state</span><br></pre></td></tr></table></figure>

<h4 id="一般关注的指标点"><a href="#一般关注的指标点" class="headerlink" title="一般关注的指标点"></a>一般关注的指标点</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.作业状态: 作业是否出故障,作业是否存活,作业是否稳定运行</span><br><span class="line">可参考监控脚本的相关文章</span><br><span class="line">[https:&#x2F;&#x2F;jxeditor.github.io&#x2F;2020&#x2F;08&#x2F;10&#x2F;shell%E8%84%9A%E6%9C%AC%E7%9B%91%E6%8E%A7flink%E9%A1%B9%E7%9B%AE%E6%98%AF%E5%90%A6%E6%AD%A3%E5%B8%B8%E8%BF%90%E8%A1%8C&#x2F;]</span><br><span class="line">2.作业性能:作业的处理延迟,数据倾斜问题是否存在,性能瓶颈情况(技术选型必须测试内容)</span><br><span class="line">1) flink接收kafka的数据性能情况的压力测试(每秒上万没有问题)</span><br><span class="line">2) flink读取或者写入hbase的数据性能情况测试</span><br><span class="line">分为客户端方式和批处理读写方式</span><br><span class="line">i:客户端,每秒的处理速度5000条左右</span><br><span class="line">ii:批处理方式:5-7w左右</span><br><span class="line">3.业务逻辑:上游数据质量,新上的逻辑是否存在问题,数据是否存在丢失(新作业上线必须测试的内容)</span><br></pre></td></tr></table></figure>

<h4 id="sink落地"><a href="#sink落地" class="headerlink" title="sink落地"></a>sink落地</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主要落地有:s3,hbase,es</span><br><span class="line">1.s3主要用于实时仓库处理</span><br><span class="line">2.hbase主要用于flink的中间表,相当于维表方式;除了共享课直接提供接口的方式进行调用</span><br><span class="line">3.es主要提供给搜索系统,实时报表展示和用户行为轨迹</span><br></pre></td></tr></table></figure>

<h4 id="存储s3的方式"><a href="#存储s3的方式" class="headerlink" title="存储s3的方式"></a>存储s3的方式</h4><p>方式一:SimpleStringEncoder</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">flink1.11之前一般使用两种方式:</span><br><span class="line">方式一:SimpleStringEncoder</span><br><span class="line">按照行以文本的方式写到文件中,每行一条记录;一般来说文本存储方式无压缩</span><br><span class="line">一般将dataStream清洗成字符串拼接的方式;使用withBucketAssigner进行自定义分区</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val builder &#x3D; StreamingFileSink.forRowFormat(</span><br><span class="line">&#x2F;&#x2F;              new Path(&quot;E:\\\\aa\\\\flink&quot;),new SimpleStringEncoder[String](&quot;UTF-8&quot;))</span><br><span class="line">        new Path(config.getProperty(&quot;s3.all.path&quot;)),new SimpleStringEncoder[String](&quot;UTF-8&quot;))</span><br><span class="line">      builder.withBucketAssigner(new BucketAssigner[String,String] &#123;</span><br><span class="line">        override def getBucketId(data: String, context: BucketAssigner.Context): String &#x3D; &#123;</span><br><span class="line">          s&quot;c_date&#x3D;$&#123;data.split(&quot;\\|&quot;)(4)&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        override def getSerializer: SimpleVersionedSerializer[String] &#x3D; SimpleVersionedStringSerializer.INSTANCE</span><br><span class="line">      &#125;).withRollingPolicy(</span><br><span class="line">        DefaultRollingPolicy.builder()</span><br><span class="line">          .withRolloverInterval(TimeUnit.MINUTES.toMillis(5))     &#x2F;&#x2F;至少5分钟的数据</span><br><span class="line">          .withInactivityInterval(TimeUnit.MINUTES.toMillis(30))   &#x2F;&#x2F;最近30分钟未收到新的记录</span><br><span class="line">          .withMaxPartSize(1024*1024*1024)   &#x2F;&#x2F;文件大小达到1G</span><br><span class="line">          .build()</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">      result.filter(!_.split(&quot;\\|&quot;)(0).equals(&quot;null&quot;)).addSink(builder.build())</span><br></pre></td></tr></table></figure>
<p>方式二:CustomParquetAvroWriters(自定义函数)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parquet,使用gzip压缩,体积较小,运算效率较高;采用二进制的存储方式</span><br><span class="line">使用方式自定义相关类,进行引用调用的方式</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import org.apache.avro.Schema;</span><br><span class="line">import org.apache.avro.generic.GenericData;</span><br><span class="line">import org.apache.avro.generic.GenericRecord;</span><br><span class="line">import org.apache.avro.reflect.ReflectData;</span><br><span class="line">import org.apache.avro.specific.SpecificData;</span><br><span class="line">import org.apache.avro.specific.SpecificRecordBase;</span><br><span class="line">import org.apache.flink.formats.parquet.ParquetBuilder;</span><br><span class="line">import org.apache.flink.formats.parquet.ParquetWriterFactory;</span><br><span class="line">import org.apache.parquet.avro.AvroParquetWriter;</span><br><span class="line">import org.apache.parquet.hadoop.ParquetWriter;</span><br><span class="line">import org.apache.parquet.hadoop.metadata.CompressionCodecName;</span><br><span class="line">import org.apache.parquet.io.OutputFile;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class CustomParquetAvroWriters &#123;</span><br><span class="line">    private CustomParquetAvroWriters() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static &lt;T extends SpecificRecordBase&gt; ParquetWriterFactory&lt;T&gt; forSpecificRecord(Class&lt;T&gt; type, CompressionCodecName compressionCodecName) &#123;</span><br><span class="line">        final String schemaString &#x3D; SpecificData.get().getSchema(type).toString();</span><br><span class="line">        final ParquetBuilder&lt;T&gt; builder &#x3D; (out) -&gt; createAvroParquetWriter(schemaString, SpecificData.get(), out, compressionCodecName);</span><br><span class="line">        return new ParquetWriterFactory&lt;&gt;(builder);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;compressionCodecName 压缩算法</span><br><span class="line">    public static ParquetWriterFactory&lt;GenericRecord&gt; forGenericRecord(Schema schema, CompressionCodecName compressionCodecName) &#123;</span><br><span class="line">        final String schemaString &#x3D; schema.toString();</span><br><span class="line">        final ParquetBuilder&lt;GenericRecord&gt; builder &#x3D; (out) -&gt; createAvroParquetWriter(schemaString, GenericData.get(), out, compressionCodecName);</span><br><span class="line">        return new ParquetWriterFactory&lt;&gt;(builder);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;compressionCodecName 压缩算法</span><br><span class="line">    public static &lt;T&gt; ParquetWriterFactory&lt;T&gt; forReflectRecord(Class&lt;T&gt; type, CompressionCodecName compressionCodecName) &#123;</span><br><span class="line">        final String schemaString &#x3D; ReflectData.get().getSchema(type).toString();</span><br><span class="line">        final ParquetBuilder&lt;T&gt; builder &#x3D; (out) -&gt; createAvroParquetWriter(schemaString, ReflectData.get(), out, compressionCodecName);</span><br><span class="line">        return new ParquetWriterFactory&lt;&gt;(builder);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;compressionCodecName 压缩算法</span><br><span class="line">    private static &lt;T&gt; ParquetWriter&lt;T&gt; createAvroParquetWriter(</span><br><span class="line">            String schemaString,</span><br><span class="line">            GenericData dataModel,</span><br><span class="line">            OutputFile out,</span><br><span class="line">            CompressionCodecName compressionCodecName) throws IOException &#123;</span><br><span class="line">        final Schema schema &#x3D; new Schema.Parser().parse(schemaString);</span><br><span class="line"></span><br><span class="line">        return AvroParquetWriter.&lt;T&gt;builder(out)</span><br><span class="line">                .withSchema(schema)</span><br><span class="line">                .withDataModel(dataModel)</span><br><span class="line">                .withCompressionCodec(compressionCodecName)&#x2F;&#x2F;压缩算法</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>应用自定义类(将dataStream的数据清洗成Option(对象内容))</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val result &#x3D; dataStream.map(x &#x3D;&gt; &#123;</span><br><span class="line">      val data &#x3D; x.getMsg.asInstanceOf[Tuple3[Long, String, String]]</span><br><span class="line">      val key &#x3D; data._1</span><br><span class="line">      val c_time &#x3D; data._2</span><br><span class="line">      var c_date &#x3D; &quot;&quot;</span><br><span class="line">      val yes_day &#x3D; LocalDate.now().plusDays(-1).toString</span><br><span class="line">      if (c_time.substring(0,10) &gt;&#x3D; yes_day)&#123;</span><br><span class="line">        c_date &#x3D; c_time.substring(0,10)</span><br><span class="line">      &#125;else</span><br><span class="line">        c_date &#x3D; yes_day</span><br><span class="line">      val file_path &#x3D; data._3</span><br><span class="line">      if (key !&#x3D; 0 &amp;&amp; c_time !&#x3D; &quot;&quot; &amp;&amp; file_path !&#x3D; &quot;&quot; &amp;&amp; c_date !&#x3D; &quot;&quot;)&#123;</span><br><span class="line">        Option(DeleteFileInfo(key,c_time,file_path,c_date))</span><br><span class="line">&#x2F;&#x2F;        key + &quot;|&quot; + c_time + &quot;|&quot; + file_path + &quot;|&quot; + c_date</span><br><span class="line">      &#125;else</span><br><span class="line">        Option.empty</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">val sink &#x3D; StreamingFileSink</span><br><span class="line">      .forBulkFormat(new Path(config.getProperty(&quot;s3.delete.path&quot;)), CustomParquetAvroWriters.forReflectRecord(classOf[DeleteFileInfo],CompressionCodecName.SNAPPY))</span><br><span class="line">      .withBucketAssigner(new BucketAssigner[DeleteFileInfo, String] &#123;</span><br><span class="line">        override def getBucketId(element: DeleteFileInfo, context: BucketAssigner.Context): String &#x3D; &#123;</span><br><span class="line">          s&quot;c_date&#x3D;$&#123;element.c_date&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        override def getSerializer: SimpleVersionedSerializer[String] &#x3D; &#123;</span><br><span class="line">          SimpleVersionedStringSerializer.INSTANCE</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      .build()</span><br><span class="line"></span><br><span class="line">    result.filter(x&#x3D;&gt; x!&#x3D; None).map(x&#x3D;&gt;x.get).addSink(sink)</span><br></pre></td></tr></table></figure>

<h4 id="sink存储至hbase"><a href="#sink存储至hbase" class="headerlink" title="sink存储至hbase"></a>sink存储至hbase</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一般使用hbase的内容有两种方式:</span><br><span class="line">方式一:客户端调用的方式</span><br><span class="line">方式二:批数据存储的方式</span><br><span class="line">调用次数比较多,一般使用common-jar的形式上传到公司的maven仓库,全公司只要引用maven地址就可以直接调用</span><br></pre></td></tr></table></figure>
<p>方式一:客户端调用(hbase-util自定义可直接作为调用工具)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.etiantian.bigdata</span><br><span class="line"></span><br><span class="line">import java.util.Properties</span><br><span class="line">import java.util.concurrent.&#123;ExecutorService, Executors&#125;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration</span><br><span class="line">import org.apache.hadoop.hbase.client._</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.&#123;HBaseConfiguration, HColumnDescriptor, HTableDescriptor, TableName&#125;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job</span><br><span class="line">import scala.collection.convert.wrapAsJava._</span><br><span class="line"></span><br><span class="line">object HbaseUtil &#123;</span><br><span class="line">  @transient private var hbaseConn:Connection &#x3D; null</span><br><span class="line">  private val hbaseConf: Configuration &#x3D; HBaseConfiguration.create()</span><br><span class="line">  private val replaceToken &#x3D; &quot;#S%P#&quot;</span><br><span class="line"></span><br><span class="line">  def init(properties: Properties):Unit &#x3D; &#123;</span><br><span class="line">    hbaseConf.set(&quot;hbase.zookeeper.quorum&quot;, properties.getProperty(&quot;hbase.zookeeper.quorum&quot;))</span><br><span class="line">    hbaseConf.set(&quot;hbase.zookeeper.property.clientPort&quot;, properties.getProperty(&quot;hbase.zookeeper.property.clientPort&quot;))</span><br><span class="line">    synchronized &#123;</span><br><span class="line">      if (hbaseConn &#x3D;&#x3D; null) &#123;</span><br><span class="line">        println(&quot;##########################  init hbase properties and connection synchronized  ##########################&quot;)</span><br><span class="line">        hbaseConn &#x3D; ConnectionFactory.createConnection(</span><br><span class="line">          hbaseConf, Executors.newFixedThreadPool(properties.getOrDefault(&quot;hbase.poolSize&quot;, &quot;8&quot;).toString.toInt)</span><br><span class="line">        )</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getConn(): Connection &#x3D; &#123;</span><br><span class="line">    hbaseConn</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def getTable(tableName: String): Table &#x3D; &#123;</span><br><span class="line">    getConn.getTable(getTableName(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def getAdmin(): Admin &#x3D; &#123;</span><br><span class="line">    getConn().getAdmin</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">    * Change the method to createJob</span><br><span class="line">    * @param tableName</span><br><span class="line">    * @param family</span><br><span class="line">    * @return</span><br><span class="line">    *&#x2F;</span><br><span class="line">  @Deprecated</span><br><span class="line">  def create(tableName: TableName, family: String) &#x3D; &#123;</span><br><span class="line">    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, tableName.toString)</span><br><span class="line"></span><br><span class="line">    val job &#x3D; Job.getInstance(hbaseConf)</span><br><span class="line">    job.setOutputKeyClass(classOf[ImmutableBytesWritable])</span><br><span class="line">    job.setOutputValueClass(classOf[Put])</span><br><span class="line">    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])</span><br><span class="line"></span><br><span class="line">    job</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def createJob(tableName: TableName, family: String) &#x3D; &#123;</span><br><span class="line">    hbaseConf.set(TableOutputFormat.OUTPUT_TABLE, tableName.toString)</span><br><span class="line"></span><br><span class="line">    val job &#x3D; Job.getInstance(hbaseConf)</span><br><span class="line">    job.setOutputKeyClass(classOf[ImmutableBytesWritable])</span><br><span class="line">    job.setOutputValueClass(classOf[Put])</span><br><span class="line">    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])</span><br><span class="line"></span><br><span class="line">    job</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  private def getTableName(tableName: String): TableName &#x3D; TableName.valueOf(tableName)</span><br><span class="line"></span><br><span class="line">  def tableExists(tableName: String): Boolean &#x3D; &#123;</span><br><span class="line">    getAdmin.tableExists(getTableName(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def createTable(tableName: String, family: String*): Unit &#x3D; &#123;</span><br><span class="line">    if (!tableExists(tableName)) &#123;</span><br><span class="line">      val descriptor &#x3D; new HTableDescriptor(getTableName(tableName))</span><br><span class="line">      family.foreach(x &#x3D;&gt; &#123;</span><br><span class="line">        val hColumnDescriptor &#x3D; new HColumnDescriptor(x)</span><br><span class="line">        descriptor.addFamily(hColumnDescriptor)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      getAdmin.createTable(descriptor)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def createTable(tableName: String, family: Seq[String], splits: Seq[String]): Unit &#x3D; &#123;</span><br><span class="line">    if (!tableExists(tableName)) &#123;</span><br><span class="line">      val descriptor &#x3D; new HTableDescriptor(getTableName(tableName))</span><br><span class="line">      family.foreach(x &#x3D;&gt; &#123;</span><br><span class="line">        val hColumnDescriptor &#x3D; new HColumnDescriptor(x)</span><br><span class="line">        descriptor.addFamily(hColumnDescriptor)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">      val splitsArray &#x3D; splits.map(Bytes.toBytes).toArray</span><br><span class="line"></span><br><span class="line">      getAdmin.createTable(descriptor, splitsArray)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def truncTable(tableName: String, preserveSplits: Boolean): Unit &#x3D; &#123;</span><br><span class="line">    val admin &#x3D; getAdmin</span><br><span class="line">    admin.disableTable(getTableName(tableName))</span><br><span class="line">    admin.truncateTable(getTableName(tableName), preserveSplits)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def truncTable(tableName: String): Unit &#x3D; &#123;</span><br><span class="line">    truncTable(tableName, false)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def dropTable(tableName: String): Unit &#x3D; &#123;</span><br><span class="line">    val admin &#x3D; getAdmin</span><br><span class="line">    admin.disableTable(getTableName(tableName))</span><br><span class="line">    admin.deleteTable(getTableName(tableName))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getValueByKey(tableName: String, key: String): Result &#x3D; &#123;</span><br><span class="line">    val table &#x3D; getTable(tableName)</span><br><span class="line">    table.get(new Get(Bytes.toBytes(key)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getColumnValue(result: Result, family: String, column: String): String &#x3D; &#123;</span><br><span class="line">    val bytesValue &#x3D; result.getValue(Bytes.toBytes(family), Bytes.toBytes(column))</span><br><span class="line">    if (bytesValue &#x3D;&#x3D; null) null else Bytes.toString(bytesValue)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getColumnValue(tableName: String, key: String, family: String, column: String): String &#x3D; &#123;</span><br><span class="line">    val result &#x3D; getValueByKey(tableName, key)</span><br><span class="line">    getColumnValue(result, family, column)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def insert(tableName: String, put: Put): Unit &#x3D; &#123;</span><br><span class="line">    val table &#x3D; getTable(tableName)</span><br><span class="line">    table.put(put)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def insert(tableName: String, putList: List[Put]): Unit &#x3D; &#123;</span><br><span class="line">    val table &#x3D; getTable(tableName)</span><br><span class="line">    table.put(putList)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def insert(tableName: String, key: String, family: String, column: String, value: String): Unit &#x3D; &#123;</span><br><span class="line">    val put &#x3D; new Put(Bytes.toBytes(key))</span><br><span class="line">    put.addColumn(Bytes.toBytes(family), Bytes.toBytes(column), if (value &#x3D;&#x3D; null) null else Bytes.toBytes(value))</span><br><span class="line">    insert(tableName, put)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def deleteRow(tableName: String, key: String): Unit &#x3D; &#123;</span><br><span class="line">    getTable(tableName).delete(new Delete(Bytes.toBytes(key)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def deleteRowList(tableName: String, keyList: List[String]): Unit &#x3D; &#123;</span><br><span class="line">    getTable(tableName).delete(keyList.map(key &#x3D;&gt;new Delete(Bytes.toBytes(key))))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def deleteMulti(tableName: String, delList: List[Delete]): Unit &#x3D; &#123;</span><br><span class="line">    getTable(tableName).delete(delList)</span><br><span class="line">  &#125;</span><br><span class="line">  &#x2F;**</span><br><span class="line">    * 插入并替换原有行</span><br><span class="line">    *&#x2F;</span><br><span class="line">  def replaceRow(tableName: String, put: Put): Unit &#x3D; &#123;</span><br><span class="line">    deleteRow(tableName, Bytes.toString(put.getRow))</span><br><span class="line">    insert(tableName, put)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def columnPlus(tableName: String, key: String, family: String, column: String, plusNum: Long): Long &#x3D; &#123;</span><br><span class="line">    var columnVal &#x3D; getColumnValue(tableName, key, family, column).toLong</span><br><span class="line">    columnVal +&#x3D; plusNum</span><br><span class="line">    insert(tableName, key, family, column, columnVal.toString)</span><br><span class="line">    columnVal</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def columnPlus(tableName: String, key: String, family: String, column: String): Long &#x3D; &#123;</span><br><span class="line">    columnPlus(tableName, key, family, column, 1)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def insertList[T](tableName: String, key: String, family: String, column: String, values: List[T], separator: String): Unit &#x3D; &#123;</span><br><span class="line">    val put &#x3D; new Put(Bytes.toBytes(key))</span><br><span class="line">    put.addColumn(Bytes.toBytes(family), Bytes.toBytes(column), if (values &#x3D;&#x3D; null) null else &#123;</span><br><span class="line">      Bytes.toBytes(values.map(x &#x3D;&gt; &#123;</span><br><span class="line">        x.toString.replaceAll(separator, replaceToken)</span><br><span class="line">      &#125;).mkString(separator))</span><br><span class="line">    &#125;)</span><br><span class="line">    insert(tableName, put)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def insertList[T](tableName: String, key: String, family: String, column: String, values: List[T]): Unit &#x3D; &#123;</span><br><span class="line">    insertList[T](tableName, key, family, column, values, &quot;,&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getList[T](tableName: String, key: String, family: String, column: String, separator: String): List[T] &#x3D; &#123;</span><br><span class="line">    val value &#x3D; getColumnValue(tableName, key, family, column)</span><br><span class="line">    if (value !&#x3D; null &amp;&amp; !value.equals(&quot;&quot;)) value.split(separator).map(_.replaceAll(replaceToken, separator).asInstanceOf[T]).toList else null</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getList[T](tableName: String, key: String, family: String, column: String): List[T] &#x3D; &#123;</span><br><span class="line">    getList[T](tableName, key, family, column, &quot;,&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def addToList[T](tableName: String, key: String, family: String, column: String, t: T, separator: String): Unit &#x3D; &#123;</span><br><span class="line">    val newValue &#x3D; t.toString.replaceAll(separator, replaceToken)</span><br><span class="line">    val value &#x3D; getColumnValue(tableName, key, family, column)</span><br><span class="line">    if (value !&#x3D; null &amp;&amp; !value.equals(&quot;&quot;))</span><br><span class="line">      insert(tableName, key, family, column, value + separator + newValue)</span><br><span class="line">    else</span><br><span class="line">      insert(tableName, key, family, column, newValue)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def addToList[T](tableName: String, key: String, family: String, column: String, t: T): Unit &#x3D; &#123;</span><br><span class="line">    addToList[T](tableName, key, family, column, t, &quot;,&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">    * 添加到list中，list 会去重</span><br><span class="line">    *&#x2F;</span><br><span class="line">  def addUniqueToList[T](tableName: String, key: String, family: String, column: String, t: T, separator: String): Unit &#x3D; &#123;</span><br><span class="line">    val newValue &#x3D; t.toString.replaceAll(separator, replaceToken)</span><br><span class="line">    val value &#x3D; getColumnValue(tableName, key, family, column)</span><br><span class="line">    if (value !&#x3D; null &amp;&amp; !value.equals(&quot;&quot;)) &#123;</span><br><span class="line">      val list &#x3D; value.split(separator).toList :+ newValue</span><br><span class="line">      insertList(tableName, key, family, column, list.distinct, separator)</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">      insert(tableName, key, family, column, newValue)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;**</span><br><span class="line">    * 添加到list中，list 会去重</span><br><span class="line">    *&#x2F;</span><br><span class="line">  def addUniqueToList[T](tableName: String, key: String, family: String, column: String, t: T): Unit &#x3D; &#123;</span><br><span class="line">    addUniqueToList(tableName, key, family, column, t, &quot;,&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def removeFromList[T](tableName: String, key: String, family: String, column: String, t: T, separator: String): Unit &#x3D; &#123;</span><br><span class="line">    val newValue &#x3D; t.toString.replaceAll(separator, replaceToken)</span><br><span class="line">    val value &#x3D; getColumnValue(tableName, key, family, column)</span><br><span class="line">    if (value !&#x3D; null &amp;&amp; !value.equals(&quot;&quot;)) &#123;</span><br><span class="line">      insertList(</span><br><span class="line">        tableName,</span><br><span class="line">        key,</span><br><span class="line">        family,</span><br><span class="line">        column,</span><br><span class="line">        value.split(separator).filter(!_.equals(newValue)).toList,</span><br><span class="line">        separator</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def removeFromList[T](tableName: String, key: String, family: String, column: String, t: T): Unit &#x3D; &#123;</span><br><span class="line">    removeFromList[T](tableName, key, family, column, t, &quot;,&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">调用方式:</span><br><span class="line">val properties &#x3D; new Properties()</span><br><span class="line">properties.setProperty(&quot;hbase.zookeeper.quorum&quot;, conf.getProperty(&quot;hbase.zookeeper.quorum&quot;))</span><br><span class="line">properties.setProperty(&quot;hbase.zookeeper.property.clientPort&quot;, conf.getProperty(&quot;hbase.zookeeper.port&quot;))</span><br><span class="line"></span><br><span class="line">HbaseUtil.init(properties)</span><br><span class="line"></span><br><span class="line">val props &#x3D; columns.split(&quot;,&quot;).toList</span><br><span class="line">val rowkey &#x3D; content.get(key).toString</span><br><span class="line">val put &#x3D; new Put(rowkey.getBytes)</span><br><span class="line">props.map(prop &#x3D;&gt; &#123;</span><br><span class="line">              if (content.has(prop) &amp;&amp; !key.contains(prop)) &#123;</span><br><span class="line">                opt match &#123;</span><br><span class="line">                  case 1 &#x3D;&gt; &#123;</span><br><span class="line">                    put.addColumn(famliy.getBytes, prop.getBytes, null)</span><br><span class="line">                    HbaseUtil.insert(newTable, put)</span><br><span class="line">                  &#125;</span><br><span class="line">                  case 2 &#x3D;&gt; &#123;</span><br><span class="line">                    put.addColumn(famliy.getBytes, prop.getBytes, content.get(prop).toString.getBytes)</span><br><span class="line">                    HbaseUtil.insert(newTable, put)</span><br><span class="line">                  &#125;</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>方式二:批处理存储(主要重写了HBaseOutputFormat继承OutputFormat)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.etiantian.bigdata.common</span><br><span class="line"></span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.io.OutputFormat</span><br><span class="line">import org.apache.flink.configuration.Configuration</span><br><span class="line">import org.apache.hadoop.hbase.client.&#123;Connection, ConnectionFactory, Put, Table&#125;</span><br><span class="line">import org.apache.hadoop.hbase.&#123;HBaseConfiguration, HConstants, TableName&#125;</span><br><span class="line"></span><br><span class="line">class HBaseOutputFormat(topic:String,zk:Properties) extends OutputFormat[Put] &#123;</span><br><span class="line">  var table: Table &#x3D; null</span><br><span class="line">  var conf: org.apache.hadoop.conf.Configuration &#x3D; null</span><br><span class="line">  var flinkConf: Configuration &#x3D; new Configuration()</span><br><span class="line">  var tableName: String &#x3D; null</span><br><span class="line">  var conn: Connection &#x3D; null</span><br><span class="line"></span><br><span class="line">  def setConfiguration(configuration: Configuration) &#x3D; &#123;</span><br><span class="line">    flinkConf &#x3D; configuration</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def getConfig(field: String): Any &#x3D; &#123;</span><br><span class="line">    if (flinkConf !&#x3D; null) flinkConf.getString(field, null) else null</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def configure(configuration: Configuration) &#x3D; &#123;</span><br><span class="line">    configuration.addAll(flinkConf)</span><br><span class="line">&#x2F;&#x2F;    println(&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;+ configuration+&quot;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;)</span><br><span class="line">    conf &#x3D; HBaseConfiguration.create()</span><br><span class="line">    conf.set(HConstants.ZOOKEEPER_QUORUM, configuration.getString(&quot;quorum&quot;,zk.getProperty(&quot;hbase.zookeeper.quorum&quot;)))</span><br><span class="line">&#x2F;&#x2F;    conf.set(HConstants.ZOOKEEPER_QUORUM, &quot;cdh132,cdh133,cdh134&quot;)</span><br><span class="line">    conf.set(HConstants.ZOOKEEPER_CLIENT_PORT, &quot;2181&quot;)</span><br><span class="line"></span><br><span class="line">    tableName &#x3D; configuration.getString(&quot;tableName&quot;, topic)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def writeRecord(it: Put) &#x3D; &#123;</span><br><span class="line">    table.put(it)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def close() &#x3D; &#123;</span><br><span class="line">    if (table !&#x3D; null) table.close</span><br><span class="line">    if (conn !&#x3D; null) conn.close</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def open(i: Int, i1: Int) &#x3D; &#123;</span><br><span class="line">    conn &#x3D; ConnectionFactory.createConnection(conf)</span><br><span class="line">    table &#x3D; conn.getTable(TableName.valueOf(tableName))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实际应用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;* 创建hbase表</span><br><span class="line">  * create &#39;aixueOnline&#39;, &#39;info&#39;,SPLITS &#x3D;&gt; [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;]</span><br><span class="line">  * create &#39;accessLogTopic&#39;, &#39;info&#39;,SPLITS &#x3D;&gt; [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;]</span><br><span class="line">  * create &#39;logTopic&#39;, &#39;info&#39;,SPLITS &#x3D;&gt; [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;]</span><br><span class="line">  * 保留分区清除数据</span><br><span class="line">  * truncate_preserve &#39;action_logs_test&#39;</span><br><span class="line">  * 修改属性:  7天过期</span><br><span class="line">  * alter &#39;aixueOnline&#39;, NAME &#x3D;&gt;&#39;info&#39;, TTL &#x3D;&gt; 604800</span><br><span class="line">  * alter &#39;accessLogTopic&#39;, NAME &#x3D;&gt;&#39;info&#39;, TTL &#x3D;&gt; 604800</span><br><span class="line">  * alter &#39;logTopic&#39;,  NAME &#x3D;&gt;&#39;info&#39;, TTL &#x3D;&gt; 604800</span><br><span class="line">  * &#x2F;</span><br><span class="line">import java.io.&#123;File, FileInputStream&#125;</span><br><span class="line">import java.util</span><br><span class="line">import java.util.Properties</span><br><span class="line"></span><br><span class="line">import com.etiantian.bigdata.common.HBaseOutputFormat</span><br><span class="line">import org.apache.flink.streaming.api.scala.&#123;DataStream, StreamExecutionEnvironment&#125;</span><br><span class="line">import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer010</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line">import org.json.JSONObject</span><br><span class="line">import org.apache.flink.api.scala._</span><br><span class="line">import org.apache.flink.runtime.state.filesystem.FsStateBackend</span><br><span class="line">import org.apache.flink.streaming.api.CheckpointingMode</span><br><span class="line">import org.apache.flink.streaming.api.environment.CheckpointConfig.ExternalizedCheckpointCleanup</span><br><span class="line"></span><br><span class="line">import scala.util.Random</span><br><span class="line"></span><br><span class="line">val configFile &#x3D; new File(args(0))</span><br><span class="line">val conf &#x3D; new Properties()</span><br><span class="line">conf.load(new FileInputStream(configFile))</span><br><span class="line">val properties &#x3D; new Properties()</span><br><span class="line">val hbaseConfig &#x3D; new Properties()</span><br><span class="line">hbaseConfig.setProperty(&quot;hbase.zookeeper.quorum&quot;, conf.getProperty(&quot;hbase.zookeeper.quorum&quot;))</span><br><span class="line">hbaseConfig.setProperty(&quot;hbase.zookeeper.property.clientPort&quot;, conf.getProperty(&quot;hbase.zookeeper.port&quot;))</span><br><span class="line"></span><br><span class="line">dataStream.filter(oneTopic(topic1,_)).map(writeToHBase(_)).writeUsingOutputFormat(new HBaseOutputFormat(topic1,hbaseConfig))</span><br></pre></td></tr></table></figure>

<h4 id="存储es方式"><a href="#存储es方式" class="headerlink" title="存储es方式"></a>存储es方式</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">可以分为dataStream和flink sql两种方式:</span><br><span class="line">方式一:dataStream.addSink(new ElasticsearchSink进行重写process方法)</span><br><span class="line"></span><br><span class="line">dataStream.addSink(new ElasticsearchSink(conf, addressList, new ElasticsearchSinkFunction[MsgContext] &#123;</span><br><span class="line">      override def process(t: MsgContext, runtimeContext: RuntimeContext, requestIndexer: RequestIndexer) &#x3D; &#123;</span><br><span class="line">        val jsonXContentBuilder &#x3D; JsonXContent.contentBuilder().startObject()</span><br><span class="line">        val message &#x3D; t.getMsg.asInstanceOf[Tuple9[String,String, String, String, String, String, String, String, String]]</span><br><span class="line">        var schoolId &#x3D; message._1</span><br><span class="line">        val cDate &#x3D; message._2</span><br><span class="line">        val schoolName &#x3D; message._3</span><br><span class="line">        val cTime &#x3D; message._4</span><br><span class="line">        val province &#x3D; message._5</span><br><span class="line">        val city &#x3D; message._6</span><br><span class="line">        val actorTye &#x3D; message._7</span><br><span class="line">        val jid &#x3D; message._8</span><br><span class="line">        val district &#x3D; message._9</span><br><span class="line"></span><br><span class="line">        val processTime &#x3D; LocalDateTime.now()</span><br><span class="line">        jsonXContentBuilder.field(&quot;process_time&quot;, processTime)</span><br><span class="line"></span><br><span class="line">        jsonXContentBuilder.field(&quot;c_time&quot;, cTime)</span><br><span class="line">        jsonXContentBuilder.field(&quot;school_name&quot;, schoolName)</span><br><span class="line">        jsonXContentBuilder.field(&quot;province&quot;, province)</span><br><span class="line">        jsonXContentBuilder.field(&quot;city&quot;, city)</span><br><span class="line">        jsonXContentBuilder.field(&quot;actor_type&quot;, actorTye)</span><br><span class="line">        jsonXContentBuilder.field(&quot;school_id&quot;, schoolId)</span><br><span class="line">        jsonXContentBuilder.field(&quot;jid&quot;, jid)</span><br><span class="line">        jsonXContentBuilder.field(&quot;district&quot;, district)</span><br><span class="line">        jsonXContentBuilder.endObject()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;捕获es插入相同id时的异常</span><br><span class="line">        val id &#x3D; jid + &quot;,&quot; + cDate</span><br><span class="line">        val index &#x3D; new UpdateRequest().index(</span><br><span class="line">          &quot;flink_count_action_user&quot;</span><br><span class="line">        ).&#96;type&#96;(</span><br><span class="line">          &quot;flink_count_action_user&quot;</span><br><span class="line">        ).id(id).docAsUpsert(true).doc(jsonXContentBuilder)</span><br><span class="line">&#x2F;&#x2F;          .create(true).id(id).source(jsonXContentBuilder)</span><br><span class="line">        requestIndexer.add(index)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, new ActionRequestFailureHandler &#123;</span><br><span class="line">      @throws(classOf[Throwable])</span><br><span class="line">      override def onFailure(index: ActionRequest, failure: Throwable, i: Int, requestIndexer: RequestIndexer): Unit &#x3D; &#123;</span><br><span class="line">        if (ExceptionUtils.findThrowable(failure, classOf[VersionConflictEngineException]).isPresent) &#123;</span><br><span class="line">          &#x2F;&#x2F;          requestIndexer.add(index)</span><br><span class="line">        &#125;</span><br><span class="line">        else &#123;</span><br><span class="line">          throw failure</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">方式二:使用flink sql直接存储的方式(insert into)</span><br><span class="line">tenv.sqlUpdate(</span><br><span class="line">      s&quot;&quot;&quot;</span><br><span class="line">         |CREATE TABLE test_zsd_01 (</span><br><span class="line">         | question_id BIGINT,</span><br><span class="line">         | bodys STRING,</span><br><span class="line">         | type BIGINT,</span><br><span class="line">         | difficult BIGINT,</span><br><span class="line">         | paper_type_id BIGINT,</span><br><span class="line">         | status BIGINT</span><br><span class="line">         |) WITH (</span><br><span class="line">         | &#39;connector.type&#39; &#x3D; &#39;elasticsearch&#39;,</span><br><span class="line">         | &#39;connector.version&#39; &#x3D; &#39;6&#39;,</span><br><span class="line">         | &#39;connector.hosts&#39; &#x3D; &#39;$esHosts&#39;,</span><br><span class="line">         | &#39;connector.index&#39; &#x3D; &#39;test_zsd&#39;,</span><br><span class="line">         | &#39;connector.document-type&#39; &#x3D; &#39;test_zsd&#39;,</span><br><span class="line">         | &#39;update-mode&#39; &#x3D; &#39;upsert&#39;,</span><br><span class="line">         | &#39;format.type&#39; &#x3D; &#39;json&#39;,</span><br><span class="line">         | &#39;connector.bulk-flush.max-actions&#39;&#x3D;&#39;1&#39;</span><br><span class="line">         |)</span><br><span class="line">         |&quot;&quot;&quot;.stripMargin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    tenv.sqlUpdate(</span><br><span class="line">      &quot;&quot;&quot;</span><br><span class="line">        | INSERT INTO test_zsd_01</span><br><span class="line">        | select</span><br><span class="line">        |question_id ,</span><br><span class="line">        |LAST_VALUE(bodys) bodys,</span><br><span class="line">        |LAST_VALUE(ques_type) ques_type,</span><br><span class="line">        |LAST_VALUE(difficult) difficult,</span><br><span class="line">        |LAST_VALUE(paper_type_id) paper_type_id,</span><br><span class="line">        |LAST_VALUE(status) status from</span><br><span class="line">        |(SELECT CAST(question_id as BIGINT) question_id,</span><br><span class="line">        |bodys,</span><br><span class="line">        |CAST(ques_type as BIGINT) ques_type,</span><br><span class="line">        |CAST(difficult as BIGINT) difficult,</span><br><span class="line">        |CAST(paper_type_id as BIGINT) paper_type_id,</span><br><span class="line">        |CAST(status as BIGINT) status from result_table) aa group by question_id</span><br><span class="line">      &quot;&quot;&quot;.stripMargin)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之一StreamGraph生成</title>
    <url>/2020/05/09/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E4%B8%80StreamGraph%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<blockquote>
<p>在之前的文章中介绍过FlinkSQL底层是怎么实现的，以及Flink的一些概念性知识。一直想整理Flink执行层面的源码，但是因为过于庞大，不知道如何下手，只能一步一步来。</p>
</blockquote>
<span id="more"></span>

<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StreamGraph</span><br><span class="line">    JobGraph</span><br><span class="line">        ExecutionGraph</span><br><span class="line">            物理执行图</span><br><span class="line"></span><br><span class="line">Executor</span><br><span class="line">    ExecutorBase</span><br><span class="line">        BatchExecutor</span><br><span class="line">        StreamExecutor</span><br><span class="line">    StreamExecutor</span><br><span class="line"></span><br><span class="line">Planner</span><br><span class="line">    PlannerBase</span><br><span class="line">        BatchPlanner</span><br><span class="line">        StreamPlanner</span><br><span class="line">    StreamPlan</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="任务的创建流程"><a href="#任务的创建流程" class="headerlink" title="任务的创建流程"></a>任务的创建流程</h2><h3 id="入口实例"><a href="#入口实例" class="headerlink" title="入口实例"></a>入口实例</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这里我们还是以FlinkSQL举例</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"><span class="keyword">val</span> settings = <span class="type">EnvironmentSettings</span>.newInstance().useBlinkPlanner().inStreamingMode().build</span><br><span class="line"><span class="keyword">val</span> tEnv = <span class="type">StreamTableEnvironment</span>.create(env, settings)</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成JobExecutionResult</span></span><br><span class="line">tEnv.execute(<span class="string">&quot;job name&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="结构流程"><a href="#结构流程" class="headerlink" title="结构流程"></a>结构流程</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">TableEnvironment</span>接口</span><br><span class="line"><span class="type">StreamTableEnvironment</span>接口</span><br><span class="line"><span class="type">StreamTableEnvironmentImpl</span>类</span><br><span class="line"><span class="type">StreamTableEnvironmentImpl</span>又继承自<span class="type">TableEnvironmentImpl</span>类</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以看到StreamTableEnvironment实例真正创建是在StreamTableEnvironmentImpl中实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(executionEnvironment: <span class="type">StreamExecutionEnvironment</span>): <span class="type">StreamTableEnvironment</span> = &#123;</span><br><span class="line">    create(</span><br><span class="line">      executionEnvironment,</span><br><span class="line">      <span class="type">EnvironmentSettings</span>.newInstance().build())</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(</span><br><span class="line">      executionEnvironment: <span class="type">StreamExecutionEnvironment</span>,</span><br><span class="line">      settings: <span class="type">EnvironmentSettings</span>)</span><br><span class="line">    : <span class="type">StreamTableEnvironment</span> = &#123;</span><br><span class="line">    <span class="type">StreamTableEnvironmentImpl</span>.create(executionEnvironment, settings, <span class="keyword">new</span> <span class="type">TableConfig</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// StreamTableEnvironmentImpl</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(</span><br><span class="line">      executionEnvironment: <span class="type">StreamExecutionEnvironment</span>,</span><br><span class="line">      settings: <span class="type">EnvironmentSettings</span>,</span><br><span class="line">      tableConfig: <span class="type">TableConfig</span>)</span><br><span class="line">    : <span class="type">StreamTableEnvironmentImpl</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!settings.isStreamingMode) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TableException</span>(</span><br><span class="line">        <span class="string">&quot;StreamTableEnvironment can not run in batch mode for now, please use TableEnvironment.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// temporary solution until FLINK-15635 is fixed</span></span><br><span class="line">    <span class="keyword">val</span> classLoader = <span class="type">Thread</span>.currentThread.getContextClassLoader</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Module管理器</span></span><br><span class="line">    <span class="keyword">val</span> moduleManager = <span class="keyword">new</span> <span class="type">ModuleManager</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Catalog管理器</span></span><br><span class="line">    <span class="keyword">val</span> catalogManager = <span class="type">CatalogManager</span>.newBuilder</span><br><span class="line">      .classLoader(classLoader)</span><br><span class="line">      .config(tableConfig.getConfiguration)</span><br><span class="line">      .defaultCatalog(</span><br><span class="line">        settings.getBuiltInCatalogName,</span><br><span class="line">        <span class="keyword">new</span> <span class="type">GenericInMemoryCatalog</span>(</span><br><span class="line">          settings.getBuiltInCatalogName,</span><br><span class="line">          settings.getBuiltInDatabaseName))</span><br><span class="line">      .executionConfig(executionEnvironment.getConfig)</span><br><span class="line">      .build</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 内置函数</span></span><br><span class="line">    <span class="keyword">val</span> functionCatalog = <span class="keyword">new</span> <span class="type">FunctionCatalog</span>(tableConfig, catalogManager, moduleManager)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Executor配置</span></span><br><span class="line">    <span class="keyword">val</span> executorProperties = settings.toExecutorProperties</span><br><span class="line">    <span class="keyword">val</span> executor = lookupExecutor(executorProperties, executionEnvironment)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Planner配置</span></span><br><span class="line">    <span class="keyword">val</span> plannerProperties = settings.toPlannerProperties</span><br><span class="line">    <span class="keyword">val</span> planner = <span class="type">ComponentFactoryService</span>.find(classOf[<span class="type">PlannerFactory</span>], plannerProperties)</span><br><span class="line">      .create(</span><br><span class="line">        plannerProperties,</span><br><span class="line">        executor,</span><br><span class="line">        tableConfig,</span><br><span class="line">        functionCatalog,</span><br><span class="line">        catalogManager)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">StreamTableEnvironmentImpl</span>(</span><br><span class="line">      catalogManager,</span><br><span class="line">      moduleManager,</span><br><span class="line">      functionCatalog,</span><br><span class="line">      tableConfig,</span><br><span class="line">      executionEnvironment,</span><br><span class="line">      planner,</span><br><span class="line">      executor,</span><br><span class="line">      settings.isStreamingMode</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="execute"><a href="#execute" class="headerlink" title="execute"></a>execute</h3><blockquote>
<p>使用Java代码查看了,Scala源码包没下载下来</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>将Operation转换为Transformation</span><br><span class="line"><span class="number">2.</span>将生成对应jobName的StreamGraph</span><br><span class="line"></span><br><span class="line"><span class="comment">// TableEnvironmentImpl</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(String jobName)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 将Operation转换为Transformation,并添加到StreamExecutionEnvironment,Operation怎么来的可以参考之前的文章</span></span><br><span class="line">	translate(bufferedModifyOperations);</span><br><span class="line">	bufferedModifyOperations.clear();</span><br><span class="line">    <span class="comment">// 调用StreamExecutionEnvironment.execute</span></span><br><span class="line">	<span class="keyword">return</span> execEnv.execute(jobName);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 转换</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">translate</span><span class="params">(List&lt;ModifyOperation&gt; modifyOperations)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 怎么转换的之前文章有说明,这里其实就已经将SQL转为DataStream了</span></span><br><span class="line">	List&lt;Transformation&lt;?&gt;&gt; transformations = planner.translate(modifyOperations);</span><br><span class="line">    <span class="comment">// 添加Transformation</span></span><br><span class="line">	execEnv.apply(transformations);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 添加进StreamExecutionEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(List&lt;Transformation&lt;?&gt;&gt; transformations)</span> </span>&#123;</span><br><span class="line">	transformations.forEach(executionEnvironment::addOperator);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// StreamExecutionEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addOperator</span><span class="params">(Transformation&lt;?&gt; transformation)</span> </span>&#123;</span><br><span class="line">	Preconditions.checkNotNull(transformation, <span class="string">&quot;transformation must not be null.&quot;</span>);</span><br><span class="line">	<span class="keyword">this</span>.transformations.add(transformation);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// StreamExecutionEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(String jobName)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	Preconditions.checkNotNull(jobName, <span class="string">&quot;Streaming Job name should not be null.&quot;</span>);</span><br><span class="line">    <span class="comment">// 到这里应该可以很清晰了,这此处生成了StreamGraph</span></span><br><span class="line">    <span class="comment">// 然后通过调用execute生成JobExecutionResult</span></span><br><span class="line">	<span class="keyword">return</span> execute(getStreamGraph(jobName));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">getStreamGraph</span><span class="params">(String jobName)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> getStreamGraph(jobName, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">getStreamGraph</span><span class="params">(String jobName, <span class="keyword">boolean</span> clearTransformations)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 生成StreamGraph</span></span><br><span class="line">	StreamGraph streamGraph = getStreamGraphGenerator().setJobName(jobName).generate();</span><br><span class="line">	<span class="keyword">if</span> (clearTransformations) &#123;</span><br><span class="line">		<span class="keyword">this</span>.transformations.clear();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> streamGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="getStreamGraphGenerator"><a href="#getStreamGraphGenerator" class="headerlink" title="getStreamGraphGenerator"></a>getStreamGraphGenerator</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将Transformation传给StreamGraph构造器</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> StreamGraphGenerator <span class="title">getStreamGraphGenerator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (transformations.size() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;No operators defined in streaming topology. Cannot execute.&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">new</span> StreamGraphGenerator(transformations, config, checkpointCfg)</span><br><span class="line">		.setStateBackend(defaultStateBackend)</span><br><span class="line">		.setChaining(isChainingEnabled)</span><br><span class="line">		.setUserArtifacts(cacheFile)</span><br><span class="line">		.setTimeCharacteristic(timeCharacteristic)</span><br><span class="line">		.setDefaultBufferTimeout(bufferTimeout);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="generate"><a href="#generate" class="headerlink" title="generate"></a>generate</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> StreamGraph <span class="title">generate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    streamGraph = <span class="keyword">new</span> StreamGraph(executionConfig, checkpointConfig, savepointRestoreSettings);</span><br><span class="line">    streamGraph.setStateBackend(stateBackend);</span><br><span class="line">    streamGraph.setChaining(chaining);</span><br><span class="line">    streamGraph.setScheduleMode(scheduleMode);</span><br><span class="line">    streamGraph.setUserArtifacts(userArtifacts);</span><br><span class="line">    streamGraph.setTimeCharacteristic(timeCharacteristic);</span><br><span class="line">    streamGraph.setJobName(jobName);</span><br><span class="line">    streamGraph.setBlockingConnectionsBetweenChains(blockingConnectionsBetweenChains);</span><br><span class="line"></span><br><span class="line">    alreadyTransformed = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Transformation&lt;?&gt; transformation: transformations) &#123;</span><br><span class="line">        <span class="comment">// transformation设置进StreamGraph,对转换树每个Transformation进行转换</span></span><br><span class="line">        transform(transformation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> StreamGraph builtStreamGraph = streamGraph;</span><br><span class="line"></span><br><span class="line">    alreadyTransformed.clear();</span><br><span class="line">    alreadyTransformed = <span class="keyword">null</span>;</span><br><span class="line">    streamGraph = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> builtStreamGraph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对具体的一个Transformation进行转换</span></span><br><span class="line"><span class="comment">// 生成StreamGraph中的StreamNode和StreamEdge</span></span><br><span class="line"><span class="comment">// 返回值为Transformation的id集合</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">transform</span><span class="params">(Transformation&lt;?&gt; transform)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 跳过已经转换过的Transformation</span></span><br><span class="line">    <span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">        <span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    LOG.debug(<span class="string">&quot;Transforming &quot;</span> + transform);</span><br><span class="line">    <span class="comment">// 设置最大并行度</span></span><br><span class="line">    <span class="keyword">if</span> (transform.getMaxParallelism() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// if the max parallelism hasn&#x27;t been set, then first use the job wide max parallelism</span></span><br><span class="line">        <span class="comment">// from the ExecutionConfig.</span></span><br><span class="line">        <span class="keyword">int</span> globalMaxParallelismFromConfig = executionConfig.getMaxParallelism();</span><br><span class="line">        <span class="keyword">if</span> (globalMaxParallelismFromConfig &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            transform.setMaxParallelism(globalMaxParallelismFromConfig);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// call at least once to trigger exceptions about MissingTypeInfo</span></span><br><span class="line">    <span class="comment">// 为了触发MissingTypeInfo异常,字段类型</span></span><br><span class="line">    transform.getOutputType();</span><br><span class="line"></span><br><span class="line">    Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">    <span class="comment">// 获取transformedIds,并且根据Transformation的类型设置节点</span></span><br><span class="line">    <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation&lt;?, ?&gt;) &#123;</span><br><span class="line">        transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</span><br><span class="line">        transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SideOutputTransformation&lt;?&gt;) &#123;</span><br><span class="line">        transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Unknown transformation: &quot;</span> + transform);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// need this check because the iterate transformation adds itself before</span></span><br><span class="line">    <span class="comment">// transforming the feedback edges</span></span><br><span class="line">    <span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">        alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (transform.getBufferTimeout() &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        streamGraph.setBufferTimeout(transform.getId(), defaultBufferTimeout);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        streamGraph.setTransformationUID(transform.getId(), transform.getUid());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (transform.getUserProvidedNodeHash() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!streamGraph.getExecutionConfig().hasAutoGeneratedUIDsEnabled()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PhysicalTransformation &amp;&amp;</span><br><span class="line">                transform.getUserProvidedNodeHash() == <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">                transform.getUid() == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">&quot;Auto generated UIDs have been disabled &quot;</span> +</span><br><span class="line">                <span class="string">&quot;but no UID or hash has been assigned to operator &quot;</span> + transform.getName());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (transform.getMinResources() != <span class="keyword">null</span> &amp;&amp; transform.getPreferredResources() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    streamGraph.setManagedMemoryWeight(transform.getId(), transform.getManagedMemoryWeight());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> transformedIds;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Transformation"><a href="#Transformation" class="headerlink" title="Transformation"></a>Transformation</h2><h3 id="种类"><a href="#种类" class="headerlink" title="种类"></a>种类</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">是对Operation的转换,通过之前的FlinkSQL源码分析</span><br><span class="line">可以知道Operator其实就是SQL解析得来的</span><br><span class="line">Transformation</span><br><span class="line">    PhysicalTransformation</span><br><span class="line">        OneInputTransformation</span><br><span class="line">        SinkTransformation</span><br><span class="line">        SourceTransformation</span><br><span class="line">        TwoInputTransformation</span><br><span class="line">    CoFeedbackTransformation</span><br><span class="line">    FeedbackTransformation</span><br><span class="line">    PartitionTransformation</span><br><span class="line">    SelectTransformation</span><br><span class="line">    SideOutputTransformation</span><br><span class="line">    SplitTransformation</span><br><span class="line">    UnionTransformation</span><br></pre></td></tr></table></figure>

<h2 id="StreamGraph"><a href="#StreamGraph" class="headerlink" title="StreamGraph"></a>StreamGraph</h2><h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamNode(节点)</span><br><span class="line">StreamEdge(节点与节点的联系)</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Map&lt;Integer, StreamNode&gt; streamNodes; <span class="comment">// 对应OneInputTransform,TwoInputTransform</span></span><br><span class="line"><span class="keyword">private</span> Set&lt;Integer&gt; sources; <span class="comment">// 对应SourceTransformation</span></span><br><span class="line"><span class="keyword">private</span> Set&lt;Integer&gt; sinks; <span class="comment">// 对应SinkTransformation</span></span><br><span class="line"><span class="keyword">private</span> Map&lt;Integer, Tuple2&lt;Integer, List&lt;String&gt;&gt;&gt; virtualSelectNodes; <span class="comment">// 对应SplitTransformation,SelectTransformation,FeedbackTransformation,CoFeedbackTransformation</span></span><br><span class="line"><span class="keyword">private</span> Map&lt;Integer, Tuple2&lt;Integer, OutputTag&gt;&gt; virtualSideOutputNodes; <span class="comment">// 对应FeedbackTransformation,CoFeedbackTransformation,SideOutputTransformation</span></span><br><span class="line"><span class="keyword">private</span> Map&lt;Integer, Tuple3&lt;Integer, StreamPartitioner&lt;?&gt;, ShuffleMode&gt;&gt; virtualPartitionNodes; <span class="comment">// 对应FeedbackTransformation,CoFeedbackTransformation,PartitionTransformation</span></span><br><span class="line"><span class="keyword">protected</span> Map&lt;Integer, String&gt; vertexIDtoBrokerID;</span><br><span class="line"><span class="keyword">protected</span> Map&lt;Integer, Long&gt; vertexIDtoLoopTimeout;</span><br><span class="line"><span class="keyword">private</span> StateBackend stateBackend;</span><br><span class="line"><span class="keyword">private</span> Set&lt;Tuple2&lt;StreamNode, StreamNode&gt;&gt; iterationSourceSinkPairs;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Transformation如何装载进StreamGraph的"><a href="#Transformation如何装载进StreamGraph的" class="headerlink" title="Transformation如何装载进StreamGraph的"></a>Transformation如何装载进StreamGraph的</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">在StreamGraphGenerator的generate方法中,对所有的Transformation进行转换</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以SourceTransformation为例</span></span><br><span class="line"><span class="keyword">private</span> &lt;T&gt; <span class="function">Collection&lt;Integer&gt; <span class="title">transformSource</span><span class="params">(SourceTransformation&lt;T&gt; source)</span> </span>&#123;</span><br><span class="line">    String slotSharingGroup = determineSlotSharingGroup(source.getSlotSharingGroup(), Collections.emptyList());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// streamGraph添加Source节点</span></span><br><span class="line">    streamGraph.addSource(source.getId(),</span><br><span class="line">            slotSharingGroup,</span><br><span class="line">            source.getCoLocationGroupKey(),</span><br><span class="line">            source.getOperatorFactory(),</span><br><span class="line">            <span class="keyword">null</span>,</span><br><span class="line">            source.getOutputType(),</span><br><span class="line">            <span class="string">&quot;Source: &quot;</span> + source.getName());</span><br><span class="line">    <span class="keyword">if</span> (source.getOperatorFactory() <span class="keyword">instanceof</span> InputFormatOperatorFactory) &#123;</span><br><span class="line">        <span class="comment">// 设置InputFormat</span></span><br><span class="line">        streamGraph.setInputFormat(source.getId(),</span><br><span class="line">                ((InputFormatOperatorFactory&lt;T&gt;) source.getOperatorFactory()).getInputFormat());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> parallelism = source.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT ?</span><br><span class="line">        source.getParallelism() : executionConfig.getParallelism();</span><br><span class="line">    streamGraph.setParallelism(source.getId(), parallelism);</span><br><span class="line">    streamGraph.setMaxParallelism(source.getId(), source.getMaxParallelism());</span><br><span class="line">    <span class="keyword">return</span> Collections.singleton(source.getId());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// StreamGraph</span></span><br><span class="line"><span class="keyword">public</span> &lt;IN, OUT&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSource</span><span class="params">(Integer vertexID,</span></span></span><br><span class="line"><span class="function"><span class="params">	<span class="meta">@Nullable</span> String slotSharingGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">	<span class="meta">@Nullable</span> String coLocationGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">	StreamOperatorFactory&lt;OUT&gt; operatorFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">	TypeInformation&lt;IN&gt; inTypeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">	TypeInformation&lt;OUT&gt; outTypeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">	String operatorName)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 创建StreamNode,并设置序列化</span></span><br><span class="line">	addOperator(vertexID, slotSharingGroup, coLocationGroup, operatorFactory, inTypeInfo, outTypeInfo, operatorName);</span><br><span class="line">    <span class="comment">// 添加节点ID</span></span><br><span class="line">	sources.add(vertexID);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> &lt;IN, OUT&gt; <span class="function"><span class="keyword">void</span> <span class="title">addOperator</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			Integer vertexID,</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="meta">@Nullable</span> String slotSharingGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="meta">@Nullable</span> String coLocationGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">			StreamOperatorFactory&lt;OUT&gt; operatorFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">			TypeInformation&lt;IN&gt; inTypeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">			TypeInformation&lt;OUT&gt; outTypeInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">			String operatorName)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 添加StreamNode</span></span><br><span class="line">    <span class="keyword">if</span> (operatorFactory.isStreamSource()) &#123;</span><br><span class="line">        addNode(vertexID, slotSharingGroup, coLocationGroup, SourceStreamTask.class, operatorFactory, operatorName);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        addNode(vertexID, slotSharingGroup, coLocationGroup, OneInputStreamTask.class, operatorFactory, operatorName);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 序列化</span></span><br><span class="line">    TypeSerializer&lt;IN&gt; inSerializer = inTypeInfo != <span class="keyword">null</span> &amp;&amp; !(inTypeInfo <span class="keyword">instanceof</span> MissingTypeInfo) ? inTypeInfo.createSerializer(executionConfig) : <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    TypeSerializer&lt;OUT&gt; outSerializer = outTypeInfo != <span class="keyword">null</span> &amp;&amp; !(outTypeInfo <span class="keyword">instanceof</span> MissingTypeInfo) ? outTypeInfo.createSerializer(executionConfig) : <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    setSerializers(vertexID, inSerializer, <span class="keyword">null</span>, outSerializer);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (operatorFactory.isOutputTypeConfigurable() &amp;&amp; outTypeInfo != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// sets the output type which must be know at StreamGraph creation time</span></span><br><span class="line">        operatorFactory.setOutputType(outTypeInfo, executionConfig);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (operatorFactory.isInputTypeConfigurable()) &#123;</span><br><span class="line">        operatorFactory.setInputType(inTypeInfo, executionConfig);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        LOG.debug(<span class="string">&quot;Vertex: &#123;&#125;&quot;</span>, vertexID);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 添加节点</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> StreamNode <span class="title">addNode</span><span class="params">(Integer vertexID,</span></span></span><br><span class="line"><span class="function"><span class="params">	<span class="meta">@Nullable</span> String slotSharingGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">	<span class="meta">@Nullable</span> String coLocationGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">	Class&lt;? extends AbstractInvokable&gt; vertexClass,</span></span></span><br><span class="line"><span class="function"><span class="params">	StreamOperatorFactory&lt;?&gt; operatorFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">	String operatorName)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (streamNodes.containsKey(vertexID)) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">&quot;Duplicate vertexID &quot;</span> + vertexID);</span><br><span class="line">	&#125;</span><br><span class="line">	StreamNode vertex = <span class="keyword">new</span> StreamNode(</span><br><span class="line">		vertexID,</span><br><span class="line">		slotSharingGroup,</span><br><span class="line">		coLocationGroup,</span><br><span class="line">		operatorFactory,</span><br><span class="line">		operatorName,</span><br><span class="line">		<span class="keyword">new</span> ArrayList&lt;OutputSelector&lt;?&gt;&gt;(),</span><br><span class="line">		vertexClass);</span><br><span class="line">    <span class="comment">// 装载入streamNodes中</span></span><br><span class="line">	streamNodes.put(vertexID, vertex);</span><br><span class="line">	<span class="keyword">return</span> vertex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之二JobGraph生成</title>
    <url>/2020/05/11/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E4%BA%8CJobGraph%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<blockquote>
<p>接着上一篇StreamGraph已经被生成出来了,根据Flink的四层图结构,接下来就是JobGraph的生成</p>
</blockquote>
<span id="more"></span>

<p>StreamGraph和JobGraph都是在Client端生成的</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># PipelineExecutor类</span><br><span class="line">PipelineExecutor</span><br><span class="line">    AbstractJobClusterExecutor</span><br><span class="line">    AbstractSessionClusterExecutor</span><br><span class="line">        RemoteExecutor</span><br><span class="line">    LocalExecutor</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="流程概览"><a href="#流程概览" class="headerlink" title="流程概览"></a>流程概览</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StreamExecutionEnvironment.execute(JobName)-&gt;execute(StreamGraph)-&gt;executeAsync(StreamGraph)</span><br><span class="line">    PipelineExecutor.execute()[有多种情况,这里选RemoteExecutor]</span><br><span class="line">        ExecutorUtils.getJobGraph()-&gt;FlinkPipelineTranslationUtil.getJobGraph()</span><br><span class="line">            -&gt;FlinkPipelineTranslator.translateToJobGraph()[选StreamGraphTranslator]</span><br><span class="line">                -&gt;StreamGraph.getJobGraph()</span><br><span class="line">                    -&gt;StreamingJobGraphGenerator.createJobGraph()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="流程解析"><a href="#流程解析" class="headerlink" title="流程解析"></a>流程解析</h2><h3 id="StreamExecutionEnvironment入口"><a href="#StreamExecutionEnvironment入口" class="headerlink" title="StreamExecutionEnvironment入口"></a>StreamExecutionEnvironment入口</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// StreamExecutionEnvironment</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(String jobName)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	Preconditions.checkNotNull(jobName, <span class="string">&quot;Streaming Job name should not be null.&quot;</span>);</span><br><span class="line">    <span class="comment">// 拿到StreamGraph</span></span><br><span class="line">	<span class="keyword">return</span> execute(getStreamGraph(jobName));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 触发程序执行,执行导致Sink操作的所有部分,打印结果或将结果转发到消息队列</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobExecutionResult <span class="title">execute</span><span class="params">(StreamGraph streamGraph)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 获取JobClient</span></span><br><span class="line">	<span class="keyword">final</span> JobClient jobClient = executeAsync(streamGraph);</span><br><span class="line">    <span class="comment">// 这一块以后再深入了解</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">final</span> JobExecutionResult jobExecutionResult;</span><br><span class="line">		<span class="keyword">if</span> (configuration.getBoolean(DeploymentOptions.ATTACHED)) &#123;</span><br><span class="line">			jobExecutionResult = jobClient.getJobExecutionResult(userClassloader).get();</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			jobExecutionResult = <span class="keyword">new</span> DetachedJobExecutionResult(jobClient.getJobID());</span><br><span class="line">		&#125;</span><br><span class="line">		jobListeners.forEach(jobListener -&gt; jobListener.onJobExecuted(jobExecutionResult, <span class="keyword">null</span>));</span><br><span class="line">		<span class="keyword">return</span> jobExecutionResult;</span><br><span class="line">	&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">		jobListeners.forEach(jobListener -&gt; &#123;</span><br><span class="line">			jobListener.onJobExecuted(<span class="keyword">null</span>, ExceptionUtils.stripExecutionException(t));</span><br><span class="line">		&#125;);</span><br><span class="line">		ExceptionUtils.rethrowException(t);</span><br><span class="line">		<span class="comment">// never reached, only make javac happy</span></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个PipelineExecutor工厂类,选择并实例化适当的PipelineExecutor</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobClient <span class="title">executeAsync</span><span class="params">(StreamGraph streamGraph)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	checkNotNull(streamGraph, <span class="string">&quot;StreamGraph cannot be null.&quot;</span>);</span><br><span class="line">	checkNotNull(configuration.get(DeploymentOptions.TARGET), <span class="string">&quot;No execution.target specified in your configuration file.&quot;</span>);</span><br><span class="line">        <span class="comment">// 工厂类</span></span><br><span class="line">	<span class="keyword">final</span> PipelineExecutorFactory executorFactory =</span><br><span class="line">		executorServiceLoader.getExecutorFactory(configuration);</span><br><span class="line">	checkNotNull(</span><br><span class="line">		executorFactory,</span><br><span class="line">		<span class="string">&quot;Cannot find compatible factory for specified execution.target (=%s)&quot;</span>,</span><br><span class="line">		configuration.get(DeploymentOptions.TARGET));</span><br><span class="line">        <span class="comment">// 内部实现JobGraph生成逻辑</span></span><br><span class="line">	CompletableFuture&lt;JobClient&gt; jobClientFuture = executorFactory</span><br><span class="line">		.getExecutor(configuration)</span><br><span class="line">		.execute(streamGraph, configuration);</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		JobClient jobClient = jobClientFuture.get();</span><br><span class="line">		jobListeners.forEach(jobListener -&gt; jobListener.onJobSubmitted(jobClient, <span class="keyword">null</span>));</span><br><span class="line">		<span class="keyword">return</span> jobClient;</span><br><span class="line">	&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">		jobListeners.forEach(jobListener -&gt; jobListener.onJobSubmitted(<span class="keyword">null</span>, t));</span><br><span class="line">		ExceptionUtils.rethrow(t);</span><br><span class="line">		<span class="comment">// make javac happy, this code path will not be reached</span></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="AbstractSessionClusterExecutor执行器"><a href="#AbstractSessionClusterExecutor执行器" class="headerlink" title="AbstractSessionClusterExecutor执行器"></a>AbstractSessionClusterExecutor执行器</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 远程执行</span></span><br><span class="line"><span class="comment">// RemoteExecutor的父类,execute方法在这里被实现</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;JobClient&gt; <span class="title">execute</span><span class="params">(<span class="meta">@Nonnull</span> <span class="keyword">final</span> Pipeline pipeline, <span class="meta">@Nonnull</span> <span class="keyword">final</span> Configuration configuration)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// 获取JobGraph</span></span><br><span class="line">	<span class="keyword">final</span> JobGraph jobGraph = ExecutorUtils.getJobGraph(pipeline, configuration);</span><br><span class="line">	<span class="keyword">try</span> (<span class="keyword">final</span> ClusterDescriptor&lt;ClusterID&gt; clusterDescriptor = clusterClientFactory.createClusterDescriptor(configuration)) &#123;</span><br><span class="line">		<span class="keyword">final</span> ClusterID clusterID = clusterClientFactory.getClusterId(configuration);</span><br><span class="line">		checkState(clusterID != <span class="keyword">null</span>);</span><br><span class="line">		<span class="keyword">final</span> ClusterClientProvider&lt;ClusterID&gt; clusterClientProvider = clusterDescriptor.retrieve(clusterID);</span><br><span class="line">		ClusterClient&lt;ClusterID&gt; clusterClient = clusterClientProvider.getClusterClient();</span><br><span class="line">                <span class="comment">// 提交JobGraph,Client是如何将生成好的JobGraph提交给远程执行,就在这里实现了</span></span><br><span class="line">		<span class="keyword">return</span> clusterClient</span><br><span class="line">				.submitJob(jobGraph)</span><br><span class="line">				.thenApplyAsync(jobID -&gt; (JobClient) <span class="keyword">new</span> ClusterClientJobClientAdapter&lt;&gt;(</span><br><span class="line">						clusterClientProvider,</span><br><span class="line">						jobID))</span><br><span class="line">				.whenComplete((ignored1, ignored2) -&gt; clusterClient.close());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="ExecutorUtils"><a href="#ExecutorUtils" class="headerlink" title="ExecutorUtils"></a>ExecutorUtils</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> JobGraph <span class="title">getJobGraph</span><span class="params">(<span class="meta">@Nonnull</span> <span class="keyword">final</span> Pipeline pipeline, <span class="meta">@Nonnull</span> <span class="keyword">final</span> Configuration configuration)</span> </span>&#123;</span><br><span class="line">	checkNotNull(pipeline);</span><br><span class="line">	checkNotNull(configuration);</span><br><span class="line">        <span class="comment">// Configuration公开的配置访问器</span></span><br><span class="line">	<span class="keyword">final</span> ExecutionConfigAccessor executionConfigAccessor = ExecutionConfigAccessor.fromConfiguration(configuration);</span><br><span class="line">        <span class="comment">// 获取JobGraph,并行度传递过去了</span></span><br><span class="line">	<span class="keyword">final</span> JobGraph jobGraph = FlinkPipelineTranslationUtil</span><br><span class="line">			.getJobGraph(pipeline, configuration, executionConfigAccessor.getParallelism());</span><br><span class="line">        <span class="comment">// Jar,ClassPath,检查点设置</span></span><br><span class="line">	jobGraph.addJars(executionConfigAccessor.getJars());</span><br><span class="line">	jobGraph.setClasspaths(executionConfigAccessor.getClasspaths());</span><br><span class="line">	jobGraph.setSavepointRestoreSettings(executionConfigAccessor.getSavepointRestoreSettings());</span><br><span class="line">	<span class="keyword">return</span> jobGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="FlinkPipelineTranslationUtil"><a href="#FlinkPipelineTranslationUtil" class="headerlink" title="FlinkPipelineTranslationUtil"></a>FlinkPipelineTranslationUtil</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> JobGraph <span class="title">getJobGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		Pipeline pipeline,</span></span></span><br><span class="line"><span class="function"><span class="params">		Configuration optimizerConfiguration,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">int</span> defaultParallelism)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 将Pipeline转换为JobGraph,Flink支持不同PipelineAPI实现(这里使用StreamGraphTranslator)</span></span><br><span class="line">	FlinkPipelineTranslator pipelineTranslator = getPipelineTranslator(pipeline);</span><br><span class="line">	<span class="keyword">return</span> pipelineTranslator.translateToJobGraph(pipeline,</span><br><span class="line">			optimizerConfiguration,</span><br><span class="line">			defaultParallelism);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="StreamGraphTranslator"><a href="#StreamGraphTranslator" class="headerlink" title="StreamGraphTranslator"></a>StreamGraphTranslator</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> JobGraph <span class="title">translateToJobGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		Pipeline pipeline,</span></span></span><br><span class="line"><span class="function"><span class="params">		Configuration optimizerConfiguration,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">int</span> defaultParallelism)</span> </span>&#123;</span><br><span class="line">        checkArgument(pipeline <span class="keyword">instanceof</span> StreamGraph,</span><br><span class="line">			<span class="string">&quot;Given pipeline is not a DataStream StreamGraph.&quot;</span>);</span><br><span class="line">        StreamGraph streamGraph = (StreamGraph) pipeline;</span><br><span class="line">        <span class="comment">// 直接调用StreamGraph.getJobGraph就可以获取JobGraph</span></span><br><span class="line">        <span class="keyword">return</span> streamGraph.getJobGraph(<span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="StreamingJobGraphGenerator入口"><a href="#StreamingJobGraphGenerator入口" class="headerlink" title="StreamingJobGraphGenerator入口"></a>StreamingJobGraphGenerator入口</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">StreamingJobGraphGenerator的成员变量都是为了辅助生成最终的JobGraph</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobGraph <span class="title">getJobGraph</span><span class="params">(<span class="meta">@Nullable</span> JobID jobID)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 可以看见JobGraph的入口函数是StreamingJobGraphGenerator.createJobGraph()</span></span><br><span class="line">    <span class="keyword">return</span> StreamingJobGraphGenerator.createJobGraph(<span class="keyword">this</span>, jobID);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// StreamingJobGraphGenerator</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> JobGraph <span class="title">createJobGraph</span><span class="params">(StreamGraph streamGraph, <span class="meta">@Nullable</span> JobID jobID)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 初始化StreamingJobGraphGenerator,再调用createJobGraph</span></span><br><span class="line">	<span class="keyword">return</span> <span class="keyword">new</span> StreamingJobGraphGenerator(streamGraph, jobID).createJobGraph();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里介绍下StreamingJobGraphGenerator中有什么成员</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> StreamGraph streamGraph; <span class="comment">// StreamGraph,输入</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, JobVertex&gt; jobVertices; <span class="comment">// id-&gt;JobVertex </span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> JobGraph jobGraph; <span class="comment">// JobGraph,输出</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Collection&lt;Integer&gt; builtVertices; <span class="comment">// 已经构建的JobVertex集合</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> List&lt;StreamEdge&gt; physicalEdgesInOrder; <span class="comment">// 保存StreamEdge的集合,在connect/setPhysicalEdges方法中使用</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, Map&lt;Integer, StreamConfig&gt;&gt; chainedConfigs; <span class="comment">// 保存chain信息,部署时用来构建OperatorChain,startNodeId-&gt;(currentNodeId-&gt;StreamConfig)</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, StreamConfig&gt; vertexConfigs; <span class="comment">// 所有节点的配置信息,id-&gt;StreamConfig</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, String&gt; chainedNames; <span class="comment">// 保存每个节点的名字</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, ResourceSpec&gt; chainedMinResources;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, ResourceSpec&gt; chainedPreferredResources;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, InputOutputFormatContainer&gt; chainedInputOutputFormats;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> StreamGraphHasher defaultStreamGraphHasher;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> List&lt;StreamGraphHasher&gt; legacyStreamGraphHashers;</span><br></pre></td></tr></table></figure>
<h3 id="createJobGraph"><a href="#createJobGraph" class="headerlink" title="createJobGraph"></a>createJobGraph</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">首先为所有节点生成一个唯一的HashID,如果节点在多次提交没有改变(并行度,上下游)</span><br><span class="line">那么该ID也不会改变,主要用于故障恢复</span><br><span class="line">StreamNode.id是一个从<span class="number">1</span>开始的静态计数变量,同样的Job可能会得到不一样的id</span><br><span class="line">接着就是chaining处理,生成JobVertex,JobEdge</span><br><span class="line">写入各种配置信息</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> JobGraph <span class="title">createJobGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	preValidate();</span><br><span class="line">	<span class="comment">// make sure that all vertices start immediately</span></span><br><span class="line">        <span class="comment">// 设置调度模式,streaming模式下,所有节点一起启动</span></span><br><span class="line">	jobGraph.setScheduleMode(streamGraph.getScheduleMode());</span><br><span class="line">	<span class="comment">// Generate deterministic hashes for the nodes in order to identify them across</span></span><br><span class="line">	<span class="comment">// submission iff they didn&#x27;t change.</span></span><br><span class="line">        <span class="comment">// 广度优先遍历StreamGraph并且为每个StreamNode生成hashId</span></span><br><span class="line">        <span class="comment">// 保证如果提交的拓扑没有改变,则每次生成的hash一样</span></span><br><span class="line">	Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes = defaultStreamGraphHasher.traverseStreamGraphAndGenerateHashes(streamGraph);</span><br><span class="line">	<span class="comment">// Generate legacy version hashes for backwards compatibility</span></span><br><span class="line">	List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes = <span class="keyword">new</span> ArrayList&lt;&gt;(legacyStreamGraphHashers.size());</span><br><span class="line">	<span class="keyword">for</span> (StreamGraphHasher hasher : legacyStreamGraphHashers) &#123;</span><br><span class="line">		legacyHashes.add(hasher.traverseStreamGraphAndGenerateHashes(streamGraph));</span><br><span class="line">	&#125;</span><br><span class="line">	Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="comment">// 最重要的函数,生成JobVertex,JobEdge等</span></span><br><span class="line">        <span class="comment">// 并尽可能的将多个节点chain在一起</span></span><br><span class="line">	setChaining(hashes, legacyHashes, chainedOperatorHashes);</span><br><span class="line">        <span class="comment">// 将每个JobVertex的入边集合也序列化到该JobVertex的StreamConfig中</span></span><br><span class="line">        <span class="comment">// (出边集合已经在setChaining的时候写入了)</span></span><br><span class="line">	setPhysicalEdges();</span><br><span class="line">        <span class="comment">// 根据group name,为每个JobVertex指定所属的SlotSharingGroup</span></span><br><span class="line">        <span class="comment">// 以及针对Iteration的头尾设置CoLocationGroup</span></span><br><span class="line">        setSlotSharingAndCoLocation();</span><br><span class="line">	setManagedMemoryFraction(</span><br><span class="line">		Collections.unmodifiableMap(jobVertices),</span><br><span class="line">		Collections.unmodifiableMap(vertexConfigs),</span><br><span class="line">		Collections.unmodifiableMap(chainedConfigs),</span><br><span class="line">		id -&gt; streamGraph.getStreamNode(id).getMinResources(),</span><br><span class="line">		id -&gt; streamGraph.getStreamNode(id).getManagedMemoryWeight());</span><br><span class="line">        <span class="comment">// 配置CK</span></span><br><span class="line">	configureCheckpointing();</span><br><span class="line">        <span class="comment">// 设置Savepoint恢复配置 </span></span><br><span class="line">	jobGraph.setSavepointRestoreSettings(streamGraph.getSavepointRestoreSettings());</span><br><span class="line">	JobGraphGenerator.addUserArtifactEntries(streamGraph.getUserArtifacts(), jobGraph);</span><br><span class="line">	<span class="comment">// set the ExecutionConfig last when it has been finalized</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		jobGraph.setExecutionConfig(streamGraph.getExecutionConfig());</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> IllegalConfigurationException(<span class="string">&quot;Could not serialize the ExecutionConfig.&quot;</span> +</span><br><span class="line">				<span class="string">&quot;This indicates that non-serializable types (like custom serializers) were registered&quot;</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> jobGraph;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="setChaining"><a href="#setChaining" class="headerlink" title="setChaining"></a>setChaining</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setChaining</span><span class="params">(Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes, List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes, Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 从Source开始建立Node Chains</span></span><br><span class="line">    <span class="comment">// 递归创建所有JobVertex实例</span></span><br><span class="line">    <span class="keyword">for</span> (Integer sourceNodeId : streamGraph.getSourceIDs()) &#123;</span><br><span class="line">        createChain(sourceNodeId, sourceNodeId, hashes, legacyHashes, <span class="number">0</span>, chainedOperatorHashes);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 构建node chains,返回当前节点的物理出边</span></span><br><span class="line"><span class="comment">// startNodeId != currentNodeId时,说明currentNode是chain中的子节点</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> List&lt;StreamEdge&gt; <span class="title">createChain</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			Integer startNodeId,</span></span></span><br><span class="line"><span class="function"><span class="params">			Integer currentNodeId,</span></span></span><br><span class="line"><span class="function"><span class="params">			Map&lt;Integer, <span class="keyword">byte</span>[]&gt; hashes,</span></span></span><br><span class="line"><span class="function"><span class="params">			List&lt;Map&lt;Integer, <span class="keyword">byte</span>[]&gt;&gt; legacyHashes,</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> chainIndex,</span></span></span><br><span class="line"><span class="function"><span class="params">			Map&lt;Integer, List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; chainedOperatorHashes)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!builtVertices.contains(startNodeId)) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 过渡用的出边集合,用来生成最终的JobEdge</span></span><br><span class="line">        <span class="comment">// 注意不包括chain内部的边</span></span><br><span class="line">        List&lt;StreamEdge&gt; transitiveOutEdges = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">        List&lt;StreamEdge&gt; chainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line">        List&lt;StreamEdge&gt; nonChainableOutputs = <span class="keyword">new</span> ArrayList&lt;StreamEdge&gt;();</span><br><span class="line"></span><br><span class="line">        StreamNode currentNode = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将当前节点的出边分成chainable和nonChainable两类</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge outEdge : currentNode.getOutEdges()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (isChainable(outEdge, streamGraph)) &#123;</span><br><span class="line">                chainableOutputs.add(outEdge);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                nonChainableOutputs.add(outEdge);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 递归调用</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge chainable : chainableOutputs) &#123;</span><br><span class="line">            transitiveOutEdges.addAll(</span><br><span class="line">                    createChain(startNodeId, chainable.getTargetId(), hashes, legacyHashes, chainIndex + <span class="number">1</span>, chainedOperatorHashes));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (StreamEdge nonChainable : nonChainableOutputs) &#123;</span><br><span class="line">            transitiveOutEdges.add(nonChainable);</span><br><span class="line">            createChain(nonChainable.getTargetId(), nonChainable.getTargetId(), hashes, legacyHashes, <span class="number">0</span>, chainedOperatorHashes);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        List&lt;Tuple2&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; operatorHashes =</span><br><span class="line">            chainedOperatorHashes.computeIfAbsent(startNodeId, k -&gt; <span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span>[] primaryHashBytes = hashes.get(currentNodeId);</span><br><span class="line">        OperatorID currentOperatorId = <span class="keyword">new</span> OperatorID(primaryHashBytes);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map&lt;Integer, <span class="keyword">byte</span>[]&gt; legacyHash : legacyHashes) &#123;</span><br><span class="line">            operatorHashes.add(<span class="keyword">new</span> Tuple2&lt;&gt;(primaryHashBytes, legacyHash.get(currentNodeId)));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 生成当前节点的显示名</span></span><br><span class="line">        chainedNames.put(currentNodeId, createChainedName(currentNodeId, chainableOutputs));</span><br><span class="line">        chainedMinResources.put(currentNodeId, createChainedMinResources(currentNodeId, chainableOutputs));</span><br><span class="line">        chainedPreferredResources.put(currentNodeId, createChainedPreferredResources(currentNodeId, chainableOutputs));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNode.getInputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId).addInputFormat(currentOperatorId, currentNode.getInputFormat());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNode.getOutputFormat() != <span class="keyword">null</span>) &#123;</span><br><span class="line">            getOrCreateFormatContainer(startNodeId).addOutputFormat(currentOperatorId, currentNode.getOutputFormat());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果当前节点是起始节点,则直接创建JobVertex并返回StreamConfig</span></span><br><span class="line">        <span class="comment">// 否则先创建一个空的StreamConfig</span></span><br><span class="line">        <span class="comment">// createJobVertex函数根据StreamNode创建对应的JobVertex,并返回空的StreamConfig</span></span><br><span class="line">        StreamConfig config = currentNodeId.equals(startNodeId)</span><br><span class="line">                ? createJobVertex(startNodeId, hashes, legacyHashes, chainedOperatorHashes)</span><br><span class="line">                : <span class="keyword">new</span> StreamConfig(<span class="keyword">new</span> Configuration());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置JobVertex的StreamConfig,基本上是序列化StreamNode中的配置到StreamConfig中</span></span><br><span class="line">        <span class="comment">// 其中包括序列化器,StreamOperator,CK等相关配置</span></span><br><span class="line">        setVertexConfig(currentNodeId, config, chainableOutputs, nonChainableOutputs);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (currentNodeId.equals(startNodeId)) &#123;</span><br><span class="line">            <span class="comment">// 如果是chain起始节点(不是chain中的节点,也会被标记成chain start)</span></span><br><span class="line">            config.setChainStart();</span><br><span class="line">            config.setChainIndex(<span class="number">0</span>);</span><br><span class="line">            <span class="comment">// 把物理出边写入配置,以便部署时使用</span></span><br><span class="line">            config.setOperatorName(streamGraph.getStreamNode(currentNodeId).getOperatorName());</span><br><span class="line">            config.setOutEdgesInOrder(transitiveOutEdges);</span><br><span class="line">            config.setOutEdges(streamGraph.getStreamNode(currentNodeId).getOutEdges());</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将当前节点与所有出边相连</span></span><br><span class="line">            <span class="keyword">for</span> (StreamEdge edge : transitiveOutEdges) &#123;</span><br><span class="line">                <span class="comment">// 通过StreamEdge构建出JobEdge,创建IntermediateDataset,用来将JobVertex和JobEdge相连</span></span><br><span class="line">                connect(startNodeId, edge);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将chain中所有子节点的StreamConfig写入到headOfChain节点的CHAINED_TASK_CONFIG配置中</span></span><br><span class="line">            config.setTransitiveChainedTaskConfigs(chainedConfigs.get(startNodeId));</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 如果是chain中的子节点</span></span><br><span class="line">            chainedConfigs.computeIfAbsent(startNodeId, k -&gt; <span class="keyword">new</span> HashMap&lt;Integer, StreamConfig&gt;());</span><br><span class="line"></span><br><span class="line">            config.setChainIndex(chainIndex);</span><br><span class="line">            StreamNode node = streamGraph.getStreamNode(currentNodeId);</span><br><span class="line">            config.setOperatorName(node.getOperatorName());</span><br><span class="line">            <span class="comment">// 将当前节点的StreamConfig添加到该chain的config集合中</span></span><br><span class="line">            chainedConfigs.get(startNodeId).put(currentNodeId, config);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        config.setOperatorID(currentOperatorId);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (chainableOutputs.isEmpty()) &#123;</span><br><span class="line">            config.setChainEnd();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> transitiveOutEdges;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之七Checkpoint流程</title>
    <url>/2020/06/18/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E4%B8%83Checkpoint%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<blockquote>
<p>Flink的CK机制可以说是一个亮点,基于Chandy-Lamport算法<br>CK的配置在生成ExecutionGroup时就已经被设置 <a href="https://jxeditor.github.io/2020/05/28/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E5%9B%9BExecutionGraph%E7%94%9F%E6%88%90/">传送门</a></p>
</blockquote>
<span id="more"></span>

<h2 id="CK的触发"><a href="#CK的触发" class="headerlink" title="CK的触发"></a>CK的触发</h2><h3 id="定时器"><a href="#定时器" class="headerlink" title="定时器"></a>定时器</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ExecutionGroup.enableCheckpointing</span></span><br><span class="line">checkpointCoordinatorTimer = Executors.newSingleThreadScheduledExecutor(</span><br><span class="line">    <span class="keyword">new</span> DispatcherThreadFactory(</span><br><span class="line">        Thread.currentThread().getThreadGroup(), <span class="string">&quot;Checkpoint Timer&quot;</span>));</span><br><span class="line"><span class="comment">// 创建定时器,并传给CheckpointCoordinator</span></span><br></pre></td></tr></table></figure>
<h3 id="监听器的生成与注册"><a href="#监听器的生成与注册" class="headerlink" title="监听器的生成与注册"></a>监听器的生成与注册</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ExecutionGroup,在enableCheckpointing方法中</span></span><br><span class="line"><span class="comment">// 如果chkConfig的CK时间设置不等于Long.MAX_VALUE</span></span><br><span class="line"><span class="comment">// 则创建一个监听器并注册</span></span><br><span class="line"><span class="keyword">if</span> (chkConfig.getCheckpointInterval() != Long.MAX_VALUE) &#123;</span><br><span class="line">    <span class="comment">// the periodic checkpoint scheduler is activated and deactivated as a result of</span></span><br><span class="line">    <span class="comment">// job status changes (running -&gt; on, all other states -&gt; off)</span></span><br><span class="line">    registerJobStatusListener(checkpointCoordinator.createActivatorDeactivator());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="触发CK"><a href="#触发CK" class="headerlink" title="触发CK"></a>触发CK</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// CheckpointCoordinatorDeActivator</span></span><br><span class="line"><span class="comment">// 当Job状态改变时调用startCheckpointScheduler()</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">jobStatusChanges</span><span class="params">(JobID jobId, JobStatus newJobStatus, <span class="keyword">long</span> timestamp, Throwable error)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (newJobStatus == JobStatus.RUNNING) &#123;</span><br><span class="line">        <span class="comment">// start the checkpoint scheduler</span></span><br><span class="line">        coordinator.startCheckpointScheduler();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// anything else should stop the trigger for now</span></span><br><span class="line">        coordinator.stopCheckpointScheduler();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckpointCoordinator</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">startCheckpointScheduler</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">		<span class="keyword">if</span> (shutdown) &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Checkpoint coordinator is shut down&quot;</span>);</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">// make sure all prior timers are cancelled</span></span><br><span class="line">        <span class="comment">// 确保启动定时CK前,没有定时器</span></span><br><span class="line">        stopCheckpointScheduler();</span><br><span class="line">        periodicScheduling = <span class="keyword">true</span>;</span><br><span class="line">        <span class="comment">// 创建定时器,获取延迟时间</span></span><br><span class="line">        currentPeriodicTrigger = scheduleTriggerWithDelay(getRandomInitDelay());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> ScheduledFuture&lt;?&gt; scheduleTriggerWithDelay(<span class="keyword">long</span> initDelay) &#123;</span><br><span class="line">	<span class="keyword">return</span> timer.scheduleAtFixedRate(</span><br><span class="line">            <span class="comment">// 触发操作</span></span><br><span class="line">            <span class="keyword">new</span> ScheduledTrigger(),</span><br><span class="line">            initDelay, baseInterval, TimeUnit.MILLISECONDS);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckpointCoordinator.ScheduledTrigger</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ScheduledTrigger</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 触发CK</span></span><br><span class="line">            triggerCheckpoint(System.currentTimeMillis(), <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(<span class="string">&quot;Exception while triggering checkpoint for job &#123;&#125;.&quot;</span>, job, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// triggerCheckpoint主要进行以下操作</span></span><br><span class="line"><span class="comment">// 1.根据配置进行CK需要的检验</span></span><br><span class="line"><span class="comment">// 2.检查需要触发CK的所有任务是否正在运行,生成executions</span></span><br><span class="line"><span class="comment">// 3.检查需要确认CK的所有任务是否正在运行,生成ackTasks</span></span><br><span class="line"><span class="comment">// 4.根据配置确定是CK还是SP</span></span><br><span class="line"><span class="comment">// 5.构造PendingCheckpoint</span></span><br><span class="line"><span class="comment">// 6.注册清除超时CK的定时器</span></span><br><span class="line"><span class="comment">// 7.调用execution.triggerCheckpoint()触发CK</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Execution</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">triggerCheckpointHelper</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp, CheckpointOptions checkpointOptions, <span class="keyword">boolean</span> advanceToEndOfEventTime)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> CheckpointType checkpointType = checkpointOptions.getCheckpointType();</span><br><span class="line">    <span class="keyword">if</span> (advanceToEndOfEventTime &amp;&amp; !(checkpointType.isSynchronous() &amp;&amp; checkpointType.isSavepoint())) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Only synchronous savepoints are allowed to advance the watermark to MAX.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> LogicalSlot slot = assignedResource;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (slot != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">final</span> TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发送CK</span></span><br><span class="line">        taskManagerGateway.triggerCheckpoint(attemptId, getVertex().getJobId(), checkpointId, timestamp, checkpointOptions, advanceToEndOfEventTime);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        LOG.debug(<span class="string">&quot;The execution has no slot assigned. This indicates that the execution is no longer running.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 后续流程</span></span><br><span class="line"><span class="comment">// RpcTaskManagerGateWay</span></span><br><span class="line"><span class="comment">// TaskExecutor</span></span><br></pre></td></tr></table></figure>
<h3 id="CK执行"><a href="#CK执行" class="headerlink" title="CK执行"></a>CK执行</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// TaskExecutor</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> CompletableFuture&lt;Acknowledge&gt; <span class="title">triggerCheckpoint</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ExecutionAttemptID executionAttemptID,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> checkpointId,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> checkpointTimestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> advanceToEndOfEventTime)</span> </span>&#123;</span><br><span class="line">    log.debug(<span class="string">&quot;Trigger checkpoint &#123;&#125;@&#123;&#125; for &#123;&#125;.&quot;</span>, checkpointId, checkpointTimestamp, executionAttemptID);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> CheckpointType checkpointType = checkpointOptions.getCheckpointType();</span><br><span class="line">    <span class="keyword">if</span> (advanceToEndOfEventTime &amp;&amp; !(checkpointType.isSynchronous() &amp;&amp; checkpointType.isSavepoint())) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Only synchronous savepoints are allowed to advance the watermark to MAX.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取具体Task</span></span><br><span class="line">    <span class="keyword">final</span> Task task = taskSlotTable.getTask(executionAttemptID);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (task != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 触发CheckpointBarrier</span></span><br><span class="line">        task.triggerCheckpointBarrier(checkpointId, checkpointTimestamp, checkpointOptions, advanceToEndOfEventTime);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> CompletableFuture.completedFuture(Acknowledge.get());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> String message = <span class="string">&quot;TaskManager received a checkpoint request for unknown task &quot;</span> + executionAttemptID + <span class="string">&#x27;.&#x27;</span>;</span><br><span class="line"></span><br><span class="line">        log.debug(message);</span><br><span class="line">        <span class="keyword">return</span> FutureUtils.completedExceptionally(<span class="keyword">new</span> CheckpointException(message, CheckpointFailureReason.TASK_CHECKPOINT_FAILURE));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Task</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">triggerCheckpointBarrier</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">long</span> checkpointID,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">long</span> checkpointTimestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> <span class="keyword">boolean</span> advanceToEndOfEventTime)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> AbstractInvokable invokable = <span class="keyword">this</span>.invokable;</span><br><span class="line">    <span class="comment">// 创建CheckpointMetaData</span></span><br><span class="line">    <span class="keyword">final</span> CheckpointMetaData checkpointMetaData = <span class="keyword">new</span> CheckpointMetaData(checkpointID, checkpointTimestamp);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (executionState == ExecutionState.RUNNING &amp;&amp; invokable != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 异步CK</span></span><br><span class="line">            invokable.triggerCheckpointAsync(checkpointMetaData, checkpointOptions, advanceToEndOfEventTime);</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// StreamTask</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;Boolean&gt; <span class="title">triggerCheckpointAsync</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		CheckpointMetaData checkpointMetaData,</span></span></span><br><span class="line"><span class="function"><span class="params">		CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">boolean</span> advanceToEndOfEventTime)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 注意这个MailboxProcessor,这是Flink1.10后的一个新特性,会在后续章节中详细说明</span></span><br><span class="line">    <span class="keyword">return</span> mailboxProcessor.getMainMailboxExecutor().submit(</span><br><span class="line">            () -&gt; triggerCheckpoint(checkpointMetaData, checkpointOptions, advanceToEndOfEventTime),</span><br><span class="line">            <span class="string">&quot;checkpoint %s with %s&quot;</span>,</span><br><span class="line">        checkpointMetaData,</span><br><span class="line">        checkpointOptions);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">triggerCheckpoint</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			CheckpointMetaData checkpointMetaData,</span></span></span><br><span class="line"><span class="function"><span class="params">			CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">boolean</span> advanceToEndOfEventTime)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// No alignment if we inject a checkpoint</span></span><br><span class="line">        <span class="comment">// 指标</span></span><br><span class="line">        CheckpointMetrics checkpointMetrics = <span class="keyword">new</span> CheckpointMetrics()</span><br><span class="line">            .setBytesBufferedInAlignment(<span class="number">0L</span>)</span><br><span class="line">            .setAlignmentDurationNanos(<span class="number">0L</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开始CK</span></span><br><span class="line">        <span class="keyword">boolean</span> success = performCheckpoint(checkpointMetaData, checkpointOptions, checkpointMetrics, advanceToEndOfEventTime);</span><br><span class="line">        <span class="keyword">if</span> (!success) &#123;</span><br><span class="line">            declineCheckpoint(checkpointMetaData.getCheckpointId());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> success;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">performCheckpoint</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointMetaData checkpointMetaData,</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointMetrics checkpointMetrics,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">boolean</span> advanceToEndOfTime)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    LOG.debug(<span class="string">&quot;Starting checkpoint (&#123;&#125;) &#123;&#125; on task &#123;&#125;&quot;</span>,</span><br><span class="line">        checkpointMetaData.getCheckpointId(), checkpointOptions.getCheckpointType(), getName());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> checkpointId = checkpointMetaData.getCheckpointId();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isRunning) &#123;</span><br><span class="line">        actionExecutor.runThrowing(() -&gt; &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (checkpointOptions.getCheckpointType().isSynchronous()) &#123;</span><br><span class="line">                setSynchronousSavepointId(checkpointId);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (advanceToEndOfTime) &#123;</span><br><span class="line">                    advanceToEndOfEventTime();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 从barriers和records/watermarks/timers/callbacks的角度来看，以下所有步骤都是原子步骤</span></span><br><span class="line">            <span class="comment">// 我们通常会尽量尽快释放检查点屏障，以免影响下游的检查点对齐</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// Step (1): ck之前,operators做一些pre-barrier工作</span></span><br><span class="line">            <span class="comment">// 一般来说,pre-barrier工作应该为0或最小</span></span><br><span class="line">            operatorChain.prepareSnapshotPreBarrier(checkpointId);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Step (2): 发送checkpoint barrier给下游</span></span><br><span class="line">            operatorChain.broadcastCheckpointBarrier(</span><br><span class="line">                    checkpointId,</span><br><span class="line">                    checkpointMetaData.getTimestamp(),</span><br><span class="line">                    checkpointOptions);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Step (3): 进行State快照. 很大程序是异步进行,避免影响流式拓扑的进度</span></span><br><span class="line">            checkpointState(checkpointMetaData, checkpointOptions, checkpointMetrics);</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        actionExecutor.runThrowing(() -&gt; &#123;</span><br><span class="line">            <span class="comment">// 不能执行检查点,让下游operators不等待该operator的任何输入</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 我们无法在operator chain上广播取消CKMarker,因为它可能尚未创建</span></span><br><span class="line">            <span class="keyword">final</span> CancelCheckpointMarker message = <span class="keyword">new</span> CancelCheckpointMarker(checkpointMetaData.getCheckpointId());</span><br><span class="line">            recordWriter.broadcastEvent(message);</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkpointState</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		CheckpointMetaData checkpointMetaData,</span></span></span><br><span class="line"><span class="function"><span class="params">		CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">		CheckpointMetrics checkpointMetrics)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// CK数据保存工厂类</span></span><br><span class="line">    CheckpointStreamFactory storage = checkpointStorage.resolveCheckpointStorageLocation(</span><br><span class="line">			checkpointMetaData.getCheckpointId(),</span><br><span class="line">			checkpointOptions.getTargetLocation());</span><br><span class="line">    <span class="comment">// CK操作类,根据CKMetaData,CK配置,CK保存点,CK指标执行CK</span></span><br><span class="line">    CheckpointingOperation checkpointingOperation = <span class="keyword">new</span> CheckpointingOperation(</span><br><span class="line">        <span class="keyword">this</span>,</span><br><span class="line">        checkpointMetaData,</span><br><span class="line">        checkpointOptions,</span><br><span class="line">        storage,</span><br><span class="line">        checkpointMetrics);</span><br><span class="line">    <span class="comment">// 执行CK</span></span><br><span class="line">    checkpointingOperation.executeCheckpointing();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="CK的执行"><a href="#CK的执行" class="headerlink" title="CK的执行"></a>CK的执行</h2><h3 id="CheckpointingOperation"><a href="#CheckpointingOperation" class="headerlink" title="CheckpointingOperation"></a>CheckpointingOperation</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 该类在StreamTask中被实现</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">executeCheckpointing</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    startSyncPartNano = System.nanoTime();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (StreamOperator&lt;?&gt; op : allOperators) &#123;</span><br><span class="line">            <span class="comment">// 对各Operator进行CK调用,同步调用</span></span><br><span class="line">            checkpointStreamOperator(op);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;Finished synchronous checkpoints for checkpoint &#123;&#125; on task &#123;&#125;&quot;</span>,</span><br><span class="line">                checkpointMetaData.getCheckpointId(), owner.getName());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        startAsyncPartNano = System.nanoTime();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// CK指标间隔时间设置</span></span><br><span class="line">        checkpointMetrics.setSyncDurationMillis((startAsyncPartNano - startSyncPartNano) / <span class="number">1_000_000</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we are transferring ownership over snapshotInProgressList for cleanup to the thread, active on submit</span></span><br><span class="line">        AsyncCheckpointRunnable asyncCheckpointRunnable = <span class="keyword">new</span> AsyncCheckpointRunnable(</span><br><span class="line">            owner,</span><br><span class="line">            operatorSnapshotsInProgress,</span><br><span class="line">            checkpointMetaData,</span><br><span class="line">            checkpointMetrics,</span><br><span class="line">            startAsyncPartNano);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 当前活跃的后台物理线程,注册一个Closeable</span></span><br><span class="line">        owner.cancelables.registerCloseable(asyncCheckpointRunnable);</span><br><span class="line">        <span class="comment">// 异步快照工作线程池</span></span><br><span class="line">        owner.asyncOperationsThreadPool.execute(asyncCheckpointRunnable);</span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;&#123;&#125; - finished synchronous part of checkpoint &#123;&#125;. &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;Alignment duration: &#123;&#125; ms, snapshot duration &#123;&#125; ms&quot;</span>,</span><br><span class="line">                owner.getName(), checkpointMetaData.getCheckpointId(),</span><br><span class="line">                checkpointMetrics.getAlignmentDurationNanos() / <span class="number">1_000_000</span>,</span><br><span class="line">                checkpointMetrics.getSyncDurationMillis());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">checkpointStreamOperator</span><span class="params">(StreamOperator&lt;?&gt; op)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != op) &#123;</span><br><span class="line">            <span class="comment">// 对于每个Operator执行snapshotState进行CK,生成OperatorSnapshotFutures</span></span><br><span class="line">            OperatorSnapshotFutures snapshotInProgress = op.snapshotState(</span><br><span class="line">                    checkpointMetaData.getCheckpointId(),</span><br><span class="line">                    checkpointMetaData.getTimestamp(),</span><br><span class="line">                    checkpointOptions,</span><br><span class="line">                    storageLocation);</span><br><span class="line">            <span class="comment">// 存入operatorSnapshotsInProgress</span></span><br><span class="line">            operatorSnapshotsInProgress.put(op.getOperatorID(), snapshotInProgress);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="同步执行"><a href="#同步执行" class="headerlink" title="同步执行"></a>同步执行</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AbstractStreamOperator</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> OperatorSnapshotFutures <span class="title">snapshotState</span><span class="params">(<span class="keyword">long</span> checkpointId, <span class="keyword">long</span> timestamp, CheckpointOptions checkpointOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">        CheckpointStreamFactory factory)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    KeyGroupRange keyGroupRange = <span class="keyword">null</span> != keyedStateBackend ?</span><br><span class="line">            keyedStateBackend.getKeyGroupRange() : KeyGroupRange.EMPTY_KEY_GROUP_RANGE;</span><br><span class="line"></span><br><span class="line">    OperatorSnapshotFutures snapshotInProgress = <span class="keyword">new</span> OperatorSnapshotFutures();</span><br><span class="line"></span><br><span class="line">    StateSnapshotContextSynchronousImpl snapshotContext = <span class="keyword">new</span> StateSnapshotContextSynchronousImpl(</span><br><span class="line">        checkpointId,</span><br><span class="line">        timestamp,</span><br><span class="line">        factory,</span><br><span class="line">        keyGroupRange,</span><br><span class="line">        getContainingTask().getCancelables());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 调用算子/函数的snapshot方法</span></span><br><span class="line">        snapshotState(snapshotContext);</span><br><span class="line"></span><br><span class="line">        snapshotInProgress.setKeyedStateRawFuture(snapshotContext.getKeyedStateStreamFuture());</span><br><span class="line">        snapshotInProgress.setOperatorStateRawFuture(snapshotContext.getOperatorStateStreamFuture());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != operatorStateBackend) &#123;</span><br><span class="line">            snapshotInProgress.setOperatorStateManagedFuture(</span><br><span class="line">                <span class="comment">// operatorStateBackend持久化</span></span><br><span class="line">                operatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != keyedStateBackend) &#123;</span><br><span class="line">            snapshotInProgress.setKeyedStateManagedFuture(</span><br><span class="line">                <span class="comment">// keyedStateBackend持久化</span></span><br><span class="line">                keyedStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception snapshotException) &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> snapshotInProgress;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(StateSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> KeyedStateBackend&lt;?&gt; keyedStateBackend = getKeyedStateBackend();</span><br><span class="line">    <span class="comment">//TODO all of this can be removed once heap-based timers are integrated with RocksDB incremental snapshots</span></span><br><span class="line">    <span class="keyword">if</span> (keyedStateBackend <span class="keyword">instanceof</span> AbstractKeyedStateBackend &amp;&amp;</span><br><span class="line">        ((AbstractKeyedStateBackend&lt;?&gt;) keyedStateBackend).requiresLegacySynchronousTimerSnapshots()) &#123;</span><br><span class="line"></span><br><span class="line">        KeyedStateCheckpointOutputStream out;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            out = context.getRawKeyedOperatorStateOutput();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">&quot;Could not open raw keyed operator state stream for &quot;</span> +</span><br><span class="line">                getOperatorName() + <span class="string">&#x27;.&#x27;</span>, exception);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            KeyGroupsList allKeyGroups = out.getKeyGroupList();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> keyGroupIdx : allKeyGroups) &#123;</span><br><span class="line">                out.startNewKeyGroup(keyGroupIdx);</span><br><span class="line"></span><br><span class="line">                timeServiceManager.snapshotStateForKeyGroup(</span><br><span class="line">                    <span class="keyword">new</span> DataOutputViewStreamWrapper(out), keyGroupIdx);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            ...</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="异步执行"><a href="#异步执行" class="headerlink" title="异步执行"></a>异步执行</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// AsyncCheckpointRunnable</span></span><br><span class="line">当同步CK完成之后,异步实现具体信息写入,由AsyncOperations-thread-*线程执行</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    FileSystemSafetyNet.initializeSafetyNetForThread();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">        TaskStateSnapshot jobManagerTaskOperatorSubtaskStates =</span><br><span class="line">            <span class="keyword">new</span> TaskStateSnapshot(operatorSnapshotsInProgress.size());</span><br><span class="line"></span><br><span class="line">        TaskStateSnapshot localTaskOperatorSubtaskStates =</span><br><span class="line">            <span class="keyword">new</span> TaskStateSnapshot(operatorSnapshotsInProgress.size());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;OperatorID, OperatorSnapshotFutures&gt; entry : operatorSnapshotsInProgress.entrySet()) &#123;</span><br><span class="line"></span><br><span class="line">            OperatorID operatorID = entry.getKey();</span><br><span class="line">            OperatorSnapshotFutures snapshotInProgress = entry.getValue();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// finalize the async part of all by executing all snapshot runnables</span></span><br><span class="line">            <span class="comment">// 等待各Future执行完成</span></span><br><span class="line">            OperatorSnapshotFinalizer finalizedSnapshots =</span><br><span class="line">                <span class="keyword">new</span> OperatorSnapshotFinalizer(snapshotInProgress);</span><br><span class="line"></span><br><span class="line">            jobManagerTaskOperatorSubtaskStates.putSubtaskStateByOperatorID(</span><br><span class="line">                operatorID,</span><br><span class="line">                finalizedSnapshots.getJobManagerOwnedState());</span><br><span class="line"></span><br><span class="line">            localTaskOperatorSubtaskStates.putSubtaskStateByOperatorID(</span><br><span class="line">                operatorID,</span><br><span class="line">                finalizedSnapshots.getTaskLocalState());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> asyncEndNanos = System.nanoTime();</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> asyncDurationMillis = (asyncEndNanos - asyncStartNanos) / <span class="number">1_000_000L</span>;</span><br><span class="line"></span><br><span class="line">        checkpointMetrics.setAsyncDurationMillis(asyncDurationMillis);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (asyncCheckpointState.compareAndSet(CheckpointingOperation.AsyncCheckpointState.RUNNING,</span><br><span class="line">            CheckpointingOperation.AsyncCheckpointState.COMPLETED)) &#123;</span><br><span class="line">            <span class="comment">// 上报CK状态给JobManager</span></span><br><span class="line">            reportCompletedSnapshotStates(</span><br><span class="line">                jobManagerTaskOperatorSubtaskStates,</span><br><span class="line">                localTaskOperatorSubtaskStates,</span><br><span class="line">                asyncDurationMillis);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;&#123;&#125; - asynchronous part of checkpoint &#123;&#125; could not be completed because it was closed before.&quot;</span>,</span><br><span class="line">                owner.getName(),</span><br><span class="line">                checkpointMetaData.getCheckpointId());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;&#123;&#125; - asynchronous part of checkpoint &#123;&#125; could not be completed.&quot;</span>,</span><br><span class="line">                owner.getName(),</span><br><span class="line">                checkpointMetaData.getCheckpointId(),</span><br><span class="line">                e);</span><br><span class="line">        &#125;</span><br><span class="line">        handleExecutionException(e);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        owner.cancelables.unregisterCloseable(<span class="keyword">this</span>);</span><br><span class="line">        FileSystemSafetyNet.closeSafetyNetAndGuardedResourcesForThread();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="非Source的消息处理"><a href="#非Source的消息处理" class="headerlink" title="非Source的消息处理"></a>非Source的消息处理</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">上游在进行CK时,会向下游发送CheckBarrier消息,而下游的task正是拿到该消息,进行CK操作</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckpointedInputGate</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Optional&lt;BufferOrEvent&gt; <span class="title">pollNext</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ...</span><br><span class="line">        BufferOrEvent bufferOrEvent = next.get();</span><br><span class="line">        <span class="comment">// 如果消息对应的channel已经被block,则将消息缓存到bufferStorage</span></span><br><span class="line">        <span class="keyword">if</span> (barrierHandler.isBlocked(offsetChannelIndex(bufferOrEvent.getChannelIndex()))) &#123;</span><br><span class="line">            <span class="comment">// if the channel is blocked, we just store the BufferOrEvent</span></span><br><span class="line">            bufferStorage.add(bufferOrEvent);</span><br><span class="line">            <span class="keyword">if</span> (bufferStorage.isFull()) &#123;</span><br><span class="line">                barrierHandler.checkpointSizeLimitExceeded(bufferStorage.getMaxBufferedBytes());</span><br><span class="line">                bufferStorage.rollOver();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果是一般的消息,返回继续下一步处理</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (bufferOrEvent.isBuffer()) &#123;</span><br><span class="line">            <span class="keyword">return</span> next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 对消息类型进行判断,如果是CheckpointBarrier类型的消息</span></span><br><span class="line">        <span class="comment">// 进一步判断是否需要对齐或者进行CK</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (bufferOrEvent.getEvent().getClass() == CheckpointBarrier.class) &#123;</span><br><span class="line">            CheckpointBarrier checkpointBarrier = (CheckpointBarrier) bufferOrEvent.getEvent();</span><br><span class="line">            <span class="keyword">if</span> (!endOfInputGate) &#123;</span><br><span class="line">                <span class="comment">// process barriers only if there is a chance of the checkpoint completing</span></span><br><span class="line">                <span class="comment">// 对齐Barrier</span></span><br><span class="line">                <span class="keyword">if</span> (barrierHandler.processBarrier(checkpointBarrier, offsetChannelIndex(bufferOrEvent.getChannelIndex()), bufferStorage.getPendingBytes())) &#123;</span><br><span class="line">                    bufferStorage.rollOver();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果是CancelCheckpointMarker类型,处理该消息</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (bufferOrEvent.getEvent().getClass() == CancelCheckpointMarker.class) &#123;</span><br><span class="line">            <span class="keyword">if</span> (barrierHandler.processCancellationBarrier((CancelCheckpointMarker) bufferOrEvent.getEvent())) &#123;</span><br><span class="line">                bufferStorage.rollOver();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (bufferOrEvent.getEvent().getClass() == EndOfPartitionEvent.class) &#123;</span><br><span class="line">                <span class="keyword">if</span> (barrierHandler.processEndOfPartition()) &#123;</span><br><span class="line">                    bufferStorage.rollOver();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CheckpointBarrierAligner</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">processBarrier</span><span class="params">(CheckpointBarrier receivedBarrier, <span class="keyword">int</span> channelIndex, <span class="keyword">long</span> bufferedBytes)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> barrierId = receivedBarrier.getId();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// fast path for single channel cases</span></span><br><span class="line">    <span class="comment">// 只有一个输入Channel</span></span><br><span class="line">    <span class="keyword">if</span> (totalNumberOfInputChannels == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">// 调用notifyCheckpoint触发CK</span></span><br><span class="line">        <span class="keyword">if</span> (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">            <span class="comment">// new checkpoint</span></span><br><span class="line">            currentCheckpointId = barrierId;</span><br><span class="line">            notifyCheckpoint(receivedBarrier, bufferedBytes, latestAlignmentDurationNanos);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> checkpointAborted = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// -- general code path for multiple input channels --</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (numBarriersReceived &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// this is only true if some alignment is already progress and was not canceled</span></span><br><span class="line">        <span class="comment">// 不是首次接收到CheckpointBarrier</span></span><br><span class="line">        <span class="keyword">if</span> (barrierId == currentCheckpointId) &#123;</span><br><span class="line">            <span class="comment">// regular case</span></span><br><span class="line">            <span class="comment">// 将该消息的channel block,并numBarriersReceived++</span></span><br><span class="line">            onBarrier(channelIndex);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">            <span class="comment">// we did not complete the current checkpoint, another started before</span></span><br><span class="line">            LOG.warn(<span class="string">&quot;&#123;&#125;: Received checkpoint barrier for checkpoint &#123;&#125; before completing current checkpoint &#123;&#125;. &quot;</span> +</span><br><span class="line">                    <span class="string">&quot;Skipping current checkpoint.&quot;</span>,</span><br><span class="line">                taskName,</span><br><span class="line">                barrierId,</span><br><span class="line">                currentCheckpointId);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// let the task know we are not completing this</span></span><br><span class="line">            notifyAbort(currentCheckpointId,</span><br><span class="line">                <span class="keyword">new</span> CheckpointException(</span><br><span class="line">                    <span class="string">&quot;Barrier id: &quot;</span> + barrierId,</span><br><span class="line">                    CheckpointFailureReason.CHECKPOINT_DECLINED_SUBSUMED));</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 对齐之前的CK</span></span><br><span class="line">            <span class="comment">// abort the current checkpoint</span></span><br><span class="line">            releaseBlocksAndResetBarriers();</span><br><span class="line">            checkpointAborted = <span class="keyword">true</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 进行新一次的CK</span></span><br><span class="line">            <span class="comment">// begin a the new checkpoint</span></span><br><span class="line">            beginNewAlignment(barrierId, channelIndex);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// ignore trailing barrier from an earlier checkpoint (obsolete now)</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (barrierId &gt; currentCheckpointId) &#123;</span><br><span class="line">        <span class="comment">// 首次接收到CK,开启新的对齐操作,将currentCheckpoinitId设置为barrierId</span></span><br><span class="line">        <span class="comment">// 并将该消息的channel block</span></span><br><span class="line">        <span class="comment">// numBarriersReceived++</span></span><br><span class="line">        <span class="comment">// first barrier of a new checkpoint</span></span><br><span class="line">        beginNewAlignment(barrierId, channelIndex);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// either the current checkpoint was canceled (numBarriers == 0) or</span></span><br><span class="line">        <span class="comment">// this barrier is from an old subsumed checkpoint</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check if we have all barriers - since canceled checkpoints always have zero barriers</span></span><br><span class="line">    <span class="comment">// this can only happen on a non canceled checkpoint</span></span><br><span class="line">    <span class="comment">// 对齐完成</span></span><br><span class="line">    <span class="keyword">if</span> (numBarriersReceived + numClosedChannels == totalNumberOfInputChannels) &#123;</span><br><span class="line">        <span class="comment">// actually trigger checkpoint</span></span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;&#123;&#125;: Received all barriers, triggering checkpoint &#123;&#125; at &#123;&#125;.&quot;</span>,</span><br><span class="line">                taskName,</span><br><span class="line">                receivedBarrier.getId(),</span><br><span class="line">                receivedBarrier.getTimestamp());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 直接将blockedChannels均设置为非block状态</span></span><br><span class="line">        <span class="comment">// numBarriersReceived设置为0</span></span><br><span class="line">        <span class="comment">// 调用notifyCheckpoint触发CK</span></span><br><span class="line">        releaseBlocksAndResetBarriers();</span><br><span class="line">        notifyCheckpoint(receivedBarrier, bufferedBytes, latestAlignmentDurationNanos);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> checkpointAborted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="CK执行完成后的上报"><a href="#CK执行完成后的上报" class="headerlink" title="CK执行完成后的上报"></a>CK执行完成后的上报</h2><h3 id="上报逻辑"><a href="#上报逻辑" class="headerlink" title="上报逻辑"></a>上报逻辑</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// StreamTask,本章前面异步执行时有提到当CK同步完成后,会将CK状态上报给JobManager</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">reportCompletedSnapshotStates</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        TaskStateSnapshot acknowledgedTaskStateSnapshot,</span></span></span><br><span class="line"><span class="function"><span class="params">        TaskStateSnapshot localTaskStateSnapshot,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">long</span> asyncDurationMillis)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    TaskStateManager taskStateManager = owner.getEnvironment().getTaskStateManager();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> hasAckState = acknowledgedTaskStateSnapshot.hasState();</span><br><span class="line">    <span class="keyword">boolean</span> hasLocalState = localTaskStateSnapshot.hasState();</span><br><span class="line"></span><br><span class="line">    Preconditions.checkState(hasAckState || !hasLocalState,</span><br><span class="line">        <span class="string">&quot;Found cached state but no corresponding primary state is reported to the job &quot;</span> +</span><br><span class="line">            <span class="string">&quot;manager. This indicates a problem.&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we signal stateless tasks by reporting null, so that there are no attempts to assign empty state</span></span><br><span class="line">    <span class="comment">// to stateless tasks on restore. This enables simple job modifications that only concern</span></span><br><span class="line">    <span class="comment">// stateless without the need to assign them uids to match their (always empty) states.</span></span><br><span class="line">    taskStateManager.reportTaskStateSnapshots(</span><br><span class="line">        checkpointMetaData,</span><br><span class="line">        checkpointMetrics,</span><br><span class="line">        hasAckState ? acknowledgedTaskStateSnapshot : <span class="keyword">null</span>,</span><br><span class="line">        hasLocalState ? localTaskStateSnapshot : <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// TaskStateManagerImpl</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reportTaskStateSnapshots</span><span class="params">(<span class="meta">@Nonnull</span> CheckpointMetaData checkpointMetaData, <span class="meta">@Nonnull</span> CheckpointMetrics checkpointMetrics, <span class="meta">@Nullable</span> TaskStateSnapshot acknowledgedState, <span class="meta">@Nullable</span> TaskStateSnapshot localState)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> checkpointId = checkpointMetaData.getCheckpointId();</span><br><span class="line">    <span class="keyword">this</span>.localStateStore.storeLocalState(checkpointId, localState);</span><br><span class="line">    <span class="comment">// jobId,执行Id,ckId,ck指标,acknowledgeState</span></span><br><span class="line">    <span class="keyword">this</span>.checkpointResponder.acknowledgeCheckpoint(<span class="keyword">this</span>.jobId, <span class="keyword">this</span>.executionAttemptID, checkpointId, checkpointMetrics, acknowledgedState);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="JobManager接收处理逻辑"><a href="#JobManager接收处理逻辑" class="headerlink" title="JobManager接收处理逻辑"></a>JobManager接收处理逻辑</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// CheckpointCoordinator</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">receiveAcknowledgeMessage</span><span class="params">(AcknowledgeCheckpoint message, String taskManagerLocationInfo)</span> <span class="keyword">throws</span> CheckpointException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (shutdown || message == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!job.equals(message.getJob())) &#123;</span><br><span class="line">        LOG.error(<span class="string">&quot;Received wrong AcknowledgeCheckpoint message for job &#123;&#125; from &#123;&#125; : &#123;&#125;&quot;</span>, job, taskManagerLocationInfo, message);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> checkpointId = message.getCheckpointId();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">        <span class="comment">// we need to check inside the lock for being shutdown as well, otherwise we</span></span><br><span class="line">        <span class="comment">// get races and invalid error log messages</span></span><br><span class="line">        <span class="keyword">if</span> (shutdown) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> PendingCheckpoint checkpoint = pendingCheckpoints.get(checkpointId);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (checkpoint != <span class="keyword">null</span> &amp;&amp; !checkpoint.isDiscarded()) &#123;</span><br><span class="line">            <span class="comment">// acknowledgeTask对一些状态进行清理</span></span><br><span class="line">            <span class="comment">// 根据不同情况返回不同的状态</span></span><br><span class="line">            <span class="keyword">switch</span> (checkpoint.acknowledgeTask(message.getTaskExecutionId(), message.getSubtaskState(), message.getCheckpointMetrics())) &#123;</span><br><span class="line">                <span class="comment">// 根据不同的返回状态进行响应处理</span></span><br><span class="line">                <span class="keyword">case</span> SUCCESS:</span><br><span class="line">                    LOG.debug(<span class="string">&quot;Received acknowledge message for checkpoint &#123;&#125; from task &#123;&#125; of job &#123;&#125; at &#123;&#125;.&quot;</span>,</span><br><span class="line">                        checkpointId, message.getTaskExecutionId(), message.getJob(), taskManagerLocationInfo);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (checkpoint.areTasksFullyAcknowledged()) &#123;</span><br><span class="line">                        <span class="comment">// 所有的Task都反馈了Ack信息</span></span><br><span class="line">                        completePendingCheckpoint(checkpoint);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">case</span> DUPLICATE:</span><br><span class="line">                    ...</span><br><span class="line">                <span class="keyword">case</span> UNKNOWN:</span><br><span class="line">                    ...</span><br><span class="line">                <span class="keyword">case</span> DISCARDED:</span><br><span class="line">                    ...</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (checkpoint != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">completePendingCheckpoint</span><span class="params">(PendingCheckpoint pendingCheckpoint)</span> <span class="keyword">throws</span> CheckpointException </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> checkpointId = pendingCheckpoint.getCheckpointId();</span><br><span class="line">    <span class="keyword">final</span> CompletedCheckpoint completedCheckpoint;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// As a first step to complete the checkpoint, we register its state with the registry</span></span><br><span class="line">    Map&lt;OperatorID, OperatorState&gt; operatorStates = pendingCheckpoint.getOperatorStates();</span><br><span class="line">    sharedStateRegistry.registerAll(operatorStates.values());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            completedCheckpoint = pendingCheckpoint.finalizeCheckpoint();</span><br><span class="line">            failureManager.handleCheckpointSuccess(pendingCheckpoint.getCheckpointId());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e1) &#123;</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 完成后必须放弃挂起的CK</span></span><br><span class="line">        Preconditions.checkState(pendingCheckpoint.isDiscarded() &amp;&amp; completedCheckpoint != <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            completedCheckpointStore.addCheckpoint(completedCheckpoint);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception exception) &#123;</span><br><span class="line">            ...</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        pendingCheckpoints.remove(checkpointId);</span><br><span class="line"></span><br><span class="line">        triggerQueuedRequests();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    rememberRecentCheckpointId(checkpointId);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除在完成CK之前挂起的CK</span></span><br><span class="line">    dropSubsumedCheckpoints(checkpointId);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录完成此操作的时间,以计算检查点之间的最小延迟</span></span><br><span class="line">    lastCheckpointCompletionRelativeTime = clock.relativeTimeMillis();</span><br><span class="line">    </span><br><span class="line">    LOG.info(<span class="string">&quot;Completed checkpoint &#123;&#125; for job &#123;&#125; (&#123;&#125; bytes in &#123;&#125; ms).&quot;</span>, checkpointId, job,</span><br><span class="line">        completedCheckpoint.getStateSize(), completedCheckpoint.getDuration());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        builder.append(<span class="string">&quot;Checkpoint state: &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (OperatorState state : completedCheckpoint.getOperatorStates().values()) &#123;</span><br><span class="line">            builder.append(state);</span><br><span class="line">            builder.append(<span class="string">&quot;, &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Remove last two chars &quot;, &quot;</span></span><br><span class="line">        builder.setLength(builder.length() - <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        LOG.debug(builder.toString());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向所有vertices发送完成通知</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> timestamp = completedCheckpoint.getTimestamp();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (ExecutionVertex ev : tasksToCommitTo) &#123;</span><br><span class="line">        Execution ee = ev.getCurrentExecutionAttempt();</span><br><span class="line">        <span class="keyword">if</span> (ee != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 通知CK完成</span></span><br><span class="line">            ee.notifyCheckpointComplete(checkpointId, timestamp);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink源码解析之四ExecutionGraph生成</title>
    <url>/2020/05/28/Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E5%9B%9BExecutionGraph%E7%94%9F%E6%88%90/</url>
    <content><![CDATA[<blockquote>
<p>介绍JobGraph被提交之后,JobManager如何接收到该请求,以及如何生成ExecutionGraph</p>
</blockquote>
<span id="more"></span>

<h2 id="JobManager的启动"><a href="#JobManager的启动" class="headerlink" title="JobManager的启动"></a>JobManager的启动</h2><h3 id="JobSubmitHandler"><a href="#JobSubmitHandler" class="headerlink" title="JobSubmitHandler"></a>JobSubmitHandler</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">此处理程序可用于将作业提交到Flink集群</span><br><span class="line">处理请求,调用submitJob,启动JobManagerRunner</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> CompletableFuture&lt;JobSubmitResponseBody&gt; <span class="title">handleRequest</span><span class="params">(<span class="meta">@Nonnull</span> HandlerRequest&lt;JobSubmitRequestBody, EmptyMessageParameters&gt; request, <span class="meta">@Nonnull</span> DispatcherGateway gateway)</span> <span class="keyword">throws</span> RestHandlerException </span>&#123;</span><br><span class="line">    <span class="comment">// 从request中获取上传的文件</span></span><br><span class="line">    <span class="keyword">final</span> Collection&lt;File&gt; uploadedFiles = request.getUploadedFiles();</span><br><span class="line">    <span class="comment">// 获取&lt;名称,文件路径&gt;</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;String, Path&gt; nameToFile = uploadedFiles.stream().collect(Collectors.toMap(</span><br><span class="line">        File::getName,</span><br><span class="line">        Path::fromLocalFile</span><br><span class="line">    ));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果上传文件数量与名称对应数量不匹配报错</span></span><br><span class="line">    <span class="keyword">if</span> (uploadedFiles.size() != nameToFile.size()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RestHandlerException(</span><br><span class="line">            String.format(<span class="string">&quot;The number of uploaded files was %s than the expected count. Expected: %s Actual %s&quot;</span>,</span><br><span class="line">                uploadedFiles.size() &lt; nameToFile.size() ? <span class="string">&quot;lower&quot;</span> : <span class="string">&quot;higher&quot;</span>,</span><br><span class="line">                nameToFile.size(),</span><br><span class="line">                uploadedFiles.size()),</span><br><span class="line">            HttpResponseStatus.BAD_REQUEST</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取请求体</span></span><br><span class="line">    <span class="keyword">final</span> JobSubmitRequestBody requestBody = request.getRequestBody();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (requestBody.jobGraphFileName == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RestHandlerException(</span><br><span class="line">            String.format(<span class="string">&quot;The %s field must not be omitted or be null.&quot;</span>,</span><br><span class="line">                JobSubmitRequestBody.FIELD_NAME_JOB_GRAPH),</span><br><span class="line">            HttpResponseStatus.BAD_REQUEST);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载JobGraph</span></span><br><span class="line">    CompletableFuture&lt;JobGraph&gt; jobGraphFuture = loadJobGraph(requestBody, nameToFile);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取Jar</span></span><br><span class="line">    Collection&lt;Path&gt; jarFiles = getJarFilesToUpload(requestBody.jarFileNames, nameToFile);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取artifacts</span></span><br><span class="line">    Collection&lt;Tuple2&lt;String, Path&gt;&gt; artifacts = getArtifactFilesToUpload(requestBody.artifactFileNames, nameToFile);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向BLOBServer上传信息</span></span><br><span class="line">    CompletableFuture&lt;JobGraph&gt; finalizedJobGraphFuture = uploadJobGraphFiles(gateway, jobGraphFuture, jarFiles, artifacts, configuration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Ack通信,向服务端提交任务</span></span><br><span class="line">    CompletableFuture&lt;Acknowledge&gt; jobSubmissionFuture = finalizedJobGraphFuture.thenCompose(jobGraph -&gt; gateway.submitJob(jobGraph, timeout));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> jobSubmissionFuture.thenCombine(jobGraphFuture,</span><br><span class="line">        (ack, jobGraph) -&gt; <span class="keyword">new</span> JobSubmitResponseBody(<span class="string">&quot;/jobs/&quot;</span> + jobGraph.getJobID()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 接下来依旧是Dispatcher中submitJob函数调用逻辑</span></span><br><span class="line"><span class="comment">// 详情可参考上一篇</span></span><br></pre></td></tr></table></figure>

<h3 id="JobManagerRunnerImpl"><a href="#JobManagerRunnerImpl" class="headerlink" title="JobManagerRunnerImpl"></a>JobManagerRunnerImpl</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">了解一下JobManagerRunnerImpl内做了什么</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">JobManagerRunnerImpl</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobGraph jobGraph,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobMasterServiceFactory jobMasterFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> HighAvailabilityServices haServices,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> LibraryCacheManager libraryCacheManager,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Executor executor,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> FatalErrorHandler fatalErrorHandler)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.resultFuture = <span class="keyword">new</span> CompletableFuture&lt;&gt;();</span><br><span class="line">    <span class="keyword">this</span>.terminationFuture = <span class="keyword">new</span> CompletableFuture&lt;&gt;();</span><br><span class="line">    <span class="keyword">this</span>.leadershipOperation = CompletableFuture.completedFuture(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// make sure we cleanly shut down out JobManager services if initialization fails</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.jobGraph = checkNotNull(jobGraph);</span><br><span class="line">        <span class="keyword">this</span>.libraryCacheManager = checkNotNull(libraryCacheManager);</span><br><span class="line">        <span class="keyword">this</span>.executor = checkNotNull(executor);</span><br><span class="line">        <span class="keyword">this</span>.fatalErrorHandler = checkNotNull(fatalErrorHandler);</span><br><span class="line"></span><br><span class="line">        checkArgument(jobGraph.getNumberOfVertices() &gt; <span class="number">0</span>, <span class="string">&quot;The given job is empty&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// libraries and class loader first</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 向BlobLibraryCacheManager注册Job</span></span><br><span class="line">            libraryCacheManager.registerJob(</span><br><span class="line">                    jobGraph.getJobID(), jobGraph.getUserJarBlobKeys(), jobGraph.getClasspaths());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">&quot;Cannot set up the user code libraries: &quot;</span> + e.getMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取类加载器</span></span><br><span class="line">        <span class="keyword">final</span> ClassLoader userCodeLoader = libraryCacheManager.getClassLoader(jobGraph.getJobID());</span><br><span class="line">        <span class="keyword">if</span> (userCodeLoader == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">&quot;The user code class loader could not be initialized.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// high availability services next</span></span><br><span class="line">        <span class="keyword">this</span>.runningJobsRegistry = haServices.getRunningJobsRegistry();</span><br><span class="line">        <span class="comment">// 为Job获取Leader选举服务</span></span><br><span class="line">        <span class="keyword">this</span>.leaderElectionService = haServices.getJobManagerLeaderElectionService(jobGraph.getJobID());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.leaderGatewayFuture = <span class="keyword">new</span> CompletableFuture&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// now start the JobManager</span></span><br><span class="line">        <span class="comment">// 启动JobMaster</span></span><br><span class="line">        <span class="keyword">this</span>.jobMasterService = jobMasterFactory.createJobMasterService(jobGraph, <span class="keyword">this</span>, userCodeLoader);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        terminationFuture.completeExceptionally(t);</span><br><span class="line">        resultFuture.completeExceptionally(t);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobGraph.getJobID(), <span class="string">&quot;Could not set up JobManager&quot;</span>, t);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ExecutionGroup的生成"><a href="#ExecutionGroup的生成" class="headerlink" title="ExecutionGroup的生成"></a>ExecutionGroup的生成</h2><h3 id="JobMaster"><a href="#JobMaster" class="headerlink" title="JobMaster"></a>JobMaster</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 入口DefaultJobMasterServiceFactory.createJobMasterService</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobMaster <span class="title">createJobMasterService</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        JobGraph jobGraph,</span></span></span><br><span class="line"><span class="function"><span class="params">        OnCompletionActions jobCompletionActions,</span></span></span><br><span class="line"><span class="function"><span class="params">        ClassLoader userCodeClassloader)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> JobMaster(</span><br><span class="line">    rpcService,</span><br><span class="line">    jobMasterConfiguration,</span><br><span class="line">    ResourceID.generate(),</span><br><span class="line">    jobGraph,</span><br><span class="line">    haServices,</span><br><span class="line">    slotPoolFactory,</span><br><span class="line">    schedulerFactory,</span><br><span class="line">    jobManagerSharedServices,</span><br><span class="line">    heartbeatServices,</span><br><span class="line">    jobManagerJobMetricGroupFactory,</span><br><span class="line">    jobCompletionActions,</span><br><span class="line">    fatalErrorHandler,</span><br><span class="line">    userCodeClassloader,</span><br><span class="line">    schedulerNGFactory,</span><br><span class="line">    shuffleMaster,</span><br><span class="line">    lookup -&gt; <span class="keyword">new</span> JobMasterPartitionTrackerImpl(</span><br><span class="line">        jobGraph.getJobID(),</span><br><span class="line">        shuffleMaster,</span><br><span class="line">        lookup</span><br><span class="line">    ));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// JobMaster.createScheduler</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> SchedulerNG <span class="title">createScheduler</span><span class="params">(<span class="keyword">final</span> JobManagerJobMetricGroup jobManagerJobMetricGroup)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 创建SchedulerNG</span></span><br><span class="line">    <span class="keyword">return</span> schedulerNGFactory.createInstance(</span><br><span class="line">        log,</span><br><span class="line">        jobGraph,</span><br><span class="line">        backPressureStatsTracker,</span><br><span class="line">        scheduledExecutorService,</span><br><span class="line">        jobMasterConfiguration.getConfiguration(),</span><br><span class="line">        scheduler,</span><br><span class="line">        scheduledExecutorService,</span><br><span class="line">        userCodeLoader,</span><br><span class="line">        highAvailabilityServices.getCheckpointRecoveryFactory(),</span><br><span class="line">        rpcTimeout,</span><br><span class="line">        blobWriter,</span><br><span class="line">        jobManagerJobMetricGroup,</span><br><span class="line">        jobMasterConfiguration.getSlotRequestTimeout(),</span><br><span class="line">        shuffleMaster,</span><br><span class="line">        partitionTracker);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="SchedulerNGFactory"><a href="#SchedulerNGFactory" class="headerlink" title="SchedulerNGFactory"></a>SchedulerNGFactory</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SchedulerNGFactory</span><br><span class="line">    DefaultSchedulerFactory</span><br><span class="line">    LegacySchedulerFactory</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分析DefaultSchedulerFactory</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SchedulerNG <span class="title">createInstance</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Logger log,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobGraph jobGraph,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> BackPressureStatsTracker backPressureStatsTracker,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Executor ioExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Configuration jobMasterConfiguration,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> SlotProvider slotProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> ScheduledExecutorService futureExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> ClassLoader userCodeLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> CheckpointRecoveryFactory checkpointRecoveryFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Time rpcTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> BlobWriter blobWriter,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobManagerJobMetricGroup jobManagerJobMetricGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> Time slotRequestTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> ShuffleMaster&lt;?&gt; shuffleMaster,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">final</span> JobMasterPartitionTracker partitionTracker)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据JobGraph不同的调度模式获取SchedulingStrategyFactory</span></span><br><span class="line">    <span class="comment">// LAZY_FROM_SOURCES批处理</span></span><br><span class="line">    <span class="comment">// EAGER流处理</span></span><br><span class="line">    <span class="keyword">final</span> SchedulingStrategyFactory schedulingStrategyFactory = createSchedulingStrategyFactory(jobGraph.getScheduleMode());</span><br><span class="line">    <span class="comment">// 重启策略</span></span><br><span class="line">    <span class="keyword">final</span> RestartBackoffTimeStrategy restartBackoffTimeStrategy = RestartBackoffTimeStrategyFactoryLoader</span><br><span class="line">        .createRestartBackoffTimeStrategyFactory(</span><br><span class="line">            jobGraph</span><br><span class="line">                .getSerializedExecutionConfig()</span><br><span class="line">                .deserializeValue(userCodeLoader)</span><br><span class="line">                .getRestartStrategy(),</span><br><span class="line">            jobMasterConfiguration,</span><br><span class="line">            jobGraph.isCheckpointingEnabled())</span><br><span class="line">        .create();</span><br><span class="line">    log.info(<span class="string">&quot;Using restart back off time strategy &#123;&#125; for &#123;&#125; (&#123;&#125;).&quot;</span>, restartBackoffTimeStrategy, jobGraph.getName(), jobGraph.getJobID());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Slot分配策略</span></span><br><span class="line">    <span class="keyword">final</span> SlotProviderStrategy slotProviderStrategy = SlotProviderStrategy.from(</span><br><span class="line">        jobGraph.getScheduleMode(),</span><br><span class="line">        slotProvider,</span><br><span class="line">        slotRequestTimeout);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回DefaultScheduler</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DefaultScheduler(</span><br><span class="line">        log,</span><br><span class="line">        jobGraph,</span><br><span class="line">        backPressureStatsTracker,</span><br><span class="line">        ioExecutor,</span><br><span class="line">        jobMasterConfiguration,</span><br><span class="line">        slotProvider,</span><br><span class="line">        futureExecutor,</span><br><span class="line">        <span class="keyword">new</span> ScheduledExecutorServiceAdapter(futureExecutor),</span><br><span class="line">        userCodeLoader,</span><br><span class="line">        checkpointRecoveryFactory,</span><br><span class="line">        rpcTimeout,</span><br><span class="line">        blobWriter,</span><br><span class="line">        jobManagerJobMetricGroup,</span><br><span class="line">        slotRequestTimeout,</span><br><span class="line">        shuffleMaster,</span><br><span class="line">        partitionTracker,</span><br><span class="line">        schedulingStrategyFactory,</span><br><span class="line">        FailoverStrategyFactoryLoader.loadFailoverStrategyFactory(jobMasterConfiguration),</span><br><span class="line">        restartBackoffTimeStrategy,</span><br><span class="line">        <span class="keyword">new</span> DefaultExecutionVertexOperations(),</span><br><span class="line">        <span class="keyword">new</span> ExecutionVertexVersioner(),</span><br><span class="line">        <span class="keyword">new</span> DefaultExecutionSlotAllocatorFactory(slotProviderStrategy));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="DefaultScheduler-gt-SchedulerBase"><a href="#DefaultScheduler-gt-SchedulerBase" class="headerlink" title="DefaultScheduler-&gt;SchedulerBase"></a>DefaultScheduler-&gt;SchedulerBase</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">这里主要是分析其父类SchedulerBase</span><br><span class="line"></span><br><span class="line"><span class="keyword">this</span>.executionGraph = createAndRestoreExecutionGraph(jobManagerJobMetricGroup, checkNotNull(shuffleMaster), checkNotNull(partitionTracker));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建,恢复ExecutionGraph</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> ExecutionGraph <span class="title">createAndRestoreExecutionGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		JobManagerJobMetricGroup currentJobManagerJobMetricGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">		ShuffleMaster&lt;?&gt; shuffleMaster,</span></span></span><br><span class="line"><span class="function"><span class="params">		JobMasterPartitionTracker partitionTracker)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 创建ExecutionGraph</span></span><br><span class="line">    ExecutionGraph newExecutionGraph = createExecutionGraph(currentJobManagerJobMetricGroup, shuffleMaster, partitionTracker);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> CheckpointCoordinator checkpointCoordinator = newExecutionGraph.getCheckpointCoordinator();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (checkpointCoordinator != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// check whether we find a valid checkpoint</span></span><br><span class="line">        <span class="keyword">if</span> (!checkpointCoordinator.restoreLatestCheckpointedState(</span><br><span class="line">            <span class="keyword">new</span> HashSet&lt;&gt;(newExecutionGraph.getAllVertices().values()),</span><br><span class="line">            <span class="keyword">false</span>,</span><br><span class="line">            <span class="keyword">false</span>)) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// check whether we can restore from a savepoint</span></span><br><span class="line">            tryRestoreExecutionGraphFromSavepoint(newExecutionGraph, jobGraph.getSavepointRestoreSettings());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> newExecutionGraph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> ExecutionGraph <span class="title">createExecutionGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		JobManagerJobMetricGroup currentJobManagerJobMetricGroup,</span></span></span><br><span class="line"><span class="function"><span class="params">		ShuffleMaster&lt;?&gt; shuffleMaster,</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="keyword">final</span> JobMasterPartitionTracker partitionTracker)</span> <span class="keyword">throws</span> JobExecutionException, JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 失败策略</span></span><br><span class="line">    <span class="keyword">final</span> FailoverStrategy.Factory failoverStrategy = legacyScheduling ?</span><br><span class="line">        FailoverStrategyLoader.loadFailoverStrategy(jobMasterConfiguration, log) :</span><br><span class="line">        <span class="keyword">new</span> NoOpFailoverStrategy.Factory();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// buildGraph初始化ExecutionGraphBuilder</span></span><br><span class="line">    <span class="keyword">return</span> ExecutionGraphBuilder.buildGraph(</span><br><span class="line">        <span class="keyword">null</span>,</span><br><span class="line">        jobGraph,</span><br><span class="line">        jobMasterConfiguration,</span><br><span class="line">        futureExecutor,</span><br><span class="line">        ioExecutor,</span><br><span class="line">        slotProvider,</span><br><span class="line">        userCodeLoader,</span><br><span class="line">        checkpointRecoveryFactory,</span><br><span class="line">        rpcTimeout,</span><br><span class="line">        restartStrategy,</span><br><span class="line">        currentJobManagerJobMetricGroup,</span><br><span class="line">        blobWriter,</span><br><span class="line">        slotRequestTimeout,</span><br><span class="line">        log,</span><br><span class="line">        shuffleMaster,</span><br><span class="line">        partitionTracker,</span><br><span class="line">        failoverStrategy);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="ExecutionGraphBuilder"><a href="#ExecutionGraphBuilder" class="headerlink" title="ExecutionGraphBuilder"></a>ExecutionGraphBuilder</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 将JobGraph构建成ExecutionGraph</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutionGraph <span class="title">buildGraph</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		<span class="meta">@Nullable</span> ExecutionGraph prior,</span></span></span><br><span class="line"><span class="function"><span class="params">		JobGraph jobGraph,</span></span></span><br><span class="line"><span class="function"><span class="params">		Configuration jobManagerConfig,</span></span></span><br><span class="line"><span class="function"><span class="params">		ScheduledExecutorService futureExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">		Executor ioExecutor,</span></span></span><br><span class="line"><span class="function"><span class="params">		SlotProvider slotProvider,</span></span></span><br><span class="line"><span class="function"><span class="params">		ClassLoader classLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">		CheckpointRecoveryFactory recoveryFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">		Time rpcTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">		RestartStrategy restartStrategy,</span></span></span><br><span class="line"><span class="function"><span class="params">		MetricGroup metrics,</span></span></span><br><span class="line"><span class="function"><span class="params">		BlobWriter blobWriter,</span></span></span><br><span class="line"><span class="function"><span class="params">		Time allocationTimeout,</span></span></span><br><span class="line"><span class="function"><span class="params">		Logger log,</span></span></span><br><span class="line"><span class="function"><span class="params">		ShuffleMaster&lt;?&gt; shuffleMaster,</span></span></span><br><span class="line"><span class="function"><span class="params">		JobMasterPartitionTracker partitionTracker,</span></span></span><br><span class="line"><span class="function"><span class="params">		FailoverStrategy.Factory failoverStrategyFactory)</span> <span class="keyword">throws</span> JobExecutionException, JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    checkNotNull(jobGraph, <span class="string">&quot;job graph cannot be null&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> String jobName = jobGraph.getName();</span><br><span class="line">    <span class="keyword">final</span> JobID jobId = jobGraph.getJobID();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Job信息</span></span><br><span class="line">    <span class="keyword">final</span> JobInformation jobInformation = <span class="keyword">new</span> JobInformation(</span><br><span class="line">        jobId,</span><br><span class="line">        jobName,</span><br><span class="line">        jobGraph.getSerializedExecutionConfig(),</span><br><span class="line">        jobGraph.getJobConfiguration(),</span><br><span class="line">        jobGraph.getUserJarBlobKeys(),</span><br><span class="line">        jobGraph.getClasspaths());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 历史记录中保留的先前执行尝试的最大次数。</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> maxPriorAttemptsHistoryLength =</span><br><span class="line">            jobManagerConfig.getInteger(JobManagerOptions.MAX_ATTEMPTS_HISTORY_SIZE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 中间结果分区</span></span><br><span class="line">    <span class="keyword">final</span> PartitionReleaseStrategy.Factory partitionReleaseStrategyFactory =</span><br><span class="line">        PartitionReleaseStrategyFactoryLoader.loadPartitionReleaseStrategyFactory(jobManagerConfig);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create a new execution graph, if none exists so far</span></span><br><span class="line">    <span class="keyword">final</span> ExecutionGraph executionGraph;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 创建新的ExecutionGraph</span></span><br><span class="line">        executionGraph = (prior != <span class="keyword">null</span>) ? prior :</span><br><span class="line">            <span class="keyword">new</span> ExecutionGraph(</span><br><span class="line">                jobInformation,</span><br><span class="line">                futureExecutor,</span><br><span class="line">                ioExecutor,</span><br><span class="line">                rpcTimeout,</span><br><span class="line">                restartStrategy,</span><br><span class="line">                maxPriorAttemptsHistoryLength,</span><br><span class="line">                failoverStrategyFactory,</span><br><span class="line">                slotProvider,</span><br><span class="line">                classLoader,</span><br><span class="line">                blobWriter,</span><br><span class="line">                allocationTimeout,</span><br><span class="line">                partitionReleaseStrategyFactory,</span><br><span class="line">                shuffleMaster,</span><br><span class="line">                partitionTracker,</span><br><span class="line">                jobGraph.getScheduleMode());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> JobException(<span class="string">&quot;Could not create the ExecutionGraph.&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set the basic properties</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 解析JobGraph生成Json形式的执行计划</span></span><br><span class="line">        executionGraph.setJsonPlan(JsonPlanGenerator.generatePlan(jobGraph));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">        log.warn(<span class="string">&quot;Cannot create JSON plan for job&quot;</span>, t);</span><br><span class="line">        <span class="comment">// give the graph an empty plan</span></span><br><span class="line">        executionGraph.setJsonPlan(<span class="string">&quot;&#123;&#125;&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize the vertices that have a master initialization hook</span></span><br><span class="line">    <span class="comment">// file output formats create directories here, input formats create splits</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> initMasterStart = System.nanoTime();</span><br><span class="line">    log.info(<span class="string">&quot;Running initialization on master for job &#123;&#125; (&#123;&#125;).&quot;</span>, jobName, jobId);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取JobVertex,并在Master初始化</span></span><br><span class="line">    <span class="keyword">for</span> (JobVertex vertex : jobGraph.getVertices()) &#123;</span><br><span class="line">        String executableClass = vertex.getInvokableClassName();</span><br><span class="line">        <span class="keyword">if</span> (executableClass == <span class="keyword">null</span> || executableClass.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobSubmissionException(jobId,</span><br><span class="line">                    <span class="string">&quot;The vertex &quot;</span> + vertex.getID() + <span class="string">&quot; (&quot;</span> + vertex.getName() + <span class="string">&quot;) has no invokable class.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            vertex.initializeOnMaster(classLoader);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId,</span><br><span class="line">                        <span class="string">&quot;Cannot initialize task &#x27;&quot;</span> + vertex.getName() + <span class="string">&quot;&#x27;: &quot;</span> + t.getMessage(), t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    log.info(<span class="string">&quot;Successfully ran initialization on master in &#123;&#125; ms.&quot;</span>,</span><br><span class="line">            (System.nanoTime() - initMasterStart) / <span class="number">1_000_000</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// topologically sort the job vertices and attach the graph to the existing one</span></span><br><span class="line">    <span class="comment">// 获取从Source开始排序完成的DAG拓扑</span></span><br><span class="line">    List&lt;JobVertex&gt; sortedTopology = jobGraph.getVerticesSortedTopologicallyFromSources();</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;Adding &#123;&#125; vertices from job graph &#123;&#125; (&#123;&#125;).&quot;</span>, sortedTopology.size(), jobName, jobId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将JobGraph转化为ExecutionGraph操作</span></span><br><span class="line">    executionGraph.attachJobGraph(sortedTopology);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled()) &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;Successfully created execution graph from job graph &#123;&#125; (&#123;&#125;).&quot;</span>, jobName, jobId);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// configure the state checkpointing</span></span><br><span class="line">    <span class="comment">// 配置CK</span></span><br><span class="line">    JobCheckpointingSettings snapshotSettings = jobGraph.getCheckpointingSettings();</span><br><span class="line">    <span class="keyword">if</span> (snapshotSettings != <span class="keyword">null</span>) &#123;</span><br><span class="line">        List&lt;ExecutionJobVertex&gt; triggerVertices =</span><br><span class="line">                idToVertex(snapshotSettings.getVerticesToTrigger(), executionGraph);</span><br><span class="line"></span><br><span class="line">        List&lt;ExecutionJobVertex&gt; ackVertices =</span><br><span class="line">                idToVertex(snapshotSettings.getVerticesToAcknowledge(), executionGraph);</span><br><span class="line"></span><br><span class="line">        List&lt;ExecutionJobVertex&gt; confirmVertices =</span><br><span class="line">                idToVertex(snapshotSettings.getVerticesToConfirm(), executionGraph);</span><br><span class="line"></span><br><span class="line">        CompletedCheckpointStore completedCheckpoints;</span><br><span class="line">        CheckpointIDCounter checkpointIdCounter;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> maxNumberOfCheckpointsToRetain = jobManagerConfig.getInteger(</span><br><span class="line">                    CheckpointingOptions.MAX_RETAINED_CHECKPOINTS);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (maxNumberOfCheckpointsToRetain &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="comment">// warning and use 1 as the default value if the setting in</span></span><br><span class="line">                <span class="comment">// state.checkpoints.max-retained-checkpoints is not greater than 0.</span></span><br><span class="line">                log.warn(<span class="string">&quot;The setting for &#x27;&#123;&#125; : &#123;&#125;&#x27; is invalid. Using default value of &#123;&#125;&quot;</span>,</span><br><span class="line">                        CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.key(),</span><br><span class="line">                        maxNumberOfCheckpointsToRetain,</span><br><span class="line">                        CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.defaultValue());</span><br><span class="line"></span><br><span class="line">                maxNumberOfCheckpointsToRetain = CheckpointingOptions.MAX_RETAINED_CHECKPOINTS.defaultValue();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            completedCheckpoints = recoveryFactory.createCheckpointStore(jobId, maxNumberOfCheckpointsToRetain, classLoader);</span><br><span class="line">            checkpointIdCounter = recoveryFactory.createCheckpointIDCounter(jobId);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId, <span class="string">&quot;Failed to initialize high-availability checkpoint handler&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Maximum number of remembered checkpoints</span></span><br><span class="line">        <span class="keyword">int</span> historySize = jobManagerConfig.getInteger(WebOptions.CHECKPOINTS_HISTORY_SIZE);</span><br><span class="line"></span><br><span class="line">        CheckpointStatsTracker checkpointStatsTracker = <span class="keyword">new</span> CheckpointStatsTracker(</span><br><span class="line">                historySize,</span><br><span class="line">                ackVertices,</span><br><span class="line">                snapshotSettings.getCheckpointCoordinatorConfiguration(),</span><br><span class="line">                metrics);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// load the state backend from the application settings</span></span><br><span class="line">        <span class="keyword">final</span> StateBackend applicationConfiguredBackend;</span><br><span class="line">        <span class="keyword">final</span> SerializedValue&lt;StateBackend&gt; serializedAppConfigured = snapshotSettings.getDefaultStateBackend();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (serializedAppConfigured == <span class="keyword">null</span>) &#123;</span><br><span class="line">            applicationConfiguredBackend = <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                applicationConfiguredBackend = serializedAppConfigured.deserializeValue(classLoader);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException | ClassNotFoundException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId,</span><br><span class="line">                        <span class="string">&quot;Could not deserialize application-defined state backend.&quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StateBackend rootBackend;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            rootBackend = StateBackendLoader.fromApplicationOrConfigOrDefault(</span><br><span class="line">                    applicationConfiguredBackend, jobManagerConfig, classLoader, log);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">catch</span> (IllegalConfigurationException | IOException | DynamicCodeLoadingException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId, <span class="string">&quot;Could not instantiate configured state backend&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// instantiate the user-defined checkpoint hooks</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> SerializedValue&lt;MasterTriggerRestoreHook.Factory[]&gt; serializedHooks = snapshotSettings.getMasterHooks();</span><br><span class="line">        <span class="keyword">final</span> List&lt;MasterTriggerRestoreHook&lt;?&gt;&gt; hooks;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (serializedHooks == <span class="keyword">null</span>) &#123;</span><br><span class="line">            hooks = Collections.emptyList();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> MasterTriggerRestoreHook.Factory[] hookFactories;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                hookFactories = serializedHooks.deserializeValue(classLoader);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">catch</span> (IOException | ClassNotFoundException e) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> JobExecutionException(jobId, <span class="string">&quot;Could not instantiate user-defined checkpoint hooks&quot;</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">final</span> Thread thread = Thread.currentThread();</span><br><span class="line">            <span class="keyword">final</span> ClassLoader originalClassLoader = thread.getContextClassLoader();</span><br><span class="line">            thread.setContextClassLoader(classLoader);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                hooks = <span class="keyword">new</span> ArrayList&lt;&gt;(hookFactories.length);</span><br><span class="line">                <span class="keyword">for</span> (MasterTriggerRestoreHook.Factory factory : hookFactories) &#123;</span><br><span class="line">                    hooks.add(MasterHooks.wrapHook(factory.create(), classLoader));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">finally</span> &#123;</span><br><span class="line">                thread.setContextClassLoader(originalClassLoader);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> CheckpointCoordinatorConfiguration chkConfig = snapshotSettings.getCheckpointCoordinatorConfiguration();</span><br><span class="line"></span><br><span class="line">        executionGraph.enableCheckpointing(</span><br><span class="line">            chkConfig,</span><br><span class="line">            triggerVertices,</span><br><span class="line">            ackVertices,</span><br><span class="line">            confirmVertices,</span><br><span class="line">            hooks,</span><br><span class="line">            checkpointIdCounter,</span><br><span class="line">            completedCheckpoints,</span><br><span class="line">            rootBackend,</span><br><span class="line">            checkpointStatsTracker);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create all the metrics for the Execution Graph</span></span><br><span class="line">    <span class="comment">// 创建监控指标</span></span><br><span class="line">    metrics.gauge(RestartTimeGauge.METRIC_NAME, <span class="keyword">new</span> RestartTimeGauge(executionGraph));</span><br><span class="line">    metrics.gauge(DownTimeGauge.METRIC_NAME, <span class="keyword">new</span> DownTimeGauge(executionGraph));</span><br><span class="line">    metrics.gauge(UpTimeGauge.METRIC_NAME, <span class="keyword">new</span> UpTimeGauge(executionGraph));</span><br><span class="line">    <span class="comment">// 注册监控</span></span><br><span class="line">    executionGraph.getFailoverStrategy().registerMetrics(metrics);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> executionGraph;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">attachJobGraph</span><span class="params">(List&lt;JobVertex&gt; topologiallySorted)</span> <span class="keyword">throws</span> JobException </span>&#123;</span><br><span class="line"></span><br><span class="line">    assertRunningInJobMasterMainThread();</span><br><span class="line"></span><br><span class="line">    LOG.debug(<span class="string">&quot;Attaching &#123;&#125; topologically sorted vertices to existing job graph with &#123;&#125; &quot;</span> +</span><br><span class="line">            <span class="string">&quot;vertices and &#123;&#125; intermediate results.&quot;</span>,</span><br><span class="line">        topologiallySorted.size(),</span><br><span class="line">        tasks.size(),</span><br><span class="line">        intermediateResults.size());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> ArrayList&lt;ExecutionJobVertex&gt; newExecJobVertices = <span class="keyword">new</span> ArrayList&lt;&gt;(topologiallySorted.size());</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> createTimestamp = System.currentTimeMillis();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历JobVertex</span></span><br><span class="line">    <span class="keyword">for</span> (JobVertex jobVertex : topologiallySorted) &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (jobVertex.isInputVertex() &amp;&amp; !jobVertex.isStoppable()) &#123;</span><br><span class="line">            <span class="keyword">this</span>.isStoppable = <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create the execution job vertex and attach it to the graph</span></span><br><span class="line">        ExecutionJobVertex ejv = <span class="keyword">new</span> ExecutionJobVertex(</span><br><span class="line">                <span class="keyword">this</span>,</span><br><span class="line">                jobVertex, <span class="comment">// JobVertex</span></span><br><span class="line">                <span class="number">1</span>, <span class="comment">// 并行度</span></span><br><span class="line">                maxPriorAttemptsHistoryLength, <span class="comment">// 历史记录中保留的先前执行尝试的最大次数</span></span><br><span class="line">                rpcTimeout, <span class="comment">// RPC通信的超时时间</span></span><br><span class="line">                globalModVersion, <span class="comment">// 全局恢复版本,每次全局恢复时都会递增</span></span><br><span class="line">                createTimestamp); <span class="comment">// 创建时间</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理JobEdge,对每个JobEdge,获取对应的intermediateResults,并记录到本节点的输入上</span></span><br><span class="line">        <span class="comment">// 最后把每个ExecutorVertex和对应的intermediateResults关联起来</span></span><br><span class="line">        ejv.connectToPredecessors(<span class="keyword">this</span>.intermediateResults);</span><br><span class="line"></span><br><span class="line">        ExecutionJobVertex previousTask = <span class="keyword">this</span>.tasks.putIfAbsent(jobVertex.getID(), ejv);</span><br><span class="line">        <span class="keyword">if</span> (previousTask != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobException(String.format(<span class="string">&quot;Encountered two job vertices with ID %s : previous=[%s] / new=[%s]&quot;</span>,</span><br><span class="line">                jobVertex.getID(), ejv, previousTask));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (IntermediateResult res : ejv.getProducedDataSets()) &#123;</span><br><span class="line">            IntermediateResult previousDataSet = <span class="keyword">this</span>.intermediateResults.putIfAbsent(res.getId(), res);</span><br><span class="line">            <span class="keyword">if</span> (previousDataSet != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> JobException(String.format(<span class="string">&quot;Encountered two intermediate data set with ID %s : previous=[%s] / new=[%s]&quot;</span>,</span><br><span class="line">                    res.getId(), res, previousDataSet));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.verticesInCreationOrder.add(ejv);</span><br><span class="line">        <span class="keyword">this</span>.numVerticesTotal += ejv.getParallelism();</span><br><span class="line">        newExecJobVertices.add(ejv);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// the topology assigning should happen before notifying new vertices to failoverStrategy</span></span><br><span class="line">    executionTopology = <span class="keyword">new</span> DefaultExecutionTopology(<span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">    failoverStrategy.notifyNewVertices(newExecJobVertices);</span><br><span class="line"></span><br><span class="line">    partitionReleaseStrategy = partitionReleaseStrategyFactory.createInstance(getSchedulingTopology());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink系列问题</title>
    <url>/2019/09/23/Flink%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<blockquote>
<p>针对Flink的一些知识问答</p>
</blockquote>
<span id="more"></span>

<h3 id="简单介绍一下Flink"><a href="#简单介绍一下Flink" class="headerlink" title="简单介绍一下Flink"></a>简单介绍一下Flink</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Apache Flink是一个分布式大数据处理引擎，可对有限数据流和无限数据流进行有状态计算。可部署在各种集群环境，对各种大小的数据规模进行快速计算。</span><br></pre></td></tr></table></figure>

<h3 id="Flink相比传统的Spark-Streaming有什么区别？和Spark中的Structured-Streaming相比呢？Flink相比Spark-Streaming和Storm有什么优势？"><a href="#Flink相比传统的Spark-Streaming有什么区别？和Spark中的Structured-Streaming相比呢？Flink相比Spark-Streaming和Storm有什么优势？" class="headerlink" title="Flink相比传统的Spark Streaming有什么区别？和Spark中的Structured Streaming相比呢？Flink相比Spark Streaming和Storm有什么优势？"></a>Flink相比传统的Spark Streaming有什么区别？和Spark中的Structured Streaming相比呢？Flink相比Spark Streaming和Storm有什么优势？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Flink相比传统的Spark Streaming有什么区别？</span><br><span class="line">架构模型上：Spark Streaming 的task运行依赖driver 和 executor和worker，当然driver和excutor还依赖于集群管理器Standalone或者yarn等。而Flink运行时主要是JobManager、TaskManage和TaskSlot。另外一个最核心的区别是：Spark Streaming 是微批处理，运行的时候需要指定批处理的时间，每次运行 job 时处理一个批次的数据；Flink 是基于事件驱动的，事件可以理解为消息。事件驱动的应用程序是一种状态应用程序，它会从一个或者多个流中注入事件，通过触发计算更新状态，或外部动作对注入的事件作出反应。</span><br><span class="line">任务调度上：Spark Streaming的调度分为构建 DGA 图，划分 stage，生成 taskset，调度 task等步骤而Flink首先会生成 StreamGraph，接着生成 JobGraph，然后将 jobGraph 提交给 Jobmanager 由它完成 jobGraph 到 ExecutionGraph 的转变，最后由 jobManager 调度执行。</span><br><span class="line">时间机制上：flink 支持三种时间机制事件时间，注入时间，处理时间，同时支持 watermark 机制处理滞后数据。Spark Streaming 只支持处理时间，Structured streaming则支持了事件时间和watermark机制。</span><br><span class="line">容错机制上：二者保证exactly-once的方式不同。spark streaming 通过保存offset和事务的方式；Flink 则使用两阶段提交协议来解决这个问题。</span><br><span class="line"></span><br><span class="line"># 相比Spark Streaming和Storm有什么优势</span><br><span class="line">Spark streaming的本质还是一款基于Microbatch计算的引擎。这种引擎一个天生的缺点就是每个Microbatch的调度开销比较大，当我们要求越低的延迟时，额外的开销就越大。这就导致了Spark Streaming实际上不是特别适合于做秒级甚至亚秒级的计算。</span><br><span class="line">Storm在最开始的流处理中扮演了很重要的角色，可以亚秒级的响应，阿里开源的JStorm，在很多互联网公司，被广泛应用于流计算处理。Storm是一个没有批处理能力的数据流处理器，除此之外Storm只提供了非常底层的API，用户需要自己实现很多复杂的逻辑。另外，Storm在状态管理和容错能力上不足。种种原因，Storm也无法满足我们的需求。</span><br><span class="line">Flink不同于Spark，Flink是一个真正意义上的流计算引擎，和Storm类似，Flink是通过流水线数据传输实现低延迟的流处理；Flink使用了经典的Chandy-Lamport算法，能够在满足低延迟和低failover开销的基础之上，完美地解决exactly once的目标；如果要用一套引擎来统一流处理和批处理，那就必须以流处理引擎为基础。Flink还提供了SQL／TableAPI这两个API，为批和流在Query层的统一又铺平了道路。因此Flink是最合适的批和流统一的引擎；Flink在设计之初就非常在意性能相关的任务状态state和流控等关键技术的设计，这些都使得用Flink执行复杂的大规模任务时性能更胜一筹。</span><br></pre></td></tr></table></figure>

<h3 id="Flink的组件栈是怎么样的？"><a href="#Flink的组件栈是怎么样的？" class="headerlink" title="Flink的组件栈是怎么样的？"></a>Flink的组件栈是怎么样的？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># API &amp; Libraries 层</span><br><span class="line">作为分布式数据处理框架，Flink同时提供了支撑流计算和批计算的接口，同时在此基础上抽象出不同的应用类型的组件库，如基于流处理的CEP（复杂事件处理库），SQL &amp; TABLE库和基于批处理的FlinkML（机器学习库），Gelly（图处理库）等。</span><br><span class="line">API层包括构建流计算应用的DataStream API和批计算应用的DataSet API，两者都是提供给用户丰富的数据处理高级API，例如Map，FlatMap 等，同时也提供比较低级的Process Function API ，用户可以直接操作状态和时间等底层数据。</span><br><span class="line"></span><br><span class="line"># Runtime 核心层</span><br><span class="line">该层主要负责对上层不同接口提供基础服务，也是Flink 分布式计算框架的核心实现层，支持分布式Stream作业的执行、JobGraph到ExecutionGraph的映射转换、任务调度等。</span><br><span class="line">将DataStream和DataSet转成统一的可执行的Task Operator，达到在流式引擎下同时处理批量计算和流式计算的目的</span><br><span class="line"></span><br><span class="line"># 物理部署层</span><br><span class="line">该层主要涉及Flink的部署模式，目前Flink支持多种部署模式：本地、集群（Standalone &#x2F; YARN）、云（GCE &#x2F; EC2）、kubenetes。</span><br><span class="line">Flink能够通过该层支持不同平台的部署，用户可以根据需要选择使用对应的部署模式。</span><br></pre></td></tr></table></figure>

<h3 id="Flink的基础编程模型了解吗？"><a href="#Flink的基础编程模型了解吗？" class="headerlink" title="Flink的基础编程模型了解吗？"></a><a href="https://www.cnblogs.com/cxhfuujust/p/10925843.html">Flink的基础编程模型了解吗？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink 程序的基础构建单元是流（streams）与转换（transformations）。DataSet API 中使用的数据集也是一种流。数据流（stream）就是一组永远不会停止的数据记录流，而转换（transformation）是将一个或多个流作为输入，并生成一个或多个输出流的操作。</span><br><span class="line">执行时，Flink程序映射到 streaming dataflows，由流（streams）和转换操作（transformation operators）组成。每个 dataflow 从一个或多个源（source）开始，在一个或多个接收器（sink）中结束。</span><br></pre></td></tr></table></figure>

<h3 id="说说Flink架构中的角色和作用？"><a href="#说说Flink架构中的角色和作用？" class="headerlink" title="说说Flink架构中的角色和作用？"></a>说说Flink架构中的角色和作用？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink是主从架构模式，Flink集群中有主节点和从节点，另外在集群之外还有客户端节点，在这些节点</span><br><span class="line">之中又有许多内部的角色，这些角色联合运作，完成整个Flink程序的计算任务。</span><br><span class="line"></span><br><span class="line"># Client </span><br><span class="line">进行提交任务,提交完成后可以选择关闭进程或等待返回结果.不是Flink集群运行的一部分.</span><br><span class="line">当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。</span><br><span class="line">Client会将用户提交的Flink程序组装一个JobGraph， 并且是以JobGraph的形式提交的。一个JobGraph是一个Flink Dataflow，它由多个JobVertex组成的DAG。</span><br><span class="line">其中，一个JobGraph包含了一个Flink程序的如下信息：JobID、Job名称、配置信息、一组JobVertex等。</span><br><span class="line"></span><br><span class="line"># Actor System(AKKA) </span><br><span class="line">Flink内部使用AKKA角色系统来管理JobManager和TaskManager</span><br><span class="line"></span><br><span class="line"># JobManager </span><br><span class="line">主要职责是分布式执行,并协调任务做CK,协调故障恢复等,从客户端接收到任务后,首先生成优化过的执行计划,再调度到TaskManager中执行</span><br><span class="line">JobManager是Flink系统的协调者，它负责接收Flink Job，调度组成Job的多个Task的执行。同时，JobManager还负责收集Job的状态信息，并管理Flink集群中从节点TaskManager。</span><br><span class="line"></span><br><span class="line"># Scheduler </span><br><span class="line">调度器,Flink的执行者被定义为任务槽.每个任务管理器都需要管理一个或多个任务槽.在内部,Flink决定哪些任务需要共享该插槽以及哪些任务必须被放置在特定的插槽中.它通过SlotSharingGroup和CoLocationGroup完成.</span><br><span class="line"></span><br><span class="line"># Checkpoint Coordinator</span><br><span class="line">Checkpoint协调器,负责checkpoint管理执行</span><br><span class="line"></span><br><span class="line"># Memory Manager</span><br><span class="line">内存管理器,会根据配置等信息计算内存的分配</span><br><span class="line"></span><br><span class="line"># TaskManager</span><br><span class="line">主要职责是从JobManager处接收任务,并部署和启动任务,接收上游的数据并处理,TaskManager在创建之初就设置好了Slot , 每个Slot可以执行一个任务,每个TaskManager是一个进程.</span><br><span class="line">TaskManager也是一个Actor，它是实际负责执行计算的Worker，在其上执行Flink Job的一组Task。每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。</span><br><span class="line"></span><br><span class="line"># Task Slot</span><br><span class="line">任务槽，负责具体任务的执行,每一个slot是一个线程</span><br></pre></td></tr></table></figure>

<h3 id="说说Flink中常用的算子？用过哪些？"><a href="#说说Flink中常用的算子？用过哪些？" class="headerlink" title="说说Flink中常用的算子？用过哪些？"></a><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/">说说Flink中常用的算子？用过哪些？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">map</span><br><span class="line">flatMap</span><br><span class="line">filter</span><br><span class="line">keyBy</span><br><span class="line">reduce</span><br><span class="line">fold</span><br><span class="line">aggregation</span><br><span class="line">min&#x2F;minBy</span><br><span class="line">max&#x2F;maxBy</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的分区策略有哪几种？"><a href="#Flink中的分区策略有哪几种？" class="headerlink" title="Flink中的分区策略有哪几种？"></a>Flink中的分区策略有哪几种？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GlobalPartitioner,GLOBAL分区。将记录输出到下游Operator的第一个实例。</span><br><span class="line"></span><br><span class="line">ShufflePartitioner,SHUFFLE分区。将记录随机输出到下游Operator的每个实例。</span><br><span class="line"></span><br><span class="line">RebalancePartitioner,REBALANCE分区。将记录以循环的方式输出到下游Operator的每个实例。</span><br><span class="line"></span><br><span class="line">RescalePartitioner,RESCALE分区。基于上下游Operator的并行度，将记录以循环的方式输出到下游Operator的每个实例。举例: 上游并行度是2，下游是4，则上游一个并行度以循环的方式将记录输出到下游的两个并行度上;上游另一个并行度以循环的方式将记录输出到下游另两个并行度上。若上游并行度是4，下游并行度是2，则上游两个并行度将记录输出到下游一个并行度上；上游另两个并行度将记录输出到下游另一个并行度上。</span><br><span class="line"></span><br><span class="line">BroadcastPartitioner,BROADCAST分区。广播分区将上游数据集输出到下游Operator的每个实例中。适合于大数据集Join小数据集的场景。</span><br><span class="line"></span><br><span class="line">ForwardPartitioner,FORWARD分区。将记录输出到下游本地的operator实例。ForwardPartitioner分区器要求上下游算子并行度一样。上下游Operator同属一个SubTasks。</span><br><span class="line"></span><br><span class="line">KeyGroupStreamPartitioner,HASH分区。将记录按Key的Hash值输出到下游Operator实例。</span><br><span class="line"></span><br><span class="line">CustomPartitionerWrapper,CUSTOM分区。通过Partitioner实例的partition方法(自定义的)将记录输出到下游。</span><br></pre></td></tr></table></figure>

<h3 id="Flink的并行度有了解吗？Flink中设置并行度需要注意什么？"><a href="#Flink的并行度有了解吗？Flink中设置并行度需要注意什么？" class="headerlink" title="Flink的并行度有了解吗？Flink中设置并行度需要注意什么？"></a>Flink的并行度有了解吗？Flink中设置并行度需要注意什么？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink程序由多个任务（Source、Transformation、Sink）组成。任务被分成多个并行实例来执行，每个并行实例处理任务的输入数据的子集。任务的并行实例的数量称之为并行度。</span><br><span class="line"></span><br><span class="line">Flink中任务的并行度可以从多个不同层面设置：</span><br><span class="line">操作算子层面(Operator Level)、执行环境层面(Execution Environment Level)、客户端层面(Client Level)、系统层面(System Level)。</span><br><span class="line"></span><br><span class="line">Flink可以设置好几个level的parallelism，其中包括Operator Level、Execution Environment Level、Client Level、System Level</span><br><span class="line"></span><br><span class="line">在flink-conf.yaml中通过parallelism.default配置项给所有execution environments指定系统级的默认parallelism；</span><br><span class="line">在ExecutionEnvironment里头可以通过setParallelism来给operators、data sources、data sinks设置默认的parallelism；</span><br><span class="line">如果operators、data sources、data sinks自己有设置parallelism则会覆盖ExecutionEnvironment设置的parallelism。 </span><br></pre></td></tr></table></figure>

<h3 id="Flink支持哪几种重启策略？分别如何配置？"><a href="#Flink支持哪几种重启策略？分别如何配置？" class="headerlink" title="Flink支持哪几种重启策略？分别如何配置？"></a><a href="https://www.jianshu.com/p/22409ccc7905">Flink支持哪几种重启策略？分别如何配置？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">重启策略种类：</span><br><span class="line">固定延迟重启策略（Fixed Delay Restart Strategy）</span><br><span class="line">故障率重启策略（Failure Rate Restart Strategy）</span><br><span class="line">无重启策略（No Restart Strategy）</span><br><span class="line">Fallback重启策略（Fallback Restart Strategy）</span><br></pre></td></tr></table></figure>

<h3 id="Flink的分布式缓存有什么作用？如何使用？"><a href="#Flink的分布式缓存有什么作用？如何使用？" class="headerlink" title="Flink的分布式缓存有什么作用？如何使用？"></a><a href="https://www.jianshu.com/p/7770f9aec75d">Flink的分布式缓存有什么作用？如何使用？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink提供了一个分布式缓存，类似于hadoop，可以使用户在并行函数中很方便的读取本地文件，并把它放在taskmanager节点中，防止task重复拉取。</span><br><span class="line"></span><br><span class="line">此缓存的工作机制如下：程序注册一个文件或者目录(本地或者远程文件系统，例如hdfs或者s3)，通过ExecutionEnvironment注册缓存文件并为它起一个名称。</span><br><span class="line"></span><br><span class="line">当程序执行，Flink自动将文件或者目录复制到所有taskmanager节点的本地文件系统，仅会执行一次。用户可以通过这个指定的名称查找文件或者目录，然后从taskmanager节点的本地文件系统访问它。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的广播变量，使用广播变量需要注意什么事项？"><a href="#Flink中的广播变量，使用广播变量需要注意什么事项？" class="headerlink" title="Flink中的广播变量，使用广播变量需要注意什么事项？"></a><a href="https://www.jianshu.com/p/3b6698ec10d8">Flink中的广播变量，使用广播变量需要注意什么事项？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在Flink中，同一个算子可能存在若干个不同的并行实例，计算过程可能不在同一个Slot中进行，不同算子之间更是如此，因此不同算子的计算数据之间不能像Java数组之间一样互相访问，而广播变量Broadcast便是解决这种情况的。</span><br><span class="line"></span><br><span class="line">我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中对窗口的支持包括哪几种？说说他们的使用场景"><a href="#Flink中对窗口的支持包括哪几种？说说他们的使用场景" class="headerlink" title="Flink中对窗口的支持包括哪几种？说说他们的使用场景"></a>Flink中对窗口的支持包括哪几种？说说他们的使用场景</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 窗口类型</span><br><span class="line">1.flink支持两种划分窗口的方式（time和count） 如果根据时间划分窗口，那么它就是一个time-window 如果根据数据划分窗口，那么它就是一个count-window</span><br><span class="line">2.flink支持窗口的两个重要属性（size和interval）</span><br><span class="line">    如果size&#x3D;interval,那么就会形成tumbling-window(无重叠数据)</span><br><span class="line">    如果size&gt;interval,那么就会形成sliding-window(有重叠数据)</span><br><span class="line">    如果size&lt;interval,那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。</span><br><span class="line">3.通过组合可以得出四种基本窗口</span><br><span class="line">    &#96;time-tumbling-window&#96; 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5))</span><br><span class="line">    &#96;time-sliding-window&#96; 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3))</span><br><span class="line">    &#96;count-tumbling-window&#96;无重叠数据的数量窗口，设置方式举例：countWindow(5)</span><br><span class="line">    &#96;count-sliding-window&#96; 有重叠数据的数量窗口，设置方式举例：countWindow(5,3)</span><br><span class="line">4.flink支持在stream上的通过key去区分多个窗口 </span><br><span class="line"></span><br><span class="line"># 窗口的实现方式</span><br><span class="line">1.Tumbling Time Window</span><br><span class="line">假如我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成不重叠的窗口，每一个事件只能属于一个窗口。</span><br><span class="line">    &#x2F;&#x2F; 用户id和购买数量 stream</span><br><span class="line">    val counts: DataStream[(Int, Int)] &#x3D; ...</span><br><span class="line">    val tumblingCnts: DataStream[(Int, Int)] &#x3D; counts</span><br><span class="line">    &#x2F;&#x2F; 用userId分组</span><br><span class="line">    .keyBy(0)</span><br><span class="line">    &#x2F;&#x2F; 1分钟的翻滚窗口宽度</span><br><span class="line">    .timeWindow(Time.minutes(1))</span><br><span class="line">    &#x2F;&#x2F; 计算购买数量</span><br><span class="line">    .sum(1)</span><br><span class="line">2.Sliding Time Window</span><br><span class="line">我们可以每30秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。通过使用 DataStream API，我们可以这样实现</span><br><span class="line">    val slidingCnts: DataStream[(Int, Int)] &#x3D; buyCnts</span><br><span class="line">    .keyBy(0)</span><br><span class="line">    .timeWindow(Time.minutes(1), Time.seconds(30))</span><br><span class="line">    .sum(1)</span><br><span class="line">3.Tumbling Count Window</span><br><span class="line">当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window）。通过使用 DataStream API，我们可以这样实现</span><br><span class="line">    &#x2F;&#x2F; Stream of (userId, buyCnts)</span><br><span class="line">    val buyCnts: DataStream[(Int, Int)] &#x3D; ...</span><br><span class="line">    val tumblingCnts: DataStream[(Int, Int)] &#x3D; buyCnts</span><br><span class="line">    &#x2F;&#x2F; key stream by sensorId</span><br><span class="line">    .keyBy(0)</span><br><span class="line">    &#x2F;&#x2F; tumbling count window of 100 elements size</span><br><span class="line">    .countWindow(100)</span><br><span class="line">    &#x2F;&#x2F; compute the buyCnt sum</span><br><span class="line">    .sum(1)</span><br><span class="line">4.Session Window</span><br><span class="line">在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流）。</span><br><span class="line">    &#x2F;&#x2F; Stream of (userId, buyCnts)</span><br><span class="line">    val buyCnts: DataStream[(Int, Int)] &#x3D; ...</span><br><span class="line">    val sessionCnts: DataStream[(Int, Int)] &#x3D; vehicleCnts</span><br><span class="line">    .keyBy(0)</span><br><span class="line">    &#x2F;&#x2F; session window based on a 30 seconds session gap interval</span><br><span class="line">    .window(ProcessingTimeSessionWindows.withGap(Time.seconds(30)))</span><br><span class="line">    .sum(1)</span><br></pre></td></tr></table></figure>

<h3 id="Flink-中的-State-Backends是什么？有什么作用？分成哪几类？说说他们各自的优缺点？"><a href="#Flink-中的-State-Backends是什么？有什么作用？分成哪几类？说说他们各自的优缺点？" class="headerlink" title="Flink 中的 State Backends是什么？有什么作用？分成哪几类？说说他们各自的优缺点？"></a><a href="https://www.cnblogs.com/029zz010buct/p/9403283.html">Flink 中的 State Backends是什么？有什么作用？分成哪几类？说说他们各自的优缺点？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink流计算中可能有各种方式来保存状态：</span><br><span class="line">    窗口操作</span><br><span class="line">    使用了KV操作的函数</span><br><span class="line">    继承了CheckpointedFunction的函数</span><br><span class="line">    当开始做checkpointing的时候，状态会被持久化到checkpoints里来规避数据丢失和状态恢复。选择的状态存储策略不同，会导致状态持久化如何和checkpoints交互。</span><br><span class="line">    Flink内部提供了这些状态后端:</span><br><span class="line">    MemoryStateBackend</span><br><span class="line">    FsStateBackend</span><br><span class="line">    RocksDBStateBackend</span><br><span class="line">    如果没有其他配置，系统将使用MemoryStateBackend。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的时间种类有哪些？各自介绍一下？"><a href="#Flink中的时间种类有哪些？各自介绍一下？" class="headerlink" title="Flink中的时间种类有哪些？各自介绍一下？"></a><a href="https://www.jianshu.com/p/0a135391ff41">Flink中的时间种类有哪些？各自介绍一下？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink中的时间与现实世界中的时间是不一致的，在flink中被划分为事件时间，摄入时间，处理时间三种。</span><br><span class="line">如果以EventTime为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime</span><br><span class="line">如果以IngesingtTime为基准来定义时间窗口将形成IngestingTimeWindow,以source的systemTime为准。</span><br><span class="line">如果以ProcessingTime基准来定义时间窗口将形成ProcessingTimeWindow，以operator的systemTime为准。</span><br></pre></td></tr></table></figure>

<h3 id="WaterMark是什么？是用来解决什么问题？如何生成水印？水印的原理是什么？"><a href="#WaterMark是什么？是用来解决什么问题？如何生成水印？水印的原理是什么？" class="headerlink" title="WaterMark是什么？是用来解决什么问题？如何生成水印？水印的原理是什么？"></a><a href="https://www.jianshu.com/p/1c2542f11da0">WaterMark是什么？是用来解决什么问题？如何生成水印？水印的原理是什么？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Watermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳。</span><br><span class="line">Watermark是用于处理乱序事件的，处理乱序事件通常用watermark机制结合window来实现。</span><br></pre></td></tr></table></figure>

<h3 id="Flink的table和SQL熟悉吗？Table-API和SQL中TableEnvironment这个类有什么作用？"><a href="#Flink的table和SQL熟悉吗？Table-API和SQL中TableEnvironment这个类有什么作用？" class="headerlink" title="Flink的table和SQL熟悉吗？Table API和SQL中TableEnvironment这个类有什么作用？"></a>Flink的table和SQL熟悉吗？Table API和SQL中TableEnvironment这个类有什么作用？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TableEnvironment是Table API和SQL集成的核心概念。它负责：</span><br><span class="line">    1.在内部catalog中注册表</span><br><span class="line">    2.注册外部catalog</span><br><span class="line">    3.执行SQL查询</span><br><span class="line">    4.注册用户定义（标量，表或聚合）函数</span><br><span class="line">    5.将DataStream或DataSet转换为表</span><br><span class="line">    6.持有对ExecutionEnvironment或StreamExecutionEnvironment的引用 </span><br></pre></td></tr></table></figure>

<h3 id="Flink如何实现SQL解析的呢？"><a href="#Flink如何实现SQL解析的呢？" class="headerlink" title="Flink如何实现SQL解析的呢？"></a><a href="https://cloud.tencent.com/developer/article/1471612">Flink如何实现SQL解析的呢？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">StreamSQL API的执行原理如下：</span><br><span class="line">    1.用户使用对外提供Stream SQL的语法开发业务应用；</span><br><span class="line">    2.用calcite对StreamSQL进行语法检验，语法检验通过后，转换成calcite的逻辑树节点；最终形成calcite的逻辑计划；</span><br><span class="line">    3.采用Flink自定义的优化规则和calcite火山模型、启发式模型共同对逻辑树进行优化，生成最优的Flink物理计划；</span><br><span class="line">    4.对物理计划采用janino codegen生成代码，生成用低阶API DataStream 描述的流应用，提交到Flink平台执行</span><br></pre></td></tr></table></figure>

<h3 id="Flink是如何做到批处理与流处理统一的？"><a href="#Flink是如何做到批处理与流处理统一的？" class="headerlink" title="Flink是如何做到批处理与流处理统一的？"></a><a href="https://cloud.tencent.com/developer/article/1501348">Flink是如何做到批处理与流处理统一的？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink设计者认为：有限流处理是无限流处理的一种特殊情况，它只不过在某个时间点停止而已。Flink通过一个底层引擎同时支持流处理和批处理。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的数据传输模式是怎么样的？"><a href="#Flink中的数据传输模式是怎么样的？" class="headerlink" title="Flink中的数据传输模式是怎么样的？"></a><a href="https://www.cnblogs.com/029zz010buct/p/10156836.html">Flink中的数据传输模式是怎么样的？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在一个运行的application中，它的tasks在持续交换数据。TaskManager负责做数据传输。</span><br><span class="line">TaskManager的网络组件首先从缓冲buffer中收集records，然后再发送。也就是说，records并不是一个接一个的发送，而是先放入缓冲，然后再以batch的形式发送。这个技术可以高效使用网络资源，并达到高吞吐。类似于网络或磁盘 I&#x2F;O 协议中使用的缓冲技术。</span><br></pre></td></tr></table></figure>

<h3 id="Flink的容错机制知道吗？"><a href="#Flink的容错机制知道吗？" class="headerlink" title="Flink的容错机制知道吗？"></a><a href="https://www.jianshu.com/p/1fca8fb61f86">Flink的容错机制知道吗？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink基于分布式快照与可部分重发的数据源实现了容错。用户可自定义对整个Job进行快照的时间间隔，当任务失败时，Flink会将整个Job恢复到最近一次快照，并从数据源重发快照之后的数据。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的分布式快照机制是怎么样的？"><a href="#Flink中的分布式快照机制是怎么样的？" class="headerlink" title="Flink中的分布式快照机制是怎么样的？"></a>Flink中的分布式快照机制是怎么样的？</h3><p><a href="https://zhuanlan.zhihu.com/p/43536305">参考1</a> <a href="https://blog.csdn.net/u014589856/article/details/94346801">参考2</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink容错机制的核心就是持续创建分布式数据流及其状态的一致快照。这些快照在系统遇到故障时，充当可以回退的一致性检查点（checkpoint）。Lightweight Asynchronous Snapshots for Distributed Dataflows 描述了Flink创建快照的机制。此论文是受分布式快照算法 Chandy-Lamport启发，并针对Flink执行模型量身定制。</span><br></pre></td></tr></table></figure>

<h3 id="Flink是如何实现Exactly-once的？"><a href="#Flink是如何实现Exactly-once的？" class="headerlink" title="Flink是如何实现Exactly-once的？"></a><a href="https://www.jianshu.com/p/9d875f6e54f2">Flink是如何实现Exactly-once的？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink通过状态和两次提交协议来保证了端到端的exactly-once语义。</span><br></pre></td></tr></table></figure>

<h3 id="Flink的Kafka-connector是如何做到向下兼容的呢？"><a href="#Flink的Kafka-connector是如何做到向下兼容的呢？" class="headerlink" title="Flink的Kafka-connector是如何做到向下兼容的呢？"></a>Flink的Kafka-connector是如何做到向下兼容的呢？</h3><p><a href="https://www.cnblogs.com/Springmoon-venn/p/10690531.html">参考1</a> <a href="https://www.cnblogs.com/huxi2b/p/6784795.html">参考2</a> <a href="https://www.cnblogs.com/0x12345678/p/10463539.html">参考3</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在新的连接器中，Flink提供了一个基础connector模块，它是实现所有connector的核心模块，所有的connector都依赖于基础connector。</span><br><span class="line">Kafka社区也改写了Java clients底层的网络客户端代码，里面会自动地判断连接的broker端所支持client请求的最高版本，并自动创建合乎标准的请求。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的内存管理是如何做的？"><a href="#Flink中的内存管理是如何做的？" class="headerlink" title="Flink中的内存管理是如何做的？"></a><a href="https://www.cnblogs.com/ooffff/p/9508271.html">Flink中的内存管理是如何做的？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink 并不是将大量对象存在堆上，而是将对象都序列化到一个预分配的内存块上，这个内存块叫做 MemorySegment，它代表了一段固定长度的内存（默认大小为 32KB），也是 Flink 中最小的内存分配单元，并且提供了非常高效的读写方法。每条记录都会以序列化的形式存储在一个或多个MemorySegment中。</span><br><span class="line">Flink堆内存划分：</span><br><span class="line">    Network Buffers: 一定数量的32KB大小的缓存，主要用于数据的网络传输。在 TaskManager启动的时候就会分配。默认数量是2048个，可以通过 taskmanager.network.numberOfBuffers来配置。</span><br><span class="line">    Memory Manager Pool:这是一个由MemoryManager管理的，由众多MemorySegment组成的超大集合。Flink中的算法（如 sort&#x2F;shuffle&#x2F;join）会向这个内存池申请MemorySegment，将序列化后的数据存于其中，使用完后释放回内存池。默认情况下，池子占了堆内存的70% 的大小。</span><br><span class="line">    Remaining (Free) Heap: 这部分的内存是留给用户代码以及TaskManager 的数据结构使用的，可以把这里看成的新生代。</span><br><span class="line">Flink大量使用堆外内存。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的序列化是如何做的？"><a href="#Flink中的序列化是如何做的？" class="headerlink" title="Flink中的序列化是如何做的？"></a><a href="https://www.cnblogs.com/ooffff/p/9508271.html">Flink中的序列化是如何做的？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink实现了自己的序列化框架，Flink处理的数据流通常是一种类型，所以可以只保存一份对象Schema信息，节省存储空间。又因为对象类型固定，所以可以通过偏移量存取。</span><br><span class="line">Java支持任意Java或Scala类型，类型信息由TypeInformation类表示，TypeInformation支持以下几种类型：</span><br><span class="line">BasicTypeInfo:任意Java 基本类型或String类型。</span><br><span class="line">BasicArrayTypeInfo:任意Java基本类型数组或String数组。</span><br><span class="line">WritableTypeInfo:任意Hadoop Writable接口的实现类。</span><br><span class="line">TupleTypeInfo:任意的Flink Tuple类型(支持Tuple1 to Tuple25)。Flink tuples 是固定长度固定类型的Java Tuple实现。</span><br><span class="line">CaseClassTypeInfo: 任意的 Scala CaseClass(包括 Scala tuples)。</span><br><span class="line">PojoTypeInfo: 任意的 POJO (Java or Scala)，例如，Java对象的所有成员变量，要么是 public 修饰符定义，要么有 getter&#x2F;setter 方法。</span><br><span class="line">GenericTypeInfo: 任意无法匹配之前几种类型的类。</span><br><span class="line"></span><br><span class="line">针对前六种类型数据集，Flink皆可以自动生成对应的TypeSerializer，能非常高效地对数据集进行序列化和反序列化。对于最后一种数据类型，Flink会使用Kryo进行序列化和反序列化。每个TypeInformation中，都包含了serializer，类型会自动通过serializer进行序列化，然后用Java Unsafe接口写入MemorySegments。</span><br><span class="line"></span><br><span class="line">操纵二进制数据：</span><br><span class="line">Flink提供了如group、sort、join等操作，这些操作都需要访问海量数据。以sort为例:首先，Flink会从MemoryManager中申请一批 MemorySegment，用来存放排序的数据。</span><br><span class="line">这些内存会分为两部分，一个区域是用来存放所有对象完整的二进制数据。另一个区域用来存放指向完整二进制数据的指针以及定长的序列化后的key（key+pointer）。将实际的数据和point+key分开存放有两个目的。</span><br><span class="line">第一，交换定长块（key+pointer）更高效，不用交换真实的数据也不用移动其他key和pointer;</span><br><span class="line">第二，这样做是缓存友好的，因为key都是连续存储在内存中的，可以增加cache命中。排序会先比较 key 大小，这样就可以直接用二进制的 key 比较而不需要反序列化出整个对象。访问排序后的数据，可以沿着排好序的key+pointer顺序访问，通过 pointer 找到对应的真实数据。</span><br></pre></td></tr></table></figure>

<h3 id="Flink中的RPC框架选型是怎么样的？"><a href="#Flink中的RPC框架选型是怎么样的？" class="headerlink" title="Flink中的RPC框架选型是怎么样的？"></a><a href="https://www.cnblogs.com/letsfly/p/10853341.html">Flink中的RPC框架选型是怎么样的？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">对于Flink中各个组件（JobMaster、TaskManager、Dispatcher等），其底层RPC框架基于Akka实现。</span><br></pre></td></tr></table></figure>

<h3 id="Flink在使用Window时出现数据倾斜，你有什么解决办法？"><a href="#Flink在使用Window时出现数据倾斜，你有什么解决办法？" class="headerlink" title="Flink在使用Window时出现数据倾斜，你有什么解决办法？"></a><a href="https://blog.csdn.net/it_lee_j_h/article/details/88641894">Flink在使用Window时出现数据倾斜，你有什么解决办法？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">注意：这里window产生的数据倾斜指的是不同的窗口内积攒的数据量不同，主要是由源头数据的产生速度导致的差异。</span><br><span class="line">核心思路：1.重新设计key 2.在窗口计算前做预聚合</span><br></pre></td></tr></table></figure>

<h3 id="Flink-SQL在使用Groupby时出现热点数据，如何处理？"><a href="#Flink-SQL在使用Groupby时出现热点数据，如何处理？" class="headerlink" title="Flink SQL在使用Groupby时出现热点数据，如何处理？"></a><a href="https://help.aliyun.com/knowledge_detail/68645.html">Flink SQL在使用Groupby时出现热点数据，如何处理？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">数据倾斜:</span><br><span class="line">SELECT </span><br><span class="line">ID,</span><br><span class="line">COUNT(distinct NAME)</span><br><span class="line">FROM  AA</span><br><span class="line">GROUP BY ID;</span><br><span class="line"></span><br><span class="line">可以对上述SQL进行拆分</span><br><span class="line"></span><br><span class="line">去重:</span><br><span class="line">CREATE VIEW SSS AS</span><br><span class="line">SELECT</span><br><span class="line">ID,</span><br><span class="line">NAME</span><br><span class="line">FROM  AA</span><br><span class="line">GROUP BY ID,NAME;</span><br><span class="line"></span><br><span class="line">聚合:</span><br><span class="line">INSERT INTO  SS</span><br><span class="line">SELECT</span><br><span class="line">ID,</span><br><span class="line">COUNT(NAME)</span><br><span class="line">FROM  SSS</span><br><span class="line">GROUP BY ID;</span><br></pre></td></tr></table></figure>

<h3 id="现在我有Flink任务，delay极高，请问你有什么调优策略？"><a href="#现在我有Flink任务，delay极高，请问你有什么调优策略？" class="headerlink" title="现在我有Flink任务，delay极高，请问你有什么调优策略？"></a>现在我有Flink任务，delay极高，请问你有什么调优策略？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">首先要确定问题产生的原因，找到最耗时的点，确定性能瓶颈点。</span><br><span class="line">比如任务频繁反压，找到反压点。</span><br><span class="line">主要通过：资源调优、作业参数调优。</span><br><span class="line">资源调优即是对作业中的Operator的并发数（parallelism）、CPU（core）、堆内存（heap_memory）等参数进行调优。</span><br><span class="line">作业参数调优包括：并行度的设置，State的设置，checkpoint的设置。</span><br></pre></td></tr></table></figure>

<h3 id="Flink是如何处理反压的？和Spark有什么区别？Storm呢？"><a href="#Flink是如何处理反压的？和Spark有什么区别？Storm呢？" class="headerlink" title="Flink是如何处理反压的？和Spark有什么区别？Storm呢？"></a><a href="https://yq.aliyun.com/articles/64821">Flink是如何处理反压的？和Spark有什么区别？Storm呢？</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Flink 没有使用任何复杂的机制来解决反压问题，因为根本不需要那样的方案！它利用自身作为纯数据流引擎的优势来优雅地响应反压问题。</span><br><span class="line"></span><br><span class="line">Storm 是通过监控 Bolt 中的接收队列负载情况，如果超过高水位值就会将反压信息写到 Zookeeper ，Zookeeper 上的 watch 会通知该拓扑的所有 Worker 都进入反压状态，最后 Spout 停止发送 tuple。</span><br><span class="line"></span><br><span class="line">JStorm 认为直接停止 Spout 的发送太过暴力，存在大量问题。当下游出现阻塞时，上游停止发送，下游消除阻塞后，上游又开闸放水，过了一会儿，下游又阻塞，上游又限流，如此反复，整个数据流会一直处在一个颠簸状态。所以 JStorm 是通过逐级降速来进行反压的，效果会较 Storm 更为稳定，但算法也更复杂。另外 JStorm 没有引入 Zookeeper 而是通过 TopologyMaster 来协调拓扑进入反压状态，这降低了 Zookeeper 的负载。</span><br><span class="line"></span><br><span class="line">SparkStreaming的Backpressure,根据处理能力来调整输入速率，从而在流量高峰时仍能保证最大的吞吐和性能</span><br></pre></td></tr></table></figure>

<h3 id="Operator-Chains（算子链）这个概念你了解吗？Flink是如何优化的？什么情况下Operator才会chain在一起？"><a href="#Operator-Chains（算子链）这个概念你了解吗？Flink是如何优化的？什么情况下Operator才会chain在一起？" class="headerlink" title="Operator Chains（算子链）这个概念你了解吗？Flink是如何优化的？什么情况下Operator才会chain在一起？"></a>Operator Chains（算子链）这个概念你了解吗？Flink是如何优化的？什么情况下Operator才会chain在一起？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">为了更高效地分布式执行，Flink会尽可能地将operator的subtask链接（chain）在一起形成task。每个task在一个线程中执行。将operators链接成task是非常有效的优化：它能减少线程之间的切换，减少消息的序列化&#x2F;反序列化，减少数据在缓冲区的交换，减少了延迟的同时提高整体的吞吐量。</span><br><span class="line">两个operator chain在一起的的条件：</span><br><span class="line">    上下游的并行度一致</span><br><span class="line">    下游节点的入度为1 （也就是说下游节点没有来自其他节点的输入）</span><br><span class="line">    上下游节点都在同一个 slot group 中（下面会解释 slot group）</span><br><span class="line">    下游节点的 chain 策略为 ALWAYS（可以与上下游链接，map、flatmap、filter等默认是ALWAYS）</span><br><span class="line">    上游节点的 chain 策略为 ALWAYS 或 HEAD（只能与下游链接，不能与上游链接，Source默认是HEAD）</span><br><span class="line">    两个节点间数据分区方式是 forward（参考理解数据流的分区）</span><br><span class="line">    用户没有禁用 chain</span><br></pre></td></tr></table></figure>

<h3 id="讲讲一个Flink-job提交的整个流程吗？"><a href="#讲讲一个Flink-job提交的整个流程吗？" class="headerlink" title="讲讲一个Flink job提交的整个流程吗？"></a>讲讲一个Flink job提交的整个流程吗？</h3><h3 id="讲讲一个Flink-job调度和执行的流程吗？"><a href="#讲讲一个Flink-job调度和执行的流程吗？" class="headerlink" title="讲讲一个Flink job调度和执行的流程吗？"></a>讲讲一个Flink job调度和执行的流程吗？</h3><h3 id="Flink所谓”三层图”结构是哪几个”图”？它们之间是什么关系？他们之间是如何转化的？"><a href="#Flink所谓”三层图”结构是哪几个”图”？它们之间是什么关系？他们之间是如何转化的？" class="headerlink" title="Flink所谓”三层图”结构是哪几个”图”？它们之间是什么关系？他们之间是如何转化的？"></a>Flink所谓”三层图”结构是哪几个”图”？它们之间是什么关系？他们之间是如何转化的？</h3><h3 id="JobManger和TaskManager分别在集群中扮演了什么角色，说说它们都做了些什么？"><a href="#JobManger和TaskManager分别在集群中扮演了什么角色，说说它们都做了些什么？" class="headerlink" title="JobManger和TaskManager分别在集群中扮演了什么角色，说说它们都做了些什么？"></a>JobManger和TaskManager分别在集群中扮演了什么角色，说说它们都做了些什么？</h3><h3 id="简单说说Flink数据的抽象和数据的交换过程"><a href="#简单说说Flink数据的抽象和数据的交换过程" class="headerlink" title="简单说说Flink数据的抽象和数据的交换过程"></a>简单说说Flink数据的抽象和数据的交换过程</h3><h3 id="Flink的分布式快照机制是如何实现的？"><a href="#Flink的分布式快照机制是如何实现的？" class="headerlink" title="Flink的分布式快照机制是如何实现的？"></a>Flink的分布式快照机制是如何实现的？</h3><h3 id="Flink的反压是如何实现的？"><a href="#Flink的反压是如何实现的？" class="headerlink" title="Flink的反压是如何实现的？"></a>Flink的反压是如何实现的？</h3><h3 id="说说FlinkSQL是如何转化的？了解逻辑计划和和物理计划吗？FlinkSQL的维表JOIN是如何做的？了解Async-IO吗？解决了什么问题？"><a href="#说说FlinkSQL是如何转化的？了解逻辑计划和和物理计划吗？FlinkSQL的维表JOIN是如何做的？了解Async-IO吗？解决了什么问题？" class="headerlink" title="说说FlinkSQL是如何转化的？了解逻辑计划和和物理计划吗？FlinkSQL的维表JOIN是如何做的？了解Async IO吗？解决了什么问题？"></a>说说FlinkSQL是如何转化的？了解逻辑计划和和物理计划吗？FlinkSQL的维表JOIN是如何做的？了解Async IO吗？解决了什么问题？</h3>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Hudi初探</title>
    <url>/2021/05/07/Hudi%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<blockquote>
<p>从Hudi的使用到Hudi文件层次的记录,参考<a href="https://blog.csdn.net/weixin_47482194/article/details/116357831">狄杰blog</a></p>
</blockquote>
<span id="more"></span>

<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hudi.git</span><br><span class="line">mvn clean package -DskipTests</span><br><span class="line"># 详细内容可参考https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;hudi&#x2F;README.md</span><br><span class="line"></span><br><span class="line"># 最终获得Hudi捆绑包</span><br><span class="line">hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line"># 在Zeppelin配置Hudi捆绑包(这种方式会报错,放入$FLINK_HOME&#x2F;lib下)</span><br><span class="line">%flink.conf</span><br><span class="line">flink.execution.jars &#x2F;Users&#x2F;xz&#x2F;Local&#x2F;Temps&#x2F;hudi&#x2F;packaging&#x2F;hudi-flink-bundle&#x2F;target&#x2F;hudi-flink-bundle_2.11-0.9.0-SNAPSHOT.jar</span><br><span class="line"># 错误信息</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hudi.common.metrics.LocalRegistry</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Hudi使用的是Flink-1.12.*,注意版本问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要开启CK</span></span><br><span class="line">%flink.conf</span><br><span class="line">pipeline.time-characteristic EventTime</span><br><span class="line">execution.checkpointing.interval 60000</span><br><span class="line">execution.checkpointing.externalized-checkpoint-retention RETAIN_ON_CANCELLATION</span><br><span class="line">state.backend filesystem</span><br><span class="line">state.checkpoints.dir hdfs://mac:9000/flink/checkpoints</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 模拟数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> gen_data (</span><br><span class="line">    <span class="keyword">uuid</span> <span class="keyword">STRING</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">STRING</span>,</span><br><span class="line">    age  <span class="built_in">INT</span>,</span><br><span class="line">    order_time   <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>)</span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;datagen&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- CLI将结果显示模式设置为tableau,这里用zeppelin,可以注释</span></span><br><span class="line"><span class="comment">-- set execution.result-mode=tableau;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t1(</span><br><span class="line">  <span class="keyword">uuid</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>), <span class="comment">-- 要么给定uuid,要么PRIMARY KEY(field) NOT ENFORCED指定主键,否则会报错</span></span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">VARCHAR</span>(<span class="number">10</span>),</span><br><span class="line">  age <span class="built_in">INT</span>,</span><br><span class="line">  ts <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>), <span class="comment">-- ts是必须字段,在前面有介绍过,用来决定数据的新旧的</span></span><br><span class="line">  <span class="string">`partition`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>)</span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="string">`partition`</span>)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;hudi&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;path&#x27;</span> = <span class="string">&#x27;hdfs://mac:9000/t1&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;write.tasks&#x27;</span> = <span class="string">&#x27;1&#x27;</span>, <span class="comment">-- default is 4 ,required more resource</span></span><br><span class="line">  <span class="string">&#x27;compaction.tasks&#x27;</span> = <span class="string">&#x27;1&#x27;</span>, <span class="comment">-- default is 10 ,required more resource</span></span><br><span class="line">  <span class="string">&#x27;table.type&#x27;</span> = <span class="string">&#x27;MERGE_ON_READ&#x27;</span> <span class="comment">-- this creates a MERGE_ON_READ table, by default is COPY_ON_WRITE</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入初始数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> t1 <span class="keyword">VALUES</span></span><br><span class="line">  (<span class="string">&#x27;id1&#x27;</span>,<span class="string">&#x27;Danny&#x27;</span>,<span class="number">23</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:01&#x27;</span>,<span class="string">&#x27;par1&#x27;</span>),</span><br><span class="line">  (<span class="string">&#x27;id2&#x27;</span>,<span class="string">&#x27;Stephen&#x27;</span>,<span class="number">33</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:02&#x27;</span>,<span class="string">&#x27;par1&#x27;</span>),</span><br><span class="line">  (<span class="string">&#x27;id3&#x27;</span>,<span class="string">&#x27;Julian&#x27;</span>,<span class="number">53</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:03&#x27;</span>,<span class="string">&#x27;par2&#x27;</span>),</span><br><span class="line">  (<span class="string">&#x27;id4&#x27;</span>,<span class="string">&#x27;Fabian&#x27;</span>,<span class="number">31</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:04&#x27;</span>,<span class="string">&#x27;par2&#x27;</span>),</span><br><span class="line">  (<span class="string">&#x27;id5&#x27;</span>,<span class="string">&#x27;Sophia&#x27;</span>,<span class="number">18</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:05&#x27;</span>,<span class="string">&#x27;par3&#x27;</span>),</span><br><span class="line">  (<span class="string">&#x27;id6&#x27;</span>,<span class="string">&#x27;Emma&#x27;</span>,<span class="number">20</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:06&#x27;</span>,<span class="string">&#x27;par3&#x27;</span>),</span><br><span class="line">  (<span class="string">&#x27;id7&#x27;</span>,<span class="string">&#x27;Bob&#x27;</span>,<span class="number">44</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:07&#x27;</span>,<span class="string">&#x27;par4&#x27;</span>),</span><br><span class="line">  (<span class="string">&#x27;id8&#x27;</span>,<span class="string">&#x27;Han&#x27;</span>,<span class="number">56</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:08&#x27;</span>,<span class="string">&#x27;par4&#x27;</span>);</span><br><span class="line">  </span><br><span class="line"><span class="comment">-- 进行查询</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> t1;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 更新数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t1 <span class="keyword">values</span></span><br><span class="line">  (<span class="string">&#x27;id1&#x27;</span>,<span class="string">&#x27;Danny&#x27;</span>,<span class="number">27</span>,<span class="built_in">TIMESTAMP</span> <span class="string">&#x27;1970-01-01 00:00:01&#x27;</span>,<span class="string">&#x27;par1&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 流查询</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> t2(</span><br><span class="line">  <span class="keyword">uuid</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>), <span class="comment">-- you can use &#x27;PRIMARY KEY NOT ENFORCED&#x27; syntax to mark the field as record key</span></span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">VARCHAR</span>(<span class="number">10</span>),</span><br><span class="line">  age <span class="built_in">INT</span>,</span><br><span class="line">  ts <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">  <span class="string">`partition`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>)</span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="string">`partition`</span>)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;hudi&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;path&#x27;</span> = <span class="string">&#x27;hdfs://mac:9000/t2&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;table.type&#x27;</span> = <span class="string">&#x27;MERGE_ON_READ&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;read.tasks&#x27;</span> = <span class="string">&#x27;1&#x27;</span>, <span class="comment">-- default is 4 ,required more resource</span></span><br><span class="line">  <span class="string">&#x27;read.streaming.enabled&#x27;</span> = <span class="string">&#x27;true&#x27;</span>,  <span class="comment">-- this option enable the streaming read</span></span><br><span class="line">  <span class="string">&#x27;read.streaming.start-commit&#x27;</span> = <span class="string">&#x27;20210316134557&#x27;</span>, <span class="comment">-- specifies the start commit instant time</span></span><br><span class="line">  <span class="string">&#x27;read.streaming.check-interval&#x27;</span> = <span class="string">&#x27;4&#x27;</span> <span class="comment">-- specifies the check interval for finding new source commits, default 60s.</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="开始文件分析MOR"><a href="#开始文件分析MOR" class="headerlink" title="开始文件分析MOR"></a>开始文件分析MOR</h2><h3 id="文件一览"><a href="#文件一览" class="headerlink" title="文件一览"></a>文件一览</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── .hoodie</span><br><span class="line">│   ├── .aux</span><br><span class="line">│   │   ├── .bootstrap</span><br><span class="line">│   │   │   ├── .fileids</span><br><span class="line">│   │   │   └── .partitions</span><br><span class="line">│   │   ├── 20210510175131.compaction.requested</span><br><span class="line">│   │   ├── 20210510181521.compaction.requested</span><br><span class="line">│   │   ├── 20210510190422.compaction.requested</span><br><span class="line">│   │   └── 20210511091552.compaction.requested</span><br><span class="line">│   ├── .temp</span><br><span class="line">│   │   ├── 20210510175131</span><br><span class="line">│   │   │   └── par1</span><br><span class="line">│   │   │       ├── 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet.marker.CREATE</span><br><span class="line">│   │   │       ├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet.marker.CREATE</span><br><span class="line">│   │   │       └── 75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet.marker.CREATE</span><br><span class="line">│   │   ├── 20210510181521</span><br><span class="line">│   │   │   └── par1</span><br><span class="line">│   │   │       └── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510181521.parquet.marker.MERGE</span><br><span class="line">│   │   └── 20210510190422</span><br><span class="line">│   │       └── par1</span><br><span class="line">│   │           └── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510190422.parquet.marker.MERGE</span><br><span class="line">│   ├── 20210510174713.deltacommit</span><br><span class="line">│   ├── 20210510174713.deltacommit.inflight</span><br><span class="line">│   ├── 20210510174713.deltacommit.requested</span><br><span class="line">......</span><br><span class="line">│   ├── 20210510181212.rollback</span><br><span class="line">│   ├── 20210510181212.rollback.inflight</span><br><span class="line">......</span><br><span class="line">│   ├── 20210510181521.commit</span><br><span class="line">│   ├── 20210510181521.compaction.inflight</span><br><span class="line">│   ├── 20210510181521.compaction.requested</span><br><span class="line">......</span><br><span class="line">│   ├── 20210511090334.clean</span><br><span class="line">│   ├── 20210511090334.clean.inflight</span><br><span class="line">│   ├── 20210511090334.clean.requested</span><br><span class="line">......</span><br><span class="line">│   ├── archived -- 归档目录,操作未达到默认值时,没有产生对应文件</span><br><span class="line">│   └── hoodie.properties</span><br><span class="line">└── par1</span><br><span class="line">    ├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510181521.log.1_0-1-0</span><br><span class="line">    ├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510190422.log.1_0-1-0</span><br><span class="line">    ├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210511091552.log.1_0-1-2</span><br><span class="line">    ├── .hoodie_partition_metadata</span><br><span class="line">    ├── 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet</span><br><span class="line">    ├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510181521.parquet</span><br><span class="line">    ├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510190422.parquet</span><br><span class="line">    └── 75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet</span><br></pre></td></tr></table></figure>

<h3 id="hoodie文件"><a href="#hoodie文件" class="headerlink" title=".hoodie文件"></a>.hoodie文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 逐层介绍文件中都由什么构成</span><br><span class="line"># deltacommit</span><br><span class="line">cat .hoodie&#x2F;20210507163615.deltacommit</span><br><span class="line">&#123;</span><br><span class="line">  &quot;partitionToWriteStats&quot; : &#123; -- 分区明细</span><br><span class="line">    &quot;par1&quot; : [ &#123; -- 注意,这里是Json数组,我这一个分区只有一个文件所以只有一个对象</span><br><span class="line">      &quot;fileId&quot; : &quot;de05d871-e8b5-4049-beb5-bed87e50586a&quot;, -- 每一个分区中文件分片的唯一标识</span><br><span class="line">      &quot;path&quot; : &quot;par1&#x2F;.de05d871-e8b5-4049-beb5-bed87e50586a_20210507163615.log.1_0-1-0&quot;, -- 写入的日志文件路径</span><br><span class="line">      &quot;prevCommit&quot; : &quot;20210507163615&quot;, -- 上一次提交成功的标识,首次,所以指定自己</span><br><span class="line">      &quot;numWrites&quot; : 2, -- 写入记录数</span><br><span class="line">      &quot;numDeletes&quot; : 0, -- 删除记录数</span><br><span class="line">      &quot;numUpdateWrites&quot; : 0, -- 更新记录数</span><br><span class="line">      &quot;numInserts&quot; : 2, -- 插入记录数</span><br><span class="line">      &quot;totalWriteBytes&quot; : 1074, -- 总写入大小</span><br><span class="line">      &quot;totalWriteErrors&quot; : 0, --  总写入错误数</span><br><span class="line">      &quot;tempPath&quot; : null, -- 临时路径</span><br><span class="line">      &quot;partitionPath&quot; : &quot;par1&quot;, -- 分区路径</span><br><span class="line">      &quot;totalLogRecords&quot; : 0, -- 总日志记录数</span><br><span class="line">      &quot;totalLogFilesCompacted&quot; : 0, -- 总压缩日志文件数</span><br><span class="line">      &quot;totalLogSizeCompacted&quot; : 0, -- 总压缩日志大小</span><br><span class="line">      &quot;totalUpdatedRecordsCompacted&quot; : 0, -- 总压缩更新记录数</span><br><span class="line">      &quot;totalLogBlocks&quot; : 0, -- 总日志块数量</span><br><span class="line">      &quot;totalCorruptLogBlock&quot; : 0, -- 总损坏日志块数量</span><br><span class="line">      &quot;totalRollbackBlocks&quot; : 0, -- 总回滚块数量</span><br><span class="line">      &quot;fileSizeInBytes&quot; : 1074, -- 文件大小</span><br><span class="line">      &quot;minEventTime&quot; : null, -- 最小事件时间</span><br><span class="line">      &quot;maxEventTime&quot; : null, -- 最大事件时间</span><br><span class="line">      &quot;logVersion&quot; : 1, -- 日志版本</span><br><span class="line">      &quot;logOffset&quot; : 0, -- 日志偏移量</span><br><span class="line">      &quot;baseFile&quot; : &quot;&quot;, -- 基本文件,每次读取baseFile和logFiles合并,就是实时数据</span><br><span class="line">      &quot;logFiles&quot; : [ &quot;.de05d871-e8b5-4049-beb5-bed87e50586a_20210507163615.log.1_0-1-0&quot; ]</span><br><span class="line">    &#125; ],</span><br><span class="line">    &quot;par2&quot; : ...,</span><br><span class="line">    &quot;par3&quot; : ...,</span><br><span class="line">    &quot;par4&quot; : ...</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;compacted&quot; : false, -- 是否压缩合并</span><br><span class="line">  &quot;extraMetadata&quot; : &#123; -- 元数据信息</span><br><span class="line">    &quot;schema&quot; : &quot;&#123;\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;record\&quot;,\&quot;fields\&quot;:[&#123;\&quot;name\&quot;:\&quot;uuid\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;string\&quot;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;string\&quot;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;age\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;int\&quot;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;ts\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,&#123;\&quot;type\&quot;:\&quot;long\&quot;,\&quot;logicalType\&quot;:\&quot;timestamp-millis\&quot;&#125;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;partition\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;string\&quot;],\&quot;default\&quot;:null&#125;]&#125;&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;operationType&quot; : null, -- 操作类型</span><br><span class="line">  &quot;totalCreateTime&quot; : 0,</span><br><span class="line">  &quot;totalScanTime&quot; : 0,</span><br><span class="line">  &quot;totalCompactedRecordsUpdated&quot; : 0,</span><br><span class="line">  &quot;totalLogFilesCompacted&quot; : 0,</span><br><span class="line">  &quot;totalLogFilesSize&quot; : 0,</span><br><span class="line">  &quot;minAndMaxEventTime&quot; : &#123;</span><br><span class="line">    &quot;Optional.empty&quot; : &#123;</span><br><span class="line">      &quot;val&quot; : null,</span><br><span class="line">      &quot;present&quot; : false</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;fileIdAndRelativePaths&quot; : &#123; -- fileId和对应的相对路径</span><br><span class="line">    &quot;1208697c-3b27-4a48-9389-f345c137c763&quot; : &quot;par4&#x2F;.1208697c-3b27-4a48-9389-f345c137c763_20210507163615.log.1_0-1-0&quot;,</span><br><span class="line">    &quot;de05d871-e8b5-4049-beb5-bed87e50586a&quot; : &quot;par1&#x2F;.de05d871-e8b5-4049-beb5-bed87e50586a_20210507163615.log.1_0-1-0&quot;,</span><br><span class="line">    &quot;918a682e-caff-462a-93c0-8b18a178e809&quot; : &quot;par2&#x2F;.918a682e-caff-462a-93c0-8b18a178e809_20210507163615.log.1_0-1-0&quot;,</span><br><span class="line">    &quot;cfd22631-84c7-4a79-8f90-6fc13a0bbb6d&quot; : &quot;par3&#x2F;.cfd22631-84c7-4a79-8f90-6fc13a0bbb6d_20210507163615.log.1_0-1-0&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;totalRecordsDeleted&quot; : 0,</span><br><span class="line">  &quot;totalLogRecordsCompacted&quot; : 0,</span><br><span class="line">  &quot;writePartitionPaths&quot; : [ &quot;par1&quot;, &quot;par2&quot;, &quot;par3&quot;, &quot;par4&quot; ], -- 写入的分区路径</span><br><span class="line">  &quot;totalUpsertTime&quot; : 3331</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># compaction</span><br><span class="line">avro-tools tojson .hoodie&#x2F;20210510175131.compaction.requested</span><br><span class="line">&#123;</span><br><span class="line">    &quot;operations&quot;:&#123;</span><br><span class="line">        &quot;array&quot;:[</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;baseInstantTime&quot;:&#123;</span><br><span class="line">                    &quot;string&quot;:&quot;20210510174713&quot; -- 基于什么操作</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;deltaFilePaths&quot;:&#123;</span><br><span class="line">                    &quot;array&quot;:[</span><br><span class="line">                        &quot;.39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510174713.log.1_0-1-0&quot; -- log_file路径</span><br><span class="line">                    ]</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;dataFilePath&quot;:null, base_file路径,第一次所以没有</span><br><span class="line">                &quot;fileId&quot;:&#123;</span><br><span class="line">                    &quot;string&quot;:&quot;39798d42-eb9f-46c7-aece-bdbe9fed3c45&quot; -- 文件id</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;partitionPath&quot;:&#123;</span><br><span class="line">                    &quot;string&quot;:&quot;par1&quot; -- 分区</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;metrics&quot;:&#123; -- 指标</span><br><span class="line">                    &quot;map&quot;:&#123;</span><br><span class="line">                        &quot;TOTAL_LOG_FILES&quot;:&quot;1.0&quot;,</span><br><span class="line">                        &quot;TOTAL_IO_READ_MB&quot;:&quot;0.0&quot;,</span><br><span class="line">                        &quot;TOTAL_LOG_FILES_SIZE&quot;:&quot;392692.0&quot;,</span><br><span class="line">                        &quot;TOTAL_IO_WRITE_MB&quot;:&quot;120.0&quot;,</span><br><span class="line">                        &quot;TOTAL_IO_MB&quot;:&quot;120.0&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;bootstrapFilePath&quot;:null -- 引导文件</span><br><span class="line">            &#125;,</span><br><span class="line">            ...,</span><br><span class="line">            ...</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;extraMetadata&quot;:null, -- 元数据</span><br><span class="line">    &quot;version&quot;:&#123;</span><br><span class="line">        &quot;int&quot;:2 -- 版本</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># commit</span><br><span class="line">cat .hoodie&#x2F;20210510175131.commit</span><br><span class="line">&#123;</span><br><span class="line">  &quot;partitionToWriteStats&quot; : &#123;</span><br><span class="line">    &quot;par1&quot; : [ &#123;</span><br><span class="line">      &quot;fileId&quot; : &quot;39798d42-eb9f-46c7-aece-bdbe9fed3c45&quot;,</span><br><span class="line">      &quot;path&quot; : &quot;par1&#x2F;39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet&quot;, -- 合并压缩后的路径</span><br><span class="line">      &quot;prevCommit&quot; : &quot;null&quot;, -- 之前的commit</span><br><span class="line">      &quot;numWrites&quot; : 940,</span><br><span class="line">      &quot;numDeletes&quot; : 0,</span><br><span class="line">      &quot;numUpdateWrites&quot; : 0,</span><br><span class="line">      &quot;numInserts&quot; : 940,</span><br><span class="line">      &quot;totalWriteBytes&quot; : 612245,</span><br><span class="line">      &quot;totalWriteErrors&quot; : 0,</span><br><span class="line">      &quot;tempPath&quot; : null,</span><br><span class="line">      &quot;partitionPath&quot; : &quot;par1&quot;,</span><br><span class="line">      &quot;totalLogRecords&quot; : 940,</span><br><span class="line">      &quot;totalLogFilesCompacted&quot; : 1,</span><br><span class="line">      &quot;totalLogSizeCompacted&quot; : 392692,</span><br><span class="line">      &quot;totalUpdatedRecordsCompacted&quot; : 940,</span><br><span class="line">      &quot;totalLogBlocks&quot; : 4,</span><br><span class="line">      &quot;totalCorruptLogBlock&quot; : 0,</span><br><span class="line">      &quot;totalRollbackBlocks&quot; : 0,</span><br><span class="line">      &quot;fileSizeInBytes&quot; : 612245,</span><br><span class="line">      &quot;minEventTime&quot; : null,</span><br><span class="line">      &quot;maxEventTime&quot; : null</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">      &quot;fileId&quot; : &quot;75dff8c0-614a-43e9-83c7-452dfa1dc797&quot;,</span><br><span class="line">      &quot;path&quot; : &quot;par1&#x2F;75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet&quot;,</span><br><span class="line">      &quot;prevCommit&quot; : &quot;null&quot;,</span><br><span class="line">      &quot;numWrites&quot; : 300,</span><br><span class="line">      &quot;numDeletes&quot; : 0,</span><br><span class="line">      &quot;numUpdateWrites&quot; : 0,</span><br><span class="line">      &quot;numInserts&quot; : 300,</span><br><span class="line">      &quot;totalWriteBytes&quot; : 493134,</span><br><span class="line">      &quot;totalWriteErrors&quot; : 0,</span><br><span class="line">      &quot;tempPath&quot; : null,</span><br><span class="line">      &quot;partitionPath&quot; : &quot;par1&quot;,</span><br><span class="line">      &quot;totalLogRecords&quot; : 300,</span><br><span class="line">      &quot;totalLogFilesCompacted&quot; : 1,</span><br><span class="line">      &quot;totalLogSizeCompacted&quot; : 124976,</span><br><span class="line">      &quot;totalUpdatedRecordsCompacted&quot; : 300,</span><br><span class="line">      &quot;totalLogBlocks&quot; : 1,</span><br><span class="line">      &quot;totalCorruptLogBlock&quot; : 0,</span><br><span class="line">      &quot;totalRollbackBlocks&quot; : 0,</span><br><span class="line">      &quot;fileSizeInBytes&quot; : 493134,</span><br><span class="line">      &quot;minEventTime&quot; : null,</span><br><span class="line">      &quot;maxEventTime&quot; : null</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">      &quot;fileId&quot; : &quot;0a0110a8-b26b-4595-b5fb-b2388263ac54&quot;,</span><br><span class="line">      &quot;path&quot; : &quot;par1&#x2F;0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet&quot;,</span><br><span class="line">      &quot;prevCommit&quot; : &quot;null&quot;,</span><br><span class="line">      &quot;numWrites&quot; : 5,</span><br><span class="line">      &quot;numDeletes&quot; : 0,</span><br><span class="line">      &quot;numUpdateWrites&quot; : 0,</span><br><span class="line">      &quot;numInserts&quot; : 5,</span><br><span class="line">      &quot;totalWriteBytes&quot; : 437253,</span><br><span class="line">      &quot;totalWriteErrors&quot; : 0,</span><br><span class="line">      &quot;tempPath&quot; : null,</span><br><span class="line">      &quot;partitionPath&quot; : &quot;par1&quot;,</span><br><span class="line">      &quot;totalLogRecords&quot; : 5,</span><br><span class="line">      &quot;totalLogFilesCompacted&quot; : 1,</span><br><span class="line">      &quot;totalLogSizeCompacted&quot; : 2918,</span><br><span class="line">      &quot;totalUpdatedRecordsCompacted&quot; : 5,</span><br><span class="line">      &quot;totalLogBlocks&quot; : 1,</span><br><span class="line">      &quot;totalCorruptLogBlock&quot; : 0,</span><br><span class="line">      &quot;totalRollbackBlocks&quot; : 0,</span><br><span class="line">      &quot;fileSizeInBytes&quot; : 437253,</span><br><span class="line">      &quot;minEventTime&quot; : null,</span><br><span class="line">      &quot;maxEventTime&quot; : null</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;compacted&quot; : true, -- 是否合并压缩</span><br><span class="line">  &quot;extraMetadata&quot; : &#123;</span><br><span class="line">    &quot;schema&quot; : &quot;&#123;\&quot;type\&quot;:\&quot;record\&quot;,\&quot;name\&quot;:\&quot;record\&quot;,\&quot;fields\&quot;:[&#123;\&quot;name\&quot;:\&quot;uuid\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;string\&quot;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;name\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;string\&quot;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;age\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;int\&quot;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;ts\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,&#123;\&quot;type\&quot;:\&quot;long\&quot;,\&quot;logicalType\&quot;:\&quot;timestamp-millis\&quot;&#125;],\&quot;default\&quot;:null&#125;,&#123;\&quot;name\&quot;:\&quot;partition\&quot;,\&quot;type\&quot;:[\&quot;null\&quot;,\&quot;string\&quot;],\&quot;default\&quot;:null&#125;]&#125;&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;operationType&quot; : &quot;UNKNOWN&quot;,</span><br><span class="line">  &quot;totalUpsertTime&quot; : 0,</span><br><span class="line">  &quot;totalCreateTime&quot; : 0,</span><br><span class="line">  &quot;totalScanTime&quot; : 225,</span><br><span class="line">  &quot;totalCompactedRecordsUpdated&quot; : 1245,</span><br><span class="line">  &quot;totalLogFilesCompacted&quot; : 3,</span><br><span class="line">  &quot;totalLogFilesSize&quot; : 520586,</span><br><span class="line">  &quot;minAndMaxEventTime&quot; : &#123;</span><br><span class="line">    &quot;Optional.empty&quot; : &#123;</span><br><span class="line">      &quot;val&quot; : null,</span><br><span class="line">      &quot;present&quot; : false</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;fileIdAndRelativePaths&quot; : &#123; -- 压缩后的文件路径</span><br><span class="line">    &quot;39798d42-eb9f-46c7-aece-bdbe9fed3c45&quot; : &quot;par1&#x2F;39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet&quot;,</span><br><span class="line">    &quot;75dff8c0-614a-43e9-83c7-452dfa1dc797&quot; : &quot;par1&#x2F;75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet&quot;,</span><br><span class="line">    &quot;0a0110a8-b26b-4595-b5fb-b2388263ac54&quot; : &quot;par1&#x2F;0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;totalRecordsDeleted&quot; : 0,</span><br><span class="line">  &quot;totalLogRecordsCompacted&quot; : 1245,</span><br><span class="line">  &quot;writePartitionPaths&quot; : [ &quot;par1&quot; ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># rollback(取消任务再重新执行任务)</span><br><span class="line">avro-tools tojson t1&#x2F;.hoodie&#x2F;20210510181212.rollback</span><br><span class="line">&#123;</span><br><span class="line">    &quot;startRollbackTime&quot;:&quot;20210510181212&quot;, -- 开始rollback的时间</span><br><span class="line">    &quot;timeTakenInMillis&quot;:33, -- rollback耗时</span><br><span class="line">    &quot;totalFilesDeleted&quot;:0, -- 共删除文件</span><br><span class="line">    &quot;commitsRollback&quot;:[</span><br><span class="line">        &quot;20210510175331&quot; -- 回滚操作</span><br><span class="line">    ],</span><br><span class="line">    &quot;partitionMetadata&quot;:&#123; -- 元数据</span><br><span class="line">        &quot;par1&quot;:&#123;</span><br><span class="line">            &quot;partitionPath&quot;:&quot;par1&quot;,</span><br><span class="line">            &quot;successDeleteFiles&quot;:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            &quot;failedDeleteFiles&quot;:[</span><br><span class="line"></span><br><span class="line">            ],</span><br><span class="line">            &quot;rollbackLogFiles&quot;:&#123;</span><br><span class="line">                &quot;map&quot;:&#123;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;writtenLogFiles&quot;:&#123;</span><br><span class="line">                &quot;map&quot;:&#123;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;version&quot;:&#123; -- 版本</span><br><span class="line">        &quot;int&quot;:1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;instantsRollback&quot;:[ -- 回滚操作</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;commitTime&quot;:&quot;20210510175331&quot;, -- commit时间</span><br><span class="line">            &quot;action&quot;:&quot;deltacommit&quot; -- commit类型</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># clean(清理)</span><br><span class="line"># clean.requested</span><br><span class="line">&#123;</span><br><span class="line">    &quot;earliestInstantToRetain&quot;:&#123; -- 需要保留的操作,之前的操作将被清除掉</span><br><span class="line">        &quot;org.apache.hudi.avro.model.HoodieActionInstant&quot;:&#123;</span><br><span class="line">            &quot;timestamp&quot;:&quot;20210510181523&quot;,</span><br><span class="line">            &quot;action&quot;:&quot;deltacommit&quot;,</span><br><span class="line">            &quot;state&quot;:&quot;COMPLETED&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;policy&quot;:&quot;KEEP_LATEST_COMMITS&quot;, -- 清理策略,保留最新的commit</span><br><span class="line">    &quot;filesToBeDeletedPerPartition&quot;:&#123; -- 每个分区被删除的文件</span><br><span class="line">        &quot;map&quot;:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;version&quot;:&#123;</span><br><span class="line">        &quot;int&quot;:2</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;filePathsToBeDeletedPerPartition&quot;:&#123; -- 每个分区被删除的文件</span><br><span class="line">        &quot;map&quot;:&#123;</span><br><span class="line">            &quot;par1&quot;:[</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;filePath&quot;:&#123;</span><br><span class="line">                        &quot;string&quot;:&quot;hdfs:&#x2F;&#x2F;mac:9000&#x2F;t1&#x2F;par1&#x2F;39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet&quot;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &quot;isBootstrapBaseFile&quot;:&#123;</span><br><span class="line">                        &quot;boolean&quot;:false</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;filePath&quot;:&#123;</span><br><span class="line">                        &quot;string&quot;:&quot;hdfs:&#x2F;&#x2F;mac:9000&#x2F;t1&#x2F;par1&#x2F;.39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510175131.log.1_0-1-0&quot;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &quot;isBootstrapBaseFile&quot;:&#123;</span><br><span class="line">                        &quot;boolean&quot;:false</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># clean</span><br><span class="line">&#123;</span><br><span class="line">    &quot;startCleanTime&quot;:&quot;20210511090334&quot;, -- 开始清除时间</span><br><span class="line">    &quot;timeTakenInMillis&quot;:618, -- 耗时</span><br><span class="line">    &quot;totalFilesDeleted&quot;:2, -- 删除文件数</span><br><span class="line">    &quot;earliestCommitToRetain&quot;:&quot;20210510181523&quot;, -- 最早提交操作</span><br><span class="line">    &quot;partitionMetadata&quot;:&#123; -- 分区元数据</span><br><span class="line">        &quot;par1&quot;:&#123;</span><br><span class="line">            &quot;partitionPath&quot;:&quot;par1&quot;,</span><br><span class="line">            &quot;policy&quot;:&quot;KEEP_LATEST_COMMITS&quot;,</span><br><span class="line">            &quot;deletePathPatterns&quot;:[ -- 删除文件正则</span><br><span class="line">                &quot;39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet&quot;,</span><br><span class="line">                &quot;.39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510175131.log.1_0-1-0&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;successDeleteFiles&quot;:[ -- 成功删除文件</span><br><span class="line">                &quot;39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet&quot;,</span><br><span class="line">                &quot;.39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510175131.log.1_0-1-0&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;failedDeleteFiles&quot;:[</span><br><span class="line"></span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;version&quot;:&#123;</span><br><span class="line">        &quot;int&quot;:2</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;bootstrapPartitionMetadata&quot;:&#123;</span><br><span class="line">        &quot;map&quot;:&#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="aux"><a href="#aux" class="headerlink" title=".aux"></a>.aux</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── .bootstrap -- 引导文件,将已有的表转化为Hudi表的操作</span><br><span class="line">│   ├── .fileids</span><br><span class="line">│   └── .partitions</span><br><span class="line">├── 20210510175131.compaction.requested -- 和上一级目录下的压缩文件内容一致</span><br><span class="line">├── 20210510181521.compaction.requested</span><br><span class="line">├── 20210510190422.compaction.requested</span><br><span class="line">└── 20210511091552.compaction.requested</span><br></pre></td></tr></table></figure>
<h3 id="temp"><a href="#temp" class="headerlink" title=".temp"></a>.temp</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── 20210510175131 -- 首次创建</span><br><span class="line">│   └── par1</span><br><span class="line">│       ├── 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet.marker.CREATE</span><br><span class="line">│       ├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510175131.parquet.marker.CREATE</span><br><span class="line">│       └── 75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet.marker.CREATE</span><br><span class="line">├── 20210510181521 -- 第一次合并</span><br><span class="line">│   └── par1</span><br><span class="line">│       └── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510181521.parquet.marker.MERGE</span><br><span class="line">└── 20210510190422 -- 第二次合并</span><br><span class="line">    └── par1</span><br><span class="line">        └── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510190422.parquet.marker.MERGE</span><br></pre></td></tr></table></figure>
<h3 id="hoodie-properties"><a href="#hoodie-properties" class="headerlink" title="hoodie.properties"></a>hoodie.properties</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Properties saved on Mon May 10 17:47:12 CST 2021</span><br><span class="line">#Mon May 10 17:47:12 CST 2021</span><br><span class="line">hoodie.compaction.payload.class&#x3D;org.apache.hudi.common.model.OverwriteWithLatestAvroPayload -- 压缩类</span><br><span class="line">hoodie.table.name&#x3D;t1 -- 表名</span><br><span class="line">hoodie.archivelog.folder&#x3D;archived -- 归档文件</span><br><span class="line">hoodie.table.type&#x3D;MERGE_ON_READ -- 表类型</span><br><span class="line">hoodie.table.version&#x3D;1 -- 版本</span><br><span class="line">hoodie.timeline.layout.version&#x3D;1</span><br></pre></td></tr></table></figure>
<h3 id="数据文件"><a href="#数据文件" class="headerlink" title="数据文件"></a>数据文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 分为元数据,log_file和数据文件</span><br><span class="line">.</span><br><span class="line">├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510181521.log.1_0-1-0 -- log_file</span><br><span class="line">├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210510190422.log.1_0-1-0</span><br><span class="line">├── .39798d42-eb9f-46c7-aece-bdbe9fed3c45_20210511091552.log.1_0-1-2</span><br><span class="line">├── .hoodie_partition_metadata -- 元数据</span><br><span class="line">├── 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet -- 数据文件</span><br><span class="line">├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510181521.parquet</span><br><span class="line">├── 39798d42-eb9f-46c7-aece-bdbe9fed3c45_0-1-0_20210510190422.parquet</span><br><span class="line">└── 75dff8c0-614a-43e9-83c7-452dfa1dc797_0-1-0_20210510175131.parquet</span><br><span class="line"></span><br><span class="line"># .hoodie_partition_metadata</span><br><span class="line">#partition metadata</span><br><span class="line">#Mon May 10 17:47:30 CST 2021</span><br><span class="line">commitTime&#x3D;20210510174713</span><br><span class="line">partitionDepth&#x3D;1</span><br><span class="line"></span><br><span class="line"># 数据文件</span><br><span class="line">parquet-tools meta 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet &gt;&gt; meta.txt</span><br><span class="line">extra:                  org.apache.hudi.bloomfilter &#x3D; &#x2F;&#x2F;&#x2F;&#x2F;&#x2F;wAAAB4BACd9Pg.... -- 构建BloomFilter,将key值记录起来</span><br><span class="line">extra:                  hoodie_min_record_key &#x3D; 0233cab512845df34fec93538e60860bdb6dbb770731806a75eb194ca4e7dc1a13ceda88250e9f70d747697a392eb40c4cc3 -- 最小Key</span><br><span class="line">extra:                  parquet.avro.schema &#x3D; &#123;&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;record&quot;,&quot;fields&quot;:[&#123;&quot;name&quot;:&quot;_hoodie_commit_time&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_commit_seqno&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_record_key&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_partition_path&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;_hoodie_file_name&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;doc&quot;:&quot;&quot;,&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;uuid&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;name&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;age&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;int&quot;],&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;ts&quot;,&quot;type&quot;:[&quot;null&quot;,&#123;&quot;type&quot;:&quot;long&quot;,&quot;logicalType&quot;:&quot;timestamp-millis&quot;&#125;],&quot;default&quot;:null&#125;,&#123;&quot;name&quot;:&quot;partition&quot;,&quot;type&quot;:[&quot;null&quot;,&quot;string&quot;],&quot;default&quot;:null&#125;]&#125;</span><br><span class="line">extra:                  writer.model.name &#x3D; avro</span><br><span class="line">extra:                  hoodie_max_record_key &#x3D; b9ae5bdd9eca29847501c5862d330a5055ef78574260b12a19dd9a3a910f45478b46489f0ad5274c90e057ded524b1d932f6 -- 最大Key</span><br><span class="line"></span><br><span class="line">file schema:            record</span><br><span class="line">--------------------------------------------------------------------------------</span><br><span class="line">_hoodie_commit_time:    OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">_hoodie_commit_seqno:   OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">_hoodie_record_key:     OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">_hoodie_partition_path: OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">_hoodie_file_name:      OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">uuid:                   OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">name:                   OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">age:                    OPTIONAL INT32 R:0 D:1</span><br><span class="line">ts:                     OPTIONAL INT64 L:TIMESTAMP(MILLIS,true) R:0 D:1</span><br><span class="line">partition:              OPTIONAL BINARY L:STRING R:0 D:1</span><br><span class="line">extra:                  org.apache.hudi.bloomfilter &#x3D; &#x2F;&#x2F;&#x2F;&#x2F;&#x2F;wAAAB4BACd9PgAA......, num_nulls: 0]</span><br><span class="line">_hoodie_commit_seqno:    BINARY GZIP DO:0 FPO:112 SZ:86&#x2F;155&#x2F;1.80 VC:5 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 20210510175131_0_1241, max: 20210510175131_0_1245, num_nulls: 0]</span><br><span class="line">_hoodie_record_key:      BINARY GZIP DO:0 FPO:198 SZ:344&#x2F;551&#x2F;1.60 VC:5 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 0233cab512845df34fec93538e60860bdb6dbb770731806a75eb194ca4e7dc1a13ceda88250e9f70d747697a392eb40c4cc3, max: b9ae5bdd9eca29847501c5862d330a5055ef78574260b12a19dd9a3a910f45478b46489f0ad5274c90e057ded524b1d932f6, num_nulls: 0]</span><br><span class="line">_hoodie_partition_path:  BINARY GZIP DO:0 FPO:542 SZ:98&#x2F;58&#x2F;0.59 VC:5 ENC:BIT_PACKED,RLE,PLAIN_DICTIONARY ST:[min: par1, max: par1, num_nulls: 0]</span><br><span class="line">_hoodie_file_name:       BINARY GZIP DO:0 FPO:640 SZ:154&#x2F;121&#x2F;0.79 VC:5 ENC:BIT_PACKED,RLE,PLAIN_DICTIONARY ST:[min: 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet, max: 0a0110a8-b26b-4595-b5fb-b2388263ac54_0-1-0_20210510175131.parquet, num_nulls: 0]</span><br><span class="line">uuid:                    BINARY GZIP DO:0 FPO:794 SZ:344&#x2F;551&#x2F;1.60 VC:5 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 0233cab512845df34fec93538e60860bdb6dbb770731806a75eb194ca4e7dc1a13ceda88250e9f70d747697a392eb40c4cc3, max: b9ae5bdd9eca29847501c5862d330a5055ef78574260b12a19dd9a3a910f45478b46489f0ad5274c90e057ded524b1d932f6, num_nulls: 0]</span><br><span class="line">name:                    BINARY GZIP DO:0 FPO:1138 SZ:343&#x2F;551&#x2F;1.61 VC:5 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 24f402db3830a56c82a39e83b2e6278d1e629c4447cbe7ddd3edaa2eda2bfef3d2f7a241ccb61d135565c4aef105793204ff, max: e415f52b9a6bc9b9f8e71bbd493aea76c9f272f5a63ffdcd3c0cd4f5ad4911acd9bdf002226c63363b44a764c5087ff59298, num_nulls: 0]</span><br><span class="line">age:                     INT32 GZIP DO:0 FPO:1481 SZ:70&#x2F;49&#x2F;0.70 VC:5 ENC:BIT_PACKED,RLE,PLAIN ST:[min: 470087658, max: 1934377936, num_nulls: 0]</span><br><span class="line">ts:                      INT64 GZIP DO:0 FPO:1551 SZ:102&#x2F;67&#x2F;0.66 VC:5 ENC:BIT_PACKED,RLE,PLAIN_DICTIONARY ST:[min: 2021-05-10T09:48:30.721+0000, max: 2021-05-10T09:48:30.728+0000, num_nulls: 0]</span><br><span class="line">partition:               BINARY GZIP DO:0 FPO:1653 SZ:98&#x2F;58&#x2F;0.59 VC:5 ENC:BIT_PACKED,RLE,PLAIN_DICTIONARY ST:[min: par1, max: par1, num_nulls: 0]</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="开始文件分析COW"><a href="#开始文件分析COW" class="headerlink" title="开始文件分析COW"></a>开始文件分析COW</h2><h3 id="文件一览-1"><a href="#文件一览-1" class="headerlink" title="文件一览"></a>文件一览</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── .hoodie</span><br><span class="line">│   ├── .aux</span><br><span class="line">│   │   └── .bootstrap</span><br><span class="line">│   │       ├── .fileids</span><br><span class="line">│   │       └── .partitions</span><br><span class="line">│   ├── .temp</span><br><span class="line">│   ├── 20210511100234.commit</span><br><span class="line">│   ├── 20210511100234.commit.requested</span><br><span class="line">│   ├── 20210511100234.inflight</span><br><span class="line">│   ├── 20210511100304.commit</span><br><span class="line">│   ├── 20210511100304.commit.requested</span><br><span class="line">│   ├── 20210511100304.inflight</span><br><span class="line">│   ├── 20210511100402.commit</span><br><span class="line">│   ├── 20210511100402.commit.requested</span><br><span class="line">│   ├── 20210511100402.inflight</span><br><span class="line">│   ├── 20210511100503.commit.requested</span><br><span class="line">│   ├── 20210511100503.inflight</span><br><span class="line">│   ├── archived</span><br><span class="line">│   └── hoodie.properties</span><br><span class="line">└── par1</span><br><span class="line">    ├── .hoodie_partition_metadata</span><br><span class="line">    ├── 48ca22d7-d503-4073-93ec-be7a33e6e8f4_0-1-0_20210511100234.parquet</span><br><span class="line">    ├── 99aaf2d1-8e82-42d9-a982-65ed8d258f28_0-1-0_20210511100304.parquet</span><br><span class="line">    └── 99aaf2d1-8e82-42d9-a982-65ed8d258f28_0-1-0_20210511100402.parquet</span><br></pre></td></tr></table></figure>
<h3 id="hoodie文件-1"><a href="#hoodie文件-1" class="headerlink" title=".hoodie文件"></a>.hoodie文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 记录的都是commit记录</span><br><span class="line">.</span><br><span class="line">├── .aux</span><br><span class="line">│   └── .bootstrap</span><br><span class="line">│       ├── .fileids</span><br><span class="line">│       └── .partitions</span><br><span class="line">├── .temp</span><br><span class="line">├── 20210511100234.commit</span><br><span class="line">├── 20210511100234.commit.requested</span><br><span class="line">├── 20210511100234.inflight</span><br><span class="line">├── 20210511100304.commit</span><br><span class="line">├── 20210511100304.commit.requested</span><br><span class="line">├── 20210511100304.inflight</span><br><span class="line">├── 20210511100402.commit</span><br><span class="line">├── 20210511100402.commit.requested</span><br><span class="line">├── 20210511100402.inflight</span><br><span class="line">├── 20210511100503.commit.requested</span><br><span class="line">├── 20210511100503.inflight</span><br><span class="line">├── archived</span><br><span class="line">└── hoodie.properties</span><br></pre></td></tr></table></figure>
<h3 id="数据文件-1"><a href="#数据文件-1" class="headerlink" title="数据文件"></a>数据文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 未见log_file文件</span><br><span class="line">.</span><br><span class="line">├── .hoodie_partition_metadata</span><br><span class="line">├── 48ca22d7-d503-4073-93ec-be7a33e6e8f4_0-1-0_20210511100234.parquet</span><br><span class="line">├── 99aaf2d1-8e82-42d9-a982-65ed8d258f28_0-1-0_20210511100304.parquet</span><br><span class="line">└── 99aaf2d1-8e82-42d9-a982-65ed8d258f28_0-1-0_20210511100402.parquet</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hudi</tag>
      </tags>
  </entry>
  <entry>
    <title>Hudi如何集成Flink</title>
    <url>/2021/05/11/Hudi%E5%A6%82%E4%BD%95%E9%9B%86%E6%88%90Flink/</url>
    <content><![CDATA[<blockquote>
<p>直接看看hudi源码究竟做了些什么</p>
</blockquote>
<span id="more"></span>

<h2 id="配置参数的了解"><a href="#配置参数的了解" class="headerlink" title="配置参数的了解"></a>配置参数的了解</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">org.apache.hudi.configuration.FlinkOptions</span><br><span class="line">org.apache.hudi.streamer.FlinkStreamerConfig</span><br><span class="line">主要是一些可以配置的参数,对使用的时候会有帮助</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="集成开始处"><a href="#集成开始处" class="headerlink" title="集成开始处"></a>集成开始处</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">和Iceberg一致</span><br><span class="line">一般直接看resources/META-INF/services文件夹的org.apache.flink.table.factories.Factory,直接定位</span><br><span class="line">org.apache.hudi.table.HoodieTableFactory</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DynamicTableSource <span class="title">createDynamicTableSource</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">    FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(<span class="keyword">this</span>, context);</span><br><span class="line">    helper.validate();</span><br><span class="line"></span><br><span class="line">    Configuration conf = (Configuration) helper.getOptions();</span><br><span class="line">    TableSchema schema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());</span><br><span class="line">    setupConfOptions(conf, context.getObjectIdentifier().getObjectName(), context.getCatalogTable(), schema);</span><br><span class="line"></span><br><span class="line">    Path path = <span class="keyword">new</span> Path(conf.getOptional(FlinkOptions.PATH).orElseThrow(() -&gt;</span><br><span class="line">        <span class="keyword">new</span> ValidationException(<span class="string">&quot;Option [path] should not be empty.&quot;</span>)));</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> HoodieTableSource(</span><br><span class="line">        schema,</span><br><span class="line">        path,</span><br><span class="line">        context.getCatalogTable().getPartitionKeys(),</span><br><span class="line">        conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME),</span><br><span class="line">        conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 写入</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> DynamicTableSink <span class="title">createDynamicTableSink</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">    Configuration conf = FlinkOptions.fromMap(context.getCatalogTable().getOptions());</span><br><span class="line">    TableSchema schema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());</span><br><span class="line">    setupConfOptions(conf, context.getObjectIdentifier().getObjectName(), context.getCatalogTable(), schema);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> HoodieTableSink(conf, schema);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="写入做了些什么"><a href="#写入做了些什么" class="headerlink" title="写入做了些什么"></a>写入做了些什么</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># 先看下整体流程</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> SinkRuntimeProvider <span class="title">getSinkRuntimeProvider</span><span class="params">(Context context)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (DataStreamSinkProvider) dataStream -&gt; &#123;</span><br><span class="line">      <span class="comment">// 获取RowType</span></span><br><span class="line">      RowType rowType = (RowType) schema.toRowDataType().notNull().getLogicalType();</span><br><span class="line">      <span class="comment">// 获取WITH配置,决定着Sink的并行度</span></span><br><span class="line">      <span class="keyword">int</span> numWriteTasks = conf.getInteger(FlinkOptions.WRITE_TASKS);</span><br><span class="line">      StreamWriteOperatorFactory&lt;HoodieRecord&gt; operatorFactory = <span class="keyword">new</span> StreamWriteOperatorFactory&lt;&gt;(conf);</span><br><span class="line">      </span><br><span class="line">      <span class="comment">// DataStream中的RowData转换HoodieRecord</span></span><br><span class="line">      DataStream&lt;Object&gt; pipeline = dataStream</span><br><span class="line">          <span class="comment">// 转化函数</span></span><br><span class="line">          .map(<span class="keyword">new</span> RowDataToHoodieFunction&lt;&gt;(rowType, conf), TypeInformation.of(HoodieRecord.class))</span><br><span class="line">          <span class="comment">// 避免多个子任务写入一个bucket</span></span><br><span class="line">          .keyBy(HoodieRecord::getRecordKey)</span><br><span class="line">          <span class="comment">// 分配给不同的fileId</span></span><br><span class="line">          .transform(</span><br><span class="line">              <span class="string">&quot;bucket_assigner&quot;</span>,</span><br><span class="line">              TypeInformation.of(HoodieRecord.class),</span><br><span class="line">              <span class="keyword">new</span> KeyedProcessOperator&lt;&gt;(<span class="keyword">new</span> BucketAssignFunction&lt;&gt;(conf)))</span><br><span class="line">          .uid(<span class="string">&quot;uid_bucket_assigner&quot;</span>)</span><br><span class="line">          <span class="comment">// shuffle by fileId(bucket id)</span></span><br><span class="line">          .keyBy(record -&gt; record.getCurrentLocation().getFileId())</span><br><span class="line">          <span class="comment">// 写入hoodie</span></span><br><span class="line">          .transform(<span class="string">&quot;hoodie_stream_write&quot;</span>, TypeInformation.of(Object.class), operatorFactory)</span><br><span class="line">          .uid(<span class="string">&quot;uid_hoodie_stream_write&quot;</span>)</span><br><span class="line">          .setParallelism(numWriteTasks);</span><br><span class="line">      <span class="comment">// 看是否需要开启压缩合并,压缩合并自带清除操作</span></span><br><span class="line">      <span class="keyword">if</span> (StreamerUtil.needsScheduleCompaction(conf)) &#123;</span><br><span class="line">        <span class="keyword">return</span> pipeline.transform(<span class="string">&quot;compact_plan_generate&quot;</span>,</span><br><span class="line">            TypeInformation.of(CompactionPlanEvent.class),</span><br><span class="line">            <span class="keyword">new</span> CompactionPlanOperator(conf))</span><br><span class="line">            .uid(<span class="string">&quot;uid_compact_plan_generate&quot;</span>)</span><br><span class="line">            .setParallelism(<span class="number">1</span>) <span class="comment">// plan generate must be singleton</span></span><br><span class="line">            .keyBy(event -&gt; event.getOperation().hashCode())</span><br><span class="line">            .transform(<span class="string">&quot;compact_task&quot;</span>,</span><br><span class="line">                TypeInformation.of(CompactionCommitEvent.class),</span><br><span class="line">                <span class="keyword">new</span> KeyedProcessOperator&lt;&gt;(<span class="keyword">new</span> CompactFunction(conf)))</span><br><span class="line">            .setParallelism(conf.getInteger(FlinkOptions.COMPACTION_TASKS))</span><br><span class="line">            .addSink(<span class="keyword">new</span> CompactionCommitSink(conf))</span><br><span class="line">            .name(<span class="string">&quot;compact_commit&quot;</span>)</span><br><span class="line">            .setParallelism(<span class="number">1</span>); <span class="comment">// compaction commit should be singleton</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 不开启则使用clean</span></span><br><span class="line">        <span class="keyword">return</span> pipeline.addSink(<span class="keyword">new</span> CleanFunction&lt;&gt;(conf))</span><br><span class="line">            .setParallelism(<span class="number">1</span>)</span><br><span class="line">            .name(<span class="string">&quot;clean_commits&quot;</span>).uid(<span class="string">&quot;uid_clean_commits&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># RowDataToHoodieFunction</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.open(parameters);</span><br><span class="line">    <span class="comment">// Avro Schema</span></span><br><span class="line">    <span class="keyword">this</span>.avroSchema = StreamerUtil.getSourceSchema(<span class="keyword">this</span>.config);</span><br><span class="line">    <span class="comment">// 创建RowData转换Hudi的GenericRecord的converter</span></span><br><span class="line">    <span class="keyword">this</span>.converter = RowDataToAvroConverters.createConverter(<span class="keyword">this</span>.rowType);</span><br><span class="line">    <span class="comment">// 主键生成器</span></span><br><span class="line">    <span class="keyword">this</span>.keyGenerator = StreamerUtil.createKeyGenerator(FlinkOptions.flatOptions(<span class="keyword">this</span>.config));</span><br><span class="line">    <span class="comment">// 数据加载</span></span><br><span class="line">    <span class="keyword">this</span>.payloadCreation = PayloadCreation.instance(config);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 每来一条数据都会执行map方法,进行转换成HoodieRecord</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> O <span class="title">map</span><span class="params">(I i)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (O) toHoodieRecord(i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;rawtypes&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> HoodieRecord <span class="title">toHoodieRecord</span><span class="params">(I record)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    </span><br><span class="line">    GenericRecord gr = (GenericRecord) <span class="keyword">this</span>.converter.convert(<span class="keyword">this</span>.avroSchema, record);</span><br><span class="line">    <span class="keyword">final</span> HoodieKey hoodieKey = keyGenerator.getKey(gr);</span><br><span class="line">    <span class="comment">// 是否删除数据</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">boolean</span> isDelete = record.getRowKind() == RowKind.DELETE;</span><br><span class="line">    <span class="comment">// 创建Payload</span></span><br><span class="line">    HoodieRecordPayload payload = payloadCreation.createPayload(gr, isDelete);</span><br><span class="line">    <span class="comment">// Key+Payload组装成HoodieRecord</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> HoodieRecord&lt;&gt;(hoodieKey, payload);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> HoodieRecordPayload&lt;?&gt; createPayload(GenericRecord record, <span class="keyword">boolean</span> isDelete) <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">// 是否合并,由write.insert.drop.duplicates,write.operation决定</span></span><br><span class="line">    <span class="keyword">if</span> (shouldCombine) &#123;</span><br><span class="line">        ValidationUtils.checkState(preCombineField != <span class="keyword">null</span>);</span><br><span class="line">        <span class="comment">// 将重复数据进行合并,根据时间字段进行合并</span></span><br><span class="line">        Comparable&lt;?&gt; orderingVal = (Comparable&lt;?&gt;) HoodieAvroUtils.getNestedFieldVal(record,</span><br><span class="line">            preCombineField, <span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">return</span> (HoodieRecordPayload&lt;?&gt;) constructor.newInstance(</span><br><span class="line">            isDelete ? <span class="keyword">null</span> : record, orderingVal);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> (HoodieRecordPayload&lt;?&gt;) <span class="keyword">this</span>.constructor.newInstance(Option.of(record));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># BucketAssignFunction</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.open(parameters);</span><br><span class="line">    HoodieWriteConfig writeConfig = StreamerUtil.getHoodieClientConfig(<span class="keyword">this</span>.conf);</span><br><span class="line">    <span class="keyword">this</span>.hadoopConf = StreamerUtil.getHadoopConf();</span><br><span class="line">    <span class="comment">// Hadoop+FlinkRuntimeContext</span></span><br><span class="line">    <span class="keyword">this</span>.context = <span class="keyword">new</span> HoodieFlinkEngineContext(</span><br><span class="line">        <span class="keyword">new</span> SerializableConfiguration(<span class="keyword">this</span>.hadoopConf),</span><br><span class="line">        <span class="keyword">new</span> FlinkTaskContextSupplier(getRuntimeContext()));</span><br><span class="line">    <span class="comment">// Bucket分配器</span></span><br><span class="line">    <span class="keyword">this</span>.bucketAssigner = BucketAssigners.create(</span><br><span class="line">        getRuntimeContext().getIndexOfThisSubtask(), <span class="comment">// 当前子任务</span></span><br><span class="line">        getRuntimeContext().getNumberOfParallelSubtasks(), <span class="comment">// 子任务并行度</span></span><br><span class="line">        WriteOperationType.isOverwrite(WriteOperationType.fromValue(conf.getString(FlinkOptions.OPERATION))),</span><br><span class="line">        HoodieTableType.valueOf(conf.getString(FlinkOptions.TABLE_TYPE)),</span><br><span class="line">        context,</span><br><span class="line">        writeConfig);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> BucketAssigner <span class="title">create</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">int</span> taskID,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">int</span> numTasks,</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">boolean</span> isOverwrite,</span></span></span><br><span class="line"><span class="function"><span class="params">  HoodieTableType tableType,</span></span></span><br><span class="line"><span class="function"><span class="params">  HoodieFlinkEngineContext context,</span></span></span><br><span class="line"><span class="function"><span class="params">  HoodieWriteConfig config)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (isOverwrite) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> OverwriteBucketAssigner(taskID, numTasks, context, config);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">switch</span> (tableType) &#123;</span><br><span class="line">      <span class="comment">// 不同表类型</span></span><br><span class="line">      <span class="keyword">case</span> COPY_ON_WRITE:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> BucketAssigner(taskID, numTasks, context, config);</span><br><span class="line">      <span class="keyword">case</span> MERGE_ON_READ:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> DeltaBucketAssigner(taskID, numTasks, context, config);</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(I value, Context ctx, Collector&lt;O&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 将Record给BucketAssigner</span></span><br><span class="line">    <span class="comment">// 2. 查看Location的状态,有Location,发送</span></span><br><span class="line">    <span class="comment">// 3. 如果是INSERT,则BuckerAssigner确定位置,然后发送</span></span><br><span class="line">    HoodieRecord&lt;?&gt; record = (HoodieRecord&lt;?&gt;) value;</span><br><span class="line">    <span class="keyword">final</span> HoodieKey hoodieKey = record.getKey();</span><br><span class="line">    <span class="keyword">final</span> BucketInfo bucketInfo;</span><br><span class="line">    <span class="keyword">final</span> HoodieRecordLocation location;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 数据集可能很大,处理会阻塞,默认情况下禁用</span></span><br><span class="line">    <span class="keyword">if</span> (bootstrapIndex &amp;&amp; !partitionLoadState.contains(hoodieKey.getPartitionPath())) &#123;</span><br><span class="line">      <span class="comment">// 如果从未加载分区记录,先加载记录</span></span><br><span class="line">      loadRecords(hoodieKey.getPartitionPath());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 只有更改的记录才需要查找位置的索引,仅追加的记录始终被识别为插入</span></span><br><span class="line">    <span class="keyword">if</span> (isChangingRecords &amp;&amp; <span class="keyword">this</span>.indexState.contains(hoodieKey)) &#123;</span><br><span class="line">      <span class="comment">// 设置Instant为U,bucket标记为更新</span></span><br><span class="line">      location = <span class="keyword">new</span> HoodieRecordLocation(<span class="string">&quot;U&quot;</span>, <span class="keyword">this</span>.indexState.get(hoodieKey).getFileId());</span><br><span class="line">      <span class="keyword">this</span>.bucketAssigner.addUpdate(record.getPartitionPath(), location.getFileId());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      bucketInfo = <span class="keyword">this</span>.bucketAssigner.addInsert(hoodieKey.getPartitionPath());</span><br><span class="line">      <span class="keyword">switch</span> (bucketInfo.getBucketType()) &#123;</span><br><span class="line">        <span class="keyword">case</span> INSERT:</span><br><span class="line">          <span class="comment">// INSERT bucket,Instant为I,下游操作可以检查Instant,知道是否是INSERT bucket.</span></span><br><span class="line">          location = <span class="keyword">new</span> HoodieRecordLocation(<span class="string">&quot;I&quot;</span>, bucketInfo.getFileIdPrefix());</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> UPDATE:</span><br><span class="line">          location = <span class="keyword">new</span> HoodieRecordLocation(<span class="string">&quot;U&quot;</span>, bucketInfo.getFileIdPrefix());</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> AssertionError();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (isChangingRecords) &#123;</span><br><span class="line">        <span class="keyword">this</span>.indexState.put(hoodieKey, location);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    record.unseal();</span><br><span class="line">    record.setCurrentLocation(location);</span><br><span class="line">    record.seal();</span><br><span class="line">    out.collect((O) record);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># DeltaBucketAssigner---&gt;BucketAssigner</span><br><span class="line">主要还是看父类操作,子类只是获取小文件列表</span><br><span class="line"><span class="function"><span class="keyword">public</span> BucketInfo <span class="title">addUpdate</span><span class="params">(String partitionPath, String fileIdHint)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> String key = StreamerUtil.generateBucketKey(partitionPath, fileIdHint);</span><br><span class="line">    <span class="keyword">if</span> (!bucketInfoMap.containsKey(key)) &#123;</span><br><span class="line">      BucketInfo bucketInfo = <span class="keyword">new</span> BucketInfo(BucketType.UPDATE, fileIdHint, partitionPath);</span><br><span class="line">      bucketInfoMap.put(key, bucketInfo);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> bucketInfoMap.get(key);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> BucketInfo <span class="title">addInsert</span><span class="params">(String partitionPath)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 对于新的插入,根据每个分区有多少条记录来计算Bucket</span></span><br><span class="line">    List&lt;SmallFile&gt; smallFiles = getSmallFilesForPartition(partitionPath);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 先插入到小文件中</span></span><br><span class="line">    <span class="keyword">for</span> (SmallFile smallFile : smallFiles) &#123;</span><br><span class="line">      <span class="keyword">final</span> String key = StreamerUtil.generateBucketKey(partitionPath, smallFile.location.getFileId());</span><br><span class="line">      SmallFileAssignState assignState = smallFileAssignStates.get(key);</span><br><span class="line">      <span class="keyword">assert</span> assignState != <span class="keyword">null</span>;</span><br><span class="line">      <span class="keyword">if</span> (assignState.canAssign()) &#123;</span><br><span class="line">        assignState.assign();</span><br><span class="line">        <span class="comment">// 创建新的Bucket,或者重用现有Bucket</span></span><br><span class="line">        BucketInfo bucketInfo;</span><br><span class="line">        <span class="keyword">if</span> (bucketInfoMap.containsKey(key)) &#123;</span><br><span class="line">          <span class="comment">// 向现有UpdateBucket分配插入</span></span><br><span class="line">          bucketInfo = bucketInfoMap.get(key);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          bucketInfo = addUpdate(partitionPath, smallFile.location.getFileId());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> bucketInfo;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建新的InsertBucket</span></span><br><span class="line">    <span class="keyword">if</span> (newFileAssignStates.containsKey(partitionPath)) &#123;</span><br><span class="line">      NewFileAssignState newFileAssignState = newFileAssignStates.get(partitionPath);</span><br><span class="line">      <span class="keyword">if</span> (newFileAssignState.canAssign()) &#123;</span><br><span class="line">        newFileAssignState.assign();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">final</span> String key = StreamerUtil.generateBucketKey(partitionPath, newFileAssignState.fileId);</span><br><span class="line">      <span class="keyword">return</span> bucketInfoMap.get(key);</span><br><span class="line">    &#125;</span><br><span class="line">    BucketInfo bucketInfo = <span class="keyword">new</span> BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);</span><br><span class="line">    <span class="keyword">final</span> String key = StreamerUtil.generateBucketKey(partitionPath, bucketInfo.getFileIdPrefix());</span><br><span class="line">    bucketInfoMap.put(key, bucketInfo);</span><br><span class="line">    newFileAssignStates.put(partitionPath, <span class="keyword">new</span> NewFileAssignState(bucketInfo.getFileIdPrefix(), insertRecordsPerBucket));</span><br><span class="line">    <span class="keyword">return</span> bucketInfo;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># StreamWriteOperatorFactory</span><br><span class="line"><span class="comment">// 创建StreamWriteOperator</span></span><br><span class="line"><span class="comment">// 提供StreamWriteOperatorCoordinator</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line"><span class="keyword">public</span> &lt;T extends StreamOperator&lt;Object&gt;&gt; <span class="function">T <span class="title">createStreamOperator</span><span class="params">(StreamOperatorParameters&lt;Object&gt; parameters)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> OperatorID operatorID = parameters.getStreamConfig().getOperatorID();</span><br><span class="line">    <span class="keyword">final</span> OperatorEventDispatcher eventDispatcher = parameters.getOperatorEventDispatcher();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">this</span>.operator.setOperatorEventGateway(eventDispatcher.getOperatorEventGateway(operatorID));</span><br><span class="line">    <span class="keyword">this</span>.operator.setup(parameters.getContainingTask(), parameters.getStreamConfig(), parameters.getOutput());</span><br><span class="line">    <span class="keyword">this</span>.operator.setProcessingTimeService(<span class="keyword">this</span>.processingTimeService);</span><br><span class="line">    eventDispatcher.registerEventHandler(operatorID, operator);</span><br><span class="line">    <span class="keyword">return</span> (T) operator;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> OperatorCoordinator.<span class="function">Provider <span class="title">getCoordinatorProvider</span><span class="params">(String s, OperatorID operatorID)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> StreamWriteOperatorCoordinator.Provider(operatorID, <span class="keyword">this</span>.conf);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># StreamWriteOperator</span><br><span class="line"><span class="comment">// 指定StreamWriteFunction</span></span><br><span class="line"><span class="comment">// 设置OperatorEventGateway</span></span><br><span class="line"></span><br><span class="line"># StreamWriteFunction</span><br><span class="line"><span class="comment">// 处理数据,缓存数据,写出数据,完成后发送事件给OperatorCoordinator</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">bufferRecord</span><span class="params">(I value)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 计算BucketID</span></span><br><span class="line">    <span class="keyword">final</span> String bucketID = getBucketID(value);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取Bucket</span></span><br><span class="line">    DataBucket bucket = <span class="keyword">this</span>.buckets.computeIfAbsent(bucketID,</span><br><span class="line">        k -&gt; <span class="keyword">new</span> DataBucket(<span class="keyword">this</span>.config.getDouble(FlinkOptions.WRITE_BATCH_SIZE)));</span><br><span class="line">    <span class="keyword">boolean</span> flushBucket = bucket.detector.detect(value);</span><br><span class="line">    <span class="keyword">boolean</span> flushBuffer = <span class="keyword">this</span>.tracer.trace(bucket.detector.lastRecordSize);</span><br><span class="line">    <span class="comment">// 判断是否需要Flush数据</span></span><br><span class="line">    <span class="keyword">if</span> (flushBucket) &#123;</span><br><span class="line">      flushBucket(bucket);</span><br><span class="line">      <span class="keyword">this</span>.tracer.countDown(bucket.detector.totalSize);</span><br><span class="line">      bucket.reset();</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (flushBuffer) &#123;</span><br><span class="line">      <span class="comment">// 找到缓存数据最多的Bucket</span></span><br><span class="line">      List&lt;DataBucket&gt; sortedBuckets = <span class="keyword">this</span>.buckets.values().stream()</span><br><span class="line">          .sorted((b1, b2) -&gt; Long.compare(b2.detector.totalSize, b1.detector.totalSize))</span><br><span class="line">          .collect(Collectors.toList());</span><br><span class="line">      <span class="keyword">final</span> DataBucket bucketToFlush = sortedBuckets.get(<span class="number">0</span>);</span><br><span class="line">      <span class="comment">// 写入文件</span></span><br><span class="line">      flushBucket(bucketToFlush);</span><br><span class="line">      <span class="keyword">this</span>.tracer.countDown(bucketToFlush.detector.totalSize);</span><br><span class="line">      bucketToFlush.reset();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 都不满足则缓存起来</span></span><br><span class="line">    bucket.records.add((HoodieRecord&lt;?&gt;) value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;unchecked, rawtypes&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">flushBucket</span><span class="params">(DataBucket bucket)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 获取PendingInstant</span></span><br><span class="line">    <span class="keyword">final</span> String instant = <span class="keyword">this</span>.writeClient.getLastPendingInstant(<span class="keyword">this</span>.actionType);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (instant == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="comment">// in case there are empty checkpoints that has no input data</span></span><br><span class="line">      LOG.info(<span class="string">&quot;No inflight instant when flushing data, cancel.&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if we are waiting for the checkpoint notification, shift the write instant time.</span></span><br><span class="line">    <span class="keyword">boolean</span> shift = confirming &amp;&amp; StreamerUtil.equal(instant, <span class="keyword">this</span>.currentInstant);</span><br><span class="line">    <span class="keyword">final</span> String flushInstant = shift ? StreamerUtil.instantTimePlus(instant, <span class="number">1</span>) : instant;</span><br><span class="line"></span><br><span class="line">    List&lt;HoodieRecord&gt; records = bucket.records;</span><br><span class="line">    ValidationUtils.checkState(records.size() &gt; <span class="number">0</span>, <span class="string">&quot;Data bucket to flush has no buffering records&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (config.getBoolean(FlinkOptions.INSERT_DROP_DUPS)) &#123;</span><br><span class="line">      records = FlinkWriteHelper.newInstance().deduplicateRecords(records, (HoodieIndex) <span class="keyword">null</span>, -<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 写入文件</span></span><br><span class="line">    <span class="keyword">final</span> List&lt;WriteStatus&gt; writeStatus = <span class="keyword">new</span> ArrayList&lt;&gt;(writeFunction.apply(records, flushInstant));</span><br><span class="line">    <span class="keyword">final</span> BatchWriteSuccessEvent event = BatchWriteSuccessEvent.builder()</span><br><span class="line">        .taskID(taskID)</span><br><span class="line">        .instantTime(instant) <span class="comment">// the write instant may shift but the event still use the currentInstant.</span></span><br><span class="line">        .writeStatus(writeStatus)</span><br><span class="line">        .isLastBatch(<span class="keyword">false</span>)</span><br><span class="line">        .isEndInput(<span class="keyword">false</span>)</span><br><span class="line">        .build();</span><br><span class="line">    <span class="comment">// 发送成功事件给StreamWriteOperatorCoordinator</span></span><br><span class="line">    <span class="keyword">this</span>.eventGateway.sendEventToCoordinator(event);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># StreamWriteOperatorCoordinator</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleEventFromOperator</span><span class="params">(<span class="keyword">int</span> i, OperatorEvent operatorEvent)</span> </span>&#123;</span><br><span class="line">    executor.execute(</span><br><span class="line">        () -&gt; &#123;</span><br><span class="line">          <span class="comment">// no event to handle</span></span><br><span class="line">          ValidationUtils.checkState(operatorEvent <span class="keyword">instanceof</span> BatchWriteSuccessEvent,</span><br><span class="line">              <span class="string">&quot;The coordinator can only handle BatchWriteSuccessEvent&quot;</span>);</span><br><span class="line">          BatchWriteSuccessEvent event = (BatchWriteSuccessEvent) operatorEvent;</span><br><span class="line">          <span class="comment">// the write task does not block after checkpointing(and before it receives a checkpoint success event),</span></span><br><span class="line">          <span class="comment">// if it it checkpoints succeed then flushes the data buffer again before this coordinator receives a checkpoint</span></span><br><span class="line">          <span class="comment">// success event, the data buffer would flush with an older instant time.</span></span><br><span class="line">          ValidationUtils.checkState(</span><br><span class="line">              HoodieTimeline.compareTimestamps(instant, HoodieTimeline.GREATER_THAN_OR_EQUALS, event.getInstantTime()),</span><br><span class="line">              String.format(<span class="string">&quot;Receive an unexpected event for instant %s from task %d&quot;</span>,</span><br><span class="line">                  event.getInstantTime(), event.getTaskID()));</span><br><span class="line">          <span class="keyword">if</span> (<span class="keyword">this</span>.eventBuffer[event.getTaskID()] != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.eventBuffer[event.getTaskID()].mergeWith(event);</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.eventBuffer[event.getTaskID()] = event;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// 是EndInput且所有事件都被接受</span></span><br><span class="line">          <span class="keyword">if</span> (event.isEndInput() &amp;&amp; allEventsReceived()) &#123;</span><br><span class="line">            <span class="comment">// 提交当前Instant</span></span><br><span class="line">            commitInstant();</span><br><span class="line">            <span class="comment">// no compaction scheduling for batch mode</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, <span class="string">&quot;handle write success event for instant %s&quot;</span>, <span class="keyword">this</span>.instant</span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="读取做了些什么"><a href="#读取做了些什么" class="headerlink" title="读取做了些什么"></a>读取做了些什么</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ScanRuntimeProvider <span class="title">getScanRuntimeProvider</span><span class="params">(ScanContext scanContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> DataStreamScanProvider() &#123;</span><br><span class="line"></span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBounded</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 是否有界</span></span><br><span class="line">        <span class="keyword">return</span> !conf.getBoolean(FlinkOptions.READ_AS_STREAMING);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> DataStream&lt;RowData&gt; <span class="title">produceDataStream</span><span class="params">(StreamExecutionEnvironment execEnv)</span> </span>&#123;</span><br><span class="line">        <span class="meta">@SuppressWarnings(&quot;unchecked&quot;)</span></span><br><span class="line">        <span class="comment">// 数据类型</span></span><br><span class="line">        TypeInformation&lt;RowData&gt; typeInfo =</span><br><span class="line">            (TypeInformation&lt;RowData&gt;) TypeInfoDataTypeConverter.fromDataTypeToTypeInfo(getProducedDataType());</span><br><span class="line">        <span class="keyword">if</span> (conf.getBoolean(FlinkOptions.READ_AS_STREAMING)) &#123;</span><br><span class="line">          <span class="comment">// 流读,很眼熟哟,IceBerg也见过</span></span><br><span class="line">          StreamReadMonitoringFunction monitoringFunction = <span class="keyword">new</span> StreamReadMonitoringFunction(</span><br><span class="line">              conf, FilePathUtils.toFlinkPath(path), metaClient, maxCompactionMemoryInBytes);</span><br><span class="line">          InputFormat&lt;RowData, ?&gt; inputFormat = getInputFormat(<span class="keyword">true</span>);</span><br><span class="line">          <span class="keyword">if</span> (!(inputFormat <span class="keyword">instanceof</span> MergeOnReadInputFormat)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> HoodieException(<span class="string">&quot;No successful commits under path &quot;</span> + path);</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// InputFormat转换DataStream</span></span><br><span class="line">          OneInputStreamOperatorFactory&lt;MergeOnReadInputSplit, RowData&gt; factory = StreamReadOperator.factory((MergeOnReadInputFormat) inputFormat);</span><br><span class="line">          SingleOutputStreamOperator&lt;RowData&gt; source = execEnv.addSource(monitoringFunction, <span class="string">&quot;streaming_source&quot;</span>)</span><br><span class="line">              .setParallelism(<span class="number">1</span>)</span><br><span class="line">              .uid(<span class="string">&quot;uid_streaming_source&quot;</span>)</span><br><span class="line">              .transform(<span class="string">&quot;split_reader&quot;</span>, typeInfo, factory)</span><br><span class="line">              .setParallelism(conf.getInteger(FlinkOptions.READ_TASKS))</span><br><span class="line">              .uid(<span class="string">&quot;uid_split_reader&quot;</span>);</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> DataStreamSource&lt;&gt;(source);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 有界</span></span><br><span class="line">          InputFormatSourceFunction&lt;RowData&gt; func = <span class="keyword">new</span> InputFormatSourceFunction&lt;&gt;(getInputFormat(), typeInfo);</span><br><span class="line">          DataStreamSource&lt;RowData&gt; source = execEnv.addSource(func, asSummaryString(), typeInfo);</span><br><span class="line">          <span class="keyword">return</span> source.name(<span class="string">&quot;bounded_source&quot;</span>)</span><br><span class="line">              .setParallelism(conf.getInteger(FlinkOptions.READ_TASKS))</span><br><span class="line">              .uid(<span class="string">&quot;uid_bounded_source&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># StreamReadMonitoringFunction</span><br><span class="line"><span class="comment">// 监听用户提供的Hoodie表路径</span></span><br><span class="line"><span class="comment">// 决定读取和处理哪些文件</span></span><br><span class="line"><span class="comment">// 创建与这些文件对应的MergeOnReadInputSplit</span></span><br><span class="line"><span class="comment">// 将他们分配给下游任务进行处理</span></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">monitorDirAndForwardSplits</span><span class="params">(SourceContext&lt;MergeOnReadInputSplit&gt; context)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 已完成的Instant</span></span><br><span class="line">    metaClient.reloadActiveTimeline();</span><br><span class="line">    HoodieTimeline commitTimeline = metaClient.getCommitsAndCompactionTimeline().filterCompletedInstants();</span><br><span class="line">    <span class="keyword">if</span> (commitTimeline.empty()) &#123;</span><br><span class="line">      LOG.warn(<span class="string">&quot;No splits found for the table under path &quot;</span> + path);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    List&lt;HoodieInstant&gt; instants = filterInstantsWithStart(commitTimeline, <span class="keyword">this</span>.issuedInstant);</span><br><span class="line">    <span class="comment">// 获取满足条件的最新Instant</span></span><br><span class="line">    <span class="keyword">final</span> HoodieInstant instantToIssue = instants.size() == <span class="number">0</span> ? <span class="keyword">null</span> : instants.get(instants.size() - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">final</span> InstantRange instantRange;</span><br><span class="line">    <span class="keyword">if</span> (instantToIssue != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">this</span>.issuedInstant != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// had already consumed an instant</span></span><br><span class="line">        instantRange = InstantRange.getInstance(<span class="keyword">this</span>.issuedInstant, instantToIssue.getTimestamp(),</span><br><span class="line">            InstantRange.RangeType.OPEN_CLOSE);</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="keyword">this</span>.conf.getOptional(FlinkOptions.READ_STREAMING_START_COMMIT).isPresent()) &#123;</span><br><span class="line">        <span class="comment">// first time consume and has a start commit</span></span><br><span class="line">        <span class="keyword">final</span> String specifiedStart = <span class="keyword">this</span>.conf.getString(FlinkOptions.READ_STREAMING_START_COMMIT);</span><br><span class="line">        instantRange = InstantRange.getInstance(specifiedStart, instantToIssue.getTimestamp(),</span><br><span class="line">            InstantRange.RangeType.CLOSE_CLOSE);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// first time consume and no start commit,</span></span><br><span class="line">        <span class="comment">// would consume all the snapshot data PLUS incremental data set</span></span><br><span class="line">        instantRange = <span class="keyword">null</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;No new instant found for the table under path &quot;</span> + path + <span class="string">&quot;, skip reading&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 生成InputSplit:</span></span><br><span class="line">    <span class="comment">// 1. 获取元数据;</span></span><br><span class="line">    <span class="comment">// 2. 过滤相对分区路径</span></span><br><span class="line">    <span class="comment">// 3. 筛选完整文件路径</span></span><br><span class="line">    <span class="comment">// 4. 使用步骤3文件路径作为文件系统视图的备份</span></span><br><span class="line">    List&lt;HoodieCommitMetadata&gt; metadataList = instants.stream()</span><br><span class="line">        .map(instant -&gt; getCommitMetadata(instant, commitTimeline)).collect(Collectors.toList());</span><br><span class="line">    Set&lt;String&gt; writePartitions = getWritePartitionPaths(metadataList);</span><br><span class="line">    FileStatus[] fileStatuses = getWritePathsOfInstants(metadataList);</span><br><span class="line">    <span class="keyword">if</span> (fileStatuses.length == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> HoodieException(<span class="string">&quot;No files found for reading in user provided path.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    HoodieTableFileSystemView fsView = <span class="keyword">new</span> HoodieTableFileSystemView(metaClient, commitTimeline, fileStatuses);</span><br><span class="line">    <span class="keyword">final</span> String commitToIssue = instantToIssue.getTimestamp();</span><br><span class="line">    <span class="keyword">final</span> AtomicInteger cnt = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">final</span> String mergeType = <span class="keyword">this</span>.conf.getString(FlinkOptions.MERGE_TYPE);</span><br><span class="line">    List&lt;MergeOnReadInputSplit&gt; inputSplits = writePartitions.stream()</span><br><span class="line">        .map(relPartitionPath -&gt; fsView.getLatestMergedFileSlicesBeforeOrOn(relPartitionPath, commitToIssue)</span><br><span class="line">        .map(fileSlice -&gt; &#123;</span><br><span class="line">          Option&lt;List&lt;String&gt;&gt; logPaths = Option.ofNullable(fileSlice.getLogFiles()</span><br><span class="line">              .sorted(HoodieLogFile.getLogFileComparator())</span><br><span class="line">              .map(logFile -&gt; logFile.getPath().toString())</span><br><span class="line">              .collect(Collectors.toList()));</span><br><span class="line">          String basePath = fileSlice.getBaseFile().map(BaseFile::getPath).orElse(<span class="keyword">null</span>);</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> MergeOnReadInputSplit(cnt.getAndAdd(<span class="number">1</span>),</span><br><span class="line">              basePath, logPaths, commitToIssue,</span><br><span class="line">              metaClient.getBasePath(), maxCompactionMemoryInBytes, mergeType, instantRange);</span><br><span class="line">        &#125;).collect(Collectors.toList()))</span><br><span class="line">        .flatMap(Collection::stream)</span><br><span class="line">        .collect(Collectors.toList());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (MergeOnReadInputSplit split : inputSplits) &#123;</span><br><span class="line">      context.collect(split);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// update the issues instant time</span></span><br><span class="line">    <span class="keyword">this</span>.issuedInstant = commitToIssue;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># StreamReadOperator</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(StreamRecord&lt;MergeOnReadInputSplit&gt; element)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 接受到InputSplit就放入队列</span></span><br><span class="line">    splits.add(element.getValue());</span><br><span class="line">    enqueueProcessSplits();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">enqueueProcessSplits</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (currentSplitState == SplitState.IDLE &amp;&amp; !splits.isEmpty()) &#123;</span><br><span class="line">      currentSplitState = SplitState.RUNNING;</span><br><span class="line">      <span class="comment">// MailboxExecutor读取InputSplit的实际数据</span></span><br><span class="line">      executor.execute(<span class="keyword">this</span>::processSplits, <span class="string">&quot;process input split&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hudi</tag>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之二Topic创建过程</title>
    <url>/2020/05/06/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E4%BA%8CTopic%E5%88%9B%E5%BB%BA%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<blockquote>
<p>Topic的创建过程</p>
</blockquote>
<span id="more"></span>

<h2 id="Topic的创建"><a href="#Topic的创建" class="headerlink" title="Topic的创建"></a>Topic的创建</h2><h3 id="kafka-topic-sh"><a href="#kafka-topic-sh" class="headerlink" title="kafka-topic.sh"></a>kafka-topic.sh</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// exec $(dirname $0)/kafka-run-class.sh kafka.admin.TopicCommand &quot;$@&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(zkClient: <span class="type">KafkaZkClient</span>, opts: <span class="type">TopicCommandOptions</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> topic = opts.options.valueOf(opts.topicOpt)</span><br><span class="line">    <span class="keyword">val</span> configs = parseTopicConfigsToBeAdded(opts)</span><br><span class="line">    <span class="keyword">val</span> ifNotExists = opts.options.has(opts.ifNotExistsOpt)</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">Topic</span>.hasCollisionChars(topic))</span><br><span class="line">      println(<span class="string">&quot;WARNING: Due to limitations in metric names, topics with a period (&#x27;.&#x27;) or underscore (&#x27;_&#x27;) could collide. To avoid issues it is best to use either, but not both.&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> adminZkClient = <span class="keyword">new</span> <span class="type">AdminZkClient</span>(zkClient)</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (opts.options.has(opts.replicaAssignmentOpt)) &#123; <span class="comment">// 指定replica的分配,直接向zk更新</span></span><br><span class="line">        <span class="keyword">val</span> assignment = parseReplicaAssignment(opts.options.valueOf(opts.replicaAssignmentOpt))</span><br><span class="line">        adminZkClient.createOrUpdateTopicPartitionAssignmentPathInZK(topic, assignment, configs, update = <span class="literal">false</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">// 未指定replica的分配,调用自动分配算法进行分配</span></span><br><span class="line">        <span class="type">CommandLineUtils</span>.checkRequiredArgs(opts.parser, opts.options, opts.partitionsOpt, opts.replicationFactorOpt)</span><br><span class="line">        <span class="keyword">val</span> partitions = opts.options.valueOf(opts.partitionsOpt).intValue</span><br><span class="line">        <span class="keyword">val</span> replicas = opts.options.valueOf(opts.replicationFactorOpt).intValue</span><br><span class="line">        <span class="keyword">val</span> rackAwareMode = <span class="keyword">if</span> (opts.options.has(opts.disableRackAware)) <span class="type">RackAwareMode</span>.<span class="type">Disabled</span></span><br><span class="line">                            <span class="keyword">else</span> <span class="type">RackAwareMode</span>.<span class="type">Enforced</span></span><br><span class="line">        adminZkClient.createTopic(topic, partitions, replicas, configs, rackAwareMode)</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">&quot;Created topic \&quot;%s\&quot;.&quot;</span>.format(topic))</span><br><span class="line">    &#125; <span class="keyword">catch</span>  &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">TopicExistsException</span> =&gt; <span class="keyword">if</span> (!ifNotExists) <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果指定了partition各个replica的分布,那么将partition replicas的结果验证之后直接更新到zk上</span></span><br><span class="line"><span class="comment">// 验证replicas的代码在parseReplicaAssignment中实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseReplicaAssignment</span></span>(replicaAssignmentList: <span class="type">String</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">List</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> partitionList = replicaAssignmentList.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> ret = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">List</span>[<span class="type">Int</span>]]()</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until partitionList.size) &#123;</span><br><span class="line">      <span class="keyword">val</span> brokerList = partitionList(i).split(<span class="string">&quot;:&quot;</span>).map(s =&gt; s.trim().toInt)</span><br><span class="line">      <span class="keyword">val</span> duplicateBrokers = <span class="type">CoreUtils</span>.duplicates(brokerList)</span><br><span class="line">      <span class="comment">// 同一个partition对应的replica是不能相同的</span></span><br><span class="line">      <span class="keyword">if</span> (duplicateBrokers.nonEmpty)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminCommandFailedException</span>(<span class="string">&quot;Partition replica lists may not contain duplicate entries: %s&quot;</span>.format(duplicateBrokers.mkString(<span class="string">&quot;,&quot;</span>)))</span><br><span class="line">      ret.put(i, brokerList.toList)</span><br><span class="line">      <span class="comment">// 同一个topic的副本数必须相同</span></span><br><span class="line">      <span class="keyword">if</span> (ret(i).size != ret(<span class="number">0</span>).size)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">&quot;Partition &quot;</span> + i + <span class="string">&quot; has different replication factor: &quot;</span> + brokerList)</span><br><span class="line">    &#125;</span><br><span class="line">    ret.toMap</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有指定的partition replicas分配的话,将会调用adminZkClient.createTopic方法创建topic</span></span><br><span class="line"><span class="comment">// 这个方法首先会检测当前的kafka集群是否机架感知</span></span><br><span class="line"><span class="comment">// 如果有,先获取Broker的机架信息,接着使用Replica自动分配算法分配Partition的replica</span></span><br><span class="line"><span class="comment">// 最后将replicas的结果更新到zk上</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTopic</span></span>(topic: <span class="type">String</span>,</span><br><span class="line">                  partitions: <span class="type">Int</span>,</span><br><span class="line">                  replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                  topicConfig: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>,</span><br><span class="line">                  rackAwareMode: <span class="type">RackAwareMode</span> = <span class="type">RackAwareMode</span>.<span class="type">Enforced</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> brokerMetadatas = getBrokerMetadatas(rackAwareMode)</span><br><span class="line">    <span class="keyword">val</span> replicaAssignment = <span class="type">AdminUtils</span>.assignReplicasToBrokers(brokerMetadatas, partitions, replicationFactor)</span><br><span class="line">    createOrUpdateTopicPartitionAssignmentPathInZK(topic, replicaAssignment, topicConfig)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Producer创建Topic"><a href="#Producer创建Topic" class="headerlink" title="Producer创建Topic"></a>Producer创建Topic</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 只有当Server端的auto.create.topics.enable设置为true时,Producer向一个不存在的topic发送数据,该topic才会被自动创建</span></span><br><span class="line"><span class="comment">// 当Producer在向一个topic发送produce请求前,会先通过Metadata请求获取这个topic的metadata信息</span></span><br><span class="line"><span class="comment">// Server端在处理Metadata请求时,如果发现获取metadata的topic不存在,但Server允许producer自动创建</span></span><br><span class="line"><span class="comment">// 那么Server将会自动创建该topic</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Replica如何分配"><a href="#Replica如何分配" class="headerlink" title="Replica如何分配"></a>Replica如何分配</h2><h3 id="创建时指定分配"><a href="#创建时指定分配" class="headerlink" title="创建时指定分配"></a>创建时指定分配</h3><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">kafka-topics.sh --create --topic <span class="built_in">test</span> --zookeeper XXXX --replica-assignment 1:2,3:4,5:6</span><br><span class="line"><span class="comment"># 该topic有三个分区,分区0的replica分布在1和2上,分区1的replica分布在3和4上,分区2的replica分布在4和5上</span></span><br><span class="line"><span class="comment"># Server端会将该replica分布直接更新到zk上</span></span><br></pre></td></tr></table></figure>
<h3 id="replicas自动分配"><a href="#replicas自动分配" class="headerlink" title="replicas自动分配"></a>replicas自动分配</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 副本分配时,有三个原则:</span></span><br><span class="line"><span class="comment">   * 1. 将副本平均分布在所有的 Broker 上;</span></span><br><span class="line"><span class="comment">   * 2. partition 的多个副本应该分配在不同的 Broker 上;</span></span><br><span class="line"><span class="comment">   * 3. 如果所有的 Broker 有机架信息的话, partition 的副本应该分配到不同的机架上。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * 为实现上面的目标,在没有机架感知的情况下，应该按照下面两个原则分配 replica:</span></span><br><span class="line"><span class="comment">   * 1. 从 broker.list 随机选择一个 Broker,使用 round-robin 算法分配每个 partition 的第一个副本;</span></span><br><span class="line"><span class="comment">   * 2. 对于这个 partition 的其他副本,逐渐增加 Broker.id 来选择 replica 的分配。</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param brokerMetadatas</span></span><br><span class="line"><span class="comment">   * @param nPartitions</span></span><br><span class="line"><span class="comment">   * @param replicationFactor</span></span><br><span class="line"><span class="comment">   * @param fixedStartIndex</span></span><br><span class="line"><span class="comment">   * @param startPartitionId</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokers</span></span>(brokerMetadatas: <span class="type">Seq</span>[<span class="type">BrokerMetadata</span>],</span><br><span class="line">                              nPartitions: <span class="type">Int</span>,</span><br><span class="line">                              replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                              fixedStartIndex: <span class="type">Int</span> = <span class="number">-1</span>,</span><br><span class="line">                              startPartitionId: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (nPartitions &lt;= <span class="number">0</span>) <span class="comment">// 要增加的分区数要大于0</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidPartitionsException</span>(<span class="string">&quot;Number of partitions must be larger than 0.&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (replicationFactor &lt;= <span class="number">0</span>) <span class="comment">// replicas要大于0</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">&quot;Replication factor must be larger than 0.&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (replicationFactor &gt; brokerMetadatas.size) <span class="comment">// replicas超过了broker数</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">InvalidReplicationFactorException</span>(<span class="string">s&quot;Replication factor: <span class="subst">$replicationFactor</span> larger than available brokers: <span class="subst">$&#123;brokerMetadatas.size&#125;</span>.&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (brokerMetadatas.forall(_.rack.isEmpty)) <span class="comment">// 没有开启机架感知</span></span><br><span class="line">      assignReplicasToBrokersRackUnaware(nPartitions, replicationFactor, brokerMetadatas.map(_.id), fixedStartIndex,</span><br><span class="line">        startPartitionId)</span><br><span class="line">    <span class="keyword">else</span> &#123; <span class="comment">// 机架感知</span></span><br><span class="line">      <span class="keyword">if</span> (brokerMetadatas.exists(_.rack.isEmpty)) <span class="comment">// 并不是所有机器都有机架感知</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">AdminOperationException</span>(<span class="string">&quot;Not all brokers have rack information for replica rack aware assignment.&quot;</span>)</span><br><span class="line">      assignReplicasToBrokersRackAware(nPartitions, replicationFactor, brokerMetadatas, fixedStartIndex,</span><br><span class="line">        startPartitionId)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 没有开启机架感知模式,使用assignReplicasToBrokersRackUnaware实现</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">assignReplicasToBrokersRackUnaware</span></span>(nPartitions: <span class="type">Int</span>,</span><br><span class="line">                                                 replicationFactor: <span class="type">Int</span>,</span><br><span class="line">                                                 brokerList: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">                                                 fixedStartIndex: <span class="type">Int</span>,</span><br><span class="line">                                                 startPartitionId: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ret = mutable.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">Int</span>]]()</span><br><span class="line">    <span class="keyword">val</span> brokerArray = brokerList.toArray</span><br><span class="line">    <span class="keyword">val</span> startIndex = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length) <span class="comment">// 随机选择一个Broker</span></span><br><span class="line">    <span class="keyword">var</span> currentPartitionId = math.max(<span class="number">0</span>, startPartitionId) <span class="comment">// 开始增加的第一个Partition</span></span><br><span class="line">    <span class="keyword">var</span> nextReplicaShift = <span class="keyword">if</span> (fixedStartIndex &gt;= <span class="number">0</span>) fixedStartIndex <span class="keyword">else</span> rand.nextInt(brokerArray.length)</span><br><span class="line">    <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until nPartitions) &#123; <span class="comment">// 对每个partition进行分配</span></span><br><span class="line">      <span class="keyword">if</span> (currentPartitionId &gt; <span class="number">0</span> &amp;&amp; (currentPartitionId % brokerArray.length == <span class="number">0</span>))</span><br><span class="line">        nextReplicaShift += <span class="number">1</span> <span class="comment">// 防止partition过大时,其中某些partition的分配(leader,follower)完全一样</span></span><br><span class="line">      <span class="keyword">val</span> firstReplicaIndex = (currentPartitionId + startIndex) % brokerArray.length <span class="comment">// partition的第一个replica</span></span><br><span class="line">      <span class="keyword">val</span> replicaBuffer = mutable.<span class="type">ArrayBuffer</span>(brokerArray(firstReplicaIndex))</span><br><span class="line">      <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until replicationFactor - <span class="number">1</span>) <span class="comment">// 其他replica的分配</span></span><br><span class="line">        replicaBuffer += brokerArray(replicaIndex(firstReplicaIndex, nextReplicaShift, j, brokerArray.length))</span><br><span class="line">      ret.put(currentPartitionId, replicaBuffer)</span><br><span class="line">      currentPartitionId += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    ret</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为partition设置完第一个replica后,其他replica分配的计算</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">replicaIndex</span></span>(firstReplicaIndex: <span class="type">Int</span>, secondReplicaShift: <span class="type">Int</span>, replicaIndex: <span class="type">Int</span>, nBrokers: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shift = <span class="number">1</span> + (secondReplicaShift + replicaIndex) % (nBrokers - <span class="number">1</span>) <span class="comment">// 在secondReplicaShift的基础上增加一个replicaIndex</span></span><br><span class="line">    (firstReplicaIndex + shift) % nBrokers</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="replicas更新到zk后触发的操作"><a href="#replicas更新到zk后触发的操作" class="headerlink" title="replicas更新到zk后触发的操作"></a>replicas更新到zk后触发的操作</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 当一个topic的replicas更新到zk上后</span></span><br><span class="line"><span class="comment">// 监控zk这个目录的方法会被触发(KafkaTopicChangeHandler.handleChildChange())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 实际调用的是controller.TopicChange</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopicChangeHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChildChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">TopicsZNode</span>.path</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleChildChange</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">TopicChange</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">TopicChange</span> <span class="keyword">extends</span> <span class="title">ControllerEvent</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">state</span></span>: <span class="type">ControllerState</span> = <span class="type">ControllerState</span>.<span class="type">TopicChange</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!isActive) <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">val</span> topics = zkClient.getAllTopicsInCluster.toSet</span><br><span class="line">    <span class="keyword">val</span> newTopics = topics -- controllerContext.allTopics <span class="comment">// 新创建的topic列表</span></span><br><span class="line">    <span class="keyword">val</span> deletedTopics = controllerContext.allTopics -- topics <span class="comment">// 已经删除的topic列表</span></span><br><span class="line">    controllerContext.allTopics = topics</span><br><span class="line">    registerPartitionModificationsHandlers(newTopics.toSeq)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 新创建topic对应的partition列表</span></span><br><span class="line">    <span class="keyword">val</span> addedPartitionReplicaAssignment = zkClient.getReplicaAssignmentForTopics(newTopics)</span><br><span class="line">    deletedTopics.foreach(controllerContext.removeTopic) <span class="comment">// 把已经删除partition的topic过滤掉</span></span><br><span class="line">    addedPartitionReplicaAssignment.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> (topicAndPartition, newReplicas) =&gt; controllerContext.updatePartitionReplicaAssignment(topicAndPartition, newReplicas) <span class="comment">// 将新增的tp-replicas更新到缓存中</span></span><br><span class="line">    &#125;</span><br><span class="line">    info(<span class="string">s&quot;New topics: [<span class="subst">$newTopics</span>], deleted topics: [<span class="subst">$deletedTopics</span>], new partition replica assignment &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;[<span class="subst">$addedPartitionReplicaAssignment</span>]&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (addedPartitionReplicaAssignment.nonEmpty) <span class="comment">// 处理新创建的topic</span></span><br><span class="line">      onNewPartitionCreation(addedPartitionReplicaAssignment.keySet)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 此回调由topic更改回调调用，并将失败的代理列表作为输入。</span></span><br><span class="line"><span class="comment"> * 1. 将新创建的分区移到NewPartition状态</span></span><br><span class="line"><span class="comment"> * 2. 从NewPartition-&gt;OnlinePartition状态移动新创建的分区</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onNewPartitionCreation</span></span>(newPartitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</span><br><span class="line">  info(<span class="string">s&quot;New partition creation callback for <span class="subst">$&#123;newPartitions.mkString(&quot;,&quot;)&#125;</span>&quot;</span>)</span><br><span class="line">  partitionStateMachine.handleStateChanges(newPartitions.toSeq, <span class="type">NewPartition</span>) <span class="comment">// 创建Partition对象,并将其状态置为NewPartition状态</span></span><br><span class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, <span class="type">NewReplica</span>) <span class="comment">// 创建Replica对象,并将其状态置为NewReplica状态</span></span><br><span class="line">  partitionStateMachine.handleStateChanges(newPartitions.toSeq, <span class="type">OnlinePartition</span>, <span class="type">Option</span>(<span class="type">OfflinePartitionLeaderElectionStrategy</span>)) <span class="comment">// 将Partition对象从NewPartition状态改为OnlinePartition状态</span></span><br><span class="line">  replicaStateMachine.handleStateChanges(controllerContext.replicasForPartition(newPartitions).toSeq, <span class="type">OnlineReplica</span>) <span class="comment">// 将Replica对象从NewReplica改为OnlineReplica状态</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Replica状态机"><a href="#Replica状态机" class="headerlink" title="Replica状态机"></a>Replica状态机</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">七种状态</span><br><span class="line">NewReplica: The controller can create new replicas during partition reassignment. In this state, a replica can only get become follower state change request.  Valid previous state is NonExistentReplica</span><br><span class="line">OnlineReplica: Once a replica is started and part of the assigned replicas for its partition, it is in this state. In this state, it can get either become leader or become follower state change requests. Valid previous state are NewReplica, OnlineReplica or OfflineReplica</span><br><span class="line">OfflineReplica: If a replica dies, it moves to this state. This happens when the broker hosting the replica is down. Valid previous state are NewReplica, OnlineReplica</span><br><span class="line">ReplicaDeletionStarted: If replica deletion starts, it is moved to this state. Valid previous state is OfflineReplica</span><br><span class="line">ReplicaDeletionSuccessful: If replica responds with no error code in response to a delete replica request, it is moved to this state. Valid previous state is ReplicaDeletionStarted</span><br><span class="line">ReplicaDeletionIneligible: If replica deletion fails, it is moved to this state. Valid previous state is ReplicaDeletionStarted</span><br><span class="line">NonExistentReplica: If a replica is deleted successfully, it is moved to this state. Valid previous state is ReplicaDeletionSuccessful</span><br></pre></td></tr></table></figure>
<h3 id="Partition状态机"><a href="#Partition状态机" class="headerlink" title="Partition状态机"></a>Partition状态机</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">四种状态</span><br><span class="line">NonExistentPartition: Partition不存在</span><br><span class="line">NewPartition: Partition刚创建,有对应的Replicas,但还没有Leader和ISR</span><br><span class="line">OnlinePartition: Partition的Leader已经选举出来了,处于正常的工作状态</span><br><span class="line">OfflinePartition: Partition的Leader挂了</span><br><span class="line"></span><br><span class="line">只有在OnlinePartition状态,才是可用状态</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="onNewPartitionCreation详细步骤"><a href="#onNewPartitionCreation详细步骤" class="headerlink" title="onNewPartitionCreation详细步骤"></a>onNewPartitionCreation详细步骤</h2><h3 id="changeStateTo方法"><a href="#changeStateTo方法" class="headerlink" title="changeStateTo方法"></a>changeStateTo方法</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">changeStateTo</span></span>(partition: <span class="type">TopicPartition</span>, currentState: <span class="type">PartitionState</span>, targetState: <span class="type">PartitionState</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  partitionState.put(partition, targetState) <span class="comment">// 缓存partition的状态</span></span><br><span class="line">  updateControllerMetrics(partition, currentState, targetState)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="partitionStateMachine-gt-NewPartition"><a href="#partitionStateMachine-gt-NewPartition" class="headerlink" title="partitionStateMachine-&gt;NewPartition"></a>partitionStateMachine-&gt;NewPartition</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">NewPartition</span> =&gt;</span><br><span class="line">    validPartitions.foreach &#123; partition =&gt;</span><br><span class="line">      stateChangeLog.trace(<span class="string">s&quot;Changed partition <span class="subst">$partition</span> state from <span class="subst">$&#123;partitionState(partition)&#125;</span> to <span class="subst">$targetState</span> with &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;assigned replicas <span class="subst">$&#123;controllerContext.partitionReplicaAssignment(partition).mkString(&quot;,&quot;)&#125;</span>&quot;</span>)</span><br><span class="line">      changeStateTo(partition, partitionState(partition), <span class="type">NewPartition</span>) <span class="comment">// 将分区对象的状态置为NewPartition</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="replicaStateMachine-gt-NewReplica"><a href="#replicaStateMachine-gt-NewReplica" class="headerlink" title="replicaStateMachine-&gt;NewReplica"></a>replicaStateMachine-&gt;NewReplica</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">NewReplica</span> =&gt;</span><br><span class="line">  validReplicas.foreach &#123; replica =&gt;</span><br><span class="line">    <span class="keyword">val</span> partition = replica.topicPartition</span><br><span class="line">    controllerContext.partitionLeadershipInfo.get(partition) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (leaderIsrAndControllerEpoch.leaderAndIsr.leader == replicaId) &#123; <span class="comment">// 这个状态的Replica不能作为Leader</span></span><br><span class="line">          <span class="keyword">val</span> exception = <span class="keyword">new</span> <span class="type">StateChangeFailedException</span>(<span class="string">s&quot;Replica <span class="subst">$replicaId</span> for partition <span class="subst">$partition</span> cannot be moved to NewReplica state as it is being requested to become leader&quot;</span>)</span><br><span class="line">          logFailedStateChange(replica, replicaState(replica), <span class="type">OfflineReplica</span>, exception)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 向所有replicaId发送LeaderAndIsr请求,这个方法同时也会向所有的Broker发送updateMeta请求</span></span><br><span class="line">          controllerBrokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">Seq</span>(replicaId),</span><br><span class="line">            replica.topicPartition,</span><br><span class="line">            leaderIsrAndControllerEpoch,</span><br><span class="line">            controllerContext.partitionReplicaAssignment(replica.topicPartition),</span><br><span class="line">            isNew = <span class="literal">true</span>)</span><br><span class="line">          logSuccessfulTransition(replicaId, partition, replicaState(replica), <span class="type">NewReplica</span>)</span><br><span class="line">          replicaState.put(replica, <span class="type">NewReplica</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        logSuccessfulTransition(replicaId, partition, replicaState(replica), <span class="type">NewReplica</span>)</span><br><span class="line">        replicaState.put(replica, <span class="type">NewReplica</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="partitionStateMachine-gt-OnlinePartition"><a href="#partitionStateMachine-gt-OnlinePartition" class="headerlink" title="partitionStateMachine-&gt;OnlinePartition"></a>partitionStateMachine-&gt;OnlinePartition</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">OnlinePartition</span> =&gt;</span><br><span class="line">    <span class="keyword">val</span> uninitializedPartitions = validPartitions.filter(partition =&gt; partitionState(partition) == <span class="type">NewPartition</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionsToElectLeader = validPartitions.filter(partition =&gt; partitionState(partition) == <span class="type">OfflinePartition</span> || partitionState(partition) == <span class="type">OnlinePartition</span>)</span><br><span class="line">    <span class="keyword">if</span> (uninitializedPartitions.nonEmpty) &#123;</span><br><span class="line">      <span class="comment">// 为新建的Partition初始化Leader和Isr,选取Replica中第一个Replica作为Leader,所有的Replica作为ISR</span></span><br><span class="line">      <span class="comment">// 最后向所有replicaId发送LeaderAndIsr请求以及向所有的Broker发送updateMetadata请求</span></span><br><span class="line">      <span class="keyword">val</span> successfulInitializations = initializeLeaderAndIsrForPartitions(uninitializedPartitions)</span><br><span class="line">      successfulInitializations.foreach &#123; partition =&gt;</span><br><span class="line">        stateChangeLog.trace(<span class="string">s&quot;Changed partition <span class="subst">$partition</span> from <span class="subst">$&#123;partitionState(partition)&#125;</span> to <span class="subst">$targetState</span> with state &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;<span class="subst">$&#123;controllerContext.partitionLeadershipInfo(partition).leaderAndIsr&#125;</span>&quot;</span>)</span><br><span class="line">        changeStateTo(partition, partitionState(partition), <span class="type">OnlinePartition</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partitionsToElectLeader.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> successfulElections = electLeaderForPartitions(partitionsToElectLeader, partitionLeaderElectionStrategyOpt.get)</span><br><span class="line">      successfulElections.foreach &#123; partition =&gt;</span><br><span class="line">        stateChangeLog.trace(<span class="string">s&quot;Changed partition <span class="subst">$partition</span> from <span class="subst">$&#123;partitionState(partition)&#125;</span> to <span class="subst">$targetState</span> with state &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;<span class="subst">$&#123;controllerContext.partitionLeadershipInfo(partition).leaderAndIsr&#125;</span>&quot;</span>)</span><br><span class="line">        changeStateTo(partition, partitionState(partition), <span class="type">OnlinePartition</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="replicaStateMachine-gt-OnlineReplica"><a href="#replicaStateMachine-gt-OnlineReplica" class="headerlink" title="replicaStateMachine-&gt;OnlineReplica"></a>replicaStateMachine-&gt;OnlineReplica</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">OnlineReplica</span> =&gt;</span><br><span class="line">  validReplicas.foreach &#123; replica =&gt;</span><br><span class="line">    <span class="keyword">val</span> partition = replica.topicPartition</span><br><span class="line">    replicaState(replica) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">NewReplica</span> =&gt;</span><br><span class="line">        <span class="comment">// 向 the assigned replicas list 添加这个 replica(正常情况下这些 replicas 已经更新到 list 中了)</span></span><br><span class="line">        <span class="keyword">val</span> assignment = controllerContext.partitionReplicaAssignment(partition)</span><br><span class="line">        <span class="keyword">if</span> (!assignment.contains(replicaId)) &#123;</span><br><span class="line">          controllerContext.updatePartitionReplicaAssignment(partition, assignment :+ replicaId)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        controllerContext.partitionLeadershipInfo.get(partition) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">            controllerBrokerRequestBatch.addLeaderAndIsrRequestForBrokers(<span class="type">Seq</span>(replicaId),</span><br><span class="line">              replica.topicPartition,</span><br><span class="line">              leaderIsrAndControllerEpoch,</span><br><span class="line">              controllerContext.partitionReplicaAssignment(partition), isNew = <span class="literal">false</span>)</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    logSuccessfulTransition(replicaId, partition, replicaState(replica), <span class="type">OnlineReplica</span>)</span><br><span class="line">    replicaState.put(replica, <span class="type">OnlineReplica</span>)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/huxi2b/p/5923252.html">如何创建Topic</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十一副本管理读取</title>
    <url>/2020/05/08/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E4%B8%80%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86%E8%AF%BB%E5%8F%96/</url>
    <content><![CDATA[<blockquote>
<p>Kafka处理Fetch请求以及日志读取过程</p>
</blockquote>
<span id="more"></span>

<h2 id="Fetch请求的来源"><a href="#Fetch请求的来源" class="headerlink" title="Fetch请求的来源"></a>Fetch请求的来源</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.Consumer消费数据产生Fetch请求</span><br><span class="line">    Consumer产生Fetch请求在之前有说过,在poll方法中构建</span><br><span class="line">2.副本同步Fetch请求</span><br><span class="line">    ReplicaManager中有一个ReplicaFetchManager实例负责开启FetchThread进行Fetch请求构建</span><br><span class="line"></span><br><span class="line">两者的区别:</span><br><span class="line">    Replica在构造FetchRequest时,调用了setReplicaId()设置了对应的replicaId</span><br><span class="line">    Consumer没有进行设置,默认为CONSUMER_REPLICA_ID,为-1</span><br><span class="line">    这个值就是区分Consumer的Fetch请求和Replica的Fetch请求的关键值</span><br></pre></td></tr></table></figure>
<h3 id="Consumer构建"><a href="#Consumer构建" class="headerlink" title="Consumer构建"></a>Consumer构建</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Fetcher.sendFetches()</span></span><br><span class="line"><span class="keyword">final</span> FetchRequest.Builder request = FetchRequest.Builder</span><br><span class="line">        .forConsumer(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, data.toSend())</span><br><span class="line">        .isolationLevel(isolationLevel)</span><br><span class="line">        .setMaxBytes(<span class="keyword">this</span>.maxBytes)</span><br><span class="line">        .metadata(data.metadata())</span><br><span class="line">        .toForget(data.toForget());</span><br></pre></td></tr></table></figure>
<h3 id="ReplicaFetchManager构建"><a href="#ReplicaFetchManager构建" class="headerlink" title="ReplicaFetchManager构建"></a>ReplicaFetchManager构建</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ReplicaFetcherThread.buildFetch()</span></span><br><span class="line"><span class="keyword">val</span> fetchData = builder.build()</span><br><span class="line"><span class="keyword">val</span> fetchRequestOpt = <span class="keyword">if</span> (fetchData.sessionPartitions.isEmpty &amp;&amp; fetchData.toForget.isEmpty) &#123;</span><br><span class="line">  <span class="type">None</span></span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> requestBuilder = <span class="type">FetchRequest</span>.<span class="type">Builder</span></span><br><span class="line">    .forReplica(fetchRequestVersion, replicaId, maxWait, minBytes, fetchData.toSend)</span><br><span class="line">    .setMaxBytes(maxBytes)</span><br><span class="line">    .toForget(fetchData.toForget)</span><br><span class="line">    .metadata(fetchData.metadata)</span><br><span class="line">  <span class="type">Some</span>(requestBuilder)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Server端处理Fetch请求"><a href="#Server端处理Fetch请求" class="headerlink" title="Server端处理Fetch请求"></a>Server端处理Fetch请求</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ApiKeys.FETCH -&gt; handleFetchRequest()</span><br></pre></td></tr></table></figure>
<h3 id="handleFetchRequest"><a href="#handleFetchRequest" class="headerlink" title="handleFetchRequest()"></a>handleFetchRequest()</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleFetchRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> versionId = request.header.apiVersion</span><br><span class="line">    <span class="keyword">val</span> clientId = request.header.clientId</span><br><span class="line">    <span class="keyword">val</span> fetchRequest = request.body[<span class="type">FetchRequest</span>]</span><br><span class="line">    <span class="keyword">val</span> fetchContext = fetchManager.newContext(</span><br><span class="line">      fetchRequest.metadata,</span><br><span class="line">      fetchRequest.fetchData,</span><br><span class="line">      fetchRequest.toForget,</span><br><span class="line">      fetchRequest.isFromFollower)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检验错误的请求</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">errorResponse</span></span>[<span class="type">T</span> &gt;: <span class="type">MemoryRecords</span> &lt;: <span class="type">BaseRecords</span>](error: <span class="type">Errors</span>): <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>[<span class="type">T</span>](error, <span class="type">FetchResponse</span>.<span class="type">INVALID_HIGHWATERMARK</span>, <span class="type">FetchResponse</span>.<span class="type">INVALID_LAST_STABLE_OFFSET</span>,</span><br><span class="line">        <span class="type">FetchResponse</span>.<span class="type">INVALID_LOG_START_OFFSET</span>, <span class="literal">null</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> erroneous = mutable.<span class="type">ArrayBuffer</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchResponse</span>.<span class="type">PartitionData</span>[<span class="type">Records</span>])]()</span><br><span class="line">    <span class="keyword">val</span> interesting = mutable.<span class="type">ArrayBuffer</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchRequest</span>.<span class="type">PartitionData</span>)]()</span><br><span class="line">    <span class="keyword">if</span> (fetchRequest.isFromFollower) &#123;</span><br><span class="line">      <span class="comment">// The follower must have ClusterAction on ClusterResource in order to fetch partition data.</span></span><br><span class="line">      <span class="comment">// 检验tp是否存在以及是否有Describe权限</span></span><br><span class="line">      <span class="keyword">if</span> (authorize(request.session, <span class="type">ClusterAction</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;</span><br><span class="line">        fetchContext.foreachPartition &#123; (topicPartition, data) =&gt;</span><br><span class="line">          <span class="comment">// 不存在或没有Describe权限的topic,返回UNKNOWN_TOPIC_OR_PARTITION错误</span></span><br><span class="line">          <span class="keyword">if</span> (!metadataCache.contains(topicPartition))</span><br><span class="line">            erroneous += topicPartition -&gt; errorResponse(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>)</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            interesting += (topicPartition -&gt; data)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 没有ClusterAction权限,返回TOPIC_AUTHORIZATION_FAILED错误</span></span><br><span class="line">        fetchContext.foreachPartition &#123; (part, _) =&gt;</span><br><span class="line">          erroneous += part -&gt; errorResponse(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      fetchContext.foreachPartition &#123; (topicPartition, data) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (!authorize(request.session, <span class="type">Read</span>, <span class="type">Resource</span>(<span class="type">Topic</span>, topicPartition.topic, <span class="type">LITERAL</span>)))</span><br><span class="line">          <span class="comment">// 没有read权限,返回TOPIC_AUTHORIZATION_FAILED错误</span></span><br><span class="line">          erroneous += topicPartition -&gt; errorResponse(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>)</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (!metadataCache.contains(topicPartition))</span><br><span class="line">          <span class="comment">// 不存在或没有Describe权限的topic,返回UNKNOWN_TOPIC_OR_PARTITION错误</span></span><br><span class="line">          erroneous += topicPartition -&gt; errorResponse(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>)</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">          interesting += (topicPartition -&gt; data)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (interesting.isEmpty)</span><br><span class="line">      processResponseCallback(<span class="type">Seq</span>.empty)</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// call the replica manager to fetch messages from the local replica</span></span><br><span class="line">      <span class="comment">// 从replica上拉取数据,满足条件后调用函数进行返回</span></span><br><span class="line">      replicaManager.fetchMessages(</span><br><span class="line">        fetchRequest.maxWait.toLong, <span class="comment">// 拉取请求最长等待时间</span></span><br><span class="line">        fetchRequest.replicaId, <span class="comment">// ReplicaId,Consumer的为-1</span></span><br><span class="line">        fetchRequest.minBytes, <span class="comment">// 拉取请求的最小拉取字节</span></span><br><span class="line">        fetchRequest.maxBytes, <span class="comment">// 拉取请求的最大拉取字节</span></span><br><span class="line">        versionId &lt;= <span class="number">2</span>,</span><br><span class="line">        interesting,</span><br><span class="line">        replicationQuota(fetchRequest),</span><br><span class="line">        processResponseCallback,</span><br><span class="line">        fetchRequest.isolationLevel)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h3><h4 id="fetchMessages"><a href="#fetchMessages" class="headerlink" title="fetchMessages"></a>fetchMessages</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>readFromLocalLog(): 从本地日志拉取相应的数据</span><br><span class="line"><span class="number">2.</span>判断<span class="type">Fetch</span>请求来源,如果来自副本同步,那么更新该副本的the end offset记录,如果该副本不在isr中,判断是否需要更新isr</span><br><span class="line"><span class="number">3.</span>返回结果,满足条件立即返回,否则通过延迟操作,延迟返回结果</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从leader拉取数据,等待拉取到足够的数据或者达到timeout时间后返回拉取的结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetchMessages</span></span>(timeout: <span class="type">Long</span>,</span><br><span class="line">                    replicaId: <span class="type">Int</span>,</span><br><span class="line">                    fetchMinBytes: <span class="type">Int</span>,</span><br><span class="line">                    fetchMaxBytes: <span class="type">Int</span>,</span><br><span class="line">                    hardMaxBytesLimit: <span class="type">Boolean</span>,</span><br><span class="line">                    fetchInfos: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</span><br><span class="line">                    quota: <span class="type">ReplicaQuota</span> = <span class="type">UnboundedQuota</span>,</span><br><span class="line">                    responseCallback: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)] =&gt; <span class="type">Unit</span>,</span><br><span class="line">                    isolationLevel: <span class="type">IsolationLevel</span>) &#123;</span><br><span class="line">    <span class="comment">// 判断请求时来自consumer还是副本同步</span></span><br><span class="line">    <span class="keyword">val</span> isFromFollower = <span class="type">Request</span>.isValidBrokerId(replicaId)</span><br><span class="line">    <span class="comment">// 默认从leader拉取</span></span><br><span class="line">    <span class="keyword">val</span> fetchOnlyFromLeader = replicaId != <span class="type">Request</span>.<span class="type">DebuggingConsumerId</span> &amp;&amp; replicaId != <span class="type">Request</span>.<span class="type">FutureLocalReplicaId</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 如果拉取请求来自consumer(true),只拉取HW以内的数据</span></span><br><span class="line">    <span class="comment">// 如果来自Replica,则没有限制</span></span><br><span class="line">    <span class="keyword">val</span> fetchIsolation = <span class="keyword">if</span> (isFromFollower || replicaId == <span class="type">Request</span>.<span class="type">FutureLocalReplicaId</span>)</span><br><span class="line">      <span class="type">FetchLogEnd</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (isolationLevel == <span class="type">IsolationLevel</span>.<span class="type">READ_COMMITTED</span>)</span><br><span class="line">      <span class="type">FetchTxnCommitted</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="type">FetchHighWatermark</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">readFromLog</span></span>(): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)] = &#123;</span><br><span class="line">      <span class="comment">// 获取本地日志</span></span><br><span class="line">      <span class="keyword">val</span> result = readFromLocalLog(</span><br><span class="line">        replicaId = replicaId,</span><br><span class="line">        fetchOnlyFromLeader = fetchOnlyFromLeader,</span><br><span class="line">        fetchIsolation = fetchIsolation,</span><br><span class="line">        fetchMaxBytes = fetchMaxBytes,</span><br><span class="line">        hardMaxBytesLimit = hardMaxBytesLimit,</span><br><span class="line">        readPartitionInfo = fetchInfos,</span><br><span class="line">        quota = quota)</span><br><span class="line">      <span class="comment">// 如果fetch来自broker的副本同步,更新相关的log end offset</span></span><br><span class="line">      <span class="keyword">if</span> (isFromFollower) updateFollowerLogReadResults(replicaId, result)</span><br><span class="line">      <span class="keyword">else</span> result</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> logReadResults = readFromLog()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check if this fetch request can be satisfied right away</span></span><br><span class="line">    <span class="keyword">val</span> logReadResultValues = logReadResults.map &#123; <span class="keyword">case</span> (_, v) =&gt; v &#125;</span><br><span class="line">    <span class="keyword">val</span> bytesReadable = logReadResultValues.map(_.info.records.sizeInBytes).sum</span><br><span class="line">    <span class="keyword">val</span> errorReadingData = logReadResultValues.foldLeft(<span class="literal">false</span>) ((errorIncurred, readResult) =&gt;</span><br><span class="line">      errorIncurred || (readResult.error != <span class="type">Errors</span>.<span class="type">NONE</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果满足以下条件的其中一个,立马返回结果</span></span><br><span class="line">    <span class="comment">// 1.timeout达到</span></span><br><span class="line">    <span class="comment">// 2.拉取结果为空</span></span><br><span class="line">    <span class="comment">// 3.拉取到足够的数据</span></span><br><span class="line">    <span class="comment">// 4.拉取时遇到Error</span></span><br><span class="line">    <span class="keyword">if</span> (timeout &lt;= <span class="number">0</span> || fetchInfos.isEmpty || bytesReadable &gt;= fetchMinBytes || errorReadingData) &#123;</span><br><span class="line">      <span class="keyword">val</span> fetchPartitionData = logReadResults.map &#123; <span class="keyword">case</span> (tp, result) =&gt;</span><br><span class="line">        tp -&gt; <span class="type">FetchPartitionData</span>(result.error, result.highWatermark, result.leaderLogStartOffset, result.info.records,</span><br><span class="line">          result.lastStableOffset, result.info.abortedTransactions)</span><br><span class="line">      &#125;</span><br><span class="line">      responseCallback(fetchPartitionData)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// construct the fetch results from the read results</span></span><br><span class="line">      <span class="comment">// 延迟发送结果</span></span><br><span class="line">      <span class="keyword">val</span> fetchPartitionStatus = logReadResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</span><br><span class="line">        <span class="keyword">val</span> fetchInfo = fetchInfos.collectFirst &#123;</span><br><span class="line">          <span class="keyword">case</span> (tp, v) <span class="keyword">if</span> tp == topicPartition =&gt; v</span><br><span class="line">        &#125;.getOrElse(sys.error(<span class="string">s&quot;Partition <span class="subst">$topicPartition</span> not found in fetchInfos&quot;</span>))</span><br><span class="line">        (topicPartition, <span class="type">FetchPartitionStatus</span>(result.info.fetchOffsetMetadata, fetchInfo))</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> fetchMetadata = <span class="type">FetchMetadata</span>(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit, fetchOnlyFromLeader,</span><br><span class="line">        fetchIsolation, isFromFollower, replicaId, fetchPartitionStatus)</span><br><span class="line">      <span class="keyword">val</span> delayedFetch = <span class="keyword">new</span> <span class="type">DelayedFetch</span>(timeout, fetchMetadata, <span class="keyword">this</span>, quota, responseCallback)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed fetch operation</span></span><br><span class="line">      <span class="keyword">val</span> delayedFetchKeys = fetchPartitionStatus.map &#123; <span class="keyword">case</span> (tp, _) =&gt; <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(tp) &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory;</span></span><br><span class="line">      <span class="comment">// this is because while the delayed fetch operation is being created, new requests</span></span><br><span class="line">      <span class="comment">// may arrive and hence make this operation completable.</span></span><br><span class="line">      delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="readFromLocalLog-gt-readRecords"><a href="#readFromLocalLog-gt-readRecords" class="headerlink" title="readFromLocalLog()-&gt;readRecords()"></a>readFromLocalLog()-&gt;readRecords()</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>先根据要拉取的tp获取对应的partition对象,根据partition对象获取对应的<span class="type">Replica</span>对象</span><br><span class="line"><span class="number">2.</span>根据<span class="type">Replica</span>对象找到对应的<span class="type">Log</span>对象,然后调用其read()从指定位置读取数据</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按offset从tp列表中读取相应的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readRecords</span></span>(fetchOffset: <span class="type">Long</span>,</span><br><span class="line">                  currentLeaderEpoch: <span class="type">Optional</span>[<span class="type">Integer</span>],</span><br><span class="line">                  maxBytes: <span class="type">Int</span>,</span><br><span class="line">                  fetchIsolation: <span class="type">FetchIsolation</span>,</span><br><span class="line">                  fetchOnlyFromLeader: <span class="type">Boolean</span>,</span><br><span class="line">                  minOneMessage: <span class="type">Boolean</span>): <span class="type">LogReadInfo</span> = inReadLock(leaderIsrUpdateLock) &#123;</span><br><span class="line">    <span class="comment">// 获取相应的Replica对象</span></span><br><span class="line">    <span class="keyword">val</span> localReplica = localReplicaWithEpochOrException(currentLeaderEpoch, fetchOnlyFromLeader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> initialHighWatermark = localReplica.highWatermark.messageOffset</span><br><span class="line">    <span class="keyword">val</span> initialLogStartOffset = localReplica.logStartOffset</span><br><span class="line">    <span class="keyword">val</span> initialLogEndOffset = localReplica.logEndOffset.messageOffset</span><br><span class="line">    <span class="keyword">val</span> initialLastStableOffset = localReplica.lastStableOffset.messageOffset</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 获取hw位置,副本同步不用设置这个值</span></span><br><span class="line">    <span class="keyword">val</span> maxOffsetOpt = fetchIsolation <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">FetchLogEnd</span> =&gt; <span class="type">None</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">FetchHighWatermark</span> =&gt; <span class="type">Some</span>(initialHighWatermark)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">FetchTxnCommitted</span> =&gt; <span class="type">Some</span>(initialLastStableOffset)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fetchedData = localReplica.log <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</span><br><span class="line">        <span class="comment">// 从指定的offset读取数据,副本同步不需要maxOffsetOpt</span></span><br><span class="line">        log.read(fetchOffset, maxBytes, maxOffsetOpt, minOneMessage,</span><br><span class="line">          includeAbortedTxns = fetchIsolation == <span class="type">FetchTxnCommitted</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        error(<span class="string">s&quot;Leader does not have a local log&quot;</span>)</span><br><span class="line">        <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">LogReadInfo</span>(</span><br><span class="line">      fetchedData = fetchedData,</span><br><span class="line">      highWatermark = initialHighWatermark,</span><br><span class="line">      logStartOffset = initialLogStartOffset,</span><br><span class="line">      logEndOffset = initialLogEndOffset,</span><br><span class="line">      lastStableOffset = initialLastStableOffset)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h2><h3 id="Log对象"><a href="#Log对象" class="headerlink" title="Log对象"></a>Log对象</h3><h4 id="read"><a href="#read" class="headerlink" title="read()"></a>read()</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>根据要读取的起始偏移量(startOffset)读取offset索引文件中对应的物理位置</span><br><span class="line"><span class="number">2.</span>查找offset索引文件最后返回,起始偏移量对应的最近物理位置(startPosition)</span><br><span class="line"><span class="number">3.</span>根据startPosition指定定位到数据文件,然后读取数据文件内容</span><br><span class="line"><span class="number">4.</span>最多能读到数据文件的结束位置</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从指定offset开始读取数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>,</span><br><span class="line">           maxLength: <span class="type">Int</span>,</span><br><span class="line">           maxOffset: <span class="type">Option</span>[<span class="type">Long</span>],</span><br><span class="line">           minOneMessage: <span class="type">Boolean</span>,</span><br><span class="line">           includeAbortedTxns: <span class="type">Boolean</span>): <span class="type">FetchDataInfo</span> = &#123;</span><br><span class="line">    maybeHandleIOException(<span class="string">s&quot;Exception while reading from <span class="subst">$topicPartition</span> in dir <span class="subst">$&#123;dir.getParent&#125;</span>&quot;</span>) &#123;</span><br><span class="line">      trace(<span class="string">s&quot;Reading <span class="subst">$maxLength</span> bytes from offset <span class="subst">$startOffset</span> of length <span class="subst">$size</span> bytes&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Because we don&#x27;t use lock for reading, the synchronization is a little bit tricky.</span></span><br><span class="line">      <span class="comment">// We create the local variables to avoid race conditions with updates to the log.</span></span><br><span class="line">      <span class="keyword">val</span> currentNextOffsetMetadata = nextOffsetMetadata</span><br><span class="line">      <span class="keyword">val</span> next = currentNextOffsetMetadata.messageOffset</span><br><span class="line">      <span class="keyword">if</span> (startOffset == next) &#123;</span><br><span class="line">        <span class="keyword">val</span> abortedTransactions =</span><br><span class="line">          <span class="keyword">if</span> (includeAbortedTxns) <span class="type">Some</span>(<span class="type">List</span>.empty[<span class="type">AbortedTransaction</span>])</span><br><span class="line">          <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">        <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(currentNextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>, firstEntryIncomplete = <span class="literal">false</span>,</span><br><span class="line">          abortedTransactions = abortedTransactions)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 查找对应的日志分段</span></span><br><span class="line">      <span class="keyword">var</span> segmentEntry = segments.floorEntry(startOffset)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// return error on attempt to read beyond the log end offset or read below log start offset</span></span><br><span class="line">      <span class="keyword">if</span> (startOffset &gt; next || segmentEntry == <span class="literal">null</span> || startOffset &lt; logStartOffset)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetOutOfRangeException</span>(<span class="string">s&quot;Received request for offset <span class="subst">$startOffset</span> for partition <span class="subst">$topicPartition</span>, &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;but we only have log segments in the range <span class="subst">$logStartOffset</span> to <span class="subst">$next</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (segmentEntry != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> segment = segmentEntry.getValue</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果fetch请求刚好发生在the active segment上</span></span><br><span class="line">        <span class="comment">// 当多个Fetch请求同时处理,如果nextOffsetMetadata更新不及时</span></span><br><span class="line">        <span class="comment">// 可能会导致发送OffsetOutOfRangeException异常</span></span><br><span class="line">        <span class="comment">// 为了解决这个问题,这里能读取的最大位置是对应的物理位置(exposedPos),而不是the log end of the active segment</span></span><br><span class="line">        <span class="keyword">val</span> maxPosition = &#123;</span><br><span class="line">          <span class="keyword">if</span> (segmentEntry == segments.lastEntry) &#123;</span><br><span class="line">            <span class="comment">// nextOffsetMetadata对应的实际物理位置</span></span><br><span class="line">            <span class="keyword">val</span> exposedPos = nextOffsetMetadata.relativePositionInSegment.toLong</span><br><span class="line">            <span class="comment">// Check the segment again in case a new segment has just rolled out.</span></span><br><span class="line">            <span class="comment">// 可能会有新的segment产生,所以需要再次判断</span></span><br><span class="line">            <span class="keyword">if</span> (segmentEntry != segments.lastEntry)</span><br><span class="line">            <span class="comment">// New log segment has rolled out, we can read up to the file end.</span></span><br><span class="line">              segment.size</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">              exposedPos</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            segment.size</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 从segment中读取相应的数据</span></span><br><span class="line">        <span class="keyword">val</span> fetchInfo = segment.read(startOffset, maxOffset, maxLength, maxPosition, minOneMessage)</span><br><span class="line">        <span class="comment">// 如果该日志分段没有读取到数据,则读取更高的日志分段</span></span><br><span class="line">        <span class="keyword">if</span> (fetchInfo == <span class="literal">null</span>) &#123;</span><br><span class="line">          segmentEntry = segments.higherEntry(segmentEntry.getKey)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">if</span> (includeAbortedTxns)</span><br><span class="line">            addAbortedTransactions(startOffset, segmentEntry, fetchInfo)</span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            fetchInfo</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// okay we are beyond the end of the last segment with no data fetched although the start offset is in range,</span></span><br><span class="line">      <span class="comment">// this can happen when all messages with offset larger than start offsets have been deleted.</span></span><br><span class="line">      <span class="comment">// In this case, we will return the empty set with log end offset metadata</span></span><br><span class="line">      <span class="type">FetchDataInfo</span>(nextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="LogSegment对象"><a href="#LogSegment对象" class="headerlink" title="LogSegment对象"></a>LogSegment对象</h3><h4 id="read-1"><a href="#read-1" class="headerlink" title="read()"></a>read()</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>根据startOffset得到实际的物理位置(translateOffset())</span><br><span class="line"><span class="number">2.</span>计算要读取的实际物理长度</span><br><span class="line"><span class="number">3.</span>根据实际起始物理位置和要读取实际物理长度读取数据文件</span><br><span class="line"></span><br><span class="line"><span class="comment">// 读取日志分段(副本同步不会设置maxSize)</span></span><br><span class="line"><span class="meta">@threadsafe</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, maxOffset: <span class="type">Option</span>[<span class="type">Long</span>], maxSize: <span class="type">Int</span>, maxPosition: <span class="type">Long</span> = size,</span><br><span class="line">           minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (maxSize &lt; <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s&quot;Invalid max size <span class="subst">$maxSize</span> for log read from segment <span class="subst">$log</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// log文件的物理长度</span></span><br><span class="line">    <span class="keyword">val</span> logSize = log.sizeInBytes <span class="comment">// this may change, need to save a consistent copy</span></span><br><span class="line">    <span class="comment">// 将起始的offset转换为起始的实际物理位置</span></span><br><span class="line">    <span class="keyword">val</span> startOffsetAndSize = translateOffset(startOffset)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if the start position is already off the end of the log, return null</span></span><br><span class="line">    <span class="keyword">if</span> (startOffsetAndSize == <span class="literal">null</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> startPosition = startOffsetAndSize.position</span><br><span class="line">    <span class="keyword">val</span> offsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(startOffset, <span class="keyword">this</span>.baseOffset, startPosition)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> adjustedMaxSize =</span><br><span class="line">      <span class="keyword">if</span> (minOneMessage) math.max(maxSize, startOffsetAndSize.size)</span><br><span class="line">      <span class="keyword">else</span> maxSize</span><br><span class="line"></span><br><span class="line">    <span class="comment">// return a log segment but with zero size in the case below</span></span><br><span class="line">    <span class="keyword">if</span> (adjustedMaxSize == <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate the length of the message set to read based on whether or not they gave us a maxOffset</span></span><br><span class="line">    <span class="comment">// 计算读取的长度</span></span><br><span class="line">    <span class="keyword">val</span> fetchSize: <span class="type">Int</span> = maxOffset <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// 副本同步时计算方式</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// no max offset, just read until the max position</span></span><br><span class="line">        <span class="comment">// 直接读最大的位置</span></span><br><span class="line">        min((maxPosition - startPosition).toInt, adjustedMaxSize)</span><br><span class="line">      <span class="comment">// consumer拉取时计算方式</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(offset) =&gt;</span><br><span class="line">        <span class="comment">// there is a max offset, translate it to a file position and use that to calculate the max read size;</span></span><br><span class="line">        <span class="comment">// when the leader of a partition changes, it&#x27;s possible for the new leader&#x27;s high watermark to be less than the</span></span><br><span class="line">        <span class="comment">// true high watermark in the previous leader for a short window. In this window, if a consumer fetches on an</span></span><br><span class="line">        <span class="comment">// offset between new leader&#x27;s high watermark and the log end offset, we want to return an empty response.</span></span><br><span class="line">        <span class="keyword">if</span> (offset &lt; startOffset)</span><br><span class="line">          <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>, firstEntryIncomplete = <span class="literal">false</span>)</span><br><span class="line">        <span class="keyword">val</span> mapping = translateOffset(offset, startPosition)</span><br><span class="line">        <span class="keyword">val</span> endPosition =</span><br><span class="line">          <span class="keyword">if</span> (mapping == <span class="literal">null</span>)</span><br><span class="line">            logSize <span class="comment">// the max offset is off the end of the log, use the end of the file</span></span><br><span class="line">          <span class="keyword">else</span></span><br><span class="line">            mapping.position</span><br><span class="line">        min(min(maxPosition, endPosition) - startPosition, adjustedMaxSize).toInt</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据起始的物理位置和读取长度读取数据文件</span></span><br><span class="line">    <span class="type">FetchDataInfo</span>(offsetMetadata, log.slice(startPosition, fetchSize),</span><br><span class="line">      firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="translateOffset"><a href="#translateOffset" class="headerlink" title="translateOffset()"></a>translateOffset()</h4><p>具体计算方法可以参考&lt;Kafka技术内幕&gt;</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>查找offset索引文件: 调用offset索引文件的lookup()查找方法,获取里startOffset最接近的物理位置</span><br><span class="line"><span class="number">2.</span>调用数据文件的searchFor()方法,从指定的物理位置开始读取每条数据,直到找到对应的offset的物理位置</span><br><span class="line"></span><br><span class="line"><span class="meta">@threadsafe</span></span><br><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">translateOffset</span></span>(offset: <span class="type">Long</span>, startingFilePosition: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogOffsetPosition</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> mapping = offsetIndex.lookup(offset)</span><br><span class="line">  log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查找索引文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(targetOffset: <span class="type">Long</span>): <span class="type">OffsetPosition</span> = &#123;</span><br><span class="line">  <span class="comment">// 查找小于等于指定offset的最大offset,并且返回对应的offset和实际物理位置</span></span><br><span class="line">  maybeLock(lock) &#123;</span><br><span class="line">    <span class="keyword">val</span> idx = mmap.duplicate <span class="comment">// 查询时,mmap会发生变化,先复制一个</span></span><br><span class="line">    <span class="keyword">val</span> slot = largestLowerBoundSlotFor(idx, targetOffset, <span class="type">IndexSearchType</span>.<span class="type">KEY</span>) <span class="comment">// 二分查找</span></span><br><span class="line">    <span class="keyword">if</span>(slot == <span class="number">-1</span>)</span><br><span class="line">      <span class="type">OffsetPosition</span>(baseOffset, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="comment">// 计算绝对偏移量,在计算物理位置</span></span><br><span class="line">      parseEntry(idx, slot).asInstanceOf[<span class="type">OffsetPosition</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">parseEntry</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">IndexEntry</span> = &#123;</span><br><span class="line">    <span class="type">OffsetPosition</span>(baseOffset + relativeOffset(buffer, n), physical(buffer, n))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">relativeOffset</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize)</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">physical</span></span>(buffer: <span class="type">ByteBuffer</span>, n: <span class="type">Int</span>): <span class="type">Int</span> = buffer.getInt(n * entrySize + <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据文件查找,前面找到的物理位置是一个接近值</span></span><br><span class="line">public <span class="type">LogOffsetPosition</span> searchForOffsetWithSize(long targetOffset, int startingPosition) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">FileChannelRecordBatch</span> batch : batchesFrom(startingPosition)) &#123;</span><br><span class="line">        long offset = batch.lastOffset();</span><br><span class="line">        <span class="keyword">if</span> (offset &gt;= targetOffset)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">LogOffsetPosition</span>(offset, batch.position(), batch.sizeInBytes());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十四Controller</title>
    <url>/2020/05/08/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E5%9B%9BController/</url>
    <content><![CDATA[<blockquote>
<p>Controller类似于其他分布式系统的Master角色,Kafka任何一台Broker都可以作为Controller,但是同一集群同时只会有一个alive状态的Controller</p>
</blockquote>
<span id="more"></span>

<h2 id="Controller简介"><a href="#Controller简介" class="headerlink" title="Controller简介"></a>Controller简介</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Broker的上线,下线处理</span><br><span class="line">新创建的topic,已经topic的分区扩容,处理分区副本的分配,leader选举</span><br><span class="line">管理所有副本的状态机和分区状态机,处理状态机变化事件</span><br><span class="line">topic删除,副本迁移,leader切换等处理</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Controller选举"><a href="#Controller选举" class="headerlink" title="Controller选举"></a>Controller选举</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">Kafka</span>的每台<span class="type">Broker</span>启动过程中都会启动<span class="type">Controller</span>服务</span><br><span class="line"></span><br><span class="line"><span class="comment">// KafkaServer</span></span><br><span class="line">kafkaController = <span class="keyword">new</span> <span class="type">KafkaController</span>(config, zkClient, time, metrics, brokerInfo, tokenManager, threadNamePrefix)</span><br><span class="line">kafkaController.startup()</span><br></pre></td></tr></table></figure>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() = &#123;</span><br><span class="line">  <span class="comment">// zk上controller节点变化</span></span><br><span class="line">  zkClient.registerStateChangeHandler(<span class="keyword">new</span> <span class="type">StateChangeHandler</span> &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> name: <span class="type">String</span> = <span class="type">StateChangeHandlers</span>.<span class="type">ControllerHandler</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">afterInitializingSession</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      eventManager.put(<span class="type">RegisterBrokerAndReelect</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">beforeInitializingSession</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> expireEvent = <span class="keyword">new</span> <span class="type">Expire</span></span><br><span class="line">      eventManager.clearAndPut(expireEvent)</span><br><span class="line">      <span class="comment">// Block initialization of the new session until the expiration event is being handled,</span></span><br><span class="line">      <span class="comment">// which ensures that all pending events have been processed before creating the new session</span></span><br><span class="line">      expireEvent.waitUntilProcessingStarted()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">  eventManager.put(<span class="type">Startup</span>) <span class="comment">// 选举</span></span><br><span class="line">  eventManager.start()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="选举"><a href="#选举" class="headerlink" title="选举"></a>选举</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">Startup</span> <span class="keyword">extends</span> <span class="title">ControllerEvent</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state</span> </span>= <span class="type">ControllerState</span>.<span class="type">ControllerChange</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    zkClient.registerZNodeChangeHandlerAndCheckExistence(controllerChangeHandler) <span class="comment">// controller节点数据变化</span></span><br><span class="line">    elect() <span class="comment">// 在controller不存的情况下选举controller,存在的话,就是从zk获取当前的controller节点信息</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="registerZNodeChangeHandlerAndCheckExistence"><a href="#registerZNodeChangeHandlerAndCheckExistence" class="headerlink" title="registerZNodeChangeHandlerAndCheckExistence"></a>registerZNodeChangeHandlerAndCheckExistence</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>如果/controller节点内容变化,那么更新一下controller最新的节点信息,如果节点之前是controller,现在不是,需要执行关闭操作onControllerResignation()</span><br><span class="line"><span class="number">2.</span>如果/controller节点被删除,如果之前是controller,需要关闭,然后重新选举</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册controllerChangeHandler监听</span></span><br><span class="line"><span class="comment">// ControllerChange节点改变</span></span><br><span class="line"><span class="comment">// Reelect重新选举</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ControllerChangeHandler</span>(<span class="params">controller: <span class="type">KafkaController</span>, eventManager: <span class="type">ControllerEventManager</span></span>) <span class="keyword">extends</span> <span class="title">ZNodeChangeHandler</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> path: <span class="type">String</span> = <span class="type">ControllerZNode</span>.path</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleCreation</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">ControllerChange</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleDeletion</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">Reelect</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">handleDataChange</span></span>(): <span class="type">Unit</span> = eventManager.put(controller.<span class="type">ControllerChange</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="elect"><a href="#elect" class="headerlink" title="elect"></a>elect</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>获取zk的/controller节点的信息,获取controller的brokerId,如果节点不存在,那么获取controllerId为<span class="number">-1</span></span><br><span class="line"><span class="number">2.</span>如果controller不为<span class="number">-1</span>,controller已经存在,直接结束</span><br><span class="line"><span class="number">3.</span>如果controller为<span class="number">-1</span>,controller不存在,当前broker开始在zk注册controller</span><br><span class="line"><span class="number">4.</span>如果注册成功,当前broker成为controller,调用onControllerFailover()方法正式初始化controller</span><br><span class="line"><span class="number">5.</span>如果注册失败,那么直接返回</span><br><span class="line"><span class="number">6.</span>controller节点是临时节点,当前controller与zk的session断开,那么controller的临时节点会消失,会触发controller的重新选举</span><br><span class="line"></span><br><span class="line"><span class="comment">// 进行controller选举</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">elect</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timestamp = time.milliseconds</span><br><span class="line">    activeControllerId = zkClient.getControllerId.getOrElse(<span class="number">-1</span>)</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * We can get here during the initial startup and the handleDeleted ZK callback. Because of the potential race condition,</span></span><br><span class="line"><span class="comment">     * it&#x27;s possible that the controller has already been elected when we get here. This check will prevent the following</span></span><br><span class="line"><span class="comment">     * createEphemeralPath method from getting into an infinite loop if this broker is already the controller.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">if</span> (activeControllerId != <span class="number">-1</span>) &#123;</span><br><span class="line">      debug(<span class="string">s&quot;Broker <span class="subst">$activeControllerId</span> has been elected as the controller, so stopping the election process.&quot;</span>)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 没有异常就创建成功</span></span><br><span class="line">      zkClient.registerController(config.brokerId, timestamp)</span><br><span class="line">      info(<span class="string">s&quot;<span class="subst">$&#123;config.brokerId&#125;</span> successfully elected as the controller&quot;</span>)</span><br><span class="line">      activeControllerId = config.brokerId</span><br><span class="line">      onControllerFailover() <span class="comment">// 成为controller,开启监听</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">// 创建时,发现有broker提前注册成功</span></span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">NodeExistsException</span> =&gt;</span><br><span class="line">        <span class="comment">// If someone else has written the path, then</span></span><br><span class="line">        activeControllerId = zkClient.getControllerId.getOrElse(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (activeControllerId != <span class="number">-1</span>)</span><br><span class="line">          debug(<span class="string">s&quot;Broker <span class="subst">$activeControllerId</span> was elected as controller instead of broker <span class="subst">$&#123;config.brokerId&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">          warn(<span class="string">&quot;A controller has been elected but just resigned, this will result in another round of election&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 其他异常,重新选举controller</span></span><br><span class="line">      <span class="keyword">case</span> e2: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        error(<span class="string">s&quot;Error while electing or becoming controller on broker <span class="subst">$&#123;config.brokerId&#125;</span>&quot;</span>, e2)</span><br><span class="line">        triggerControllerMove()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Controller启动"><a href="#Controller启动" class="headerlink" title="Controller启动"></a>Controller启动</h2><h3 id="onControllerFailover"><a href="#onControllerFailover" class="headerlink" title="onControllerFailover"></a>onControllerFailover</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>注册<span class="type">Controller</span> epoch变化监听器</span><br><span class="line"><span class="number">2.</span>增加<span class="type">Controller</span> epoch</span><br><span class="line"><span class="number">3.</span>初始化<span class="type">Controller</span></span><br><span class="line"><span class="number">4.</span>启动<span class="type">Controller</span>的channel管理器</span><br><span class="line"><span class="number">5.</span>启动replica状态机</span><br><span class="line"><span class="number">6.</span>启动partition状态机</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">onControllerFailover</span></span>() &#123;</span><br><span class="line">    info(<span class="string">&quot;Reading controller epoch from ZooKeeper&quot;</span>)</span><br><span class="line">    readControllerEpochFromZooKeeper()</span><br><span class="line">    info(<span class="string">&quot;Incrementing controller epoch in ZooKeeper&quot;</span>)</span><br><span class="line">    incrementControllerEpoch()</span><br><span class="line">    info(<span class="string">&quot;Registering handlers&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks</span></span><br><span class="line">    <span class="comment">// 注册监听zk上controller节点的子节点变化</span></span><br><span class="line">    <span class="comment">// topic上下线,broker上下线,isr变动</span></span><br><span class="line">    <span class="keyword">val</span> childChangeHandlers = <span class="type">Seq</span>(brokerChangeHandler, topicChangeHandler, topicDeletionHandler, logDirEventNotificationHandler,</span><br><span class="line">      isrChangeNotificationHandler)</span><br><span class="line">    childChangeHandlers.foreach(zkClient.registerZNodeChildChangeHandler)</span><br><span class="line">    <span class="comment">// 最优replica leader选举,分区迁移</span></span><br><span class="line">    <span class="keyword">val</span> nodeChangeHandlers = <span class="type">Seq</span>(preferredReplicaElectionHandler, partitionReassignmentHandler)</span><br><span class="line">    nodeChangeHandlers.foreach(zkClient.registerZNodeChangeHandlerAndCheckExistence)</span><br><span class="line"></span><br><span class="line">    info(<span class="string">&quot;Deleting log dir event notifications&quot;</span>)</span><br><span class="line">    zkClient.deleteLogDirEventNotifications()</span><br><span class="line">    info(<span class="string">&quot;Deleting isr change notifications&quot;</span>)</span><br><span class="line">    zkClient.deleteIsrChangeNotifications()</span><br><span class="line">    info(<span class="string">&quot;Initializing controller context&quot;</span>)</span><br><span class="line">    <span class="comment">// 初始化controller,包括alive broker列表,partition详细信息等</span></span><br><span class="line">    initializeControllerContext()</span><br><span class="line">    info(<span class="string">&quot;Fetching topic deletions in progress&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> (topicsToBeDeleted, topicsIneligibleForDeletion) = fetchTopicDeletionsInProgress()</span><br><span class="line">    info(<span class="string">&quot;Initializing topic deletion manager&quot;</span>)</span><br><span class="line">    topicDeletionManager.init(topicsToBeDeleted, topicsIneligibleForDeletion)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We need to send UpdateMetadataRequest after the controller context is initialized and before the state machines</span></span><br><span class="line">    <span class="comment">// are started. The is because brokers need to receive the list of live brokers from UpdateMetadataRequest before</span></span><br><span class="line">    <span class="comment">// they can process the LeaderAndIsrRequests that are generated by replicaStateMachine.startup() and</span></span><br><span class="line">    <span class="comment">// partitionStateMachine.startup().</span></span><br><span class="line">    info(<span class="string">&quot;Sending update metadata request&quot;</span>)</span><br><span class="line">    <span class="comment">// 在controller初始化之后,发送UpdateMetadata请求在状态机启动之前,获取当前存活的brokerList</span></span><br><span class="line">    <span class="comment">// 因为它们需要处理来自副本状态机和分区状态机启动发送的LeaderAndIsr请求</span></span><br><span class="line">    sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化replica状态信息:replica存活状态OnlineReplica,否则ReplicaDeletionIneligible</span></span><br><span class="line">    replicaStateMachine.startup()</span><br><span class="line">    <span class="comment">// 初始化partition状态信息:leader所在的broker是active的,状态为OnlinePartition,否则OfflinePartition</span></span><br><span class="line">    <span class="comment">// 并状态为OfflinePartition的topic选举leader</span></span><br><span class="line">    partitionStateMachine.startup()</span><br><span class="line"></span><br><span class="line">    info(<span class="string">s&quot;Ready to serve as the new controller with epoch <span class="subst">$epoch</span>&quot;</span>)</span><br><span class="line">    <span class="comment">// 触发一次分区副本迁移操作</span></span><br><span class="line">    maybeTriggerPartitionReassignment(controllerContext.partitionsBeingReassigned.keySet)</span><br><span class="line">    topicDeletionManager.tryTopicDeletion()</span><br><span class="line">    <span class="keyword">val</span> pendingPreferredReplicaElections = fetchPendingPreferredReplicaElections()</span><br><span class="line">    onPreferredReplicaElection(pendingPreferredReplicaElections)</span><br><span class="line">    info(<span class="string">&quot;Starting the controller scheduler&quot;</span>)</span><br><span class="line">    kafkaScheduler.startup()</span><br><span class="line">    <span class="comment">// 如果开启自动均衡</span></span><br><span class="line">    <span class="keyword">if</span> (config.autoLeaderRebalanceEnable) &#123;</span><br><span class="line">      scheduleAutoLeaderRebalanceTask(delay = <span class="number">5</span>, unit = <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>) <span class="comment">// 发送最新的meta信息</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (config.tokenAuthEnabled) &#123;</span><br><span class="line">      info(<span class="string">&quot;starting the token expiry check scheduler&quot;</span>)</span><br><span class="line">      tokenCleanScheduler.startup()</span><br><span class="line">      tokenCleanScheduler.schedule(name = <span class="string">&quot;delete-expired-tokens&quot;</span>,</span><br><span class="line">        fun = tokenManager.expireTokens,</span><br><span class="line">        period = config.delegationTokenExpiryCheckIntervalMs,</span><br><span class="line">        unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="KafkaController内容"><a href="#KafkaController内容" class="headerlink" title="KafkaController内容"></a>KafkaController内容</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两个状态机:</span><br><span class="line">    分区状态机,副本状态机</span><br><span class="line">一个管理器:</span><br><span class="line">    Channel管理器,负责所有的Broker通信</span><br><span class="line">相关缓存:</span><br><span class="line">    Partition信息,Topic信息,BrokerId信息等</span><br><span class="line">四种Leader选举机制:</span><br><span class="line">    用于Leader下线,Broker下线,Partition分配,最优Leader选举</span><br><span class="line">    对应Handler不同的操作</span><br><span class="line">    OfflinePartitionLeaderElectionStrategy</span><br><span class="line">    ReassignPartitionLeaderElectionStrategy</span><br><span class="line">    PreferredReplicaPartitionLeaderElectionStrategy</span><br><span class="line">    ControlledShutdownPartitionLeaderElectionStrategy</span><br></pre></td></tr></table></figure>
<h3 id="initializeControllerContext初始化"><a href="#initializeControllerContext初始化" class="headerlink" title="initializeControllerContext初始化"></a>initializeControllerContext初始化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.从zk中获取所有aliveBroker列表,记录到liveBrokers</span><br><span class="line">2.从zk中获取所有topic列表,记录到allTopics</span><br><span class="line">3.registerPartitionModificationsHandlers注册分区改变监听</span><br><span class="line">4.从zk中获取所有Replica信息,updatePartitionReplicaAssignment()更新到partitionReplicaAssignmentUnderlying</span><br><span class="line">5.清除partitionLeadershipInfo,LeaderAndIsr信息</span><br><span class="line">6.注册Broker改变监听</span><br><span class="line">7.更新LeaderAndIsr信息</span><br><span class="line">8.启动Controller的ChannelManager</span><br><span class="line">9.初始化需要进行副本迁移的Partition列表</span><br></pre></td></tr></table></figure>
<h3 id="ControllerChannelManager"><a href="#ControllerChannelManager" class="headerlink" title="ControllerChannelManager"></a>ControllerChannelManager</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startChannelManager</span></span>() &#123;</span><br><span class="line">  controllerContext.controllerChannelManager = <span class="keyword">new</span> <span class="type">ControllerChannelManager</span>(controllerContext, config, time, metrics,</span><br><span class="line">    stateChangeLogger, threadNamePrefix)</span><br><span class="line">  controllerContext.controllerChannelManager.startup()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ControllerChannelManager在初始化时,会为集群中每个节点初始化一个ControllerBrokerStateInfo对象</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> brokerStateInfo = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">ControllerBrokerStateInfo</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// 该对象包括NetworkClient,Node,BlockingQueue,RequestSendThread</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ControllerBrokerStateInfo</span>(<span class="params">networkClient: <span class="type">NetworkClient</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">     brokerNode: <span class="type">Node</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">     messageQueue: <span class="type">BlockingQueue</span>[<span class="type">QueueItem</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">     requestSendThread: <span class="type">RequestSendThread</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">     queueSizeGauge: <span class="type">Gauge</span>[<span class="type">Int</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">     requestRateAndTimeMetrics: <span class="type">Timer</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="comment">// KafkaController向Broker发送请求</span></span></span><br><span class="line"><span class="class"><span class="title">private</span>[controller] <span class="title">def</span> <span class="title">sendRequest</span>(<span class="params">brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">                                    callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = null</span>) </span>= &#123;</span><br><span class="line">  <span class="comment">// 实际调用controllerChannelManager的sendRequest()</span></span><br><span class="line">  controllerContext.controllerChannelManager.sendRequest(brokerId, apiKey, request, callback)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ControllerChannelManager发送方法,并不是实际发送,而是添加到对应的queue中</span></span><br><span class="line"><span class="comment">// 真正的发送在ResultSendThread中处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRequest</span></span>(brokerId: <span class="type">Int</span>, apiKey: <span class="type">ApiKeys</span>, request: <span class="type">AbstractRequest</span>.<span class="type">Builder</span>[_ &lt;: <span class="type">AbstractRequest</span>],</span><br><span class="line">                callback: <span class="type">AbstractResponse</span> =&gt; <span class="type">Unit</span> = <span class="literal">null</span>) &#123;</span><br><span class="line">  brokerLock synchronized &#123;</span><br><span class="line">    <span class="keyword">val</span> stateInfoOpt = brokerStateInfo.get(brokerId)</span><br><span class="line">    stateInfoOpt <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(stateInfo) =&gt;</span><br><span class="line">        stateInfo.messageQueue.put(<span class="type">QueueItem</span>(apiKey, request, callback, time.milliseconds()))</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        warn(<span class="string">s&quot;Not sending request <span class="subst">$request</span> to broker <span class="subst">$brokerId</span>, since it is offline.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="四种选举策略"><a href="#四种选举策略" class="headerlink" title="四种选举策略"></a>四种选举策略</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">OfflinePartitionLeaderElectionStrategy</span><br><span class="line">    Leader掉线时触发</span><br><span class="line">ReassignPartitionLeaderElectionStrategy</span><br><span class="line">    分区副本重新分配数据同步完成后触发</span><br><span class="line">PreferredReplicaPartitionLeaderElectionStrategy</span><br><span class="line">    最优Leader选举,手动触发或自动leader均衡调度时触发</span><br><span class="line">ControlledShutdownPartitionLeaderElectionStrategy</span><br><span class="line">    Broker发送ShutDown请求主动关闭服务时触发</span><br></pre></td></tr></table></figure>
<h3 id="OfflinePartitionLeaderElectionStrategy"><a href="#OfflinePartitionLeaderElectionStrategy" class="headerlink" title="OfflinePartitionLeaderElectionStrategy"></a>OfflinePartitionLeaderElectionStrategy</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">leaderForOffline</span></span>(leaderIsrAndControllerEpochs: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LeaderIsrAndControllerEpoch</span>)]):</span><br><span class="line">  <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">Option</span>[<span class="type">LeaderAndIsr</span>], <span class="type">Seq</span>[<span class="type">Int</span>])] = &#123;</span><br><span class="line">    <span class="keyword">val</span> (partitionsWithNoLiveInSyncReplicas, partitionsWithLiveInSyncReplicas) = leaderIsrAndControllerEpochs.partition &#123; <span class="keyword">case</span> (partition, leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">      <span class="keyword">val</span> liveInSyncReplicas = leaderIsrAndControllerEpoch.leaderAndIsr.isr.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))</span><br><span class="line">      liveInSyncReplicas.isEmpty</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> (logConfigs, failed) = zkClient.getLogConfigs(partitionsWithNoLiveInSyncReplicas.map &#123; <span class="keyword">case</span> (partition, _) =&gt; partition.topic &#125;, config.originals())</span><br><span class="line">    <span class="keyword">val</span> partitionsWithUncleanLeaderElectionState = partitionsWithNoLiveInSyncReplicas.map &#123; <span class="keyword">case</span> (partition, leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (failed.contains(partition.topic)) &#123;</span><br><span class="line">        logFailedStateChange(partition, partitionState(partition), <span class="type">OnlinePartition</span>, failed(partition.topic))</span><br><span class="line">        (partition, <span class="type">None</span>, <span class="literal">false</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (partition, <span class="type">Option</span>(leaderIsrAndControllerEpoch), logConfigs(partition.topic).uncleanLeaderElectionEnable.booleanValue())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; ++ partitionsWithLiveInSyncReplicas.map &#123; <span class="keyword">case</span> (partition, leaderIsrAndControllerEpoch) =&gt; (partition, <span class="type">Option</span>(leaderIsrAndControllerEpoch), <span class="literal">false</span>) &#125;</span><br><span class="line">    partitionsWithUncleanLeaderElectionState.map &#123; <span class="keyword">case</span> (partition, leaderIsrAndControllerEpochOpt, uncleanLeaderElectionEnabled) =&gt;</span><br><span class="line">      <span class="keyword">val</span> assignment = controllerContext.partitionReplicaAssignment(partition)</span><br><span class="line">      <span class="keyword">val</span> liveReplicas = assignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))</span><br><span class="line">      <span class="keyword">if</span> (leaderIsrAndControllerEpochOpt.nonEmpty) &#123;</span><br><span class="line">        <span class="keyword">val</span> leaderIsrAndControllerEpoch = leaderIsrAndControllerEpochOpt.get</span><br><span class="line">        <span class="keyword">val</span> isr = leaderIsrAndControllerEpoch.leaderAndIsr.isr</span><br><span class="line">        <span class="keyword">val</span> leaderOpt = <span class="type">PartitionLeaderElectionAlgorithms</span>.offlinePartitionLeaderElection(assignment, isr, liveReplicas.toSet, uncleanLeaderElectionEnabled, controllerContext)</span><br><span class="line">        <span class="keyword">val</span> newLeaderAndIsrOpt = leaderOpt.map &#123; leader =&gt;</span><br><span class="line">          <span class="keyword">val</span> newIsr = <span class="keyword">if</span> (isr.contains(leader)) isr.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))</span><br><span class="line">          <span class="keyword">else</span> <span class="type">List</span>(leader)</span><br><span class="line">          leaderIsrAndControllerEpoch.leaderAndIsr.newLeaderAndIsr(leader, newIsr)</span><br><span class="line">        &#125;</span><br><span class="line">        (partition, newLeaderAndIsrOpt, liveReplicas)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        (partition, <span class="type">None</span>, liveReplicas)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="ReassignPartitionLeaderElectionStrategy"><a href="#ReassignPartitionLeaderElectionStrategy" class="headerlink" title="ReassignPartitionLeaderElectionStrategy"></a>ReassignPartitionLeaderElectionStrategy</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">leaderForReassign</span></span>(leaderIsrAndControllerEpochs: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LeaderIsrAndControllerEpoch</span>)]):</span><br><span class="line">  <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">Option</span>[<span class="type">LeaderAndIsr</span>], <span class="type">Seq</span>[<span class="type">Int</span>])] = &#123;</span><br><span class="line">    leaderIsrAndControllerEpochs.map &#123; <span class="keyword">case</span> (partition, leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">      <span class="keyword">val</span> reassignment = controllerContext.partitionsBeingReassigned(partition).newReplicas</span><br><span class="line">      <span class="keyword">val</span> liveReplicas = reassignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))</span><br><span class="line">      <span class="keyword">val</span> isr = leaderIsrAndControllerEpoch.leaderAndIsr.isr</span><br><span class="line">      <span class="keyword">val</span> leaderOpt = <span class="type">PartitionLeaderElectionAlgorithms</span>.reassignPartitionLeaderElection(reassignment, isr, liveReplicas.toSet)</span><br><span class="line">      <span class="keyword">val</span> newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderIsrAndControllerEpoch.leaderAndIsr.newLeader(leader))</span><br><span class="line">      (partition, newLeaderAndIsrOpt, reassignment)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="PreferredReplicaPartitionLeaderElectionStrategy"><a href="#PreferredReplicaPartitionLeaderElectionStrategy" class="headerlink" title="PreferredReplicaPartitionLeaderElectionStrategy"></a>PreferredReplicaPartitionLeaderElectionStrategy</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">leaderForPreferredReplica</span></span>(leaderIsrAndControllerEpochs: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LeaderIsrAndControllerEpoch</span>)]):</span><br><span class="line">  <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">Option</span>[<span class="type">LeaderAndIsr</span>], <span class="type">Seq</span>[<span class="type">Int</span>])] = &#123;</span><br><span class="line">    leaderIsrAndControllerEpochs.map &#123; <span class="keyword">case</span> (partition, leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">      <span class="keyword">val</span> assignment = controllerContext.partitionReplicaAssignment(partition)</span><br><span class="line">      <span class="keyword">val</span> liveReplicas = assignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition))</span><br><span class="line">      <span class="keyword">val</span> isr = leaderIsrAndControllerEpoch.leaderAndIsr.isr</span><br><span class="line">      <span class="keyword">val</span> leaderOpt = <span class="type">PartitionLeaderElectionAlgorithms</span>.preferredReplicaPartitionLeaderElection(assignment, isr, liveReplicas.toSet)</span><br><span class="line">      <span class="keyword">val</span> newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderIsrAndControllerEpoch.leaderAndIsr.newLeader(leader))</span><br><span class="line">      (partition, newLeaderAndIsrOpt, assignment)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="ControlledShutdownPartitionLeaderElectionStrategy"><a href="#ControlledShutdownPartitionLeaderElectionStrategy" class="headerlink" title="ControlledShutdownPartitionLeaderElectionStrategy"></a>ControlledShutdownPartitionLeaderElectionStrategy</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">leaderForControlledShutdown</span></span>(leaderIsrAndControllerEpochs: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LeaderIsrAndControllerEpoch</span>)], shuttingDownBrokers: <span class="type">Set</span>[<span class="type">Int</span>]):</span><br><span class="line">  <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">Option</span>[<span class="type">LeaderAndIsr</span>], <span class="type">Seq</span>[<span class="type">Int</span>])] = &#123;</span><br><span class="line">    leaderIsrAndControllerEpochs.map &#123; <span class="keyword">case</span> (partition, leaderIsrAndControllerEpoch) =&gt;</span><br><span class="line">      <span class="keyword">val</span> assignment = controllerContext.partitionReplicaAssignment(partition)</span><br><span class="line">      <span class="keyword">val</span> liveOrShuttingDownReplicas = assignment.filter(replica =&gt; controllerContext.isReplicaOnline(replica, partition, includeShuttingDownBrokers = <span class="literal">true</span>))</span><br><span class="line">      <span class="keyword">val</span> isr = leaderIsrAndControllerEpoch.leaderAndIsr.isr</span><br><span class="line">      <span class="keyword">val</span> leaderOpt = <span class="type">PartitionLeaderElectionAlgorithms</span>.controlledShutdownPartitionLeaderElection(assignment, isr, liveOrShuttingDownReplicas.toSet, shuttingDownBrokers)</span><br><span class="line">      <span class="keyword">val</span> newIsr = isr.filter(replica =&gt; !controllerContext.shuttingDownBrokerIds.contains(replica))</span><br><span class="line">      <span class="keyword">val</span> newLeaderAndIsrOpt = leaderOpt.map(leader =&gt; leaderIsrAndControllerEpoch.leaderAndIsr.newLeaderAndIsr(leader, newIsr))</span><br><span class="line">      (partition, newLeaderAndIsrOpt, liveOrShuttingDownReplicas)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka的概念性知识整合</title>
    <url>/2018/01/25/Kafka%E7%9A%84%E6%A6%82%E5%BF%B5%E6%80%A7%E7%9F%A5%E8%AF%86%E6%95%B4%E5%90%88/</url>
    <content><![CDATA[<blockquote>
<p>主要围绕Kafka组件,文件存储机制以及常用命令的介绍</p>
</blockquote>
<span id="more"></span>

<h2 id="Kafka简介"><a href="#Kafka简介" class="headerlink" title="Kafka简介"></a>Kafka简介</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kafka最初由Linkedin公司开发的分布式、分区的、多副本的、多订阅者的消息系统。</span><br><span class="line">它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。</span><br><span class="line">Kafka对消息保存是根据Topic进行归类，发送消息者称为Producer；消息接受者称为Consumer；此外Kafka集群有多个Kafka实例组成，每个实例(server)称为Broker。</span><br><span class="line">无论是Kafka集群，还是producer和consumer都依赖于zookeeper来保证系统可用性集群保存一些meta信息</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">kafka的0.8版本之后，producer不在依赖zookeeper保存meta信息，而是producer自己保存meta信息。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="概念介绍"><a href="#概念介绍" class="headerlink" title="概念介绍"></a>概念介绍</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Broker：</span><br><span class="line">    消息中间件处理节点，一个Kafka节点就是一个Broker，一个或者多个Broker可以组成一个Kafka集群；</span><br><span class="line">Topic：</span><br><span class="line">    主题是对一组消息的抽象分类，比如例如page view日志、click日志等都可以以topic的形式进行抽象划分类别。</span><br><span class="line">    在物理上，不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可使得数据的生产者或消费者不必关心数据存于何处；</span><br><span class="line">Partition：</span><br><span class="line">    每个主题又被分成一个或者若干个分区（Partition）。</span><br><span class="line">    每个分区在本地磁盘上对应一个文件夹，分区命名规则为主题名称后接&quot;—&quot;连接符，之后再接分区编号，分区编号从0开始至分区总数减-1；</span><br><span class="line">LogSegment：</span><br><span class="line">    每个分区又被划分为多个日志分段（LogSegment）组成，日志段是Kafka日志对象分片的最小单位；</span><br><span class="line">    LogSegment算是一个逻辑概念，对应一个具体的日志文件（&quot;.log&quot;的数据文件）和两个索引文件（&quot;.index&quot;和&quot;.timeindex&quot;，分别表示偏移量索引文件和消息时间戳索引文件）组成；</span><br><span class="line">Offset：</span><br><span class="line">    每个partition中都由一系列有序的、不可变的消息组成，这些消息被顺序地追加到partition中。</span><br><span class="line">    每个消息都有一个连续的序列号称之为offset—偏移量，用于在partition内唯一标识消息（并不表示消息在磁盘上的物理位置）；</span><br><span class="line">Message：</span><br><span class="line">    消息是Kafka中存储的最小最基本的单位，即为一个commit log，由一个固定长度的消息头和一个可变长度的消息体组成；</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Kafka的Consumer及其offset"><a href="#Kafka的Consumer及其offset" class="headerlink" title="Kafka的Consumer及其offset"></a>Kafka的Consumer及其offset</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">根据Kafka-API的版本不同,offset位置不一样,老版本维护在ZK中,新版本则是在Kafka的Topic中</span><br><span class="line"></span><br><span class="line"># offset的更新方式</span><br><span class="line">自动提交:</span><br><span class="line">    设置enable.auto.commit&#x3D;true，更新的频率根据参数[auto.commit.interval.ms]来定。</span><br><span class="line">    这种方式也被称为[at most once]，fetch到消息后就可以更新offset，无论是否消费成功。</span><br><span class="line">手动提交:</span><br><span class="line">    设置enable.auto.commit&#x3D;false，这种方式称为[at least once]。</span><br><span class="line">    fetch到消息后，等消费完成再调用方法[consumer.commitSync()]，手动更新offset；</span><br><span class="line">    如果消费失败，则offset也不会更新，此条消息会被重复消费一次。</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">一个Topic可以被多个ConsumerGroup分别消费,但是每个ConsumerGroup中只能有一个Consumer消费此消息.</span><br><span class="line">一个ConsumerGroup内的Consumer只能消费不同的Partition,即一个Partition只能被一个Consumer消费.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Kafka数据存储机制"><a href="#Kafka数据存储机制" class="headerlink" title="Kafka数据存储机制"></a>Kafka数据存储机制</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.Kafka中分区&#x2F;副本的日志文件存储分析</span><br><span class="line">创建Topic指定分区及副本</span><br><span class="line">    .&#x2F;kafka-topics.sh --create --zookeeper 192.168.3.111:2181 --replication-factor 3 --partitions  3 --topic test-01</span><br><span class="line">查询Topic状态</span><br><span class="line">    .&#x2F;kafka-topics.sh --describe --zookeeper 192.168.3.111:2181 --topic test-01</span><br><span class="line">在Producer产生信息后,可以查看Kafka的config&#x2F;server.properties配置文件中log.dirs指定的日志数据存储目录下存在三个分区目录</span><br><span class="line">同时在每个分区目录下存在很多对应的日志数据文件和日志索引文件文件</span><br><span class="line">    分区目录文件</span><br><span class="line">        drwxr-x--- 2 root root 4096 Jul 26 19:35 test-01-0</span><br><span class="line">        drwxr-x--- 2 root root 4096 Jul 24 20:15 test-01-1</span><br><span class="line">        drwxr-x--- 2 root root 4096 Jul 24 20:15 test-01-2</span><br><span class="line">    分区目录中的日志数据文件和日志索引文件</span><br><span class="line">        -rw-r----- 1 root root 512K Jul 24 19:51 00000000000000000000.index</span><br><span class="line">        -rw-r----- 1 root root 1.0G Jul 24 19:51 00000000000000000000.log</span><br><span class="line">        -rw-r----- 1 root root 768K Jul 24 19:51 00000000000000000000.timeindex</span><br><span class="line">        -rw-r----- 1 root root 512K Jul 24 20:03 00000000000022372103.index</span><br><span class="line">        -rw-r----- 1 root root 1.0G Jul 24 20:03 00000000000022372103.log</span><br><span class="line">        -rw-r----- 1 root root 768K Jul 24 20:03 00000000000022372103.timeindex</span><br><span class="line">        -rw-r----- 1 root root 512K Jul 24 20:15 00000000000044744987.index</span><br><span class="line">        -rw-r----- 1 root root 1.0G Jul 24 20:15 00000000000044744987.log</span><br><span class="line">        -rw-r----- 1 root root 767K Jul 24 20:15 00000000000044744987.timeindex</span><br><span class="line">        -rw-r----- 1 root root  10M Jul 24 20:21 00000000000067117761.index</span><br><span class="line">        -rw-r----- 1 root root 511M Jul 24 20:21 00000000000067117761.log</span><br><span class="line">        -rw-r----- 1 root root  10M Jul 24 20:21 00000000000067117761.timeindex</span><br><span class="line">可以看出，每个分区在物理上对应一个文件夹，分区的命名规则为主题名后接&quot;—&quot;连接符，之后再接分区编号，分区编号从0开始，编号的最大值为分区总数减1。</span><br><span class="line">每个分区又有1至多个副本，分区的副本分布在集群的不同代理上，以提高可用性。</span><br><span class="line">从存储的角度上来说，分区的每个副本在逻辑上可以抽象为一个日志（Log）对象，即分区副本与日志对象是相对应的。</span><br><span class="line"></span><br><span class="line"># b.Kafka中日志索引和数据文件的存储结构</span><br><span class="line">在Kafka中，每个Log对象又可以划分为多个LogSegment文件，每个LogSegment文件包括一个日志数据文件和两个索引文件（偏移量索引文件和消息时间戳索引文件）。</span><br><span class="line">其中，每个LogSegment中的日志数据文件大小均相等（该日志数据文件的大小可以通过在Kafka Broker的config&#x2F;server.properties配置文件的中的&quot;log.segment.bytes&quot;进行设置。</span><br><span class="line">默认为1G大小（1073741824字节），在顺序写入消息时如果超出该设定的阈值，将会创建一组新的日志数据和索引文件）。</span><br><span class="line">Kafka将日志文件封装成一个FileMessageSet对象，将偏移量索引文件和消息时间戳索引文件分别封装成OffsetIndex和TimerIndex对象。</span><br><span class="line">Log和LogSegment均为逻辑概念，Log是对副本在Broker上存储文件的抽象，而LogSegment是对副本存储下每个日志分段的抽象，日志与索引文件才与磁盘上的物理存储相对应；</span><br><span class="line">执行下面命令即可将日志数据文件内容dump出来</span><br><span class="line">    .&#x2F;kafka-run-class.sh kafka.tools.DumpLogSegments --files &#x2F;apps&#x2F;svr&#x2F;Kafka&#x2F;kafkalogs&#x2F;kafka-topic-01-0&#x2F;00000000000022372103.log --print-data-log &gt; 00000000000022372103_txt.log</span><br><span class="line">        Dumping &#x2F;apps&#x2F;svr&#x2F;Kafka&#x2F;kafkalogs&#x2F;kafka-topic-01-0&#x2F;00000000000022372103.log</span><br><span class="line">        Starting offset: 22372103</span><br><span class="line">        offset: 22372103 position: 0 CreateTime: 1532433067157 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 5d2697c5-d04a-4018-941d-881ac72ed9fd</span><br><span class="line">        offset: 22372104 position: 0 CreateTime: 1532433067159 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 0ecaae7d-aba5-4dd5-90df-597c8b426b47</span><br><span class="line">        offset: 22372105 position: 0 CreateTime: 1532433067159 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 87709dd9-596b-4cf4-80fa-d1609d1f2087</span><br><span class="line">        ......</span><br><span class="line">        ......</span><br><span class="line">        offset: 22372444 position: 16365 CreateTime: 1532433067166 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 8d52ec65-88cf-4afd-adf1-e940ed9a8ff9</span><br><span class="line">        offset: 22372445 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 5f5f6646-d0f5-4ad1-a257-4e3c38c74a92</span><br><span class="line">        offset: 22372446 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 51dd1da4-053e-4507-9ef8-68ef09d18cca</span><br><span class="line">        offset: 22372447 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 80d50a8e-0098-4748-8171-fd22d6af3c9b</span><br><span class="line">        ......</span><br><span class="line">        ......</span><br><span class="line">        offset: 22372785 position: 32730 CreateTime: 1532433067174 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: db80eb79-8250-42e2-ad26-1b6cfccb5c00</span><br><span class="line">        offset: 22372786 position: 32730 CreateTime: 1532433067176 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 51d95ab0-ab0d-4530-b1d1-05eeb9a6ff00</span><br><span class="line">        ......</span><br><span class="line">        ......</span><br><span class="line">同样地，dump出来的具体偏移量索引内容</span><br><span class="line">    .&#x2F;kafka-run-class.sh kafka.tools.DumpLogSegments --files &#x2F;apps&#x2F;svr&#x2F;Kafka&#x2F;kafkalogs&#x2F;kafka-topic-01-0&#x2F;00000000000022372103.index --print-data-log &gt; 00000000000022372103_txt.index</span><br><span class="line">        Dumping &#x2F;apps&#x2F;svr&#x2F;Kafka&#x2F;kafkalogs&#x2F;kafka-topic-01-0&#x2F;00000000000022372103.index</span><br><span class="line">        offset: 22372444 position: 16365</span><br><span class="line">        offset: 22372785 position: 32730</span><br><span class="line">        offset: 22373467 position: 65460</span><br><span class="line">        offset: 22373808 position: 81825</span><br><span class="line">        offset: 22374149 position: 98190</span><br><span class="line">        offset: 22374490 position: 114555</span><br><span class="line">        ......</span><br><span class="line">        ......</span><br><span class="line">dump出来的时间戳索引文件内容</span><br><span class="line">    .&#x2F;kafka-run-class.sh kafka.tools.DumpLogSegments --files &#x2F;apps&#x2F;svr&#x2F;Kafka&#x2F;kafkalogs&#x2F;kafka-topic-01-0&#x2F;00000000000022372103.timeindex --print-data-log &gt; 00000000000022372103_txt.timeindex</span><br><span class="line">        Dumping &#x2F;apps&#x2F;svr&#x2F;Kafka&#x2F;kafkalogs&#x2F;kafka-topic-01-0&#x2F;00000000000022372103.timeindex</span><br><span class="line">        timestamp: 1532433067174 offset: 22372784</span><br><span class="line">        timestamp: 1532433067191 offset: 22373466</span><br><span class="line">        timestamp: 1532433067206 offset: 22373807</span><br><span class="line">        timestamp: 1532433067214 offset: 22374148</span><br><span class="line">        timestamp: 1532433067222 offset: 22374489</span><br><span class="line">        timestamp: 1532433067230 offset: 22374830</span><br><span class="line">        ......</span><br><span class="line">        ......</span><br><span class="line">由上面dump出来的偏移量索引文件和日志数据文件的具体内容可以分析出来，偏移量索引文件中存储着大量的索引元数据，日志数据文件中存储着大量消息结构中的各个字段内容和消息体本身的值。</span><br><span class="line">索引文件中的元数据postion字段指向对应日志数据文件中message的实际位置（即为物理偏移地址）。</span><br><span class="line"></span><br><span class="line"># c.Kafka消息字段以及各个字段说明</span><br><span class="line">offset: 消息偏移量</span><br><span class="line">message size: 消息总长度</span><br><span class="line">CRC32: CRC32编码校验和</span><br><span class="line">attributes:表示为独立版本、或标识压缩类型、或编码类型</span><br><span class="line">magic: 表示本次发布Kafka服务程序协议版本号</span><br><span class="line">key length: 消息Key的长度</span><br><span class="line">key: 消息Key的实际数据</span><br><span class="line">valuesize: 消息的实际数据长度</span><br><span class="line">playload: 消息的实际数据</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="数据文件"><a href="#数据文件" class="headerlink" title="数据文件"></a>数据文件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.日志数据文件</span><br><span class="line">Kafka将生产者发送给它的消息数据内容保存至日志数据文件中，该文件以该段的基准偏移量左补齐0命名，文件后缀为“.log”。</span><br><span class="line">分区中的每条message由offset来表示它在这个分区中的偏移量，这个offset并不是该Message在分区中实际存储位置，而是逻辑上的一个值（Kafka中用8字节长度来记录这个偏移量），但它却唯一确定了分区中一条Message的逻辑位置，同一个分区下的消息偏移量按照顺序递增（这个可以类比下数据库的自增主键）。</span><br><span class="line">另外，从dump出来的日志数据文件的字符值中可以看到消息体的各个字段的内容值。</span><br><span class="line"></span><br><span class="line"># b.偏移量索引文件</span><br><span class="line">如果消息的消费者每次fetch都需要从1G大小（默认值）的日志数据文件中来查找对应偏移量的消息，那么效率一定非常低，在定位到分段后还需要顺序比对才能找到。</span><br><span class="line">Kafka在设计数据存储时，为了提高查找消息的效率，故而为分段后的每个日志数据文件均使用稀疏索引的方式建立索引，这样子既节省空间又能通过索引快速定位到日志数据文件中的消息内容。</span><br><span class="line">偏移量索引文件和数据文件一样也同样也以该段的基准偏移量左补齐0命名，文件后缀为“.index”。</span><br><span class="line">从上面dump出来的偏移量索引内容可以看出，索引条目用于将偏移量映射成为消息在日志数据文件中的实际物理位置，每个索引条目由offset和position组成，每个索引条目可以唯一确定在各个分区数据文件的一条消息。</span><br><span class="line">其中，Kafka采用稀疏索引存储的方式，每隔一定的字节数建立了一条索引，可以通过&quot;index.interval.bytes&quot;设置索引的跨度；</span><br><span class="line">有了偏移量索引文件，通过它，Kafka就能够根据指定的偏移量快速定位到消息的实际物理位置。</span><br><span class="line">具体的做法是，根据指定的偏移量，使用二分法查询定位出该偏移量对应的消息所在的分段索引文件和日志数据文件。</span><br><span class="line">然后通过二分查找法，继续查找出小于等于指定偏移量的最大偏移量，同时也得出了对应的position（实际物理位置），根据该物理位置在分段的日志数据文件中顺序扫描查找偏移量与指定偏移量相等的消息。</span><br><span class="line"></span><br><span class="line"># c.时间戳索引文件</span><br><span class="line">这种类型的索引文件是Kafka从0.10.1.1版本开始引入的的一个基于时间戳的索引文件，它们的命名方式与对应的日志数据文件和偏移量索引文件名基本一样，唯一不同的就是后缀名。</span><br><span class="line">从上面dump出来的该种类型的时间戳索引文件的内容来看，每一条索引条目都对应了一个8字节长度的时间戳字段和一个4字节长度的偏移量字段，其中时间戳字段记录的是该LogSegment到目前为止的最大时间戳，后面对应的偏移量即为此时插入新消息的偏移量。</span><br><span class="line">另外，时间戳索引文件的时间戳类型与日志数据文件中的时间类型是一致的，索引条目中的时间戳值及偏移量与日志数据文件中对应的字段值相同（ps：Kafka也提供了通过时间戳索引来访问消息的方法）。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="过期日志的处理"><a href="#过期日志的处理" class="headerlink" title="过期日志的处理"></a>过期日志的处理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kafka作为一个消息中间件，是需要定期处理数据的，否则磁盘就爆了。</span><br><span class="line">Kafka日志管理器中会有一个专门的日志删除任务来周期性检测和删除不符合保留条件的日志分段文件，这个周期可以通过broker端参数log.retention.check.interval.ms来配置，默认值为300,000，即5分钟。</span><br><span class="line">当前日志分段的保留策略有3种：基于时间的保留策略、基于日志大小的保留策略以及基于日志起始偏移量的保留策略。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># a.处理的机制</span><br><span class="line">基于时间:</span><br><span class="line">    根据数据的时间长短进行清理，例如数据在磁盘中超过多久会被清理（默认是168个小时）。</span><br><span class="line">    日志删除任务会检查当前日志文件中是否有保留时间超过设定的阈值retentionMs来寻找可删除的的日志分段文件集合deletableSegments。</span><br><span class="line">    retentionMs可以通过broker端参数log.retention.hours、log.retention.minutes以及log.retention.ms来配置，其中log.retention.ms的优先级最高，log.retention.minutes次之，log.retention.hours最低。</span><br><span class="line">    默认情况下只配置了log.retention.hours参数，其值为168，故默认情况下日志分段文件的保留时间为7天。</span><br><span class="line">基于日志大小：</span><br><span class="line">    根据文件大小的方式给进行清理，例如数据大小超过多大时，删除数据（大小是按照每个partition的大小来界定的）。</span><br><span class="line">    日志删除任务会检查当前日志的大小是否超过设定的阈值retentionSize来寻找可删除的日志分段的文件集合deletableSegments。</span><br><span class="line">    retentionSize可以通过broker端参数log.retention.bytes来配置，默认值为-1，表示无穷大。</span><br><span class="line">    注意log.retention.bytes配置的是日志文件的总大小，而不是单个的日志分段的大小，一个日志文件包含多个日志分段。</span><br><span class="line">基于日志起始偏移量：</span><br><span class="line">    一般情况下日志文件的起始偏移量logStartOffset等于第一个日志分段的baseOffset，但是这并不是绝对的，logStartOffset的值可以通过DeleteRecordsRequest请求、日志的清理和截断等操作修改。</span><br><span class="line">    基于日志起始偏移量的删除策略的判断依据是某日志分段的下一个日志分段的起始偏移量baseOffset是否小于等于logStartOffset，若是则可以删除此日志分段。</span><br><span class="line"></span><br><span class="line"># b.删除过期的日志的方式</span><br><span class="line">删除日志分段时，首先会从日志文件对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作。</span><br><span class="line">然后将日志分段文件添加上“.deleted”的后缀，当然也包括日志分段对应的索引文件。</span><br><span class="line">最后交由一个以“delete-file”命名的延迟任务来删除这些“.deleted”为后缀的文件，这个任务的延迟执行时间可以通过file.delete.delay.ms参数来设置，默认值为60000，即1分钟。</span><br><span class="line">直接删除segment文件。后台会周期性的扫描，当满足设定的条件的数据就执行删除。</span><br><span class="line">如果设置是按照大小的方式，删除segment是按照segment存在顺序进行删除，即先删除存在最久的那个segment。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="常见的专业性名词"><a href="#常见的专业性名词" class="headerlink" title="常见的专业性名词"></a>常见的专业性名词</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">AR: Assigned Replicas 分区中所有副本统称</span><br><span class="line">ISR: In Sync Replicas 所有与Leader副本保持一定程度同步的副本</span><br><span class="line">OSR: Outof Sync Replied 于Leader副本同步滞后过多的副本(不包括Leader副本)</span><br><span class="line">HW: High Watermak 特定消息的Offset,消费者只能拉取到这个offset之前的消息</span><br><span class="line">LEO: Log End Offset 表示当前日志文件下一条待写入消息的offset</span><br><span class="line">LSO: Last Start Offset 与Kafka事务有关,</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十二副本同步线程</title>
    <url>/2020/05/08/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E4%BA%8C%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E7%BA%BF%E7%A8%8B/</url>
    <content><![CDATA[<blockquote>
<p>主要介绍副本同步线程什么情况下启动,同步线程处理逻辑,以及什么情况下线程关闭</p>
</blockquote>
<span id="more"></span>

<h2 id="同步线程的启动"><a href="#同步线程的启动" class="headerlink" title="同步线程的启动"></a>同步线程的启动</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在上一章中,Replica同步发送的Fetch请求的封装,需要设计ReplicaFetchManager</span><br><span class="line">副本同步线程的启动和关闭都由这个实例操作</span><br></pre></td></tr></table></figure>
<h3 id="什么情况下启动"><a href="#什么情况下启动" class="headerlink" title="什么情况下启动"></a>什么情况下启动</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">Broker</span>分配的任何一个partition都是以replica对象实例的形式存在</span><br><span class="line">而replica在kafka上有两个角色: leader和follower</span><br><span class="line">只要这个replica是follower,那么它就会向leader进行数据同步</span><br><span class="line"></span><br><span class="line">如果<span class="type">Broker</span>的本地副本被选举为follower,那么就会启动副本同步线程</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对于给定的这些副本，将本地副本设置为 follower</span></span><br><span class="line"><span class="comment">// 从 leader partition 集合移除这些 partition；</span></span><br><span class="line"><span class="comment">// 将这些 partition 标记为 follower，之后这些 partition 就不会再接收 produce 的请求了；</span></span><br><span class="line"><span class="comment">// 停止对这些 partition 的副本同步，这样这些副本就不会再有（来自副本请求线程）的数据进行追加了；</span></span><br><span class="line"><span class="comment">// 对这些 partition 的 offset 进行 checkpoint，如果日志需要截断就进行截断操作；</span></span><br><span class="line"><span class="comment">// 清空 purgatory 中的 produce 和 fetch 请求；</span></span><br><span class="line"><span class="comment">// 如果 broker 没有掉线，向这些 partition 的新 leader 启动副本同步线程；</span></span><br><span class="line"><span class="comment">//note: 上面这些操作的顺序性，保证了这些副本在 offset checkpoint 之前将不会接收新的数据，这样的话，在 checkpoint 之前这些数据都可以保证刷到磁盘</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</span><br><span class="line">                            epoch: <span class="type">Int</span>,</span><br><span class="line">                            partitionStates: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">LeaderAndIsrRequest</span>.<span class="type">PartitionState</span>],</span><br><span class="line">                            correlationId: <span class="type">Int</span>,</span><br><span class="line">                            responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Errors</span>]) : <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">    partitionStates.foreach &#123; <span class="keyword">case</span> (partition, partitionState) =&gt;</span><br><span class="line">      stateChangeLogger.trace(<span class="string">s&quot;Handling LeaderAndIsr request correlationId <span class="subst">$correlationId</span> from controller <span class="subst">$controllerId</span> &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;epoch <span class="subst">$epoch</span> starting the become-follower transition for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> with leader &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;<span class="subst">$&#123;partitionState.basePartitionState.leader&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (partition &lt;- partitionStates.keys)</span><br><span class="line">      responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 统计follower的集合</span></span><br><span class="line">    <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// <span class="doctag">TODO:</span> Delete leaders from LeaderAndIsrRequest</span></span><br><span class="line">      partitionStates.foreach &#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</span><br><span class="line">        <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.basePartitionState.leader</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// leader是可用的</span></span><br><span class="line">          metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="comment">// Only change partition state when the leader is available</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt; <span class="comment">// partition的本地副本设置为follower</span></span><br><span class="line">              <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</span><br><span class="line">                partitionsToMakeFollower += partition</span><br><span class="line">              <span class="keyword">else</span> <span class="comment">// 这个partition的本地副本已经是follower了</span></span><br><span class="line">                stateChangeLogger.info(<span class="string">s&quot;Skipped the become-follower state change after marking its partition as &quot;</span> +</span><br><span class="line">                  <span class="string">s&quot;follower with correlation id <span class="subst">$correlationId</span> from controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> &quot;</span> +</span><br><span class="line">                  <span class="string">s&quot;for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> (last update &quot;</span> +</span><br><span class="line">                  <span class="string">s&quot;controller epoch <span class="subst">$&#123;partitionStateInfo.basePartitionState.controllerEpoch&#125;</span>) &quot;</span> +</span><br><span class="line">                  <span class="string">s&quot;since the new leader <span class="subst">$newLeaderBrokerId</span> is the same as the old leader&quot;</span>)</span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">              <span class="comment">// The leader broker should always be present in the metadata cache.</span></span><br><span class="line">              <span class="comment">// If not, we should record the error message and abort the transition process for this partition</span></span><br><span class="line">              stateChangeLogger.error(<span class="string">s&quot;Received LeaderAndIsrRequest with correlation id <span class="subst">$correlationId</span> from &quot;</span> +</span><br><span class="line">                <span class="string">s&quot;controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> &quot;</span> +</span><br><span class="line">                <span class="string">s&quot;(last update controller epoch <span class="subst">$&#123;partitionStateInfo.basePartitionState.controllerEpoch&#125;</span>) &quot;</span> +</span><br><span class="line">                <span class="string">s&quot;but cannot become follower since the new leader <span class="subst">$newLeaderBrokerId</span> is unavailable.&quot;</span>)</span><br><span class="line">              <span class="comment">// Create the local replica even if the leader is unavailable. This is required to ensure that we include</span></span><br><span class="line">              <span class="comment">// the partition&#x27;s high watermark in the checkpoint file (see KAFKA-1647)</span></span><br><span class="line">              partition.getOrCreateReplica(localBrokerId, isNew = partitionStateInfo.isNew)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</span><br><span class="line">            stateChangeLogger.error(<span class="string">s&quot;Skipped the become-follower state change with correlation id <span class="subst">$correlationId</span> from &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;(last update controller epoch <span class="subst">$&#123;partitionStateInfo.basePartitionState.controllerEpoch&#125;</span>) with leader &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;<span class="subst">$newLeaderBrokerId</span> since the replica for the partition is offline due to disk error <span class="subst">$e</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> dirOpt = getLogDir(partition.topicPartition)</span><br><span class="line">            error(<span class="string">s&quot;Error while making broker the follower for partition <span class="subst">$partition</span> with leader &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;<span class="subst">$newLeaderBrokerId</span> in dir <span class="subst">$dirOpt</span>&quot;</span>, e)</span><br><span class="line">            responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">KAFKA_STORAGE_ERROR</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 删除这些partition的副本同步线程</span></span><br><span class="line">      replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</span><br><span class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">        stateChangeLogger.trace(<span class="string">s&quot;Stopped fetchers as part of become-follower request from controller <span class="subst">$controllerId</span> &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;epoch <span class="subst">$epoch</span> with correlation id <span class="subst">$correlationId</span> for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> with leader &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;<span class="subst">$&#123;partitionStates(partition).basePartitionState.leader&#125;</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">        <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</span><br><span class="line">        tryCompleteDelayedProduce(topicPartitionOperationKey)</span><br><span class="line">        tryCompleteDelayedFetch(topicPartitionOperationKey)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 完成那些延迟请求的处理</span></span><br><span class="line">      partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">        stateChangeLogger.trace(<span class="string">s&quot;Truncated logs and checkpointed recovery boundaries for partition &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;<span class="subst">$&#123;partition.topicPartition&#125;</span> as part of become-follower request with correlation id <span class="subst">$correlationId</span> from &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> with leader <span class="subst">$&#123;partitionStates(partition).basePartitionState.leader&#125;</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (isShuttingDown.get()) &#123;</span><br><span class="line">        partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">          stateChangeLogger.trace(<span class="string">s&quot;Skipped the adding-fetcher step of the become-follower state &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;change with correlation id <span class="subst">$correlationId</span> from controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> for &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;partition <span class="subst">$&#123;partition.topicPartition&#125;</span> with leader <span class="subst">$&#123;partitionStates(partition).basePartitionState.leader&#125;</span> &quot;</span> +</span><br><span class="line">            <span class="string">&quot;since it is shutting down&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 启动副本同步线程</span></span><br><span class="line">        <span class="comment">// we do not need to check if the leader exists again since this has been done at the beginning of this process</span></span><br><span class="line">        <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map &#123; partition =&gt;</span><br><span class="line">          <span class="keyword">val</span> leader = metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get</span><br><span class="line">            .brokerEndPoint(config.interBrokerListenerName)</span><br><span class="line">          <span class="keyword">val</span> fetchOffset = partition.localReplicaOrException.highWatermark.messageOffset</span><br><span class="line">          partition.topicPartition -&gt; <span class="type">InitialFetchState</span>(leader, partition.getLeaderEpoch, fetchOffset)</span><br><span class="line">        &#125;.toMap</span><br><span class="line">        replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</span><br><span class="line"></span><br><span class="line">        partitionsToMakeFollower.foreach &#123; partition =&gt;</span><br><span class="line">          stateChangeLogger.trace(<span class="string">s&quot;Started fetcher to new leader as part of become-follower &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;request from controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> with correlation id <span class="subst">$correlationId</span> for &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;partition <span class="subst">$&#123;partition.topicPartition&#125;</span> with leader <span class="subst">$&#123;partitionStates(partition).basePartitionState.leader&#125;</span>&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        stateChangeLogger.error(<span class="string">s&quot;Error while processing LeaderAndIsr request with correlationId <span class="subst">$correlationId</span> &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;received from controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span>&quot;</span>, e)</span><br><span class="line">        <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    partitionStates.keys.foreach &#123; partition =&gt;</span><br><span class="line">      stateChangeLogger.trace(<span class="string">s&quot;Completed LeaderAndIsr request correlationId <span class="subst">$correlationId</span> from controller <span class="subst">$controllerId</span> &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;epoch <span class="subst">$epoch</span> for the become-follower transition for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> with leader &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;<span class="subst">$&#123;partitionStates(partition).basePartitionState.leader&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    partitionsToMakeFollower</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">并不一定会为每一个partition都启动一个fetcher线程,对于一个目的<span class="type">Broker</span></span><br><span class="line">只会启动num.replica.fetchers个线程</span><br><span class="line">具体这个tp会分配到那个fetcher线程上,根据topic名和partitionId计算得到的</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getFetcherId</span></span>(topic: <span class="type">String</span>, partitionId: <span class="type">Int</span>) : <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="type">Utils</span>.abs(<span class="number">31</span> * topic.hashCode() + partitionId) % numFetchers</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="线程启动"><a href="#线程启动" class="headerlink" title="线程启动"></a>线程启动</h3><h4 id="addFetcherForPartitions"><a href="#addFetcherForPartitions" class="headerlink" title="addFetcherForPartitions()"></a>addFetcherForPartitions()</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>计算tp对应的fetcherId</span><br><span class="line"><span class="number">2.</span>根据leader和fetcherId获取对应的replica fetcher线程,没有找到就调用createFetcherThread()创建一个新的</span><br><span class="line"><span class="number">3.</span>如果是新启动的replica fetcher线程,那么就直接启动</span><br><span class="line"><span class="number">4.</span>将tp记录到fetcherThreadMap中,值对应要同步的tp列表</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为tp添加replica-fetch线程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addFetcherForPartitions</span></span>(partitionAndOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">InitialFetchState</span>]) &#123;</span><br><span class="line">    lock synchronized &#123;</span><br><span class="line">      <span class="comment">// 为这些tp分配相应的fetch线程Id</span></span><br><span class="line">      <span class="keyword">val</span> partitionsPerFetcher = partitionAndOffsets.groupBy &#123; <span class="keyword">case</span> (topicPartition, brokerAndInitialFetchOffset) =&gt;</span><br><span class="line">        <span class="type">BrokerAndFetcherId</span>(brokerAndInitialFetchOffset.leader, getFetcherId(topicPartition))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">addAndStartFetcherThread</span></span>(brokerAndFetcherId: <span class="type">BrokerAndFetcherId</span>, brokerIdAndFetcherId: <span class="type">BrokerIdAndFetcherId</span>): <span class="type">AbstractFetcherThread</span> = &#123;</span><br><span class="line">        <span class="comment">// 为BrokerIdAndFetcherId构造fetcherThread</span></span><br><span class="line">        <span class="keyword">val</span> fetcherThread = createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)</span><br><span class="line">        fetcherThreadMap.put(brokerIdAndFetcherId, fetcherThread)</span><br><span class="line">        fetcherThread.start()</span><br><span class="line">        fetcherThread</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> ((brokerAndFetcherId, initialFetchOffsets) &lt;- partitionsPerFetcher) &#123;</span><br><span class="line">        <span class="keyword">val</span> brokerIdAndFetcherId = <span class="type">BrokerIdAndFetcherId</span>(brokerAndFetcherId.broker.id, brokerAndFetcherId.fetcherId)</span><br><span class="line">        <span class="keyword">val</span> fetcherThread = fetcherThreadMap.get(brokerIdAndFetcherId) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(currentFetcherThread) <span class="keyword">if</span> currentFetcherThread.sourceBroker == brokerAndFetcherId.broker =&gt;</span><br><span class="line">            <span class="comment">// reuse the fetcher thread</span></span><br><span class="line">            currentFetcherThread</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(f) =&gt;</span><br><span class="line">            f.shutdown()</span><br><span class="line">            addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            addAndStartFetcherThread(brokerAndFetcherId, brokerIdAndFetcherId)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> initialOffsetAndEpochs = initialFetchOffsets.map &#123; <span class="keyword">case</span> (tp, brokerAndInitOffset) =&gt;</span><br><span class="line">          tp -&gt; <span class="type">OffsetAndEpoch</span>(brokerAndInitOffset.initOffset, brokerAndInitOffset.currentLeaderEpoch)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 添加tp列表</span></span><br><span class="line">        fetcherThread.addPartitions(initialOffsetAndEpochs)</span><br><span class="line">        info(<span class="string">s&quot;Added fetcher to broker <span class="subst">$&#123;brokerAndFetcherId.broker&#125;</span> for partitions <span class="subst">$initialOffsetAndEpochs</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="createFetcherThread"><a href="#createFetcherThread" class="headerlink" title="createFetcherThread"></a>createFetcherThread</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createFetcherThread</span></span>(fetcherId: <span class="type">Int</span>, sourceBroker: <span class="type">BrokerEndPoint</span>): <span class="type">ReplicaFetcherThread</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> prefix = threadNamePrefix.map(tp =&gt; <span class="string">s&quot;<span class="subst">$tp</span>:&quot;</span>).getOrElse(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> threadName = <span class="string">s&quot;<span class="subst">$&#123;prefix&#125;</span>ReplicaFetcherThread-<span class="subst">$fetcherId</span>-<span class="subst">$&#123;sourceBroker.id&#125;</span>&quot;</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">ReplicaFetcherThread</span>(threadName, fetcherId, sourceBroker, brokerConfig, replicaManager,</span><br><span class="line">    metrics, time, quotaManager)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="同步线程处理过程"><a href="#同步线程处理过程" class="headerlink" title="同步线程处理过程"></a>同步线程处理过程</h2><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ReplicaFetchManager.addAndStartFetcherThread()</span><br><span class="line"> -&gt;start()</span><br><span class="line">  -&gt;ShutdownableThread.run()-&gt;doWork()</span><br><span class="line">   -&gt;AbstractFetcherThread.doWork()-&gt;maybeTruncate(),maybeFetch()</span><br><span class="line">    -&gt;ReplicaFetchThread.buildFetch()</span><br><span class="line">     -&gt;AbstractFetcherThread.processFetchRequest()</span><br><span class="line">      -&gt;ReplicaFetchThread.fetchFromLeader()    </span><br><span class="line">       -&gt;ReplicaFetchThread.processPartitionData()</span><br></pre></td></tr></table></figure>
<h3 id="doWork-gt-maybeFetch"><a href="#doWork-gt-maybeFetch" class="headerlink" title="doWork()-&gt;maybeFetch()"></a>doWork()-&gt;maybeFetch()</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>构造<span class="type">Fetch</span>请求</span><br><span class="line"><span class="number">2.</span>通过processFetchRequest()发送<span class="type">Fetch</span>请求,并对其结果进行相应的处理</span><br><span class="line"></span><br><span class="line"><span class="comment">// AbstractFetcherThread</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeFetch</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> (fetchStates, fetchRequestOpt) = inLock(partitionMapLock) &#123;</span><br><span class="line">    <span class="keyword">val</span> fetchStates = partitionStates.partitionStateMap.asScala</span><br><span class="line">    <span class="comment">// 关键在于setReplicaId,区分consumer,注意调用的是子类ReplicaFetchThread</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">ResultWithPartitions</span>(fetchRequestOpt, partitionsWithError) = buildFetch(fetchStates)</span><br><span class="line">    handlePartitionsWithErrors(partitionsWithError)</span><br><span class="line">    <span class="keyword">if</span> (fetchRequestOpt.isEmpty) &#123;</span><br><span class="line">      trace(<span class="string">s&quot;There are no active partitions. Back off for <span class="subst">$fetchBackOffMs</span> ms before sending a fetch request&quot;</span>)</span><br><span class="line">      <span class="comment">// 如果没有活跃的partition,在下次调用之前,sleep fetchBackOffMs时间</span></span><br><span class="line">      partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    (fetchStates, fetchRequestOpt)</span><br><span class="line">  &#125;</span><br><span class="line">  fetchRequestOpt.foreach &#123; fetchRequest =&gt;</span><br><span class="line">    processFetchRequest(fetchStates, fetchRequest)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="processFetchRequest"><a href="#processFetchRequest" class="headerlink" title="processFetchRequest()"></a>processFetchRequest()</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>主要实现在fetchFromLeader方法内</span><br><span class="line"><span class="number">2.</span>获取相应的response(如果遇到异常,下次发送fetch请求前,会sleep一段时间再发)</span><br><span class="line"><span class="number">3.</span>如果返回结果不为空,并且fetch请求的offset信息与返回结果的offset信息对的上</span><br><span class="line">    调用processPartitionData()方法将拉取到的数据追加到本地副本日志文件中</span><br><span class="line">    如果返回结果有错误,按相应的错误进行处理</span><br><span class="line"><span class="number">4.</span>对在fetch过程遇到的异常或返回的错误,会进行delay操作</span><br><span class="line">    下次fetch请求发生至少间隔replica.fetch.backoff.ms</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送fetch,返回相应结果</span></span><br><span class="line">responseData = fetchFromLeader(fetchRequest)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将fetch的数据追加到日志文件中</span></span><br><span class="line"><span class="keyword">val</span> logAppendInfoOpt = processPartitionData(topicPartition, currentFetchState.fetchOffset, partitionData)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="同步线程的关闭"><a href="#同步线程的关闭" class="headerlink" title="同步线程的关闭"></a>同步线程的关闭</h2><h3 id="什么情况下关闭"><a href="#什么情况下关闭" class="headerlink" title="什么情况下关闭"></a>什么情况下关闭</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.stopReplica(): broker收到了controller发来的StopReplica请求,这时会开始关闭对指定tp的同步线程</span><br><span class="line">2.makeLeaders(): 这些partition的本地副本被选举成了leader,这时会先停止对这些tp副本的同步线程</span><br><span class="line">3.makeFollowers(): 停止副本同步,然后再开启同步</span><br></pre></td></tr></table></figure>
<h3 id="stopReplica"><a href="#stopReplica" class="headerlink" title="stopReplica"></a>stopReplica</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">ReplicaManager</span>.stopReplica()</span><br><span class="line">是<span class="type">Controller</span>发送过来的,触发条件有多种,broker下线,partition replica迁移</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stopReplica</span></span>(topicPartition: <span class="type">TopicPartition</span>, deletePartition: <span class="type">Boolean</span>)  = &#123;</span><br><span class="line">    stateChangeLogger.trace(<span class="string">s&quot;Handling stop replica (delete=<span class="subst">$deletePartition</span>) for partition <span class="subst">$topicPartition</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (deletePartition) &#123;</span><br><span class="line">      <span class="keyword">val</span> removedPartition = allPartitions.remove(topicPartition)</span><br><span class="line">      <span class="keyword">if</span> (removedPartition eq <span class="type">ReplicaManager</span>.<span class="type">OfflinePartition</span>) &#123;</span><br><span class="line">        allPartitions.put(topicPartition, <span class="type">ReplicaManager</span>.<span class="type">OfflinePartition</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaStorageException</span>(<span class="string">s&quot;Partition <span class="subst">$topicPartition</span> is on an offline disk&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (removedPartition != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> topicHasPartitions = allPartitions.values.exists(partition =&gt; topicPartition.topic == partition.topic)</span><br><span class="line">        <span class="keyword">if</span> (!topicHasPartitions)</span><br><span class="line">          brokerTopicStats.removeMetrics(topicPartition.topic)</span><br><span class="line">        <span class="comment">// this will delete the local log. This call may throw exception if the log is on offline directory</span></span><br><span class="line">        removedPartition.delete()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        stateChangeLogger.trace(<span class="string">s&quot;Ignoring stop replica (delete=<span class="subst">$deletePartition</span>) for partition <span class="subst">$topicPartition</span> as replica doesn&#x27;t exist on broker&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Delete log and corresponding folders in case replica manager doesn&#x27;t hold them anymore.</span></span><br><span class="line">      <span class="comment">// This could happen when topic is being deleted while broker is down and recovers.</span></span><br><span class="line">      <span class="keyword">if</span> (logManager.getLog(topicPartition).isDefined)</span><br><span class="line">        logManager.asyncDelete(topicPartition)</span><br><span class="line">      <span class="keyword">if</span> (logManager.getLog(topicPartition, isFuture = <span class="literal">true</span>).isDefined)</span><br><span class="line">        logManager.asyncDelete(topicPartition, isFuture = <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    stateChangeLogger.trace(<span class="string">s&quot;Finished handling stop replica (delete=<span class="subst">$deletePartition</span>) for partition <span class="subst">$topicPartition</span>&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="makeLeaders"><a href="#makeLeaders" class="headerlink" title="makeLeaders"></a>makeLeaders</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">ReplicaManager</span>.makeLeaders()</span><br><span class="line">当broker上这个partition的副本被设置为leader时触发的</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</span><br><span class="line">                          epoch: <span class="type">Int</span>,</span><br><span class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">LeaderAndIsrRequest</span>.<span class="type">PartitionState</span>],</span><br><span class="line">                          correlationId: <span class="type">Int</span>,</span><br><span class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Errors</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">    partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">      stateChangeLogger.trace(<span class="string">s&quot;Handling LeaderAndIsr request correlationId <span class="subst">$correlationId</span> from &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> starting the become-leader transition for &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;partition <span class="subst">$&#123;partition.topicPartition&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (partition &lt;- partitionState.keys)</span><br><span class="line">      responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> partitionsToMakeLeaders = mutable.<span class="type">Set</span>[<span class="type">Partition</span>]()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// First stop fetchers for all the partitions</span></span><br><span class="line">      replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</span><br><span class="line">      <span class="comment">// Update the partition information to be the leader</span></span><br><span class="line">      partitionState.foreach&#123; <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId)) &#123;</span><br><span class="line">            partitionsToMakeLeaders += partition</span><br><span class="line">            stateChangeLogger.trace(<span class="string">s&quot;Stopped fetchers as part of become-leader request from &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> with correlation id <span class="subst">$correlationId</span> for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;(last update controller epoch <span class="subst">$&#123;partitionStateInfo.basePartitionState.controllerEpoch&#125;</span>)&quot;</span>)</span><br><span class="line">          &#125; <span class="keyword">else</span></span><br><span class="line">            stateChangeLogger.info(<span class="string">s&quot;Skipped the become-leader state change after marking its &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;partition as leader with correlation id <span class="subst">$correlationId</span> from controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> for &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;partition <span class="subst">$&#123;partition.topicPartition&#125;</span> (last update controller epoch <span class="subst">$&#123;partitionStateInfo.basePartitionState.controllerEpoch&#125;</span>) &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;since it is already the leader for the partition.&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</span><br><span class="line">            stateChangeLogger.error(<span class="string">s&quot;Skipped the become-leader state change with &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;correlation id <span class="subst">$correlationId</span> from controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> for partition <span class="subst">$&#123;partition.topicPartition&#125;</span> &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;(last update controller epoch <span class="subst">$&#123;partitionStateInfo.basePartitionState.controllerEpoch&#125;</span>) since &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;the replica for the partition is offline due to disk error <span class="subst">$e</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> dirOpt = getLogDir(partition.topicPartition)</span><br><span class="line">            error(<span class="string">s&quot;Error while making broker the leader for partition <span class="subst">$partition</span> in dir <span class="subst">$dirOpt</span>&quot;</span>, e)</span><br><span class="line">            responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">KAFKA_STORAGE_ERROR</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">          stateChangeLogger.error(<span class="string">s&quot;Error while processing LeaderAndIsr request correlationId <span class="subst">$correlationId</span> received &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;from controller <span class="subst">$controllerId</span> epoch <span class="subst">$epoch</span> for partition <span class="subst">$&#123;partition.topicPartition&#125;</span>&quot;</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Re-throw the exception for it to be caught in KafkaApis</span></span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    partitionState.keys.foreach &#123; partition =&gt;</span><br><span class="line">      stateChangeLogger.trace(<span class="string">s&quot;Completed LeaderAndIsr request correlationId <span class="subst">$correlationId</span> from controller <span class="subst">$controllerId</span> &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;epoch <span class="subst">$epoch</span> for the become-leader transition for partition <span class="subst">$&#123;partition.topicPartition&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    partitionsToMakeLeaders</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="makeFollowers"><a href="#makeFollowers" class="headerlink" title="makeFollowers"></a>makeFollowers</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">参考同步线程启动</span><br></pre></td></tr></table></figure>
<h3 id="removeFetcherForPartitions"><a href="#removeFetcherForPartitions" class="headerlink" title="removeFetcherForPartitions"></a>removeFetcherForPartitions</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeFetcherForPartitions</span></span>(partitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</span><br><span class="line">  lock synchronized &#123;</span><br><span class="line">    <span class="keyword">for</span> (fetcher &lt;- fetcherThreadMap.values)</span><br><span class="line">      fetcher.removePartitions(partitions)</span><br><span class="line">  &#125;</span><br><span class="line">  info(<span class="string">s&quot;Removed fetcher for partitions <span class="subst">$partitions</span>&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="removePartitions"><a href="#removePartitions" class="headerlink" title="removePartitions"></a>removePartitions</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removePartitions</span></span>(topicPartitions: <span class="type">Set</span>[<span class="type">TopicPartition</span>]) &#123;</span><br><span class="line">  partitionMapLock.lockInterruptibly()</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    topicPartitions.foreach &#123; topicPartition =&gt;</span><br><span class="line">      partitionStates.remove(topicPartition)</span><br><span class="line">      fetcherLagStats.unregister(topicPartition)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> partitionMapLock.unlock()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="shutdownIdleFetcherThreads"><a href="#shutdownIdleFetcherThreads" class="headerlink" title="shutdownIdleFetcherThreads"></a>shutdownIdleFetcherThreads</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">ReplicaManager</span>每次处理完<span class="type">LeaderAndIsr</span>请求后,都会调用<span class="type">ReplicaFetcherManager</span>的shutdownIdleFetcherThreads()方法</span><br><span class="line">如果fetcher线程要拉取的tp集合为空,就会关闭对应的fetcher线程</span><br><span class="line"><span class="comment">// 真正关闭线程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdownIdleFetcherThreads</span></span>() &#123;</span><br><span class="line">    lock synchronized &#123;</span><br><span class="line">      <span class="keyword">val</span> keysToBeRemoved = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">BrokerIdAndFetcherId</span>]</span><br><span class="line">      <span class="keyword">for</span> ((key, fetcher) &lt;- fetcherThreadMap) &#123;</span><br><span class="line">        <span class="keyword">if</span> (fetcher.partitionCount &lt;= <span class="number">0</span>) &#123; <span class="comment">// 要拉取的partition数小于0</span></span><br><span class="line">          fetcher.shutdown()</span><br><span class="line">          keysToBeRemoved += key</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      fetcherThreadMap --= keysToBeRemoved</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>任务调度结合数据结构图实现</title>
    <url>/2019/10/12/%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E7%BB%93%E5%90%88%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%9B%BE%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<blockquote>
<p>各类作业执行会操作不同的表,作业之间又有依赖关系,所以希望通过输入输出表与作业的关系来决定作业顺序,其实如果没有特殊要求,像这类调度可以使用Azkanban工具来实现,更加方便</p>
</blockquote>
<span id="more"></span>

<h2 id="已有信息"><a href="#已有信息" class="headerlink" title="已有信息"></a>已有信息</h2><ul>
<li>各spark作业jar</li>
<li>neo4j存储了作业与表的关系图</li>
<li>作业有执行的先后关系</li>
</ul>
<hr>
<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><ul>
<li>半/全自动化执行作业</li>
<li>只需要确定输入表输出表与作业的关系,就可以生成执行信息</li>
</ul>
<hr>
<h2 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.Neo4j针对输入输出表,建立图</span><br><span class="line">2.根据图去生成作业的执行顺序&lt;重点&gt;</span><br><span class="line">    eg: 现有5张表,ta,tb,tc,td,te,每个表对应1个spark程序j1,j2,j3,j4,j5</span><br><span class="line">    表的依赖关系为tc依赖于(ta,tb),te依赖于(tc,td),那么我们可以得到一个任务分层树</span><br><span class="line">    表分层:</span><br><span class="line">        第一层: ta,tb,td		</span><br><span class="line">        第二层: tc							</span><br><span class="line">        第三层: te							</span><br><span class="line">    对应作业:	</span><br><span class="line">        第一层: j1,j2,j4</span><br><span class="line">        第二层: j3</span><br><span class="line">        第三层: j5</span><br><span class="line">3.建立作业的状态表job_state(状态信息MySQL存储)</span><br><span class="line">    eg: 根据上述信息,可以了解到,树的深度是未知的,所以根据层数来指定分层表是不理想的,但是可以用父子节点的方式将作业信息存放单表中</span><br><span class="line">    每个作业应该有的信息有</span><br><span class="line">        作业id(每天根据第2步生成任务分层树生成)</span><br><span class="line">        父作业id(依赖的作业id,无为0,,可以是多个)</span><br><span class="line">        执行状态(0未执行,1,正在执行,2执行失败,6执行成功)</span><br><span class="line">        执行命令(spark-submit ...jar)</span><br><span class="line">        开始时间</span><br><span class="line">        结束时间</span><br><span class="line">4.建立脚本,每天定时执行(具体选择队列形式&lt;celery一次执行&gt;,还是重复执行&lt;crontab每N分钟执行&gt;)</span><br><span class="line">5.脚本逻辑</span><br><span class="line">    去表中获取今日份作业信息&lt;循环执行&gt;</span><br><span class="line">    判断状态值与依赖</span><br><span class="line">    状态为0且无父依赖</span><br><span class="line">        调用console执行spark作业</span><br><span class="line">    状态为0但有父依赖</span><br><span class="line">        根据父作业id,查询父作业状态select * from job_state where id &#x3D; 父作业id and 执行状态 !&#x3D; 6</span><br><span class="line">        判断状态值,为6则执行,只要有不为6的就跳过</span><br><span class="line">    状态不为0</span><br><span class="line">        跳过执行</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="图代码实现"><a href="#图代码实现" class="headerlink" title="图代码实现"></a>图代码实现</h2><h3 id="Point-TablePoint-ProjectPoint"><a href="#Point-TablePoint-ProjectPoint" class="headerlink" title="Point(TablePoint,ProjectPoint)"></a>Point(TablePoint,ProjectPoint)</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Point</span> </span>&#123;</span><br><span class="line">    HashSet&lt;String&gt; parentPoints = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    HashSet&lt;String&gt; childPoints = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ---</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProjectPoint</span> <span class="keyword">extends</span> <span class="title">Point</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;任务节点&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ---</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TablePoint</span> <span class="keyword">extends</span> <span class="title">Point</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;表节点&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, Point&gt; pointMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, HashSet&lt;String&gt;&gt; edgeMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 添加节点,Point具有两个数组,分别存父节点,子节点</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> point</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addPoint</span><span class="params">(String name, Point point)</span> </span>&#123;</span><br><span class="line">        pointMap.put(name, point);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 添加边,并给from节点添加子节点列表,给to节点添加父节点列表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> from</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> to</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">addEdge</span><span class="params">(String from, String to)</span> </span>&#123;</span><br><span class="line">        HashSet&lt;String&gt; nextPoint = edgeMap.get(from);</span><br><span class="line">        <span class="keyword">if</span> (nextPoint != <span class="keyword">null</span>) &#123;</span><br><span class="line">            nextPoint.add(to);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            HashSet&lt;String&gt; strings = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">            strings.add(to);</span><br><span class="line">            edgeMap.put(from, strings);</span><br><span class="line">        &#125;</span><br><span class="line">        Point fromPoint = pointMap.get(from);</span><br><span class="line">        Point toPoint = pointMap.get(to);</span><br><span class="line">        fromPoint.childPoints.add(to);</span><br><span class="line">        toPoint.parentPoints.add(from);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    HashMap&lt;String, HashSet&lt;String&gt;&gt; getEdgeMap() &#123;</span><br><span class="line">        <span class="keyword">return</span> edgeMap;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">HashMap&lt;String, Point&gt; <span class="title">getPointMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pointMap;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取出度</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">HashMap&lt;String, Integer&gt; <span class="title">getOutDegree</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        HashMap&lt;String, Integer&gt; outDegree = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String key : edgeMap.keySet()) &#123;</span><br><span class="line">            outDegree.put(key, edgeMap.get(key).size());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> outDegree;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取入度</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">HashMap&lt;String, Integer&gt; <span class="title">getInDegree</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        HashMap&lt;String, Integer&gt; inDegree = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String key : edgeMap.keySet()) &#123;</span><br><span class="line">            HashSet&lt;String&gt; strings = edgeMap.get(key);</span><br><span class="line">            <span class="keyword">for</span> (String s : strings) &#123;</span><br><span class="line">                inDegree.merge(s, <span class="number">1</span>, (a, b) -&gt; a + b);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> inDegree;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取points各个节点为ProjectPoint的节点,HashSet去重</span></span><br><span class="line"><span class="comment">     * 目前暂且支持table-&gt;project-&gt;table的情况</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> points</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function">HashSet&lt;String&gt; <span class="title">getParentNodes</span><span class="params">(HashSet&lt;String&gt; points)</span> </span>&#123;</span><br><span class="line">        HashSet&lt;String&gt; result = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String point : points) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!(pointMap.get(point) <span class="keyword">instanceof</span> ProjectPoint)) &#123;</span><br><span class="line">                result.addAll(getParentNodes(pointMap.get(point).parentPoints));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                result.add(point);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="GraphDemo"><a href="#GraphDemo" class="headerlink" title="GraphDemo"></a>GraphDemo</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GraphDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Graph graph = <span class="keyword">new</span> Graph();</span><br><span class="line">        <span class="comment">// 构建图</span></span><br><span class="line">        graph.addPoint(<span class="string">&quot;a&quot;</span>, <span class="keyword">new</span> TablePoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;b&quot;</span>, <span class="keyword">new</span> TablePoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;c&quot;</span>, <span class="keyword">new</span> TablePoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;d&quot;</span>, <span class="keyword">new</span> ProjectPoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;e&quot;</span>, <span class="keyword">new</span> TablePoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;f&quot;</span>, <span class="keyword">new</span> ProjectPoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;g&quot;</span>, <span class="keyword">new</span> TablePoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;h&quot;</span>, <span class="keyword">new</span> TablePoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;i&quot;</span>, <span class="keyword">new</span> TablePoint());</span><br><span class="line">        graph.addPoint(<span class="string">&quot;j&quot;</span>, <span class="keyword">new</span> ProjectPoint());</span><br><span class="line">        graph.addEdge(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;d&quot;</span>, <span class="string">&quot;e&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;e&quot;</span>, <span class="string">&quot;f&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;f&quot;</span>, <span class="string">&quot;g&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;f&quot;</span>, <span class="string">&quot;h&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;g&quot;</span>, <span class="string">&quot;j&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;j&quot;</span>);</span><br><span class="line">        graph.addEdge(<span class="string">&quot;i&quot;</span>, <span class="string">&quot;j&quot;</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(graph.getPointMap());</span><br><span class="line">        System.out.println(graph.getEdgeMap());</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, HashSet&lt;String&gt;&gt; edgeMap = graph.getEdgeMap();</span><br><span class="line">        HashMap&lt;String, Point&gt; pointMap = graph.getPointMap();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 出度</span></span><br><span class="line">        System.out.println(graph.getOutDegree());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 入度</span></span><br><span class="line">        System.out.println(graph.getInDegree());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 生成ProjectPoint节点,以及其父ProjectPoint节点信息</span></span><br><span class="line">        <span class="keyword">for</span> (String key : pointMap.keySet()) &#123;</span><br><span class="line">            <span class="comment">// System.out.println(key+&quot;:&quot;+pointMap.get(key).parentPoints);</span></span><br><span class="line">            <span class="keyword">if</span> (pointMap.get(key) <span class="keyword">instanceof</span> ProjectPoint) &#123;</span><br><span class="line">                HashSet&lt;String&gt; parentNodes = graph.getParentNodes(pointMap.get(key).parentPoints);</span><br><span class="line">                System.out.println(key + <span class="string">&quot;:&quot;</span> + parentNodes);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="图升级版实现-lt-附思路和代码-gt"><a href="#图升级版实现-lt-附思路和代码-gt" class="headerlink" title="图升级版实现&lt;附思路和代码&gt;"></a>图升级版实现&lt;附思路和代码&gt;</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">我们已经获取了一个图,并且确定了每个节点的父节点与子节点</span><br><span class="line">那么获取所有没有入度的节点</span><br><span class="line">执行完没有入度的节点之后,删除节点,与其对应的边,再去获取没有入度的节点</span><br><span class="line">一直循环,直到结束</span><br><span class="line"></span><br><span class="line">点集:存放节点信息</span><br><span class="line">边集:存放(from点,to点)关系对</span><br><span class="line"></span><br><span class="line">执行节点: </span><br><span class="line">执行时,将正在执行的节点保存起来&lt;runningNode&gt;;</span><br><span class="line">什么是执行节点,获取没有入度的节点并且不存在于&lt;runningNode&gt;的节点就是执行节点</span><br><span class="line">没有入度的节点:遍历边集的to节点,不存在与点集的都是没有入度的节点</span><br><span class="line">执行前或执行后删除都可以,顺序如下</span><br><span class="line">[1,2,3,4,5,6]-&gt;[(1,2),(1,3),(4,5),(3,6),(5,6)]</span><br><span class="line">to(2,3,5,6) 执行1,4</span><br><span class="line">[2,3,5,6]-&gt;[(3,6),(5,6)]</span><br><span class="line">to(6) 执行2,3,5</span><br><span class="line">[6]-&gt;[]</span><br><span class="line">to() 执行6</span><br><span class="line"></span><br><span class="line">具体实现开启多线程执行作业</span><br></pre></td></tr></table></figure>

<h3 id="缩边"><a href="#缩边" class="headerlink" title="缩边"></a>缩边</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">任务调度图是由表与作业两个节点进行连接生成的</span><br><span class="line">最终我们需要的是一个完全由作业组成的作业流图</span><br><span class="line">实现:</span><br><span class="line">a.获取以作业节点为起点的边集-&gt;(作业节点,表节点&#x2F;作业节点)</span><br><span class="line">b.获取(作业节点,作业节点)类型的边集</span><br><span class="line">c.根据a的边集获取对应表节点为起点的边集-&gt;(表节点,作业节点)</span><br><span class="line">d.对ac的结果集进行缩边,将表节点转成作业节点</span><br><span class="line">eg:</span><br><span class="line">    p1,p2,p3是作业节点</span><br><span class="line">    t1,t2,t3,t4是表节点</span><br><span class="line">    依赖为:(t1,t2)-&gt;p1-&gt;t3-&gt;p2-&gt;t4-&gt;p3</span><br><span class="line">    a将产生[(p1,t3),(p2-&gt;t4)]]</span><br><span class="line">    b是空</span><br><span class="line">    c根据a的t3,t4将产生[(t3,p2),(t4,p3)]</span><br><span class="line">    d产生[(p1,p2),(p2,p4)]</span><br><span class="line">    </span><br><span class="line">注意:</span><br><span class="line">    不支持存在表节点-&gt;表节点关系的图</span><br></pre></td></tr></table></figure>

<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> graph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.<span class="type">FileInputStream</span></span><br><span class="line"><span class="keyword">import</span> java.time.&#123;<span class="type">Instant</span>, <span class="type">LocalDateTime</span>, <span class="type">ZoneOffset</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.time.format.<span class="type">DateTimeFormatter</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Graph</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>(<span class="params">name: <span class="type">String</span>, flag: <span class="type">String</span>, cost: <span class="type">Long</span>, path: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">case</span> <span class="title">class</span> <span class="title">Edge</span>(<span class="params">from: <span class="type">String</span>, to: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">var</span> <span class="title">mysqlData</span></span>: <span class="type">MySQLData</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> nodeSet: <span class="type">Set</span>[<span class="type">Node</span>] = <span class="type">Set</span>[<span class="type">Node</span>]()</span><br><span class="line">  <span class="keyword">var</span> edgeSet: <span class="type">Set</span>[<span class="type">Edge</span>] = <span class="type">Set</span>[<span class="type">Edge</span>]()</span><br><span class="line">  <span class="keyword">var</span> eSet = <span class="type">Set</span>[<span class="type">Edge</span>]()</span><br><span class="line">  <span class="keyword">var</span> nSet = <span class="type">Set</span>[<span class="type">Node</span>]()</span><br><span class="line">  <span class="keyword">var</span> recoverSet = <span class="type">Set</span>[<span class="type">Node</span>]()</span><br><span class="line">  <span class="keyword">var</span> pickedHeadSet = <span class="type">Set</span>[<span class="type">Node</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> nodeMap: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Node</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Node</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> <span class="type">THREAD_COUNT</span> = <span class="number">3</span></span><br><span class="line">  <span class="keyword">val</span> formatter = <span class="type">DateTimeFormatter</span>.ofPattern(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">LOCK</span> = <span class="number">1</span>l</span><br><span class="line">  <span class="keyword">var</span> recover = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 初始化</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">init</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 替换成MySQL</span></span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;d&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="number">7</span>, <span class="string">&quot;执行命令d&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;x&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;e&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;f&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="number">6</span>, <span class="string">&quot;执行命令f&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;g&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;i&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="number">0</span>, <span class="string">&quot;执行命令&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;j&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="number">5</span>, <span class="string">&quot;执行命令j&quot;</span>)</span><br><span class="line">    nodeSet += <span class="type">Node</span>(<span class="string">&quot;k&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="number">4</span>, <span class="string">&quot;执行命令k&quot;</span>)</span><br><span class="line"></span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;d&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;b&quot;</span>, <span class="string">&quot;d&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;d&quot;</span>, <span class="string">&quot;e&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;e&quot;</span>, <span class="string">&quot;f&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;f&quot;</span>, <span class="string">&quot;g&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;f&quot;</span>, <span class="string">&quot;h&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;g&quot;</span>, <span class="string">&quot;j&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;j&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;k&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;i&quot;</span>, <span class="string">&quot;j&quot;</span>)</span><br><span class="line">    edgeSet += <span class="type">Edge</span>(<span class="string">&quot;j&quot;</span>, <span class="string">&quot;i&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//    val url = prop.getProperty(&quot;url&quot;)</span></span><br><span class="line">    <span class="comment">//    val username = prop.getProperty(&quot;username&quot;)</span></span><br><span class="line">    <span class="comment">//    val password = prop.getProperty(&quot;password&quot;)</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    mysqlData = MySQLData(url, username, password)</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    val resultSet = mysqlData.executeQuery(&quot;select name, flag, cost, path from job_node&quot;)</span></span><br><span class="line">    <span class="comment">//    while (resultSet.next()) &#123;</span></span><br><span class="line">    <span class="comment">//      val name = resultSet.getString(1)</span></span><br><span class="line">    <span class="comment">//      val cost = resultSet.getLong(2)</span></span><br><span class="line">    <span class="comment">//      val path = resultSet.getString(3)</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//      nodeSet += Node(name, &quot;&quot;, cost, path)</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="comment">//    resultSet.close()</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    // job edge</span></span><br><span class="line">    <span class="comment">//    val edgeResultSet = mysqlData.executeQuery(&quot;select before_job, after_job from job_edge&quot;)</span></span><br><span class="line">    <span class="comment">//    while (edgeResultSet.next()) &#123;</span></span><br><span class="line">    <span class="comment">//      val from = edgeResultSet.getString(1)</span></span><br><span class="line">    <span class="comment">//      val to = edgeResultSet.getString(2)</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//      edgeSet += Edge(from, to)</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="comment">//    edgeResultSet.close()</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    nodeMap = nodeSet.map(x =&gt; (x.name, x)).toMap[String, Node]</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//    // failed job</span></span><br><span class="line">    <span class="comment">//    val failedResultSet = mysqlData.executeQuery(</span></span><br><span class="line">    <span class="comment">//      s&quot;select name from job_failed_node where c_time&gt; &#x27;$&#123;LocalDate.now().toString&#125;&#x27;&quot;</span></span><br><span class="line">    <span class="comment">//    )</span></span><br><span class="line">    <span class="comment">//    while (failedResultSet.next()) &#123;</span></span><br><span class="line">    <span class="comment">//      val name = failedResultSet.getString(1)</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//      if (nodeMap.contains(name)) &#123;</span></span><br><span class="line">    <span class="comment">//        recoverSet += nodeMap(name)</span></span><br><span class="line">    <span class="comment">//      &#125;</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="comment">//    failedResultSet.close()</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 缩边,重置点集边集</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">generate</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    nodeMap = nodeSet.map(x =&gt; (x.name, x)).toMap[<span class="type">String</span>, <span class="type">Node</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取&lt;作业节点,表节点/作业节点&gt;边</span></span><br><span class="line">    <span class="keyword">val</span> edge1 = edgeSet.filter(x =&gt; nodeMap.filter(_._2.flag == <span class="string">&quot;p&quot;</span>).keys.toList.contains(x.from)).map(x =&gt; (x.from, x.to)).toList</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取&lt;作业节点,作业节点&gt;边</span></span><br><span class="line">    <span class="keyword">val</span> p2p = edge1.filter(x =&gt; &#123;</span><br><span class="line">      nodeMap(x._1).flag.equals(<span class="string">&quot;p&quot;</span>) &amp;&amp; nodeMap(x._2).flag.equals(<span class="string">&quot;p&quot;</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 根据edge1获取&lt;表节点,作业节点&gt;边,注意,在初始图中不应该存在&lt;表节点,表节点&gt;这样的边</span></span><br><span class="line">    <span class="keyword">val</span> edge2 = edgeSet.filter(x =&gt; edge1.map(_._2).contains(x.from)).map(x =&gt; (x.from, x.to)).toList</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重置点边</span></span><br><span class="line">    nodeSet = nodeSet.filter(_.flag == <span class="string">&quot;p&quot;</span>)</span><br><span class="line">    nodeMap = nodeSet.map(x =&gt; (x.name, x)).toMap[<span class="type">String</span>, <span class="type">Node</span>]</span><br><span class="line">    edgeSet = edge1.map(x =&gt; (x, edge2.filter(_._1 == x._2).map(_._2))).map(x =&gt; &#123;</span><br><span class="line">      x._2.map(y =&gt; &#123;</span><br><span class="line">        (x._1._1, y)</span><br><span class="line">      &#125;)</span><br><span class="line"><span class="comment">//    &#125;).flatMap(_.toList).++(p2p).filter(x =&gt; x._1 != x._2).map(x =&gt; &#123;</span></span><br><span class="line">    &#125;).flatMap(_.toList).++(p2p).map(x =&gt; &#123;</span><br><span class="line">      <span class="type">Edge</span>(x._1, x._2)</span><br><span class="line">    &#125;).toSet</span><br><span class="line">    println(edge1)</span><br><span class="line">    println(edge2)</span><br><span class="line">    println(nodeSet)</span><br><span class="line">    println(edgeSet)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 验证图</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">validate</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    edgeSet = edgeSet.filter(x =&gt; nodeMap.get(x.from).isDefined &amp;&amp; nodeMap.get(x.to).isDefined)</span><br><span class="line">    eSet = edgeSet</span><br><span class="line">    nSet = nodeSet</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 加载需要恢复的节点</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    * @param rSet</span></span><br><span class="line"><span class="comment">    * @return</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getReset</span></span>(rSet: <span class="type">Set</span>[<span class="type">Node</span>]): <span class="type">Set</span>[<span class="type">Node</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> set = <span class="keyword">if</span> (rSet.size &lt; <span class="number">1</span>) <span class="type">Set</span>[<span class="type">Node</span>]() <span class="keyword">else</span> getReset(edgeSet.filter(x =&gt; &#123;</span><br><span class="line">      rSet.map(_.name).contains(x.from)</span><br><span class="line">    &#125;).map(x =&gt; nodeMap(x.to)))</span><br><span class="line">    set ++ rSet</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    prop.load(<span class="keyword">new</span> <span class="type">FileInputStream</span>(<span class="string">&quot;D:\\Projects\\IdeaProjects\\Demo\\zsd-test\\src\\main\\resource\\prop.properties&quot;</span>))</span><br><span class="line">    <span class="type">THREAD_COUNT</span> = prop.getProperty(<span class="string">&quot;worker_count&quot;</span>).toInt</span><br><span class="line">    <span class="keyword">if</span> (args.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      recover = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化(表-作业)图</span></span><br><span class="line">    init()</span><br><span class="line">    <span class="comment">// 生成作业图</span></span><br><span class="line">    generate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// validate()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//    if (recover &amp;&amp; recoverSet != null &amp;&amp; recoverSet.nonEmpty) &#123; // 如果是恢复流程</span></span><br><span class="line">    <span class="comment">//      println(&quot;#############################  Recover job  #############################&quot;)</span></span><br><span class="line">    <span class="comment">//      val rSet = getReset(recoverSet) //加载需要恢复的节点</span></span><br><span class="line">    <span class="comment">//      nodeSet = rSet</span></span><br><span class="line">    <span class="comment">//      nSet = nodeSet</span></span><br><span class="line">    <span class="comment">//      nodeMap = nodeSet.map(x =&gt; (x.name, x)).toMap[String, Node]</span></span><br><span class="line">    <span class="comment">//      validate() // 验证边和点信息</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until <span class="type">THREAD_COUNT</span>) &#123; <span class="comment">// 启动线程</span></span><br><span class="line">      <span class="keyword">val</span> thread = <span class="keyword">new</span> <span class="type">Worker</span>()</span><br><span class="line">      thread.start()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Worker</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> runtime: <span class="type">Runtime</span> = <span class="type">Runtime</span>.getRuntime</span><br><span class="line">    <span class="keyword">var</span> history: <span class="type">ListBuffer</span>[<span class="type">Node</span>] = <span class="type">ListBuffer</span>[<span class="type">Node</span>]()</span><br><span class="line">    <span class="keyword">var</span> cost = <span class="number">0</span>d</span><br><span class="line">    <span class="keyword">var</span> enable = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">while</span> (nSet.nonEmpty &amp;&amp; enable) &#123;</span><br><span class="line">        <span class="keyword">var</span> pickedNode: <span class="type">Node</span> = <span class="literal">null</span></span><br><span class="line">        <span class="type">LOCK</span>.synchronized &#123;</span><br><span class="line">          <span class="keyword">val</span> headSet = getHead <span class="comment">// 获取可运行的头结点</span></span><br><span class="line">          println(headSet)</span><br><span class="line">          <span class="keyword">if</span> (headSet.nonEmpty) &#123; <span class="comment">// 如果有可用头结点，则挑选一个来运行</span></span><br><span class="line">            <span class="keyword">val</span> (pn, length) = pickNode(headSet) <span class="comment">// 挑选加权深度最大的一个node运行</span></span><br><span class="line">            history += pn <span class="comment">// 记录本线程运行历史</span></span><br><span class="line">            pickedHeadSet += pn <span class="comment">// 记录正在运行的头结点</span></span><br><span class="line">            eSet = eSet.filter(_.from != pn.name) <span class="comment">// 去除边</span></span><br><span class="line">            nSet = nSet.filter(_ != pn) <span class="comment">// 去除点</span></span><br><span class="line">            pickedNode = pn</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">else</span> &#123; <span class="comment">// 如果没有可用头结点就等待10s</span></span><br><span class="line">            println(<span class="string">s&quot;Thread <span class="subst">$&#123;Thread.currentThread().getId&#125;</span> Sleeping !!&quot;</span>)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">10000</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行程序</span></span><br><span class="line">        <span class="keyword">if</span> (pickedNode != <span class="literal">null</span>) &#123;</span><br><span class="line">          println(<span class="string">s&quot;Thread <span class="subst">$&#123;Thread.currentThread().getId&#125;</span> ==&gt; <span class="subst">$&#123;pickedNode.name&#125;</span>&quot;</span>)</span><br><span class="line">          <span class="keyword">val</span> t1 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">          <span class="keyword">var</span> t2 = t1</span><br><span class="line">          <span class="keyword">var</span> rtn = <span class="number">0</span></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            println(<span class="string">s&quot;Exec <span class="subst">$&#123;pickedNode.path&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment">// val path = pickedNode.path.substring(0, pickedNode.path.lastIndexOf(&quot;/&quot;))</span></span><br><span class="line">            <span class="type">Thread</span>.sleep(pickedNode.cost * <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">//            val cmd: Array[String] = Array(&quot;/bin/sh&quot;, &quot;-c&quot;, s&quot;nohup $&#123;pickedNode.path&#125; &gt;&gt; $path/msg.log 2&gt;&amp;1&quot;)</span></span><br><span class="line">            <span class="comment">//            val process = runtime.exec(cmd)</span></span><br><span class="line">            <span class="comment">//            rtn = process.waitFor()</span></span><br><span class="line">            println(pickedNode.path)</span><br><span class="line">            t2 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">            <span class="keyword">if</span> (rtn != <span class="number">0</span>) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">s&quot;Return Code <span class="subst">$rtn</span> !!&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">              println(<span class="string">s&quot;Run Success in Code <span class="subst">$rtn</span> !! Cost <span class="subst">$&#123;(t2 - t1) / 1000&#125;</span> Seconds&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="type">LOCK</span>.synchronized &#123;</span><br><span class="line">              pickedHeadSet -= pickedNode <span class="comment">// 如果执行成功了，就将节点从正在执行的set中移除</span></span><br><span class="line">            &#125;</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">RuntimeException</span> =&gt; &#123;</span><br><span class="line">              <span class="type">LOCK</span>.synchronized &#123;</span><br><span class="line">                println(<span class="string">s&quot;### Node <span class="subst">$&#123;pickedNode.name&#125;</span> # is error !!&quot;</span>)</span><br><span class="line">                printChildren(pickedNode, pickedNode) <span class="comment">// 执行失败，则记录错误信息</span></span><br><span class="line">                println(e)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125; <span class="keyword">finally</span> &#123; <span class="comment">// 最终，更新node信息，记录最后运行时间和花费时间</span></span><br><span class="line">            <span class="keyword">val</span> time = formatter.format(<span class="type">LocalDateTime</span>.ofInstant(<span class="type">Instant</span>.ofEpochMilli(t1), <span class="type">ZoneOffset</span>.ofHours(<span class="number">8</span>)))</span><br><span class="line">            <span class="keyword">val</span> sql = <span class="keyword">new</span> <span class="type">StringBuilder</span>(<span class="string">s&quot;update job_node set last_run=&#x27;<span class="subst">$time</span>&#x27;&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> (rtn == <span class="number">0</span>) &#123;</span><br><span class="line">              cost += (t2 - t1) / <span class="number">1000</span></span><br><span class="line">              sql.append(<span class="string">s&quot;, cost = <span class="subst">$&#123;(t2 - t1) / 1000&#125;</span>&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            sql.append(<span class="string">s&quot; where name=&#x27;<span class="subst">$&#123;pickedNode.name&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">            <span class="comment">//            val count = mysqlData.executeUpdate(sql.toString())</span></span><br><span class="line">            <span class="comment">//            println(s&quot;Infect Count $count&quot;)</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      println(<span class="string">s&quot;@@ Thread <span class="subst">$&#123;Thread.currentThread().getId&#125;</span> Cost: <span class="subst">$cost</span> ==&gt; <span class="subst">$history</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printChildren</span></span>(node: <span class="type">Node</span>, error_root: <span class="type">Node</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">//      mysqlData.executeUpdate(</span></span><br><span class="line">      <span class="comment">//        s&quot;insert into job_failed_node(name, error_root) values(&#x27;$&#123;node.name&#125;&#x27;, &#x27;$&#123;error_root.name&#125;&#x27;)&quot;</span></span><br><span class="line">      <span class="comment">//      )</span></span><br><span class="line">      println(<span class="string">s&quot;Record the Error Node <span class="subst">$&#123;node.name&#125;</span>!!&quot;</span>)</span><br><span class="line">      nSet -= node</span><br><span class="line">      edgeSet.filter(_.from == node.name).foreach(x =&gt; &#123;</span><br><span class="line">        printChildren(nodeMap(x.to), error_root)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getDepth</span></span>(node: <span class="type">Node</span>, nodeSeq: <span class="type">Seq</span>[<span class="type">Node</span>], depth: <span class="type">Double</span> = <span class="number">0</span>): (<span class="type">Seq</span>[<span class="type">Node</span>], <span class="type">Double</span>) = &#123;</span><br><span class="line">      <span class="keyword">val</span> subEdgeSet = edgeSet.filter(_.from == node.name)</span><br><span class="line">      <span class="keyword">if</span> (subEdgeSet.isEmpty) &#123;</span><br><span class="line">        (nodeSeq :+ node, node.cost + depth)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        subEdgeSet.map(x =&gt; &#123;</span><br><span class="line">          getDepth(nodeMap(x.to), nodeSeq :+ node, node.cost + depth)</span><br><span class="line">        &#125;).maxBy(_._2)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pickNode</span></span>(headSet: <span class="type">Set</span>[<span class="type">Node</span>]): (<span class="type">Node</span>, <span class="type">Double</span>) = &#123;</span><br><span class="line">      <span class="keyword">val</span> maxNode = headSet.map(node =&gt; (node, getDepth(node, <span class="type">Seq</span>[<span class="type">Node</span>]()))).maxBy(_._2._2) <span class="comment">// 挑选运行估计时间最长的</span></span><br><span class="line">      (maxNode._1, maxNode._2._2)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getHead</span></span>: <span class="type">Set</span>[<span class="type">Node</span>] = &#123;</span><br><span class="line">      nSet.filterNot(node =&gt; &#123; <span class="comment">// 这些情况的不挑</span></span><br><span class="line">        eSet.map(_.to).contains(node.name) || <span class="comment">// 有入度的节点</span></span><br><span class="line">          pickedHeadSet.map(_.name).contains(node.name) || <span class="comment">// 正在执行的节点</span></span><br><span class="line">          edgeSet.filter(x =&gt; pickedHeadSet.map(_.name).contains(x.from)).map(_.to).contains(node.name) <span class="comment">//  正在执行的节点的子节点</span></span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> graph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.&#123;<span class="type">Connection</span>, <span class="type">DriverManager</span>, <span class="type">ResultSet</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQLData</span>(<span class="params">url: <span class="type">String</span>, username: <span class="type">String</span>, password: <span class="type">String</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> lock = <span class="number">1</span>l</span><br><span class="line">  <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initConn</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    lock.synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (conn == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="type">Class</span>.forName(<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">        conn = <span class="type">DriverManager</span>.getConnection(url, username, password)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">executeQuery</span></span>(sql: <span class="type">String</span>): <span class="type">ResultSet</span> = &#123;</span><br><span class="line">    initConn()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> statement = conn.createStatement()</span><br><span class="line">    statement.executeQuery(sql)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">executeUpdate</span></span>(sql: <span class="type">String</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    initConn()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> statement = conn.createStatement()</span><br><span class="line">    statement.executeUpdate(sql)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MySQLData</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(</span><br><span class="line">             url: <span class="type">String</span>,</span><br><span class="line">             username: <span class="type">String</span>,</span><br><span class="line">             password: <span class="type">String</span></span><br><span class="line">           ): <span class="type">MySQLData</span> = <span class="keyword">new</span> <span class="type">MySQLData</span>(url, username, password)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法</category>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据理论知识合集</title>
    <url>/2019/11/29/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%90%88%E9%9B%86/</url>
    <content><![CDATA[<blockquote>
<p>对于大数据理论方面知识进行搜集整理</p>
</blockquote>
<span id="more"></span>

<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="1-HDFS的组成"><a href="#1-HDFS的组成" class="headerlink" title="1.HDFS的组成"></a>1.HDFS的组成</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.NameNode：</span><br><span class="line">    管理HDFS的名称空间；</span><br><span class="line">    管理数据块的映射信息；</span><br><span class="line">    配置副本策略；</span><br><span class="line">    处理客户端的读写请求。</span><br><span class="line">    Fsimage：</span><br><span class="line">        存储了文件的基本信息，如文件路径，文件副本集个数，文件块的信息，文件所在的主机信息。</span><br><span class="line">    Editslog：</span><br><span class="line">        存了客户端对hdfs中各种写操作的日志（指令）。</span><br><span class="line">2.DataNode：</span><br><span class="line">    存储实际的数据块(Block，默认128M)；</span><br><span class="line">    执行数据块的读写操作。</span><br><span class="line">3.Secondary NameNode：</span><br><span class="line">    是NameNode的冷备；</span><br><span class="line">    辅助NameNode，分担其工作量；</span><br><span class="line">    定期（每一小时，editslog大小超过64M）合并fsimage和fsedits，并推送给NameNode；</span><br><span class="line">    在紧急情况下，可辅助恢复NameNode。</span><br><span class="line">4.Client：</span><br><span class="line">    文件切分；</span><br><span class="line">    与NameNode交互，获取文件的位置信息；</span><br><span class="line">    与DataNode交互，读取或者写入数据；</span><br><span class="line">    Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；</span><br><span class="line">    Client可以通过一些命令来访问HDFS。</span><br></pre></td></tr></table></figure>

<h2 id="2-HDFS读写数据流程"><a href="#2-HDFS读写数据流程" class="headerlink" title="2.HDFS读写数据流程"></a>2.HDFS读写数据流程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">读</span><br><span class="line">    1.HDFS客户端开启分布式文件系统</span><br><span class="line">    2.分布式文件系统向NameNode获取数据块信息</span><br><span class="line">    3.HDFS客户端通过数据块信息创建文件系统数据输入流</span><br><span class="line">    4.流通过数据块信息去各个DataNode读取信息</span><br><span class="line">    5.HDFS客户端关闭流</span><br><span class="line">写</span><br><span class="line">    1.HDFS客户端开启分布式文件系统</span><br><span class="line">    2.分布式文件系统创建NameNode</span><br><span class="line">    3.HDFS客户端创建文件系统数据输出流</span><br><span class="line">    4.通过流写入DataNode---逐步备份</span><br><span class="line">    5.逐步返回ack---流检测ack是否成功</span><br><span class="line">    6.HDFS客户端关闭流</span><br><span class="line">    7.分布式文件系统向NameNode提交完成</span><br></pre></td></tr></table></figure>

<h2 id="3-MR的shuffle过程"><a href="#3-MR的shuffle过程" class="headerlink" title="3.MR的shuffle过程"></a>3.MR的shuffle过程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Map端Shuffle</span><br><span class="line">Split切片,一般根据HDFS上的Block数量决定</span><br><span class="line">频繁的磁盘I&#x2F;O操作会严重降低效率,&quot;中间结果&quot;不会立马写入磁盘,而是写入一个环形缓冲区</span><br><span class="line">在写入过程中进行分区,对于每个键值增加一个partition属性值</span><br><span class="line">连同键值对一起序列化成字节数组写入缓冲区(默认100M)</span><br><span class="line">当写入的数据量达到预先设置的阙值后(默认0.8)便会启动溢写</span><br><span class="line">将缓冲区数据写到磁盘的临时文件中(可以压缩,默认不压缩),写入前根据Key进行排序(Sort)和合并(Combine,可选操作)</span><br><span class="line">当整个Map操作完成溢写后,会对磁盘这个Map产生的所有临时文件进行归并(Merge),生成正式输出文件</span><br><span class="line">此事的归并会将所有临时文件中相同partition合并到一起,并对每一个partition的数据再一次排序(Sort)</span><br><span class="line"></span><br><span class="line">Reduce端Shuffle</span><br><span class="line">不断拉取当前Job中每个MapTask的最终结果</span><br><span class="line">从不同地方拉取过来的数据不断做Merge,合并成分区相同的大文件</span><br><span class="line">对这个文件的键值对按Key进行Sort</span><br><span class="line">排好序之后紧接着分组,分组完成后才将整个文件交给ReduceTask处理</span><br><span class="line"></span><br><span class="line">注意</span><br><span class="line">1.Combine和Merge的区别</span><br><span class="line">    &lt;a,1&gt;,&lt;a,1&gt;变成&lt;a,2&gt;是Combine</span><br><span class="line">    &lt;a,1&gt;,&lt;a,1&gt;变成&lt;a,&lt;1,1&gt;&gt;是Merge</span><br><span class="line">2.Shuffle中的排序</span><br><span class="line">    内存缓冲区使用的是快速排序</span><br><span class="line">    文件合并阶段使用的归并排序</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><h2 id="1-Yarn的组成"><a href="#1-Yarn的组成" class="headerlink" title="1.Yarn的组成"></a>1.Yarn的组成</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.ResourceManager：</span><br><span class="line">    处理客户端请求；</span><br><span class="line">    启动或监控ApplicationMaster；</span><br><span class="line">    监控NodeManager；</span><br><span class="line">    负责整个集群的资源管理和调度。</span><br><span class="line">2.NodeManager：</span><br><span class="line">    处理来自ResourceManager上的命令；</span><br><span class="line">    处理来自ApplicationMaster上的命令；</span><br><span class="line">    负责单个节点上的资源管理。</span><br><span class="line">3.ApplicationMaster：</span><br><span class="line">    负责数据切分；</span><br><span class="line">    为应用程序申请资源并分配给内部的任务；</span><br><span class="line">    负责应用程序相关的事务，比如任务调度，任务监控和容错等。</span><br></pre></td></tr></table></figure>

<h2 id="2-Yarn执行流程"><a href="#2-Yarn执行流程" class="headerlink" title="2.Yarn执行流程"></a>2.Yarn执行流程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.作业提交</span><br><span class="line">    A.Client调用job.waitForCompletion方法，向集群提交MR作业；</span><br><span class="line">    B.ResourceManager分配JobID；</span><br><span class="line">    C.Client核实作业的输出，计算输入的Split，将作业的资源(Jar包，配置文件，Split信息等)拷贝给HDFS；</span><br><span class="line">    D.调用ResourceManager的submitApplication()提交作业。</span><br><span class="line">2.作业初始化</span><br><span class="line">    A.ResourceManager收到请求，将请求发送给调度器（scheduler），调度器分配Container，然后ResourceManager在Container中启动ApplicationMaster，由NodeManager进行监控；</span><br><span class="line">    B.ApplicationMaster通过bookkeeping对象来监控作业的进度，得到任务的进度和完成报告；</span><br><span class="line">    C.通过HDFS得到Client计算好的Split，为每个输入Split创建Map任务，根据mapreduce.job.reduces创建Reduce任务对象。</span><br><span class="line">3.任务分配</span><br><span class="line">    A.如果作业小，ApplicationMaster会选择在自己的JVM中运行任务；</span><br><span class="line">    B.如果作业大，ApplicationMaster会向ResourceManager请求Container来运行所有的map和reduce任务，请求通过心跳机制传输；</span><br><span class="line">    C.调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点，或者退而分配给和存放Split的节点相同机架的节点。</span><br><span class="line">4.任务运行</span><br><span class="line">    A.当任务由ResourceManager的调度分配给Container后，ApplicationMaster通过联系NodeManager来启动Container；</span><br><span class="line">    B.任务由一个主类为YarnChild的Java应用执行（YarnChild运行在一个专用的JVM中，Yarn不支持JVM重用）；</span><br><span class="line">    C.在运行任务之前首先需要本地化任务需要的资源，比如作业配置，Jar包，以及分布式缓存的所有文件；</span><br><span class="line">    D.运行Map或Reduce任务。</span><br><span class="line">5.进度和状态更新</span><br><span class="line">    A.Yarn中的任务将其进度和状态返回给ApplicationMaster；</span><br><span class="line">    B.Client每秒向ApplicationMaster请求进度更新，展示给用户。</span><br><span class="line">6.任务完成</span><br><span class="line">    A.除了向ApplicationMaster请求作业进度外，Client每5分钟检查作业是否完成通过调用waitForCompletion()方法；</span><br><span class="line">    B.作业完成之后，ApplicationMaster和Container会清理工作状态，OutputCommiter的作业清理方法也会被调用；</span><br><span class="line">    C.作业的信息会被作业历史服务器存储以备用户核查。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h2 id="1-Hive怎么解决数据倾斜的问题"><a href="#1-Hive怎么解决数据倾斜的问题" class="headerlink" title="1.Hive怎么解决数据倾斜的问题?"></a>1.Hive怎么解决数据倾斜的问题?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">使map的输出数据更均匀的分布到reduce中去，是我们的最终目标</span><br></pre></td></tr></table></figure>

<h2 id="2-数据倾斜有哪些原因"><a href="#2-数据倾斜有哪些原因" class="headerlink" title="2.数据倾斜有哪些原因?"></a>2.数据倾斜有哪些原因?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">key分布不均匀</span><br><span class="line">业务数据本身的欠缺性</span><br><span class="line">建表设计方法不对</span><br><span class="line">有些SQL难免会有一些数据倾斜不可避免</span><br><span class="line"></span><br><span class="line">表现的形式:</span><br><span class="line">    任务完成进度卡死在99%，或者进度完成度在100%</span><br><span class="line">    但是查看任务监控，发现还是有少量（1个或几个）reduce子任务未完成</span><br><span class="line">    因为其处理的数据量和其他reduce差异过大</span><br><span class="line">    单一reduce的记录数与平均记录数差异过大</span><br><span class="line">    通常可能达到3倍甚至更多</span><br><span class="line">    最长时长远大于平均时长</span><br></pre></td></tr></table></figure>

<h2 id="3-Hive的工作过程"><a href="#3-Hive的工作过程" class="headerlink" title="3.Hive的工作过程"></a>3.Hive的工作过程</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A.用户提交查询等任务给Driver；</span><br><span class="line">B.编译器获得该用户的任务Plan；</span><br><span class="line">C.编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息；</span><br><span class="line">D.编译器Compiler得到元数据信息，对任务进行编译，先将HQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询快转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理计划（MapReduce），最后选择最优策略；</span><br><span class="line">E.将最终的计划提交给Driver；</span><br><span class="line">F.Driver将计划Plan转交给ExecutorEngine去执行，获取元数据信息，提交给JobTracker或者ResourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作；</span><br><span class="line">G.获取执行的结果；</span><br><span class="line">H.取得并返回执行结果。</span><br></pre></td></tr></table></figure>

<h2 id="4-Hive优化器的主要功能"><a href="#4-Hive优化器的主要功能" class="headerlink" title="4.Hive优化器的主要功能"></a>4.Hive优化器的主要功能</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A.将多Multiple Join合并为一个Muti-Way Join；</span><br><span class="line">B.对Join，group by和自定义的MapReduce操作重新进行划分；</span><br><span class="line">C.消减不必要的列；</span><br><span class="line">D.在表的扫描操作中推行使用断言；</span><br><span class="line">E.对于已分区的表，消减不必要的分区；</span><br><span class="line">F.在抽样查询中，消减不必要的桶；</span><br><span class="line">G.优化器还增加了局部聚合操作用于处理大分组聚合和增加再分区操作用于处理不对称的分组聚合。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h2 id="1-HBase的系统架构"><a href="#1-HBase的系统架构" class="headerlink" title="1.HBase的系统架构"></a>1.HBase的系统架构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.Client</span><br><span class="line">    包含了访问HBase的接口，Client维护着一些cache来加快对HBase的访问，比如Region的位置信息。</span><br><span class="line">2.Zookeeper</span><br><span class="line">    A.保证任何时候，集群中只有一个Master；</span><br><span class="line">    B.存储所有Region的寻址入口，Root表在哪台服务器上；</span><br><span class="line">    C.实时监控Region Server的状态，将Region Server的上线和下线信息实时通知给Master；</span><br><span class="line">    D.存储HBase的Schema，包括有哪些table，每个table有哪些column family。</span><br><span class="line">3.Master</span><br><span class="line">    A.为Region Server分配Region；</span><br><span class="line">    B.负责Region Server的负载均衡；</span><br><span class="line">    C.发现失效的Region Server并重新分配其上的Region；</span><br><span class="line">    D.HDFS上的垃圾文件回收；</span><br><span class="line">    E.处理schema更新请求。</span><br><span class="line">4.Region Server</span><br><span class="line">    A.Region Server维护Master分配给它的Region，处理对这些Region的IO请求；</span><br><span class="line">    B.Region Server负责切分在运行过程中变得过大的Region；</span><br><span class="line">    C.Client访问HBase上数据的过程并不需要Master参与(寻址访问Zookeeper和Region Server，数据读写访问Region Server)</span><br><span class="line">        Master仅仅维护table和Region的元数据信息，负载很低。</span><br></pre></td></tr></table></figure>

<h2 id="2-HBase的存储结构"><a href="#2-HBase的存储结构" class="headerlink" title="2.HBase的存储结构"></a>2.HBase的存储结构</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.基本结构</span><br><span class="line">    A.Table中的所有行都按照rowKey的字典排列；</span><br><span class="line">    B.Table在行的方向上分割为多个Region；</span><br><span class="line">    C.Region按大小分割，每个表一开始只有一个Region，随着数据的不断插入，Region不断增大</span><br><span class="line">        当增大到一个阈值的时候，Region就会等分成两个新的Region，当table中的行数不断增多，就会有越来越多的Region；</span><br><span class="line">    D.Region是HBase中分布式存储和负载均衡的最小单元。</span><br><span class="line">        最小单元表示不同的Region可以分布在不同的Region Server上，但是一个Region是不会拆分到不同的Region Server上的；</span><br><span class="line">    E.Region虽然是分布式存储的最小单元，但并不是物理存储的最小单元。</span><br><span class="line">2.MemStore与StoreFile</span><br><span class="line">    A.Region由一个或者多个Store组成，每个Store保存一个column family；</span><br><span class="line">    B.每个Store由一个MemStore和0至多个StoreFile组成；</span><br><span class="line">    C.写操作先写入MemStore，当数据量达到一个阈值，Region Server启动flashcache进程写入StoreFile；</span><br><span class="line">    D.当Store大小超过一定阈值，会把当前的Region分割成两个，并由Master分配给相应的Region Server（先下线Region，切割好之后加入Meta元信息，加入到原本的RegionServer中，最后汇报Master）；</span><br><span class="line">    E.客户端检索数据时，先在MemStore找，找不到再找StoreFile；</span><br><span class="line">    F.StoreFile以HFile的格式保存在HDFS上。</span><br><span class="line">3.HFile</span><br><span class="line">    A.Data Block段：保存表中的数据，可被压缩；</span><br><span class="line">    B.Meta Block段：保存用户自定义的kv对，可被压缩；</span><br><span class="line">    C.File Info段：HFile的元信息，不可压缩，用户也可以在这一部分添加自己的元信息；</span><br><span class="line">    D.Data Block Index段：Data Block的索引；</span><br><span class="line">    E.Meta Block Index段：Meta Block的索引；</span><br><span class="line">    F.Trailer段：保存每一段的偏移量，读取一个HFile时，会首先读取Trailer。</span><br><span class="line">4.HLog(（WAL Log）</span><br><span class="line">    A.类似MySQL的binlog，做灾难恢复，记录数据的所有变更；</span><br><span class="line">    B.每个Region Server维护一个HLog；</span><br><span class="line">    C.如果Region Server下线，需要对其上的HLog进行拆分，分发到其他Region Server进行恢复。</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h2 id="1-SparkSQL和SparkStreaming哪个比较熟"><a href="#1-SparkSQL和SparkStreaming哪个比较熟" class="headerlink" title="1.SparkSQL和SparkStreaming哪个比较熟?"></a>1.SparkSQL和SparkStreaming哪个比较熟?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">都还行，SparkSQL的DataFrame或者DataSet和SparkStreaming的DStream都是基于SparkCore的;</span><br><span class="line">最终都会转化为SparkTask执行;</span><br><span class="line">我们可以交流一下本质的东西SparkCore，而SparkCore的核心又是RDD。</span><br></pre></td></tr></table></figure>

<h2 id="2-说一下SparkShuffle"><a href="#2-说一下SparkShuffle" class="headerlink" title="2.说一下SparkShuffle"></a>2.说一下SparkShuffle</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Spark的shuffle也是一处理问题的思想：分而治之。</span><br><span class="line">Shuffle一般称为洗牌，一般会有Shuffle Write阶段和Shuffle Read阶段。</span><br><span class="line">在Spark中实现Shuffle的方式有两种，一种是HashShuffle(2.0弃用)，一种是SortShuffle。</span><br><span class="line">Shuffle的性能是影响Spark应用程序性能的关键。</span><br><span class="line">Shuffle发生在stage之间，stage中用的pipline的计算模式。</span><br><span class="line"></span><br><span class="line">决定Shuffle后数据属于哪个分区,主要由分区器决定(HashPartitioner,RangePartitioner)</span><br><span class="line">Hash: 由于Hash碰撞的存在,极端情况下可能出现数据倾斜</span><br><span class="line">Range: 划分范围,分区与分区之间是有序的,分区内不保证顺序</span><br></pre></td></tr></table></figure>

<h2 id="3-SparkShuffle的调优点"><a href="#3-SparkShuffle的调优点" class="headerlink" title="3.SparkShuffle的调优点?"></a>3.SparkShuffle的调优点?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Shuffle的选择</span><br><span class="line">缓冲区的大小</span><br><span class="line">拉取的数据量的大小</span><br><span class="line">间隔时间重试次数</span><br></pre></td></tr></table></figure>

<h2 id="4-缓存这块熟悉吗-介绍缓存级别"><a href="#4-缓存这块熟悉吗-介绍缓存级别" class="headerlink" title="4.缓存这块熟悉吗,介绍缓存级别"></a>4.缓存这块熟悉吗,介绍缓存级别</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Spark的缓存机制是Spark优化的一个重要点；</span><br><span class="line">它将需要重复使用或者共用的RDD缓存在内存中，可以提高Spark的性能。</span><br><span class="line">Spark的底层源码中使用StorageLevel来表示缓存机制，</span><br><span class="line">其中包括：</span><br><span class="line">    使用内存，使用磁盘，使用序列化，使用堆外内存</span><br><span class="line">在他的半生对象中基于这几种方式提供了一些实现：</span><br><span class="line">    不使用缓存，Memory_Only，Disk_only，OffHeap</span><br><span class="line">分别都有相应的序列化，副本，组合的实现提供选择。</span><br><span class="line">持久化的级别StorageLevel可以自定义，但是一般不自定义。</span><br><span class="line">如何选择RDD的缓存级别的本质是在内存的利用率和CPU的利用率之间的权衡。</span><br><span class="line">一般默认选择的是Memory_only, 其次是Memery_only_Ser, 再次是Memory_only_and_Dis</span><br><span class="line">至于怎么选择你得自己权衡。</span><br></pre></td></tr></table></figure>

<h2 id="5-说一下cache和checkpoint的区别"><a href="#5-说一下cache和checkpoint的区别" class="headerlink" title="5.说一下cache和checkpoint的区别"></a>5.说一下cache和checkpoint的区别</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">catche是将共用的或者重复使用的RDD按照持久化的级别进行缓存</span><br><span class="line">checkpoint是将业务场景非常长的逻辑计算的中间结果缓存到HDFS上</span><br><span class="line">ck的实现原理是:</span><br><span class="line">    首先找到stage最后的finalRDD，然后按照RDD的依赖关系进行回溯</span><br><span class="line">    找到使用了checkPoint的RDD</span><br><span class="line">    然后标记这个使用了checkPoint的RDD</span><br><span class="line">    重新的启动一个线程来将checkPoint之前的RDD缓存到HDFS上面</span><br><span class="line">    最后将RDD的依赖关系从checkPoint的位置切断</span><br></pre></td></tr></table></figure>

<h2 id="6-Spark运行模式local、local-和local-分别是什么"><a href="#6-Spark运行模式local、local-和local-分别是什么" class="headerlink" title="6.Spark运行模式local、local[]和local[*]分别是什么?"></a>6.Spark运行模式<code>local</code>、<code>local[]</code>和<code>local[*]</code>分别是什么?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算</span><br><span class="line">通常用来验证开发出来的应用程序逻辑上有没有问题</span><br><span class="line">其中N代表可以使用N个线程，每个线程拥有一个core。</span><br><span class="line">如果不指定N，则默认是1个线程（该线程有1个core）。</span><br><span class="line">如果是local[*]，则代表Run Spark locally with as many worker threads as logical cores on your machine(在本地运行Spark，与您的机器上的逻辑内核一样多的工作线程)。</span><br></pre></td></tr></table></figure>

<h2 id="7-Spark怎么设置垃圾回收机制"><a href="#7-Spark怎么设置垃圾回收机制" class="headerlink" title="7.Spark怎么设置垃圾回收机制?"></a>7.Spark怎么设置垃圾回收机制?</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Spark中各个角色的JVM参数设置:</span><br><span class="line">Driver的JVM参数</span><br><span class="line">    GC方式，如果是yarn-client模式，默认读取的是spark-class文件中的JAVAOPTS；</span><br><span class="line">    如果是yarn-cluster模式，则读取的是spark-default.conf文件中的spark.driver.extraJavaOptions对应的参数值。</span><br><span class="line">Executor的JVM参数</span><br><span class="line">    GC方式，两种模式都是读取的是spark-default.conf文件中的spark.executor.extraJavaOptions对应的JVM参数值。</span><br></pre></td></tr></table></figure>

<h2 id="8-一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？"><a href="#8-一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？" class="headerlink" title="8.一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？"></a>8.一台节点上以root用户执行一个spark程序，以其他非root用户也同时在执行一个spark程序，这时以spark用户登录，这个节点上，使用Jps会看到哪些线程？</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">单独的用户只能看自己的进程</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h2 id="1-Flink中TM内存管理"><a href="#1-Flink中TM内存管理" class="headerlink" title="1.Flink中TM内存管理"></a>1.Flink中TM内存管理</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Total Process Memory 总内存(设置多少就是多少)</span><br><span class="line">    Total Flink Memory 总Flink使用内存</span><br><span class="line">        JVM Heap 堆内内存</span><br><span class="line">            Framework Heap 框架堆内内存,默认128M</span><br><span class="line">            Task Heap 任务堆内内存,总Flink使用内存-框架堆内内存-管理内存-网络内存</span><br><span class="line">        Off-Heap Memory 堆外内存</span><br><span class="line">            Managed Memory 管理内存,总Flink使用内存的0.4</span><br><span class="line">            Direct Memory</span><br><span class="line">                Framework Off-Heap 框架堆外内存,默认128M</span><br><span class="line">                Task Off-Heap 任务的堆外内存,默认0</span><br><span class="line">                Network 网络缓存大小,总Flink使用内存的0.1</span><br><span class="line">    JVM Metaspace 默认占用256M</span><br><span class="line">    JVM Overhead  默认值1GB</span><br><span class="line"></span><br><span class="line">注意:JVM Metaspace和Overhead也属于堆外内存</span><br><span class="line"></span><br><span class="line">FrameWork Heap是Task Executor本身占用的堆内内存大小,不用于执行Task.</span><br><span class="line">FrameWork Off-Heap是Task Executor保留的堆外内存大小.</span><br><span class="line">Task Heap是专门用来执行Task的堆内内存.</span><br><span class="line">Task Off-Heap是Task的堆外内存,在Flink程序中有调用Native方法,可以配置.</span><br><span class="line">Managed Memory主要用于排序,哈希表和中间结果缓存,RocksDB的Backend.</span><br><span class="line">Network Memory用于Task之间的数据交换.</span><br><span class="line">JVM Metaspace是JVM的元数据空间大小</span><br><span class="line">JVM Overhead是保留给JVM其他内存开销,GC,ThreadStack,code cache等.</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库存储过程及触发器</title>
    <url>/2018/05/31/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%8F%8A%E8%A7%A6%E5%8F%91%E5%99%A8/</url>
    <content><![CDATA[<blockquote>
<p>存储过程和触发器</p>
</blockquote>
<span id="more"></span>

<h3 id="一-关键语法"><a href="#一-关键语法" class="headerlink" title="一.关键语法"></a>一.关键语法</h3><ul>
<li><strong>DELIMITER</strong> 声明语句结束符</li>
<li><strong>CREATE PROCEDURE</strong> 声明存储过程</li>
<li><strong>BEGIN … END</strong> 储存过程开始和结束符号</li>
<li><strong>SET</strong> 变量赋值</li>
<li><strong>DECLARE</strong> 变量定义</li>
<li><strong>DEFINER</strong> 存储过程的权限</li>
</ul>
<hr>
<h3 id="二-入门实例"><a href="#二-入门实例" class="headerlink" title="二.入门实例"></a>二.入门实例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER $$</span><br><span class="line">USE &#96;test&#96;$$</span><br><span class="line">DROP PROCEDURE IF EXISTS &#96;test&#96;$$</span><br><span class="line"></span><br><span class="line">CREATE DEFINER&#x3D;&#96;root&#96;@&#96;localhost&#96; PROCEDURE &#96;test&#96;()</span><br><span class="line">BEGIN</span><br><span class="line">SELECT COUNT(*) FROM &#96;test&#96;.&#96;info&#96;;</span><br><span class="line">END$$</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="三-存储过程使用"><a href="#三-存储过程使用" class="headerlink" title="三.存储过程使用"></a>三.存储过程使用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CALL test(); 调用存储过程</span><br><span class="line">SET @ip&#x3D;1; 存储过程外定义变量</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="四-参数"><a href="#四-参数" class="headerlink" title="四.参数"></a>四.参数</h3><ul>
<li><strong>IN</strong> 输入参数,表示该参数的值必须在调用存储过程时指定，在存储过程中修改该参数的值不能被返回，为默认值</li>
<li><strong>OUT</strong> 输出参数,该值可在存储过程内部被改变，并可返回</li>
<li><strong>INOUT</strong> 输入输出参数,调用时指定，并且可被改变和返回</li>
</ul>
<h4 id="1-IN参数实例"><a href="#1-IN参数实例" class="headerlink" title="1.IN参数实例"></a>1.IN参数实例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set @ip&#x3D;1;</span><br><span class="line"></span><br><span class="line">DELIMITER $$</span><br><span class="line">USE &#96;test&#96;$$</span><br><span class="line">DROP PROCEDURE IF EXISTS &#96;test&#96;$$</span><br><span class="line"></span><br><span class="line">CREATE DEFINER&#x3D;&#96;root&#96;@&#96;localhost&#96; PROCEDURE &#96;test&#96;(IN ip INT)</span><br><span class="line">BEGIN</span><br><span class="line">SELECT ip; &#x2F;&#x2F; 1</span><br><span class="line">SET ip&#x3D;10;</span><br><span class="line">SELECT ip; &#x2F;&#x2F; 10</span><br><span class="line">END$$</span><br><span class="line"></span><br><span class="line">DELIMITER ;</span><br><span class="line"></span><br><span class="line">CALL test(@ip);</span><br><span class="line"></span><br><span class="line">select @ip; &#x2F;&#x2F; 1</span><br></pre></td></tr></table></figure>
<p>外影响内,内并不影响外的值</p>
<h4 id="2-OUT参数实例"><a href="#2-OUT参数实例" class="headerlink" title="2.OUT参数实例"></a>2.OUT参数实例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set @ip&#x3D;1;</span><br><span class="line"></span><br><span class="line">DELIMITER $$</span><br><span class="line">USE &#96;test&#96;$$</span><br><span class="line">DROP PROCEDURE IF EXISTS &#96;test&#96;$$</span><br><span class="line"></span><br><span class="line">CREATE DEFINER&#x3D;&#96;root&#96;@&#96;localhost&#96; PROCEDURE &#96;test&#96;(OUT ip INT)</span><br><span class="line">BEGIN</span><br><span class="line">SELECT ip; &#x2F;&#x2F; NULL</span><br><span class="line">SET ip&#x3D;10;</span><br><span class="line">SELECT ip; &#x2F;&#x2F; 10</span><br><span class="line">END$$</span><br><span class="line"></span><br><span class="line">DELIMITER ;</span><br><span class="line"></span><br><span class="line">CALL test(@ip);</span><br><span class="line"></span><br><span class="line">select @ip; &#x2F;&#x2F; 10</span><br></pre></td></tr></table></figure>
<p>存储过程内的会影响外部的变量,但是外部的不会影响存储过程内的变量</p>
<h4 id="3-INOUT参数实例"><a href="#3-INOUT参数实例" class="headerlink" title="3.INOUT参数实例"></a>3.INOUT参数实例</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set @ip&#x3D;1;</span><br><span class="line"></span><br><span class="line">DELIMITER $$</span><br><span class="line">USE &#96;test&#96;$$</span><br><span class="line">DROP PROCEDURE IF EXISTS &#96;test&#96;$$</span><br><span class="line"></span><br><span class="line">CREATE DEFINER&#x3D;&#96;root&#96;@&#96;localhost&#96; PROCEDURE &#96;test&#96;(INOUT ip INT)</span><br><span class="line">BEGIN</span><br><span class="line">SELECT ip; &#x2F;&#x2F; 1</span><br><span class="line">SET ip&#x3D;10;</span><br><span class="line">SELECT ip; &#x2F;&#x2F; 10</span><br><span class="line">END$$</span><br><span class="line"></span><br><span class="line">DELIMITER ;</span><br><span class="line"></span><br><span class="line">CALL test(@ip);</span><br><span class="line"></span><br><span class="line">select @ip; &#x2F;&#x2F; 10</span><br></pre></td></tr></table></figure>
<p>内外都能影响</p>
<hr>
<h3 id="五-变量"><a href="#五-变量" class="headerlink" title="五.变量"></a>五.变量</h3><h4 id="1-变量定义"><a href="#1-变量定义" class="headerlink" title="1.变量定义"></a>1.变量定义</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">局部变量声明一定要放在存储过程体的开始</span><br><span class="line">DECLARE l_int int unsigned default 4000000;</span><br><span class="line">DECLARE l_numeric number(8,2) DEFAULT 9.95;</span><br><span class="line">DECLARE l_date date DEFAULT &#39;1999-12-31&#39;;  </span><br><span class="line">DECLARE l_datetime datetime DEFAULT &#39;1999-12-31 23:59:59&#39;;  </span><br><span class="line">DECLARE l_varchar varchar(255) DEFAULT &#39;This will not be padded&#39;;</span><br></pre></td></tr></table></figure>

<h4 id="2-变量赋值"><a href="#2-变量赋值" class="headerlink" title="2.变量赋值"></a>2.变量赋值</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set ip &#x3D; value;</span><br></pre></td></tr></table></figure>

<h4 id="3-用户变量"><a href="#3-用户变量" class="headerlink" title="3.用户变量"></a>3.用户变量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT &#39;Hello World&#39; into @x;</span><br><span class="line">SELECT @x;</span><br><span class="line">SET @y&#x3D;&#39;Goodbye Cruel World&#39;;</span><br><span class="line">SELECT @y;</span><br><span class="line">SET @z&#x3D;1+2+3;</span><br><span class="line">SELECT @z;</span><br><span class="line">&#x2F;&#x2F; 存储过程中使用用户变量</span><br><span class="line">CREATE PROCEDURE GreetWorld() SELECT CONCAT(@greeting,&#39; World&#39;);</span><br><span class="line">SET @greeting&#x3D;&#39;Hello&#39;;</span><br><span class="line">CALL GreetWorld();  </span><br><span class="line">&#x2F;&#x2F; 存储过程间传递全局范围的用户变量</span><br><span class="line">CREATE PROCEDURE p1() SET @last_procedure&#x3D;&#39;p1&#39;;</span><br><span class="line">CREATE PROCEDURE p2() SELECT CONCAT(&#39;Last procedure was &#39;,@last_procedure);</span><br><span class="line">CALL p1();</span><br><span class="line">CALL p2();</span><br></pre></td></tr></table></figure>
<p>注意:</p>
<ul>
<li>用户变量名一般以@开头</li>
<li>滥用用户变量会导致程序难以理解及管理</li>
</ul>
<hr>
<h3 id="六-注释"><a href="#六-注释" class="headerlink" title="六.注释"></a>六.注释</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER $$</span><br><span class="line">USE &#96;test&#96;$$</span><br><span class="line">DROP PROCEDURE IF EXISTS &#96;test&#96;$$</span><br><span class="line"></span><br><span class="line">CREATE DEFINER&#x3D;&#96;root&#96;@&#96;localhost&#96; PROCEDURE &#96;test&#96;(INOUT ip INT) -- 创建存储过程</span><br><span class="line">BEGIN</span><br><span class="line">SELECT ip; -- 输出 1</span><br><span class="line">&#x2F;*</span><br><span class="line">SET ip&#x3D;10;</span><br><span class="line">SELECT ip; -- 输出 10</span><br><span class="line">*&#x2F;</span><br><span class="line">END$$</span><br><span class="line"></span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="七-存储过程的增删改查"><a href="#七-存储过程的增删改查" class="headerlink" title="七.存储过程的增删改查"></a>七.存储过程的增删改查</h3><h4 id="1-创建"><a href="#1-创建" class="headerlink" title="1.创建"></a>1.创建</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE DEFINER&#x3D;&#96;root&#96;@&#96;localhost&#96; PROCEDURE 存储过程名;</span><br></pre></td></tr></table></figure>
<h4 id="2-删除"><a href="#2-删除" class="headerlink" title="2.删除"></a>2.删除</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DROP PROCEDURE 存储过程名;</span><br></pre></td></tr></table></figure>
<h4 id="3-修改"><a href="#3-修改" class="headerlink" title="3.修改"></a>3.修改</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER PROCEDURE 存储过程名</span><br></pre></td></tr></table></figure>
<h4 id="4-查看"><a href="#4-查看" class="headerlink" title="4.查看"></a>4.查看</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT NAME FROM mysql.proc WHERE db&#x3D;&#39;数据库名&#39;;</span><br><span class="line">SELECT routine_name FROM information_schema.routines WHERE routine_schema&#x3D;&#39;数据库名&#39;;</span><br><span class="line">SHOW PROCEDURE STATUS WHERE db&#x3D;&#39;数据库名&#39;;</span><br><span class="line">SHOW CREATE PROCEDURE 数据库.存储过程名;</span><br></pre></td></tr></table></figure>

<h3 id="八-存储过程的控制语句"><a href="#八-存储过程的控制语句" class="headerlink" title="八.存储过程的控制语句"></a>八.存储过程的控制语句</h3><h4 id="1-变量的作用域"><a href="#1-变量的作用域" class="headerlink" title="1.变量的作用域"></a>1.变量的作用域</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">CREATE PROCEDURE proc3(OUT x1 VARCHAR(5))  </span><br><span class="line">BEGIN </span><br><span class="line">DECLARE x1 VARCHAR(5) DEFAULT &#39;outer&#39;;  </span><br><span class="line">BEGIN </span><br><span class="line">DECLARE x1 VARCHAR(5) DEFAULT &#39;inner&#39;;  </span><br><span class="line">SELECT x1; -- inner</span><br><span class="line">END;  </span><br><span class="line">SELECT x1; -- outer</span><br><span class="line">END;  </span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">DELIMITER ;  </span><br></pre></td></tr></table></figure>
<p>只在自已的定义域中有效,出了定义域无效,可以通过out参数或者将其值指派给会话变量来保存其值。</p>
<h4 id="2-if-then-else语句"><a href="#2-if-then-else语句" class="headerlink" title="2.if -then -else语句"></a>2.if -then -else语句</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">CREATE PROCEDURE proc2(IN parameter INT)  </span><br><span class="line">BEGIN </span><br><span class="line">DECLARE var INT;  </span><br><span class="line">SET var&#x3D;parameter+1;  </span><br><span class="line"></span><br><span class="line">IF var&#x3D;0 THEN </span><br><span class="line">    INSERT INTO t VALUES(17);  </span><br><span class="line">END IF;  </span><br><span class="line"></span><br><span class="line">IF parameter&#x3D;0 THEN </span><br><span class="line">    UPDATE t SET s1&#x3D;s1+1;  </span><br><span class="line">ELSE </span><br><span class="line">    UPDATE t SET s1&#x3D;s1+2;  </span><br><span class="line">END IF;  </span><br><span class="line">END;</span><br><span class="line">&#x2F;&#x2F;  </span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>
<h4 id="3-case语句"><a href="#3-case语句" class="headerlink" title="3.case语句"></a>3.case语句</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">CREATE PROCEDURE proc3 (IN parameter INT)  </span><br><span class="line">BEGIN </span><br><span class="line">DECLARE var INT;  </span><br><span class="line">SET var&#x3D;parameter+1;  </span><br><span class="line">CASE var  </span><br><span class="line">WHEN 0 THEN   </span><br><span class="line">    INSERT INTO t VALUES(17);  </span><br><span class="line">WHEN 1 THEN   </span><br><span class="line">    INSERT INTO t VALUES(18);  </span><br><span class="line">ELSE   </span><br><span class="line">    INSERT INTO t VALUES(19);  </span><br><span class="line">END CASE;  </span><br><span class="line">END;  </span><br><span class="line">&#x2F;&#x2F;  </span><br><span class="line">DELIMITER ; </span><br></pre></td></tr></table></figure>
<h4 id="4-while-条件-DO-…-end-while-语句"><a href="#4-while-条件-DO-…-end-while-语句" class="headerlink" title="4.while 条件 DO … end while;语句"></a>4.while 条件 DO … end while;语句</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">CREATE PROCEDURE proc4()  </span><br><span class="line">BEGIN </span><br><span class="line">DECLARE var INT;  </span><br><span class="line">SET var&#x3D;0;  </span><br><span class="line">WHILE var&lt;6 DO  </span><br><span class="line">    INSERT INTO t VALUES(var);  </span><br><span class="line">    SET var&#x3D;var+1;  </span><br><span class="line">END WHILE;</span><br><span class="line">END;  </span><br><span class="line">&#x2F;&#x2F;  </span><br><span class="line">DELIMITER ; </span><br></pre></td></tr></table></figure>
<p>在执行循环体之前进行判断</p>
<h4 id="5-repeat-…until-条件-end-repeat-语句"><a href="#5-repeat-…until-条件-end-repeat-语句" class="headerlink" title="5.repeat …until 条件 end repeat;语句"></a>5.repeat …until 条件 end repeat;语句</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">CREATE PROCEDURE proc5 ()  </span><br><span class="line">BEGIN   </span><br><span class="line">DECLARE v INT;  </span><br><span class="line">SET v&#x3D;0;  </span><br><span class="line">REPEAT  </span><br><span class="line">    INSERT INTO t VALUES(v);  </span><br><span class="line">    SET v&#x3D;v+1;  </span><br><span class="line">UNTIL v&gt;&#x3D;5</span><br><span class="line">END REPEAT;  </span><br><span class="line">END;  </span><br><span class="line">&#x2F;&#x2F;  </span><br><span class="line">DELIMITER ;  </span><br></pre></td></tr></table></figure>
<p>在执行循环体之后进行判断</p>
<h4 id="6-loop-…-leave-end-loop-语句"><a href="#6-loop-…-leave-end-loop-语句" class="headerlink" title="6.loop … leave end loop;语句"></a>6.loop … leave end loop;语句</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">CREATE PROCEDURE proc6 ()  </span><br><span class="line">BEGIN </span><br><span class="line">DECLARE v INT;  </span><br><span class="line">SET v&#x3D;0;  </span><br><span class="line">LOOP_LABLE:LOOP  </span><br><span class="line">    INSERT INTO t VALUES(v);  </span><br><span class="line">    SET v&#x3D;v+1;  </span><br><span class="line">    IF v &gt;&#x3D;5 THEN </span><br><span class="line">        LEAVE LOOP_LABLE;  </span><br><span class="line">    END IF;  </span><br><span class="line">END LOOP;  </span><br><span class="line">END;  </span><br><span class="line">&#x2F;&#x2F;  </span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>
<p>leave语句的意义是离开循环,LOOP_LABEL是自定义的标号</p>
<h4 id="7-LABLES-标号"><a href="#7-LABLES-标号" class="headerlink" title="7.LABLES 标号:"></a>7.LABLES 标号:</h4><p>在使用loop的时候，使用到的labels标号，对于labels可以用到while，loop，rrepeat等循环控制语句中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create procedure pro13()</span><br><span class="line">label_1:begin</span><br><span class="line">label_2:while 0&#x3D;1 do leave label_2;end while;</span><br><span class="line">label_3:repeat leave label_3;until 0&#x3D;0 end repeat;</span><br><span class="line">label_4:loop leave label_4;end loop;</span><br><span class="line">end;</span><br><span class="line"></span><br><span class="line">-- 加了结束标号的语句,功能相同</span><br><span class="line">create procedure pro14()</span><br><span class="line">label_1:begin</span><br><span class="line">label_2:while 0&#x3D;1 do leave label_2;end while label_2;</span><br><span class="line">label_3:repeat leave label_3;until 0&#x3D;0 end repeat label_3;</span><br><span class="line">label_4:loop leave label_4;end loop label_4;</span><br><span class="line">end label_1;</span><br></pre></td></tr></table></figure>

<h4 id="8-ITERATE迭代"><a href="#8-ITERATE迭代" class="headerlink" title="8.ITERATE迭代"></a>8.ITERATE迭代</h4><p>ITERATE只能出现在LOOP，REPEAT和WHILE语句中，它的意思是“再次循环”,类似于continue</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">CREATE PROCEDURE proc11()  </span><br><span class="line">BEGIN </span><br><span class="line">DECLARE v INT;  </span><br><span class="line">SET v&#x3D;0;  </span><br><span class="line">LOOP_LABLE:LOOP  </span><br><span class="line">    IF v&#x3D;3 THEN   </span><br><span class="line">        SET v&#x3D;v+1;  </span><br><span class="line">	    ITERATE LOOP_LABLE;  </span><br><span class="line">	END IF;  </span><br><span class="line">    INSERT INTO t VALUES(v);  </span><br><span class="line">    SET v&#x3D;v+1;  </span><br><span class="line">	IF v&gt;&#x3D;5 THEN </span><br><span class="line">	    LEAVE LOOP_LABLE;  </span><br><span class="line">    END IF;  </span><br><span class="line">END LOOP;  </span><br><span class="line">END;  </span><br><span class="line">&#x2F;&#x2F;  </span><br><span class="line">DELIMITER ; </span><br></pre></td></tr></table></figure>
<p>首先i的值为0，条件判断语句if i=3 then判断为假，跳过if语段，向数据库中插入0，然后i+1，同样后面的if i&gt;=5 then判断也为假，也跳过；继续循环，同样插入1和2；在i=3的时候条件判断语句if i=3 then判断为真，执行i=i+1，i值为4，然后执行迭代iterate loop_label;，即语句执行到iterate loop_label;后直接跳到if i=3 then判断语句，执行判断，这个时候由于i=4，if i=3 then判断为假，跳过IF语段，将4添加到表中，i变为5，条件判断if i&gt;=5 then判断为真，执行leave loop_label;跳出loop循环，然后执行end;//，结束整个存储过程。</p>
<h3 id="九-存储过程中的基本函数"><a href="#九-存储过程中的基本函数" class="headerlink" title="九.存储过程中的基本函数"></a>九.存储过程中的基本函数</h3><h4 id="1-字符串类"><a href="#1-字符串类" class="headerlink" title="1.字符串类"></a>1.字符串类</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CHARSET(str) &#x2F;&#x2F;返回字串字符集</span><br><span class="line">CONCAT (string2 [,... ]) &#x2F;&#x2F;连接字串</span><br><span class="line">INSTR (string ,substring ) &#x2F;&#x2F;返回substring首次在string中出现的位置,不存在返回0</span><br><span class="line">LCASE (string2 ) &#x2F;&#x2F;转换成小写</span><br><span class="line">LEFT (string2 ,length ) &#x2F;&#x2F;从string2中的左边起取length个字符</span><br><span class="line">LENGTH (string ) &#x2F;&#x2F;string长度</span><br><span class="line">LOAD_FILE (file_name ) &#x2F;&#x2F;从文件读取内容</span><br><span class="line">LOCATE (substring , string [,start_position ] ) 同INSTR,但可指定开始位置</span><br><span class="line">LPAD (string2 ,length ,pad ) &#x2F;&#x2F;重复用pad加在string开头,直到字串长度为length</span><br><span class="line">LTRIM (string2 ) &#x2F;&#x2F;去除前端空格</span><br><span class="line">REPEAT (string2 ,count ) &#x2F;&#x2F;重复count次</span><br><span class="line">REPLACE (str ,search_str ,replace_str ) &#x2F;&#x2F;在str中用replace_str替换search_str</span><br><span class="line">RPAD (string2 ,length ,pad) &#x2F;&#x2F;在str后用pad补充,直到长度为length</span><br><span class="line">RTRIM (string2 ) &#x2F;&#x2F;去除后端空格</span><br><span class="line">STRCMP (string1 ,string2 ) &#x2F;&#x2F;逐字符比较两字串大小,str1比str2小返回-1,大返回1,相同返回0</span><br><span class="line">SUBSTRING (str , position [,length ]) &#x2F;&#x2F;从str的position开始,取length个字符</span><br><span class="line">注：mysql中处理字符串时，默认第一个字符下标为1，即参数position必须大于等于1</span><br><span class="line">TRIM([[BOTH|LEADING|TRAILING][padding] FROM]string2) &#x2F;&#x2F;去除指定位置的指定字符</span><br><span class="line">UCASE (string2 ) &#x2F;&#x2F;转换成大写</span><br><span class="line">RIGHT(string2,length) &#x2F;&#x2F;取string2最后length个字符</span><br><span class="line">SPACE(count) &#x2F;&#x2F;生成count个空格</span><br></pre></td></tr></table></figure>
<h4 id="2-数学类"><a href="#2-数学类" class="headerlink" title="2.数学类"></a>2.数学类</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ABS (number2 ) &#x2F;&#x2F;绝对值</span><br><span class="line">BIN (decimal_number ) &#x2F;&#x2F;十进制转二进制</span><br><span class="line">CEILING (number2 ) &#x2F;&#x2F;向上取整</span><br><span class="line">CONV(number2,from_base,to_base) &#x2F;&#x2F;进制转换</span><br><span class="line">FLOOR (number2 ) &#x2F;&#x2F;向下取整</span><br><span class="line">FORMAT (number,decimal_places ) &#x2F;&#x2F;保留小数位数</span><br><span class="line">HEX (DecimalNumber ) &#x2F;&#x2F;转十六进制</span><br><span class="line">注：HEX()中可传入字符串，则返回其ASC-11码，如HEX(&#39;DEF&#39;)返回4142143</span><br><span class="line">也可以传入十进制整数，返回其十六进制编码，如HEX(25)返回19</span><br><span class="line">LEAST (number , number2 [,..]) &#x2F;&#x2F;求最小值</span><br><span class="line">MOD (numerator ,denominator ) &#x2F;&#x2F;求余</span><br><span class="line">POWER (number ,power ) &#x2F;&#x2F;求指数</span><br><span class="line">RAND([seed]) &#x2F;&#x2F;随机数</span><br><span class="line">ROUND (number [,decimals ]) &#x2F;&#x2F;四舍五入,decimals为小数位数]</span><br><span class="line">注：返回类型并非均为整数,默认为整数</span><br><span class="line">SQRT(number2) &#x2F;&#x2F;开平方</span><br></pre></td></tr></table></figure>
<h4 id="3-日期时间类"><a href="#3-日期时间类" class="headerlink" title="3.日期时间类"></a>3.日期时间类</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ADDTIME (date2 ,time_interval)&#x2F;&#x2F;将time_interval加到date2</span><br><span class="line">CONVERT_TZ (datetime2 ,fromTZ ,toTZ) &#x2F;&#x2F;转换时区</span><br><span class="line">CURRENT_DATE () &#x2F;&#x2F;当前日期</span><br><span class="line">CURRENT_TIME () &#x2F;&#x2F;当前时间</span><br><span class="line">CURRENT_TIMESTAMP () &#x2F;&#x2F;当前时间戳</span><br><span class="line">DATE (datetime) &#x2F;&#x2F;返回datetime的日期部分</span><br><span class="line">DATE_ADD (date2 , INTERVAL d_value d_type) &#x2F;&#x2F;在date2中加上日期或时间</span><br><span class="line">DATE_FORMAT (datetime ,FormatCodes) &#x2F;&#x2F;使用formatcodes格式显示datetime</span><br><span class="line">DATE_SUB (date2 , INTERVAL d_value d_type) &#x2F;&#x2F;在date2上减去一个时间</span><br><span class="line">DATEDIFF (date1 ,date2) &#x2F;&#x2F;两个日期差</span><br><span class="line">DAY (date) &#x2F;&#x2F;返回日期的天</span><br><span class="line">DAYNAME (date) &#x2F;&#x2F;英文星期</span><br><span class="line">DAYOFWEEK (date) &#x2F;&#x2F;星期(1-7) ,1为星期天</span><br><span class="line">DAYOFYEAR (date) &#x2F;&#x2F;一年中的第几天</span><br><span class="line">EXTRACT (interval_name FROM date) &#x2F;&#x2F;从date中提取日期的指定部分</span><br><span class="line">MAKEDATE (year ,day) &#x2F;&#x2F;给出年及年中的第几天,生成日期串</span><br><span class="line">MAKETIME (hour ,minute ,second) &#x2F;&#x2F;生成时间串</span><br><span class="line">MONTHNAME (date ) &#x2F;&#x2F;英文月份名</span><br><span class="line">NOW () &#x2F;&#x2F;当前时间</span><br><span class="line">SEC_TO_TIME (seconds) &#x2F;&#x2F;秒数转成时间</span><br><span class="line">STR_TO_DATE (string ,format) &#x2F;&#x2F;字串转成时间,以format格式显示</span><br><span class="line">TIMEDIFF (datetime1 ,datetime2) &#x2F;&#x2F;两个时间差</span><br><span class="line">TIME_TO_SEC (time) &#x2F;&#x2F;时间转秒数]</span><br><span class="line">WEEK (date_time [,start_of_week ]) &#x2F;&#x2F;第几周</span><br><span class="line">YEAR (datetime) &#x2F;&#x2F;年份</span><br><span class="line">DAYOFMONTH(datetime) &#x2F;&#x2F;月的第几天</span><br><span class="line">HOUR(datetime) &#x2F;&#x2F;小时</span><br><span class="line">LAST_DAY(date) &#x2F;&#x2F;date的月的最后日期</span><br><span class="line">MICROSECOND(datetime) &#x2F;&#x2F;微秒</span><br><span class="line">MONTH(datetime) &#x2F;&#x2F;月</span><br><span class="line">MINUTE(datetime) &#x2F;&#x2F;分返回符号,正负或0</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="十-分页存储过程"><a href="#十-分页存储过程" class="headerlink" title="十.分页存储过程"></a>十.分页存储过程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER &#x2F;&#x2F;  </span><br><span class="line">DROP PROCEDURE IF EXISTS pr_pager;</span><br><span class="line">CREATE PROCEDURE pr_pager(</span><br><span class="line">    IN p_table_name VARCHAR(1024),      -- 表名      </span><br><span class="line">    IN p_fields VARCHAR(1024),          -- 列名</span><br><span class="line">    IN p_page_size INT,                 -- 一页的大小</span><br><span class="line">    IN p_page_now INT,                  -- 页码</span><br><span class="line">    IN p_order_string VARCHAR(128),     -- 排序语句</span><br><span class="line">    IN p_where_string VARCHAR(1024),    -- where条件</span><br><span class="line">    OUT p_out_rows INT                    </span><br><span class="line">)</span><br><span class="line">NOT DETERMINISTIC</span><br><span class="line">SQL SECURITY DEFINER</span><br><span class="line">COMMENT &#39;分页存储过程&#39;</span><br><span class="line"></span><br><span class="line">BEGIN</span><br><span class="line">  </span><br><span class="line">DECLARE m_begin_row INT DEFAULT 0;</span><br><span class="line">DECLARE m_limit_string CHAR(64);</span><br><span class="line"></span><br><span class="line">SET m_begin_row &#x3D; (p_page_now - 1) * p_page_size;   -- 开始行&#x3D;(页面-1)*一页的大小</span><br><span class="line">SET m_limit_string &#x3D; CONCAT(&#39; LIMIT &#39;, m_begin_row, &#39;, &#39;, p_page_size);   -- 分页拼接 LIMIT 开始行,一页的大小</span><br><span class="line"></span><br><span class="line">-- SELECT COUNT(*) INTO @ROWS_TOTAL FROM 表名 where条件); 将总行数赋值给ROWS_TOTAL,作为返回值</span><br><span class="line">SET @COUNT_STRING &#x3D; CONCAT(&#39;SELECT COUNT(*) INTO @ROWS_TOTAL FROM &#39;, p_table_name, &#39; &#39;, p_where_string);</span><br><span class="line">-- SELECT 列名 FROM 表名 where条件 order排序 limit语句; 最后的输出页数据</span><br><span class="line">SET @MAIN_STRING &#x3D; CONCAT(&#39;SELECT &#39;, p_fields, &#39; FROM &#39;, p_table_name, &#39; &#39;, p_where_string, &#39; &#39;, p_order_string,m_limit_string);</span><br><span class="line"></span><br><span class="line">PREPARE count_stmt FROM @COUNT_STRING;  -- 预定义sql</span><br><span class="line">EXECUTE count_stmt;                     -- 执行sql    </span><br><span class="line">DEALLOCATE PREPARE count_stmt;          -- 释放掉连接</span><br><span class="line">SET p_out_rows &#x3D; @ROWS_TOTAL;</span><br><span class="line"></span><br><span class="line">PREPARE main_stmt FROM @MAIN_STRING;</span><br><span class="line">EXECUTE main_stmt;</span><br><span class="line">DEALLOCATE PREPARE main_stmt;</span><br><span class="line">    </span><br><span class="line">END;</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line">DELIMITER ;</span><br><span class="line"></span><br><span class="line">call pr_pager(&quot;t&quot;,&quot;var&quot;,3,3,&quot;&quot;,&quot;&quot;,@result);</span><br><span class="line">call pr_pager(&quot;t&quot;,&quot;var&quot;,3,2,&quot;&quot;,&quot;&quot;,@result);</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="十一-存储过程的优点"><a href="#十一-存储过程的优点" class="headerlink" title="十一.存储过程的优点"></a>十一.存储过程的优点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.存储过程只在创造时进行编译，以后每次执行存储过程都不需再重新编译，而一般 SQL 语句每执行一次就编译一次,所以使用存储过程可提高数据库执行速度。</span><br><span class="line">2.当对数据库进行复杂操作时(如对多个表进行 Update,Insert,Query,Delete 时），可将此复杂操作用存储过程封装起来与数据库提供的事务处理结合一起使用。这些操作，如果用程序来完成，就变成了一条条的 SQL 语句，可能要多次连接数据库。而换成存储，只需要连接一次数据库就可以了。</span><br><span class="line">3.存储过程可以重复使用,可减少数据库开发人员的工作量。</span><br><span class="line">4.安全性高,可设定只有某此用户才具有对指定存储过程的使用权。</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="补充-触发器"><a href="#补充-触发器" class="headerlink" title="补充:触发器"></a>补充:触发器</h3><h4 id="1-语法"><a href="#1-语法" class="headerlink" title="1.语法"></a>1.语法</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TRIGGER 触发器名</span><br><span class="line">触发时机(BEFORE,AFTER)</span><br><span class="line">触发事件(INSERT,UPDATE,DELETE) ON 被触发的表名</span><br><span class="line">FOR EACH ROW</span><br><span class="line">触发器程序体</span><br><span class="line"></span><br><span class="line">由此可见，可以建立6种触发器，即：BEFORE INSERT、BEFORE UPDATE、BEFORE DELETE、AFTER INSERT、AFTER UPDATE、AFTER DELETE。</span><br><span class="line"></span><br><span class="line">另外有一个限制是不能同时在一个表上建立2个相同类型的触发器，因此在一个表上最多建立6个触发器。</span><br></pre></td></tr></table></figure>

<h3 id="2-触发事件"><a href="#2-触发事件" class="headerlink" title="2.触发事件"></a>2.触发事件</h3><blockquote>
<ul>
<li>MySQL除了对<strong>INSERT</strong>、<strong>UPDATE</strong>、<strong>DELETE</strong>基本操作进行定义外，还定义了<strong>LOAD DATA</strong>和<strong>REPLACE</strong>语句，这两种语句也能引起上述6中类型的触发器的触发。</li>
<li><strong>LOAD DATA</strong>语句用于将一个文件装入到一个数据表中，相当与一系列的 INSERT 操作。</li>
<li><strong>REPLACE</strong>语句一般来说和 INSERT 语句很像，只是在表中有<strong>primary key **或</strong>unique<strong>索引时，如果插入的数据和原来</strong>primary key<strong>或</strong>unique**索引一致时，会先删除原来的数据，然后增加一条新数据，也就是说，一条 REPLACE 语句有时候等价于一条。</li>
<li><strong>INSERT</strong>语句，有时候等价于一条<strong>DELETE</strong>语句加上一条<strong>INSERT</strong>语句。</li>
<li><strong>INSERT型触发器</strong>：插入某一行时激活触发器，可能通过<strong>INSERT</strong>、<strong>LOAD DATA</strong>、<strong>REPLACE</strong>语句触发；</li>
<li><strong>UPDATE型触发器</strong>：更改某一行时激活触发器，可能通过<strong>UPDATE</strong>语句触发；</li>
<li><strong>DELETE型触发器</strong>：删除某一行时激活触发器，可能通过<strong>DELETE</strong>、<strong>REPLACE</strong>语句触发。</li>
</ul>
</blockquote>
<h3 id="3-触发器程序体"><a href="#3-触发器程序体" class="headerlink" title="3.触发器程序体"></a>3.触发器程序体</h3><blockquote>
<p>statement_list代表一个或多个语句的列表，列表内的每条语句都必须用分号（;）来结尾。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BEGIN</span><br><span class="line">[statement_list]</span><br><span class="line">END</span><br></pre></td></tr></table></figure>

<h3 id="4-触发器实例"><a href="#4-触发器实例" class="headerlink" title="4.触发器实例"></a>4.触发器实例</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELIMITER $</span><br><span class="line">-- 创建一个在对student表进行INSERT事件之后进行一系列操作的触发器</span><br><span class="line">create trigger tri_stuInsert after insert</span><br><span class="line">on student for each row</span><br><span class="line">begin</span><br><span class="line">declare c int;</span><br><span class="line">-- 查询原来class表的stuCount</span><br><span class="line">set c &#x3D; (select stuCount from class where classID&#x3D;new.classID);</span><br><span class="line">-- 将stuCount+1</span><br><span class="line">update class set stuCount &#x3D; c + 1 where classID &#x3D; new.classID;</span><br><span class="line">end</span><br><span class="line">$</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>

<h3 id="5-NEW和OLD详解"><a href="#5-NEW和OLD详解" class="headerlink" title="5.NEW和OLD详解"></a>5.NEW和OLD详解</h3><blockquote>
<p>MySQL中定义了<strong>NEW</strong>和<strong>OLD</strong>，用来表示触发器的所在表中，触发了触发器的那一行数据。</p>
<ul>
<li>在<strong>INSERT</strong>型触发器中，<strong>NEW</strong>用来表示将要（<strong>BEFORE</strong>）或已经（<strong>AFTER</strong>）插入的新数据；</li>
<li>在<strong>UPDATE</strong>型触发器中，<strong>OLD</strong>用来表示将要或已经被修改的原数据，<strong>NEW</strong>用来表示将要或已经修改为的新数据；</li>
<li>在<strong>DELETE</strong>型触发器中，<strong>OLD</strong>用来表示将要或已经被删除的原数据；</li>
</ul>
<p><strong>使用方法</strong>： <strong>NEW.columnName</strong>（columnName为相应数据表某一列名）</p>
<p>另外，<strong>OLD</strong>是只读的，而<strong>NEW</strong>则可以在触发器中使用SET赋值，这样不会再次触发触发器，造成循环调用（如每插入一个学生前，都在其学号前加“2013”）。</p>
</blockquote>
<h3 id="6-触发器的查找与删除"><a href="#6-触发器的查找与删除" class="headerlink" title="6.触发器的查找与删除"></a>6.触发器的查找与删除</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-- 查找</span><br><span class="line">SHOW TRIGGERS [FROM schema_name]</span><br><span class="line">SELETE TRIGGER_SCHEMA,TRIGGER_NAME,DEFINER,ACTION_STATEMENT FROM TRIGGERS WHERE TRIGGER_SCHEMA&#x3D;&#39;course&#39;;</span><br><span class="line"></span><br><span class="line">-- 删除</span><br><span class="line">DROP TRIGGER [IF EXISTS] [schema_name.]trigger_name</span><br></pre></td></tr></table></figure>

<h3 id="7-触发器的执行顺序"><a href="#7-触发器的执行顺序" class="headerlink" title="7.触发器的执行顺序"></a>7.触发器的执行顺序</h3><blockquote>
<p>建立的数据库一般都是<strong>InnoDB</strong>数据库，其上建立的表是事务性表，也就是事务安全的。这时，若SQL语句或触发器执行失败，MySQL会回滚事务，有：</p>
<ul>
<li>如果<strong>BEFORE</strong>触发器执行失败，SQL无法正确执行。</li>
<li>SQL执行失败时，<strong>AFTER</strong>型触发器不会触发。</li>
<li><strong>AFTER</strong>类型的触发器执行失败，SQL会回滚。</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>简单的FlinkTopN的操作源码操作</title>
    <url>/2021/05/17/%E7%AE%80%E5%8D%95%E7%9A%84FlinkTopN%E7%9A%84%E6%93%8D%E4%BD%9C%E6%BA%90%E7%A0%81%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<blockquote>
<p>和女票争论了一下,一开始我认为使用ROW_NUMBER计算出TOPN写入MySQL中只会进行更新操作,结果还进行了删除操作,最终结果表中只保留RK条记录</p>
</blockquote>
<span id="more"></span>

<h2 id="前置Demo"><a href="#前置Demo" class="headerlink" title="前置Demo"></a>前置Demo</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- mysql上建立结果表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`users`</span> (</span><br><span class="line">  <span class="string">`user_id`</span> <span class="built_in">bigint</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`page_id`</span> <span class="built_in">bigint</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`num`</span> <span class="built_in">bigint</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`update_at`</span> <span class="built_in">timestamp</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">CURRENT_TIMESTAMP</span> <span class="keyword">ON</span> <span class="keyword">UPDATE</span> <span class="keyword">CURRENT_TIMESTAMP</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`user_id`</span>,<span class="string">`page_id`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8mb4 <span class="keyword">COLLATE</span>=utf8mb4_0900_ai_ci;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- flinksql</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pageviews (</span><br><span class="line">   user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">   page_id <span class="built_in">BIGINT</span>,</span><br><span class="line">   view_time <span class="built_in">TIMESTAMP</span>(<span class="number">3</span>),</span><br><span class="line">   proctime <span class="keyword">AS</span> PROCTIME()</span><br><span class="line"> ) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;kafka&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;topic&#x27;</span> = <span class="string">&#x27;pageviews&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;properties.bootstrap.servers&#x27;</span> = <span class="string">&#x27;localhost:9092&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;scan.startup.mode&#x27;</span> = <span class="string">&#x27;earliest-offset&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;format&#x27;</span> = <span class="string">&#x27;json&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">users</span> (</span><br><span class="line">  user_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  page_id <span class="built_in">BIGINT</span>,</span><br><span class="line">  <span class="keyword">num</span> <span class="built_in">BIGINT</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (user_id,page_id) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">&#x27;connector&#x27;</span> = <span class="string">&#x27;jdbc&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;url&#x27;</span> = <span class="string">&#x27;jdbc:mysql://localhost:3306/flink&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;table-name&#x27;</span> = <span class="string">&#x27;users&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;username&#x27;</span> = <span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;password&#x27;</span> = <span class="string">&#x27;123456&#x27;</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">users</span></span><br><span class="line"><span class="keyword">select</span> user_id,page_id,<span class="keyword">num</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> user_id,page_id,<span class="keyword">num</span>,row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">num</span> <span class="keyword">desc</span>) rk</span><br><span class="line">    <span class="keyword">from</span> (</span><br><span class="line">        <span class="keyword">select</span> user_id,page_id,<span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">num</span></span><br><span class="line">        <span class="keyword">from</span> pageviews</span><br><span class="line">        <span class="keyword">group</span> <span class="keyword">by</span> user_id,page_id</span><br><span class="line">    ) tmp</span><br><span class="line">) a</span><br><span class="line"><span class="keyword">where</span> rk &lt;= <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> pageviews <span class="keyword">VALUES</span></span><br><span class="line">  (<span class="number">1</span>, <span class="number">101</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>)),</span><br><span class="line">  (<span class="number">1</span>, <span class="number">102</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>)),</span><br><span class="line">  (<span class="number">1</span>, <span class="number">103</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>)),</span><br><span class="line">  (<span class="number">1</span>, <span class="number">103</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>)),</span><br><span class="line">  (<span class="number">1</span>, <span class="number">104</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>)),</span><br><span class="line">  (<span class="number">1</span>, <span class="number">104</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>)),</span><br><span class="line">  (<span class="number">1</span>, <span class="number">104</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:01.00&#x27;</span>)),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">104</span>, TO_TIMESTAMP(<span class="string">&#x27;2020-11-23 15:00:00&#x27;</span>))</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; select user_id,page_id,num from users;</span><br><span class="line">+---------+---------+------+</span><br><span class="line">| user_id | page_id | num  |</span><br><span class="line">+---------+---------+------+</span><br><span class="line">|       1 |     101 |    1 |</span><br><span class="line">|       1 |     103 |    2 |</span><br><span class="line">|       1 |     104 |    3 |</span><br><span class="line">|       2 |     104 |    1 |</span><br><span class="line">+---------+---------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line"># 再次插入两条数据后</span><br><span class="line">INSERT INTO pageviews VALUES</span><br><span class="line">  (1, 102, TO_TIMESTAMP(&#39;2020-11-23 15:00:00&#39;)),</span><br><span class="line">  (1, 102, TO_TIMESTAMP(&#39;2020-11-23 15:00:00&#39;));</span><br><span class="line"></span><br><span class="line">mysql&gt; select user_id,page_id,num from users;</span><br><span class="line">+---------+---------+------+</span><br><span class="line">| user_id | page_id | num  |</span><br><span class="line">+---------+---------+------+</span><br><span class="line">|       1 |     102 |    3 |</span><br><span class="line">|       1 |     103 |    2 |</span><br><span class="line">|       1 |     104 |    3 |</span><br><span class="line">|       2 |     104 |    1 |</span><br><span class="line">+---------+---------+------+</span><br><span class="line">4 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line"># user_id&#x3D;1,page_id&#x3D;101的数据被删除了</span><br><span class="line">源码层次做了什么操作呢?(发送UPDATE&#x2F;DELETE)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="源码浏览"><a href="#源码浏览" class="headerlink" title="源码浏览"></a>源码浏览</h2><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"># SQL的转换就不再赘述了,之前有做过分析</span><br><span class="line"><span class="number">1.</span>StreamPhysicalRankRule,Rule规则转换RelNode</span><br><span class="line"><span class="function">override def <span class="title">convert</span><span class="params">(rel: RelNode)</span>: RelNode </span>= &#123;</span><br><span class="line">    val rank = rel.asInstanceOf[FlinkLogicalRank]</span><br><span class="line">    val input = rank.getInput</span><br><span class="line">    val requiredDistribution = <span class="keyword">if</span> (!rank.partitionKey.isEmpty) &#123;</span><br><span class="line">      FlinkRelDistribution.hash(rank.partitionKey.toList)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      FlinkRelDistribution.SINGLETON</span><br><span class="line">    &#125;</span><br><span class="line">    val requiredTraitSet = input.getTraitSet</span><br><span class="line">      .replace(FlinkConventions.STREAM_PHYSICAL)</span><br><span class="line">      .replace(requiredDistribution)</span><br><span class="line">    val providedTraitSet = rank.getTraitSet.replace(FlinkConventions.STREAM_PHYSICAL)</span><br><span class="line">    val newInput: RelNode = RelOptRule.convert(input, requiredTraitSet)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> StreamPhysicalRank(</span><br><span class="line">      rank.getCluster,</span><br><span class="line">      providedTraitSet,</span><br><span class="line">      newInput,</span><br><span class="line">      rank.partitionKey,</span><br><span class="line">      rank.orderKey,</span><br><span class="line">      rank.rankType,</span><br><span class="line">      rank.rankRange,</span><br><span class="line">      rank.rankNumberType,</span><br><span class="line">      rank.outputRankNumber,</span><br><span class="line">      <span class="comment">// TopN更新策略</span></span><br><span class="line">      <span class="comment">// 1.UndefinedStrategy</span></span><br><span class="line">      <span class="comment">// 2.AppendFastStrategy</span></span><br><span class="line">      <span class="comment">// 3.RetractStrategy</span></span><br><span class="line">      <span class="comment">// 4.UpdateFastStrategy</span></span><br><span class="line">      RankProcessStrategy.UNDEFINED_STRATEGY)</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">2.</span>StreamPhysicalRank,转换为ExecNode</span><br><span class="line"><span class="function">override def <span class="title">translateToExecNode</span><span class="params">()</span>: ExecNode[_] </span>= &#123;</span><br><span class="line">    val generateUpdateBefore = ChangelogPlanUtils.generateUpdateBefore(<span class="keyword">this</span>)</span><br><span class="line">    val fieldCollations = orderKey.getFieldCollations</span><br><span class="line">    <span class="keyword">new</span> StreamExecRank(</span><br><span class="line">      rankType,</span><br><span class="line">      <span class="keyword">new</span> PartitionSpec(partitionKey.toArray),</span><br><span class="line">      SortUtil.getSortSpec(fieldCollations),</span><br><span class="line">      rankRange,</span><br><span class="line">      rankStrategy,</span><br><span class="line">      outputRankNumber,</span><br><span class="line">      generateUpdateBefore,</span><br><span class="line">      InputProperty.DEFAULT,</span><br><span class="line">      FlinkTypeFactory.toLogicalRowType(getRowType),</span><br><span class="line">      getRelDetailedDescription</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br><span class="line"># 这里注意copy方法,涉及到策略的选型</span><br><span class="line"><span class="number">3.</span>StreamExecRank,转换为Operator</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> Transformation&lt;RowData&gt; <span class="title">translateToPlanInternal</span><span class="params">(PlannerBase planner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (rankType) &#123;</span><br><span class="line">        <span class="keyword">case</span> ROW_NUMBER:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> RANK:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(<span class="string">&quot;RANK() on streaming table is not supported currently&quot;</span>);</span><br><span class="line">        <span class="keyword">case</span> DENSE_RANK:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(</span><br><span class="line">                    <span class="string">&quot;DENSE_RANK() on streaming table is not supported currently&quot;</span>);</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TableException(</span><br><span class="line">                    String.format(</span><br><span class="line">                            <span class="string">&quot;Streaming tables do not support %s rank function.&quot;</span>, rankType));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ExecEdge inputEdge = getInputEdges().get(<span class="number">0</span>);</span><br><span class="line">    Transformation&lt;RowData&gt; inputTransform =</span><br><span class="line">            (Transformation&lt;RowData&gt;) inputEdge.translateToPlan(planner);</span><br><span class="line"></span><br><span class="line">    RowType inputType = (RowType) inputEdge.getOutputType();</span><br><span class="line">    InternalTypeInfo&lt;RowData&gt; inputRowTypeInfo = InternalTypeInfo.of(inputType);</span><br><span class="line">    <span class="keyword">int</span>[] sortFields = sortSpec.getFieldIndices();</span><br><span class="line">    RowDataKeySelector sortKeySelector =</span><br><span class="line">            KeySelectorUtil.getRowDataSelector(sortFields, inputRowTypeInfo);</span><br><span class="line">    <span class="comment">// create a sort spec on sort keys.</span></span><br><span class="line">    <span class="keyword">int</span>[] sortKeyPositions = IntStream.range(<span class="number">0</span>, sortFields.length).toArray();</span><br><span class="line">    SortSpec.SortSpecBuilder builder = SortSpec.builder();</span><br><span class="line">    IntStream.range(<span class="number">0</span>, sortFields.length)</span><br><span class="line">            .forEach(</span><br><span class="line">                    idx -&gt;</span><br><span class="line">                            builder.addField(</span><br><span class="line">                                    idx,</span><br><span class="line">                                    sortSpec.getFieldSpec(idx).getIsAscendingOrder(),</span><br><span class="line">                                    sortSpec.getFieldSpec(idx).getNullIsLast()));</span><br><span class="line">    SortSpec sortSpecInSortKey = builder.build();</span><br><span class="line">    TableConfig tableConfig = planner.getTableConfig();</span><br><span class="line">    GeneratedRecordComparator sortKeyComparator =</span><br><span class="line">            ComparatorCodeGenerator.gen(</span><br><span class="line">                    tableConfig,</span><br><span class="line">                    <span class="string">&quot;StreamExecSortComparator&quot;</span>,</span><br><span class="line">                    RowType.of(sortSpec.getFieldTypes(inputType)),</span><br><span class="line">                    sortSpecInSortKey);</span><br><span class="line">    <span class="keyword">long</span> cacheSize = tableConfig.getConfiguration().getLong(TABLE_EXEC_TOPN_CACHE_SIZE);</span><br><span class="line">    <span class="keyword">long</span> minIdleStateRetentionTime = tableConfig.getMinIdleStateRetentionTime();</span><br><span class="line">    <span class="keyword">long</span> maxIdleStateRetentionTime = tableConfig.getMaxIdleStateRetentionTime();</span><br><span class="line"></span><br><span class="line">    AbstractTopNFunction processFunction;</span><br><span class="line">    <span class="keyword">if</span> (rankStrategy <span class="keyword">instanceof</span> RankProcessStrategy.AppendFastStrategy) &#123;</span><br><span class="line">        processFunction =</span><br><span class="line">                <span class="keyword">new</span> AppendOnlyTopNFunction(</span><br><span class="line">                        minIdleStateRetentionTime,</span><br><span class="line">                        maxIdleStateRetentionTime,</span><br><span class="line">                        inputRowTypeInfo,</span><br><span class="line">                        sortKeyComparator,</span><br><span class="line">                        sortKeySelector,</span><br><span class="line">                        rankType,</span><br><span class="line">                        rankRange,</span><br><span class="line">                        generateUpdateBefore,</span><br><span class="line">                        outputRankNumber,</span><br><span class="line">                        cacheSize);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (rankStrategy <span class="keyword">instanceof</span> RankProcessStrategy.UpdateFastStrategy) &#123;</span><br><span class="line">        RankProcessStrategy.UpdateFastStrategy updateFastStrategy =</span><br><span class="line">                (RankProcessStrategy.UpdateFastStrategy) rankStrategy;</span><br><span class="line">        <span class="keyword">int</span>[] primaryKeys = updateFastStrategy.getPrimaryKeys();</span><br><span class="line">        RowDataKeySelector rowKeySelector =</span><br><span class="line">                KeySelectorUtil.getRowDataSelector(primaryKeys, inputRowTypeInfo);</span><br><span class="line">        processFunction =</span><br><span class="line">                <span class="keyword">new</span> UpdatableTopNFunction(</span><br><span class="line">                        minIdleStateRetentionTime,</span><br><span class="line">                        maxIdleStateRetentionTime,</span><br><span class="line">                        inputRowTypeInfo,</span><br><span class="line">                        rowKeySelector,</span><br><span class="line">                        sortKeyComparator,</span><br><span class="line">                        sortKeySelector,</span><br><span class="line">                        rankType,</span><br><span class="line">                        rankRange,</span><br><span class="line">                        generateUpdateBefore,</span><br><span class="line">                        outputRankNumber,</span><br><span class="line">                        cacheSize);</span><br><span class="line">        <span class="comment">// TODO Use UnaryUpdateTopNFunction after SortedMapState is merged</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (rankStrategy <span class="keyword">instanceof</span> RankProcessStrategy.RetractStrategy) &#123;</span><br><span class="line">        EqualiserCodeGenerator equaliserCodeGen =</span><br><span class="line">                <span class="keyword">new</span> EqualiserCodeGenerator(</span><br><span class="line">                        inputType.getFields().stream()</span><br><span class="line">                                .map(RowType.RowField::getType)</span><br><span class="line">                                .toArray(LogicalType[]::<span class="keyword">new</span>));</span><br><span class="line">        GeneratedRecordEqualiser generatedEqualiser =</span><br><span class="line">                equaliserCodeGen.generateRecordEqualiser(<span class="string">&quot;RankValueEqualiser&quot;</span>);</span><br><span class="line">        ComparableRecordComparator comparator =</span><br><span class="line">                <span class="keyword">new</span> ComparableRecordComparator(</span><br><span class="line">                        sortKeyComparator,</span><br><span class="line">                        sortKeyPositions,</span><br><span class="line">                        sortSpec.getFieldTypes(inputType),</span><br><span class="line">                        sortSpec.getAscendingOrders(),</span><br><span class="line">                        sortSpec.getNullsIsLast());</span><br><span class="line">        processFunction =</span><br><span class="line">                <span class="keyword">new</span> RetractableTopNFunction(</span><br><span class="line">                        minIdleStateRetentionTime,</span><br><span class="line">                        maxIdleStateRetentionTime,</span><br><span class="line">                        inputRowTypeInfo,</span><br><span class="line">                        comparator,</span><br><span class="line">                        sortKeySelector,</span><br><span class="line">                        rankType,</span><br><span class="line">                        rankRange,</span><br><span class="line">                        generatedEqualiser,</span><br><span class="line">                        generateUpdateBefore,</span><br><span class="line">                        outputRankNumber);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> TableException(</span><br><span class="line">                String.format(<span class="string">&quot;rank strategy:%s is not supported.&quot;</span>, rankStrategy));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    KeyedProcessOperator&lt;RowData, RowData, RowData&gt; operator =</span><br><span class="line">            <span class="keyword">new</span> KeyedProcessOperator&lt;&gt;(processFunction);</span><br><span class="line">    processFunction.setKeyContext(operator);</span><br><span class="line"></span><br><span class="line">    OneInputTransformation&lt;RowData, RowData&gt; transform =</span><br><span class="line">            <span class="keyword">new</span> OneInputTransformation&lt;&gt;(</span><br><span class="line">                    inputTransform,</span><br><span class="line">                    getDescription(),</span><br><span class="line">                    operator,</span><br><span class="line">                    InternalTypeInfo.of((RowType) getOutputType()),</span><br><span class="line">                    inputTransform.getParallelism());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set KeyType and Selector for state</span></span><br><span class="line">    RowDataKeySelector selector =</span><br><span class="line">            KeySelectorUtil.getRowDataSelector(</span><br><span class="line">                    partitionSpec.getFieldIndices(), inputRowTypeInfo);</span><br><span class="line">    transform.setStateKeySelector(selector);</span><br><span class="line">    transform.setStateKeyType(selector.getProducedType());</span><br><span class="line">    <span class="keyword">return</span> transform;</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">4.</span>使用的是RetractStrategy,主要看RetractableTopNFunction的emitRecordsWithRowNumber和retractRecordWithRowNumber</span><br><span class="line">一者进行只进行插入更新操作,一者进行插入更新删除操作</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">retractRecordWithRowNumber</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        SortedMap&lt;RowData, Long&gt; sortedMap,</span></span></span><br><span class="line"><span class="function"><span class="params">        RowData sortKey,</span></span></span><br><span class="line"><span class="function"><span class="params">        RowData inputRow,</span></span></span><br><span class="line"><span class="function"><span class="params">        Collector&lt;RowData&gt; out)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Iterator&lt;Map.Entry&lt;RowData, Long&gt;&gt; iterator = sortedMap.entrySet().iterator();</span><br><span class="line">    <span class="keyword">long</span> currentRank = <span class="number">0L</span>;</span><br><span class="line">    RowData prevRow = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">boolean</span> findsSortKey = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">        Map.Entry&lt;RowData, Long&gt; entry = iterator.next();</span><br><span class="line">        RowData key = entry.getKey();</span><br><span class="line">        <span class="keyword">if</span> (!findsSortKey &amp;&amp; key.equals(sortKey)) &#123;</span><br><span class="line">            List&lt;RowData&gt; inputs = dataState.get(key);</span><br><span class="line">            <span class="keyword">if</span> (inputs == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// Skip the data if it&#x27;s state is cleared because of state ttl.</span></span><br><span class="line">                <span class="keyword">if</span> (lenient) &#123;</span><br><span class="line">                    LOG.warn(STATE_CLEARED_WARN_MSG);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(STATE_CLEARED_WARN_MSG);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                Iterator&lt;RowData&gt; inputIter = inputs.iterator();</span><br><span class="line">                <span class="keyword">while</span> (inputIter.hasNext() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">                    RowData currentRow = inputIter.next();</span><br><span class="line">                    <span class="keyword">if</span> (!findsSortKey &amp;&amp; equaliser.equals(currentRow, inputRow)) &#123;</span><br><span class="line">                        prevRow = currentRow;</span><br><span class="line">                        findsSortKey = <span class="keyword">true</span>;</span><br><span class="line">                        inputIter.remove();</span><br><span class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (findsSortKey) &#123;</span><br><span class="line">                        collectUpdateBefore(out, prevRow, currentRank);</span><br><span class="line">                        collectUpdateAfter(out, currentRow, currentRank);</span><br><span class="line">                        prevRow = currentRow;</span><br><span class="line">                    &#125;</span><br><span class="line">                    currentRank += <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (inputs.isEmpty()) &#123;</span><br><span class="line">                    dataState.remove(key);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dataState.put(key, inputs);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (findsSortKey) &#123;</span><br><span class="line">            List&lt;RowData&gt; inputs = dataState.get(key);</span><br><span class="line">            <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (i &lt; inputs.size() &amp;&amp; isInRankEnd(currentRank)) &#123;</span><br><span class="line">                RowData currentRow = inputs.get(i);</span><br><span class="line">                collectUpdateBefore(out, prevRow, currentRank);</span><br><span class="line">                collectUpdateAfter(out, currentRow, currentRank);</span><br><span class="line">                prevRow = currentRow;</span><br><span class="line">                currentRank += <span class="number">1</span>;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            currentRank += entry.getValue();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (isInRankEnd(currentRank)) &#123;</span><br><span class="line">        <span class="comment">// there is no enough elements in Top-N, emit DELETE message for the retract record.</span></span><br><span class="line">        collectDelete(out, prevRow, currentRank);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> findsSortKey;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="在何处进行策略的更新"><a href="#在何处进行策略的更新" class="headerlink" title="在何处进行策略的更新?"></a>在何处进行策略的更新?</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">上面有提到,需要注意StreamPhysicalRank的copy方法</span><br><span class="line">在FlinkChangelogModeInferenceProgram中被调用</span><br><span class="line">推断ChangelogMode优化</span><br><span class="line"><span class="keyword">case</span> rank: StreamPhysicalRank =&gt;</span><br><span class="line">    val rankStrategies = RankProcessStrategy.analyzeRankProcessStrategies(</span><br><span class="line">      rank, rank.partitionKey, rank.orderKey)</span><br><span class="line">    visitRankStrategies(rankStrategies, requiredTrait, rankStrategy =&gt; rank.copy(rankStrategy))</span><br><span class="line"></span><br><span class="line">基于FlinkPhysicalRel,分区Key,排序Key获取更新策略RankProcessStrategy.analyzeRankProcessStrategies()</span><br><span class="line"><span class="function"><span class="keyword">static</span> List&lt;RankProcessStrategy&gt; <span class="title">analyzeRankProcessStrategies</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        StreamPhysicalRel rank, ImmutableBitSet partitionKey, RelCollation orderKey)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    RelMetadataQuery mq = rank.getCluster().getMetadataQuery();</span><br><span class="line">    List&lt;RelFieldCollation&gt; fieldCollations = orderKey.getFieldCollations();</span><br><span class="line">    <span class="comment">// 分析是不是Update流</span></span><br><span class="line">    <span class="keyword">boolean</span> isUpdateStream = !ChangelogPlanUtils.inputInsertOnly(rank);</span><br><span class="line">    RelNode input = rank.getInput(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isUpdateStream) &#123;</span><br><span class="line">        Set&lt;ImmutableBitSet&gt; uniqueKeys = mq.getUniqueKeys(input);</span><br><span class="line">        <span class="keyword">if</span> (uniqueKeys == <span class="keyword">null</span></span><br><span class="line">                || uniqueKeys.isEmpty()</span><br><span class="line">                <span class="comment">// unique key should contains partition key</span></span><br><span class="line">                || uniqueKeys.stream().noneMatch(k -&gt; k.contains(partitionKey))) &#123;</span><br><span class="line">            <span class="comment">// and we fall back to using retract rank</span></span><br><span class="line">            <span class="comment">// 返回的是RETRACT_STRATEGY</span></span><br><span class="line">            <span class="keyword">return</span> Collections.singletonList(RETRACT_STRATEGY);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            FlinkRelMetadataQuery fmq = FlinkRelMetadataQuery.reuseOrCreate(mq);</span><br><span class="line">            RelModifiedMonotonicity monotonicity = fmq.getRelModifiedMonotonicity(input);</span><br><span class="line">            <span class="keyword">boolean</span> isMonotonic = <span class="keyword">false</span>;</span><br><span class="line">            <span class="keyword">if</span> (monotonicity != <span class="keyword">null</span> &amp;&amp; !fieldCollations.isEmpty()) &#123;</span><br><span class="line">                isMonotonic =</span><br><span class="line">                        fieldCollations.stream()</span><br><span class="line">                                .allMatch(</span><br><span class="line">                                        collation -&gt; &#123;</span><br><span class="line">                                            SqlMonotonicity fieldMonotonicity =</span><br><span class="line">                                                    monotonicity</span><br><span class="line">                                                            .fieldMonotonicities()[</span><br><span class="line">                                                            collation.getFieldIndex()];</span><br><span class="line">                                            RelFieldCollation.Direction direction =</span><br><span class="line">                                                    collation.direction;</span><br><span class="line">                                            <span class="keyword">if</span> ((fieldMonotonicity == SqlMonotonicity.DECREASING</span><br><span class="line">                                                            || fieldMonotonicity</span><br><span class="line">                                                                    == SqlMonotonicity</span><br><span class="line">                                                                            .STRICTLY_DECREASING)</span><br><span class="line">                                                    &amp;&amp; direction</span><br><span class="line">                                                            == RelFieldCollation.Direction</span><br><span class="line">                                                                    .ASCENDING) &#123;</span><br><span class="line">                                                <span class="comment">// sort field is ascending and its monotonicity</span></span><br><span class="line">                                                <span class="comment">// is decreasing</span></span><br><span class="line">                                                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                                            &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((fieldMonotonicity</span><br><span class="line">                                                                    == SqlMonotonicity</span><br><span class="line">                                                                            .INCREASING</span><br><span class="line">                                                            || fieldMonotonicity</span><br><span class="line">                                                                    == SqlMonotonicity</span><br><span class="line">                                                                            .STRICTLY_INCREASING)</span><br><span class="line">                                                    &amp;&amp; direction</span><br><span class="line">                                                            == RelFieldCollation.Direction</span><br><span class="line">                                                                    .DESCENDING) &#123;</span><br><span class="line">                                                <span class="comment">// sort field is descending and its monotonicity</span></span><br><span class="line">                                                <span class="comment">// is increasing</span></span><br><span class="line">                                                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                                            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                                                <span class="comment">// sort key is a grouping key of upstream agg,</span></span><br><span class="line">                                                <span class="comment">// it is monotonic</span></span><br><span class="line">                                                <span class="keyword">return</span> fieldMonotonicity</span><br><span class="line">                                                        == SqlMonotonicity.CONSTANT;</span><br><span class="line">                                            &#125;</span><br><span class="line">                                        &#125;);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (isMonotonic) &#123;</span><br><span class="line">                <span class="comment">// <span class="doctag">TODO:</span> choose a set of primary key</span></span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(</span><br><span class="line">                        <span class="keyword">new</span> UpdateFastStrategy(uniqueKeys.iterator().next().toArray()),</span><br><span class="line">                        RETRACT_STRATEGY);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 返回RETRACT_STRATEGY</span></span><br><span class="line">                <span class="keyword">return</span> Collections.singletonList(RETRACT_STRATEGY);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 返回APPEND_FAST_STRATEGY</span></span><br><span class="line">        <span class="keyword">return</span> Collections.singletonList(APPEND_FAST_STRATEGY);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>聚类分析实践</title>
    <url>/2021/05/19/%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<blockquote>
<p>对于知道GPS数据,想要得到某个人的频繁涉及的点,<a href="https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods">传送门</a></p>
</blockquote>
<span id="more"></span>

<h2 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">已有条件,GPS经纬度数据</span><br><span class="line">A,lat,lon</span><br><span class="line">A,lat,lon</span><br><span class="line">A,lat,lon</span><br><span class="line"></span><br><span class="line">最为简单的方式就是,将经纬度转换为geohash值</span><br><span class="line">A,geohash</span><br><span class="line">A,geohash</span><br><span class="line">A,geohash</span><br><span class="line">然后对geohash截取位数,进行分组统计取出现次数的最大值</span><br><span class="line">最后再将geohash转换为经纬度</span><br><span class="line"></span><br><span class="line">高级一点,用聚类算法,将这批数据跑一遍,得出最终收敛的值</span><br><span class="line"></span><br><span class="line"># 注意</span><br><span class="line">在使用pip install时有时候不成功,可以使用conda install</span><br><span class="line">不过注意名称</span><br><span class="line">e.g: </span><br><span class="line">pip install sklearn</span><br><span class="line">conda install scikit-learn</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><h3 id="Mean-Shift"><a href="#Mean-Shift" class="headerlink" title="Mean-Shift"></a>Mean-Shift</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 均值迁移,在数据集中选定一个点,以这个点为圆心,r为半径,画一个圆</span></span><br><span class="line"><span class="comment"># 计算出点到所有点的向量平均值,圆心与向量均值的和为新的圆心</span></span><br><span class="line"><span class="comment"># 迭代整个过程,直到满足一点</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># Mean-Shift</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MeanShift, estimate_bandwidth</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">100</span></span><br><span class="line"><span class="comment">##生产数据</span></span><br><span class="line">X, _ = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.6</span>,</span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##带宽(半径),也就是以某个点为核心时的搜索半径</span></span><br><span class="line">bandwidth = estimate_bandwidth(X, quantile=<span class="number">0.2</span>, n_samples=<span class="number">500</span>)</span><br><span class="line"><span class="comment">##设置均值偏移函数</span></span><br><span class="line">ms = MeanShift(bandwidth=bandwidth, bin_seeding=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">ms.fit(X)</span><br><span class="line"><span class="comment">##每个点的标签</span></span><br><span class="line">labels = ms.labels_</span><br><span class="line">print(labels)</span><br><span class="line"><span class="comment">##簇中心的点的集合</span></span><br><span class="line">cluster_centers = ms.cluster_centers_</span><br><span class="line">print(<span class="string">&#x27;cluster_centers:&#x27;</span>,cluster_centers)</span><br><span class="line"><span class="comment">##总共的标签分类</span></span><br><span class="line">labels_unique = np.unique(labels)</span><br><span class="line">print(labels_unique)</span><br><span class="line"><span class="comment">##聚簇的个数,即分类的个数</span></span><br><span class="line">n_clusters_ = <span class="built_in">len</span>(labels_unique)</span><br><span class="line">print(<span class="string">&quot;number of estimated clusters : %d&quot;</span> % n_clusters_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">&#x27;bgrcmykbgrcmykbgrcmykbgrcmyk&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters_), colors):</span><br><span class="line">    <span class="comment">##根据lables中的值是否等于k，重新组成一个True、False的数组</span></span><br><span class="line">    my_members = labels == k</span><br><span class="line">    cluster_center = cluster_centers[k]</span><br><span class="line">    <span class="comment">##X[my_members, 0] 取出my_members对应位置为True的值的横坐标</span></span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    plt.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">             markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">14</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Estimated number of clusters: %d&#x27;</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Spectral-Clustering"><a href="#Spectral-Clustering" class="headerlink" title="Spectral Clustering"></a>Spectral Clustering</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 谱聚类,基于图论的聚类</span></span><br><span class="line"><span class="comment"># Spectral Clustering</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> spectral_clustering</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">3000</span></span><br><span class="line"><span class="comment">##生产数据</span></span><br><span class="line">X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.6</span>, </span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##变换成矩阵，输入必须是对称矩阵</span></span><br><span class="line">metrics_metrix = (-<span class="number">1</span> * metrics.pairwise.pairwise_distances(X)).astype(np.int32)</span><br><span class="line">metrics_metrix += -<span class="number">1</span> * metrics_metrix.<span class="built_in">min</span>()</span><br><span class="line"><span class="comment">##设置谱聚类函数</span></span><br><span class="line">n_clusters_= <span class="number">4</span></span><br><span class="line">lables = spectral_clustering(metrics_metrix,n_clusters=n_clusters_)</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">&#x27;bgrcmykbgrcmykbgrcmykbgrcmyk&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters_), colors):</span><br><span class="line">    <span class="comment">##根据lables中的值是否等于k，重新组成一个True、False的数组</span></span><br><span class="line">    my_members = lables == k</span><br><span class="line">    <span class="comment">##X[my_members, 0] 取出my_members对应位置为True的值的横坐标</span></span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">plt.title(<span class="string">&#x27;Estimated number of clusters: %d&#x27;</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 层次聚类,按照某种方法进行层次分类</span></span><br><span class="line"><span class="comment"># Hierarchical Clustering</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">3000</span></span><br><span class="line"><span class="comment">##生产数据</span></span><br><span class="line">X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.6</span>, </span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##设置分层聚类函数</span></span><br><span class="line">linkages = [<span class="string">&#x27;ward&#x27;</span>, <span class="string">&#x27;average&#x27;</span>, <span class="string">&#x27;complete&#x27;</span>]</span><br><span class="line">n_clusters_ = <span class="number">3</span></span><br><span class="line">ac = AgglomerativeClustering(linkage=linkages[<span class="number">2</span>],n_clusters = n_clusters_)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">ac.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">##每个数据的分类</span></span><br><span class="line">lables = ac.labels_</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">&#x27;bgrcmykbgrcmykbgrcmykbgrcmyk&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters_), colors):</span><br><span class="line">    <span class="comment">##根据lables中的值是否等于k，重新组成一个True、False的数组</span></span><br><span class="line">    my_members = lables == k</span><br><span class="line">    <span class="comment">##X[my_members, 0] 取出my_members对应位置为True的值的横坐标</span></span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">plt.title(<span class="string">&#x27;Estimated number of clusters: %d&#x27;</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 密度聚类,基于密度的空间聚类</span></span><br><span class="line"><span class="comment"># DBSCAN</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle  <span class="comment">##python自带的迭代器模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment">##产生随机数据的中心</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]]</span><br><span class="line"><span class="comment">##产生的数据个数</span></span><br><span class="line">n_samples=<span class="number">750</span></span><br><span class="line"><span class="comment">##生产数据:此实验结果受cluster_std的影响，或者说受eps 和cluster_std差值影响</span></span><br><span class="line">X, lables_true = make_blobs(n_samples=n_samples, centers= centers, cluster_std=<span class="number">0.4</span>, </span><br><span class="line">                  random_state =<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##设置分层聚类函数</span></span><br><span class="line">db = DBSCAN(eps=<span class="number">0.3</span>, min_samples=<span class="number">10</span>)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">db.fit(X)</span><br><span class="line"><span class="comment">##初始化一个全是False的bool类型的数组</span></span><br><span class="line">core_samples_mask = np.zeros_like(db.labels_, dtype=<span class="built_in">bool</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">   这里是关键点(针对这行代码：xy = X[class_member_mask &amp; ~core_samples_mask])：</span></span><br><span class="line"><span class="string">   db.core_sample_indices_  表示的是某个点在寻找核心点集合的过程中暂时被标为噪声点的点(即周围点</span></span><br><span class="line"><span class="string">   小于min_samples)，并不是最终的噪声点。在对核心点进行联通的过程中，这部分点会被进行重新归类(即标签</span></span><br><span class="line"><span class="string">   并不会是表示噪声点的-1)，也可也这样理解，这些点不适合做核心点，但是会被包含在某个核心点的范围之内</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">core_samples_mask[db.core_sample_indices_] = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##每个数据的分类</span></span><br><span class="line">lables = db.labels_</span><br><span class="line"></span><br><span class="line"><span class="comment">##分类个数：lables中包含-1，表示噪声点</span></span><br><span class="line">n_clusters_ =<span class="built_in">len</span>(np.unique(lables)) - (<span class="number">1</span> <span class="keyword">if</span> -<span class="number">1</span> <span class="keyword">in</span> lables <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">unique_labels = <span class="built_in">set</span>(lables)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">   1)np.linspace 返回[0,1]之间的len(unique_labels) 个数</span></span><br><span class="line"><span class="string">   2)plt.cm 一个颜色映射模块</span></span><br><span class="line"><span class="string">   3)生成的每个colors包含4个值，分别是rgba</span></span><br><span class="line"><span class="string">   4)其实这行代码的意思就是生成4个可以和光谱对应的颜色值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">colors = plt.cm.Spectral(np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="built_in">len</span>(unique_labels)))</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(unique_labels, colors):</span><br><span class="line">    <span class="comment">##-1表示噪声点,这里的k表示黑色</span></span><br><span class="line">    <span class="keyword">if</span> k == -<span class="number">1</span>:</span><br><span class="line">        col = <span class="string">&#x27;k&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">##生成一个True、False数组，lables == k 的设置成True</span></span><br><span class="line">    class_member_mask = (lables == k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">##两个数组做&amp;运算，找出即是核心点又等于分类k的值  markeredgecolor=&#x27;k&#x27;,</span></span><br><span class="line">    xy = X[class_member_mask &amp; core_samples_mask]</span><br><span class="line">    plt.plot(xy[:, <span class="number">0</span>], xy[:, <span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, c=col,markersize=<span class="number">14</span>)</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">       1)~优先级最高，按位对core_samples_mask 求反，求出的是噪音点的位置</span></span><br><span class="line"><span class="string">       2)&amp; 于运算之后，求出虽然刚开始是噪音点的位置，但是重新归类却属于k的点</span></span><br><span class="line"><span class="string">       3)对核心分类之后进行的扩展</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    xy = X[class_member_mask &amp; ~core_samples_mask]     </span><br><span class="line">    plt.plot(xy[:, <span class="number">0</span>], xy[:, <span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, c=col,markersize=<span class="number">6</span>)</span><br><span class="line">    </span><br><span class="line">plt.title(<span class="string">&#x27;Estimated number of clusters: %d&#x27;</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Birch"><a href="#Birch" class="headerlink" title="Birch"></a>Birch</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过聚类特征(CF)形成一个聚类特征树</span></span><br><span class="line"><span class="comment"># Birch</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> Birch</span><br><span class="line"></span><br><span class="line"><span class="comment"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=[[-<span class="number">1</span>,-<span class="number">1</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>]], cluster_std=[<span class="number">0.4</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>], </span><br><span class="line">                  random_state =<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##设置birch函数</span></span><br><span class="line">birch = Birch(n_clusters = <span class="literal">None</span>)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">y_pred = birch.fit_predict(X)</span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="GaussianMixtureModel"><a href="#GaussianMixtureModel" class="headerlink" title="GaussianMixtureModel"></a>GaussianMixtureModel</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 高斯混合,GMM,任意形状的概率分布都可以用多个高斯分布函数去近似</span></span><br><span class="line"><span class="comment"># 通过属于某一类的概率大小来判断最终的归属类别</span></span><br><span class="line"><span class="comment"># GaussianMixtureModel</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"></span><br><span class="line"><span class="comment"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共4个簇，簇中心在[-1,-1], [0,0],[1,1], [2,2]</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=[[-<span class="number">1</span>,-<span class="number">1</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>]], cluster_std=[<span class="number">0.4</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.3</span>], </span><br><span class="line">                  random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##设置gmm函数</span></span><br><span class="line">gmm = GaussianMixture(n_components=<span class="number">4</span>, covariance_type=<span class="string">&#x27;full&#x27;</span>).fit(X)</span><br><span class="line"><span class="comment">##训练数据</span></span><br><span class="line">y_pred = gmm.predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">##绘图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># K均值聚类,KMeans和MiniBatchKMeans</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans, KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances_argmin</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># 生成样本数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">45</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]]</span><br><span class="line">n_clusters = <span class="built_in">len</span>(centers)</span><br><span class="line">X, labels_true = make_blobs(n_samples=<span class="number">3000</span>, centers=centers, cluster_std=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># 用Kmeans计算聚类</span></span><br><span class="line"></span><br><span class="line">k_means = KMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, n_init=<span class="number">10</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">k_means.fit(X)</span><br><span class="line">t_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># 用MiniBatchKMeans计算聚类</span></span><br><span class="line"></span><br><span class="line">mbk = MiniBatchKMeans(init=<span class="string">&#x27;k-means++&#x27;</span>, n_clusters=<span class="number">3</span>, batch_size=batch_size,</span><br><span class="line">                      n_init=<span class="number">10</span>, max_no_improvement=<span class="number">10</span>, verbose=<span class="number">0</span>)</span><br><span class="line">t0 = time.time()</span><br><span class="line">mbk.fit(X)</span><br><span class="line">t_mini_batch = time.time() - t0</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">fig.subplots_adjust(left=<span class="number">0.02</span>, right=<span class="number">0.98</span>, bottom=<span class="number">0.05</span>, top=<span class="number">0.9</span>)</span><br><span class="line">colors = [<span class="string">&#x27;#4EACC5&#x27;</span>, <span class="string">&#x27;#FF9C34&#x27;</span>, <span class="string">&#x27;#4E9A06&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># MiniBatchKMeans和KMeans算法中的同一簇具有相同的颜色</span></span><br><span class="line">k_means_cluster_centers = k_means.cluster_centers_</span><br><span class="line">order = pairwise_distances_argmin(k_means.cluster_centers_,</span><br><span class="line">                                  mbk.cluster_centers_)</span><br><span class="line">mbk_means_cluster_centers = mbk.cluster_centers_[order]</span><br><span class="line"></span><br><span class="line">k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)</span><br><span class="line">mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = k_means_labels == k</span><br><span class="line">    cluster_center = k_means_cluster_centers[k]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;KMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>,  <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> % (</span><br><span class="line">    t_batch, k_means.inertia_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># MiniBatchKMeans</span></span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters), colors):</span><br><span class="line">    my_members = mbk_means_labels == k</span><br><span class="line">    cluster_center = mbk_means_cluster_centers[k]</span><br><span class="line">    ax.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">            markerfacecolor=col, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    ax.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">            markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">6</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;MiniBatchKMeans&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line">plt.text(-<span class="number">3.5</span>, <span class="number">1.8</span>, <span class="string">&#x27;train time: %.2fs\ninertia: %f&#x27;</span> %</span><br><span class="line">         (t_mini_batch, mbk.inertia_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示不同</span></span><br><span class="line">different = (mbk_means_labels == <span class="number">4</span>)</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n_clusters):</span><br><span class="line">    different += ((k_means_labels == k) != (mbk_means_labels == k))</span><br><span class="line"></span><br><span class="line">identic = np.logical_not(different)</span><br><span class="line">ax.plot(X[identic, <span class="number">0</span>], X[identic, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        markerfacecolor=<span class="string">&#x27;#bbbbbb&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">ax.plot(X[different, <span class="number">0</span>], X[different, <span class="number">1</span>], <span class="string">&#x27;w&#x27;</span>,</span><br><span class="line">        markerfacecolor=<span class="string">&#x27;m&#x27;</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">ax.set_title(<span class="string">&#x27;Difference&#x27;</span>)</span><br><span class="line">ax.set_xticks(())</span><br><span class="line">ax.set_yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Affinity-Propagation"><a href="#Affinity-Propagation" class="headerlink" title="Affinity Propagation"></a>Affinity Propagation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># AP算法,近邻传播/亲和力传播算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AffinityPropagation</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># 样本数据</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [-<span class="number">1</span>, -<span class="number">1</span>], [<span class="number">1</span>, -<span class="number">1</span>]]</span><br><span class="line">X, labels_true = make_blobs(n_samples=<span class="number">300</span>, centers=centers, cluster_std=<span class="number">0.5</span>,</span><br><span class="line">                            random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># 计算Affinity Propagation</span></span><br><span class="line">af = AffinityPropagation(preference=-<span class="number">50</span>).fit(X)</span><br><span class="line">cluster_centers_indices = af.cluster_centers_indices_</span><br><span class="line">labels = af.labels_</span><br><span class="line"></span><br><span class="line">n_clusters_ = <span class="built_in">len</span>(cluster_centers_indices)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Estimated number of clusters: %d&#x27;</span> % n_clusters_)</span><br><span class="line">print(<span class="string">&quot;Homogeneity: %0.3f&quot;</span> % metrics.homogeneity_score(labels_true, labels))</span><br><span class="line">print(<span class="string">&quot;Completeness: %0.3f&quot;</span> % metrics.completeness_score(labels_true, labels))</span><br><span class="line">print(<span class="string">&quot;V-measure: %0.3f&quot;</span> % metrics.v_measure_score(labels_true, labels))</span><br><span class="line">print(<span class="string">&quot;Adjusted Rand Index: %0.3f&quot;</span></span><br><span class="line">      % metrics.adjusted_rand_score(labels_true, labels))</span><br><span class="line">print(<span class="string">&quot;Adjusted Mutual Information: %0.3f&quot;</span></span><br><span class="line">      % metrics.adjusted_mutual_info_score(labels_true, labels))</span><br><span class="line">print(<span class="string">&quot;Silhouette Coefficient: %0.3f&quot;</span></span><br><span class="line">      % metrics.silhouette_score(X, labels, metric=<span class="string">&#x27;sqeuclidean&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle</span><br><span class="line"></span><br><span class="line">plt.close(<span class="string">&#x27;all&#x27;</span>)</span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">&#x27;bgrcmykbgrcmykbgrcmykbgrcmyk&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(n_clusters_), colors):</span><br><span class="line">    class_members = labels == k</span><br><span class="line">    cluster_center = X[cluster_centers_indices[k]]</span><br><span class="line">    plt.plot(X[class_members, <span class="number">0</span>], X[class_members, <span class="number">1</span>], col + <span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">    plt.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">&#x27;o&#x27;</span>, markerfacecolor=col,</span><br><span class="line">             markeredgecolor=<span class="string">&#x27;k&#x27;</span>, markersize=<span class="number">14</span>)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> X[class_members]:</span><br><span class="line">        plt.plot([cluster_center[<span class="number">0</span>], x[<span class="number">0</span>]], [cluster_center[<span class="number">1</span>], x[<span class="number">1</span>]], col)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&#x27;Estimated number of clusters: %d&#x27;</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="OPTICS"><a href="#OPTICS" class="headerlink" title="OPTICS"></a>OPTICS</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可以看做DBSCAN的扩展</span></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> OPTICS, cluster_optics_dbscan</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">n_points_per_cluster = <span class="number">250</span></span><br><span class="line"></span><br><span class="line">C1 = [-<span class="number">5</span>, -<span class="number">2</span>] + <span class="number">.8</span> * np.random.randn(n_points_per_cluster, <span class="number">2</span>)</span><br><span class="line">C2 = [<span class="number">4</span>, -<span class="number">1</span>] + <span class="number">.1</span> * np.random.randn(n_points_per_cluster, <span class="number">2</span>)</span><br><span class="line">C3 = [<span class="number">1</span>, -<span class="number">2</span>] + <span class="number">.2</span> * np.random.randn(n_points_per_cluster, <span class="number">2</span>)</span><br><span class="line">C4 = [-<span class="number">2</span>, <span class="number">3</span>] + <span class="number">.3</span> * np.random.randn(n_points_per_cluster, <span class="number">2</span>)</span><br><span class="line">C5 = [<span class="number">3</span>, -<span class="number">2</span>] + <span class="number">1.6</span> * np.random.randn(n_points_per_cluster, <span class="number">2</span>)</span><br><span class="line">C6 = [<span class="number">5</span>, <span class="number">6</span>] + <span class="number">2</span> * np.random.randn(n_points_per_cluster, <span class="number">2</span>)</span><br><span class="line">X = np.vstack((C1, C2, C3, C4, C5, C6))</span><br><span class="line"></span><br><span class="line">clust = OPTICS(min_samples=<span class="number">50</span>, xi=<span class="number">.05</span>, min_cluster_size=<span class="number">.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the fit</span></span><br><span class="line">clust.fit(X)</span><br><span class="line"></span><br><span class="line">labels_050 = cluster_optics_dbscan(reachability=clust.reachability_,</span><br><span class="line">                                   core_distances=clust.core_distances_,</span><br><span class="line">                                   ordering=clust.ordering_, eps=<span class="number">0.5</span>)</span><br><span class="line">labels_200 = cluster_optics_dbscan(reachability=clust.reachability_,</span><br><span class="line">                                   core_distances=clust.core_distances_,</span><br><span class="line">                                   ordering=clust.ordering_, eps=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">space = np.arange(<span class="built_in">len</span>(X))</span><br><span class="line">reachability = clust.reachability_[clust.ordering_]</span><br><span class="line">labels = clust.labels_[clust.ordering_]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">G = gridspec.GridSpec(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">ax1 = plt.subplot(G[<span class="number">0</span>, :])</span><br><span class="line">ax2 = plt.subplot(G[<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">ax3 = plt.subplot(G[<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">ax4 = plt.subplot(G[<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reachability plot</span></span><br><span class="line">colors = [<span class="string">&#x27;g.&#x27;</span>, <span class="string">&#x27;r.&#x27;</span>, <span class="string">&#x27;b.&#x27;</span>, <span class="string">&#x27;y.&#x27;</span>, <span class="string">&#x27;c.&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> klass, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>), colors):</span><br><span class="line">    Xk = space[labels == klass]</span><br><span class="line">    Rk = reachability[labels == klass]</span><br><span class="line">    ax1.plot(Xk, Rk, color, alpha=<span class="number">0.3</span>)</span><br><span class="line">ax1.plot(space[labels == -<span class="number">1</span>], reachability[labels == -<span class="number">1</span>], <span class="string">&#x27;k.&#x27;</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line">ax1.plot(space, np.full_like(space, <span class="number">2.</span>, dtype=<span class="built_in">float</span>), <span class="string">&#x27;k-&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">ax1.plot(space, np.full_like(space, <span class="number">0.5</span>, dtype=<span class="built_in">float</span>), <span class="string">&#x27;k-.&#x27;</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;Reachability (epsilon distance)&#x27;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;Reachability Plot&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># OPTICS</span></span><br><span class="line">colors = [<span class="string">&#x27;g.&#x27;</span>, <span class="string">&#x27;r.&#x27;</span>, <span class="string">&#x27;b.&#x27;</span>, <span class="string">&#x27;y.&#x27;</span>, <span class="string">&#x27;c.&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> klass, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>), colors):</span><br><span class="line">    Xk = X[clust.labels_ == klass]</span><br><span class="line">    ax2.plot(Xk[:, <span class="number">0</span>], Xk[:, <span class="number">1</span>], color, alpha=<span class="number">0.3</span>)</span><br><span class="line">ax2.plot(X[clust.labels_ == -<span class="number">1</span>, <span class="number">0</span>], X[clust.labels_ == -<span class="number">1</span>, <span class="number">1</span>], <span class="string">&#x27;k+&#x27;</span>, alpha=<span class="number">0.1</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Automatic Clustering\nOPTICS&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DBSCAN at 0.5</span></span><br><span class="line">colors = [<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;greenyellow&#x27;</span>, <span class="string">&#x27;olive&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> klass, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">6</span>), colors):</span><br><span class="line">    Xk = X[labels_050 == klass]</span><br><span class="line">    ax3.plot(Xk[:, <span class="number">0</span>], Xk[:, <span class="number">1</span>], color, alpha=<span class="number">0.3</span>, marker=<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">ax3.plot(X[labels_050 == -<span class="number">1</span>, <span class="number">0</span>], X[labels_050 == -<span class="number">1</span>, <span class="number">1</span>], <span class="string">&#x27;k+&#x27;</span>, alpha=<span class="number">0.1</span>)</span><br><span class="line">ax3.set_title(<span class="string">&#x27;Clustering at 0.5 epsilon cut\nDBSCAN&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DBSCAN at 2.</span></span><br><span class="line">colors = [<span class="string">&#x27;g.&#x27;</span>, <span class="string">&#x27;m.&#x27;</span>, <span class="string">&#x27;y.&#x27;</span>, <span class="string">&#x27;c.&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> klass, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">4</span>), colors):</span><br><span class="line">    Xk = X[labels_200 == klass]</span><br><span class="line">    ax4.plot(Xk[:, <span class="number">0</span>], Xk[:, <span class="number">1</span>], color, alpha=<span class="number">0.3</span>)</span><br><span class="line">ax4.plot(X[labels_200 == -<span class="number">1</span>, <span class="number">0</span>], X[labels_200 == -<span class="number">1</span>, <span class="number">1</span>], <span class="string">&#x27;k+&#x27;</span>, alpha=<span class="number">0.1</span>)</span><br><span class="line">ax4.set_title(<span class="string">&#x27;Clustering at 2.0 epsilon cut\nDBSCAN&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="Clustering-performance-evaluation"><a href="#Clustering-performance-evaluation" class="headerlink" title="Clustering performance evaluation"></a>Clustering performance evaluation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 聚类性能评估</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uniform_labelings_scores</span>(<span class="params">score_func, n_samples, n_clusters_range,</span></span></span><br><span class="line"><span class="function"><span class="params">                             fixed_n_classes=<span class="literal">None</span>, n_runs=<span class="number">5</span>, seed=<span class="number">42</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute score for 2 random uniform cluster labelings.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Both random labelings have the same number of clusters for each value</span></span><br><span class="line"><span class="string">    possible value in ``n_clusters_range``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    When fixed_n_classes is not None the first labeling is considered a ground</span></span><br><span class="line"><span class="string">    truth class assignment with fixed number of classes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    random_labels = np.random.RandomState(seed).randint</span><br><span class="line">    scores = np.zeros((<span class="built_in">len</span>(n_clusters_range), n_runs))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> fixed_n_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        labels_a = random_labels(low=<span class="number">0</span>, high=fixed_n_classes, size=n_samples)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(n_clusters_range):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n_runs):</span><br><span class="line">            <span class="keyword">if</span> fixed_n_classes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                labels_a = random_labels(low=<span class="number">0</span>, high=k, size=n_samples)</span><br><span class="line">            labels_b = random_labels(low=<span class="number">0</span>, high=k, size=n_samples)</span><br><span class="line">            scores[i, j] = score_func(labels_a, labels_b)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ami_score</span>(<span class="params">U, V</span>):</span></span><br><span class="line">    <span class="keyword">return</span> metrics.adjusted_mutual_info_score(U, V)</span><br><span class="line"></span><br><span class="line">score_funcs = [</span><br><span class="line">    metrics.adjusted_rand_score,</span><br><span class="line">    metrics.v_measure_score,</span><br><span class="line">    ami_score,</span><br><span class="line">    metrics.mutual_info_score,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 independent random clusterings with equal cluster number</span></span><br><span class="line"></span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">n_clusters_range = np.linspace(<span class="number">2</span>, n_samples, <span class="number">10</span>).astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plots = []</span><br><span class="line">names = []</span><br><span class="line"><span class="keyword">for</span> score_func <span class="keyword">in</span> score_funcs:</span><br><span class="line">    print(<span class="string">&quot;Computing %s for %d values of n_clusters and n_samples=%d&quot;</span></span><br><span class="line">          % (score_func.__name__, <span class="built_in">len</span>(n_clusters_range), n_samples))</span><br><span class="line"></span><br><span class="line">    t0 = time()</span><br><span class="line">    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)</span><br><span class="line">    print(<span class="string">&quot;done in %0.3fs&quot;</span> % (time() - t0))</span><br><span class="line">    plots.append(plt.errorbar(</span><br><span class="line">        n_clusters_range, np.median(scores, axis=<span class="number">1</span>), scores.std(axis=<span class="number">1</span>))[<span class="number">0</span>])</span><br><span class="line">    names.append(score_func.__name__)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;Clustering measures for 2 random uniform labelings\n&quot;</span></span><br><span class="line">          <span class="string">&quot;with equal number of clusters&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of clusters (Number of samples is fixed to %d)&#x27;</span> % n_samples)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Score value&#x27;</span>)</span><br><span class="line">plt.legend(plots, names)</span><br><span class="line">plt.ylim(bottom=-<span class="number">0.05</span>, top=<span class="number">1.05</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Random labeling with varying n_clusters against ground class labels</span></span><br><span class="line"><span class="comment"># with fixed number of clusters</span></span><br><span class="line"></span><br><span class="line">n_samples = <span class="number">1000</span></span><br><span class="line">n_clusters_range = np.linspace(<span class="number">2</span>, <span class="number">100</span>, <span class="number">10</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">n_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plots = []</span><br><span class="line">names = []</span><br><span class="line"><span class="keyword">for</span> score_func <span class="keyword">in</span> score_funcs:</span><br><span class="line">    print(<span class="string">&quot;Computing %s for %d values of n_clusters and n_samples=%d&quot;</span></span><br><span class="line">          % (score_func.__name__, <span class="built_in">len</span>(n_clusters_range), n_samples))</span><br><span class="line"></span><br><span class="line">    t0 = time()</span><br><span class="line">    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range,</span><br><span class="line">                                      fixed_n_classes=n_classes)</span><br><span class="line">    print(<span class="string">&quot;done in %0.3fs&quot;</span> % (time() - t0))</span><br><span class="line">    plots.append(plt.errorbar(</span><br><span class="line">        n_clusters_range, scores.mean(axis=<span class="number">1</span>), scores.std(axis=<span class="number">1</span>))[<span class="number">0</span>])</span><br><span class="line">    names.append(score_func.__name__)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;Clustering measures for random uniform labeling\n&quot;</span></span><br><span class="line">          <span class="string">&quot;against reference assignment with %d classes&quot;</span> % n_classes)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of clusters (Number of samples is fixed to %d)&#x27;</span> % n_samples)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Score value&#x27;</span>)</span><br><span class="line">plt.ylim(bottom=-<span class="number">0.05</span>, top=<span class="number">1.05</span>)</span><br><span class="line">plt.legend(plots, names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>learn</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>ES教程干货整理</title>
    <url>/2018/07/05/ES%E6%95%99%E7%A8%8B%E5%B9%B2%E8%B4%A7%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<blockquote>
<p>没有什么比官网更加详细了,<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">传送门</a></p>
</blockquote>
<span id="more"></span>

<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Near-Realtime-NRT"><a href="#Near-Realtime-NRT" class="headerlink" title="Near Realtime (NRT)"></a>Near Realtime (NRT)</h3><p>在ES中进行搜索是近实时的,意思是数据从写入ES到可以被searchable仅仅需要1秒钟,因此说基于ES执行的搜索和分析可以达到秒级</p>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p>集群 , 集群是一个或多个node的集合,他们一起保存你存放进去的数据,用户可以在所有的node之间进行检索,一般的每个集群都会有一个唯一的名称标识,默认的名称标识为elasticsearch, 这个名字很重要,因为node想加入cluster时,需要这个名称信息</p>
<p>确保别在不同的环境中使用相同的集群名称,进而避免node加错集群的情况,一颗考虑下面的集群命名风格logging-stage和logging-dev和logging-pro</p>
<h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><p>单台server就是一个node,他和cluster一样,也存在一个默认的名称,但是它的名称是通过UUID生成的随机串,当然用户也可以定制不同的名称,但是这个名字最好别重复,这个名称对于管理来说很在乎要,因为需要确定,当前网络中的哪台服务器,对应这个集群中的哪个节点</p>
<p>node存在一个默认的设置,默认的,当每一个node在启动时都会自动的去加入一个叫elasticsearch的节点,这就意味着,如果用户在网络中启动了多个node,他们会彼此发现,然后组成集群</p>
<p>在单个的cluster中,你可以拥有任意多的node,假如说你的网络上没有有其他正在运行的节点,然后你启动一个新的节点,这个新的节点自己会组件一个集群</p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><p>Index是一类拥有相似属性的document的集合,比如你可以为消费者的数据创建一个index,为产品创建一个index,为订单创建一个index</p>
<p>index名称(必须是小写的字符), 当需要对index中的文档执行索引,搜索,更新,删除,等操作时,都需要用到这个index</p>
<p>一个集群中理论上你可以创建任意数量的index</p>
<h3 id="Type"><a href="#Type" class="headerlink" title="Type"></a>Type</h3><p>Type可以作为index中的逻辑类别,为了更细的划分,比如用户数据type,评论数据type,博客数据type</p>
<p>在设计时,尽最大努力让拥有更多相同field的document会分为同一个type下</p>
<h3 id="Document"><a href="#Document" class="headerlink" title="Document"></a>Document</h3><p>document就是ES中存储的一条数据,就像mysql中的一行记录一样,可以是一条用户的记录,一个商品的记录等等</p>
<h3 id="Shards-amp-Replicas"><a href="#Shards-amp-Replicas" class="headerlink" title="Shards &amp; Replicas"></a>Shards &amp; Replicas</h3><p>elasticsearch6设置索引的默认分片数和副本数已经不是在elasticsearch.yml文件中了，而是使用了一个索引模板的东西。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建索引的时候设置</span><br><span class="line">PUT demo</span><br><span class="line">&#123;</span><br><span class="line">    &quot;settings&quot; : &#123;</span><br><span class="line">        &quot;index&quot; : &#123;</span><br><span class="line">            &quot;number_of_shards&quot; : 3, </span><br><span class="line">            &quot;number_of_replicas&quot; : 2 </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET demo</span><br><span class="line"> </span><br><span class="line"># 在创建Maping的时候设置</span><br><span class="line">PUT demo</span><br><span class="line">&#123;</span><br><span class="line">    &quot;settings&quot; : &#123;</span><br><span class="line">        &quot;number_of_shards&quot; : 1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;mappings&quot; : &#123;</span><br><span class="line">        &quot;type1&quot; : &#123;</span><br><span class="line">            &quot;properties&quot; : &#123;</span><br><span class="line">                &quot;field1&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="入门探索"><a href="#入门探索" class="headerlink" title="入门探索"></a>入门探索</h2><h3 id="集群的健康状况"><a href="#集群的健康状况" class="headerlink" title="集群的健康状况"></a>集群的健康状况</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_cat&#x2F;health?v</span><br><span class="line"></span><br><span class="line">epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent</span><br><span class="line">1572595632 16:07:12  elasticsearch yellow          1         1      5   5    0    0        5             0                  -                 50.0%</span><br></pre></td></tr></table></figure>
<p>解读上面的信息,默认的集群名是elasticsearch,当前集群的status是yellow,后续列出来的是集群的分片信息,最后一个active_shards_percent表示当前集群中仅有一半shard是可用的</p>
<h3 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h3><p>存在三种状态分别是red green yellow</p>
<ul>
<li>green : 表示当前集群所有的节点全部可用</li>
<li>yellow: 表示所有的数据是可以访问的,但是并不是所有的replica shard都是可以使用的(我现在是默认启动一个node,而ES又不允许同一个node的primary shard和replica shard共存,因此我当前的node中仅仅存在5个primary shard,为status为黄色)</li>
<li>red: 集群宕机,数据不可访问</li>
</ul>
<h3 id="集群的索引信息"><a href="#集群的索引信息" class="headerlink" title="集群的索引信息"></a>集群的索引信息</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_cat&#x2F;indices?v</span><br><span class="line"></span><br><span class="line">health status index              uuid                   pri rep docs.count docs.deleted store.size pri.store.size</span><br><span class="line">yellow open   ai_answer_question cl_oJNRPRV-bdBBBLLL05g   5   1     203459            0    172.3mb        172.3mb</span><br></pre></td></tr></table></figure>
<p>显示,状态yellow表示存在replica shard不可用, 存在5个primary shard,并且每一个primary shard都有一个replica shard , 一共20多万条文档,未删除过文档,文档占用的空间情况为172.3M</p>
<h3 id="创建index"><a href="#创建index" class="headerlink" title="创建index"></a>创建index</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT &#x2F;customer?pretty</span><br></pre></td></tr></table></figure>
<p>ES 使用的RestfulAPI,新增使用put,这是个很亲民的举动</p>
<h3 id="添加-or-修改"><a href="#添加-or-修改" class="headerlink" title="添加 or 修改"></a>添加 or 修改</h3><p><strong>如果是ES中没有过下面的数据则添加进去,如果存在了id=1的元素就修改(全量替换)</strong></p>
<ul>
<li>格式:<code>PUT /index/type/id</code></li>
</ul>
<blockquote>
<p><strong>全量替换时,原来的document是没有被删除的,而是被标记为deleted,被标记成的deleted是不会被检索出来的,当ES中数据越来越多时,才会删除它</strong></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT &#x2F;customer&#x2F;_doc&#x2F;1?pretty</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;John Doe&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot;: &quot;customer&quot;,</span><br><span class="line">  &quot;_type&quot;: &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot;: 1,</span><br><span class="line">  &quot;result&quot;: &quot;created&quot;,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 2,</span><br><span class="line">    &quot;successful&quot;: 1,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;_seq_no&quot;: 0,</span><br><span class="line">  &quot;_primary_term&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>强制创建</strong>,加添_create或者?op_type=create</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT &#x2F;customer&#x2F;_doc&#x2F;1?op_type&#x3D;create</span><br><span class="line">PUT &#x2F;customer&#x2F;_doc&#x2F;1&#x2F;_create</span><br></pre></td></tr></table></figure>
<ul>
<li>局部更新(Partial Update)</li>
</ul>
<blockquote>
<p><strong>不指定id则新增document</strong></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;customer&#x2F;_doc?pretty</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Jane Doe&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>指定id则进行doc的局部更新操作</strong></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;customer&#x2F;_doc&#x2F;1?pretty</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;Jane Doe&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>并且POST相对于上面的PUT而言,不论是否存在相同内容的doc,只要不指定id,都会使用一个随机的串当成id,完成doc的插入</strong></p>
</blockquote>
<blockquote>
<p><strong>Partial Update先获取document,再将传递过来的field更新进document的json中,将老的doc标记为deleted,再将创建document,相对于全量替换中间会省去两次网络请求</strong></p>
</blockquote>
<h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><p>格式: <code>GET /index/type/</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;customer&#x2F;_doc&#x2F;1?pretty</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot;: &quot;customer&quot;,</span><br><span class="line">  &quot;_type&quot;: &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot;: 1,</span><br><span class="line">  &quot;found&quot;: true,</span><br><span class="line">  &quot;_source&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;John Doe&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>删除一条document</p>
<blockquote>
<p><strong>大部分情况下,原来的document不会被立即删除,而是被标记为deleted,被标记成的deleted是不会被检索出来的,当ES中数据越来越多时,才会删除它</strong></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELETE &#x2F;customer&#x2F;_doc&#x2F;1</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot;: &quot;customer&quot;,</span><br><span class="line">  &quot;_type&quot;: &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot;: &quot;1&quot;,</span><br><span class="line">  &quot;_version&quot;: 2,</span><br><span class="line">  &quot;result&quot;: &quot;deleted&quot;,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 2,</span><br><span class="line">    &quot;successful&quot;: 1,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;_seq_no&quot;: 1,</span><br><span class="line">  &quot;_primary_term&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>删除index</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DELETE &#x2F;index1</span><br><span class="line">DELETE &#x2F;index1,index2</span><br><span class="line">DELETE &#x2F;index*</span><br><span class="line">DELETE &#x2F;_all</span><br><span class="line"></span><br><span class="line"># 可以在elasticsearch.yml中将下面这个设置置为ture,表示禁止使用 DELETE &#x2F;_all</span><br><span class="line">action.destructive_required_name:true</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;acknowledged&quot;: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="更新文档"><a href="#更新文档" class="headerlink" title="更新文档"></a>更新文档</h3><p>上面说了POST关键字,可以实现不指定id就完成document的插入, <code>POST</code> + <code>_update</code>关键字可以实现更新的操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;customer&#x2F;_doc&#x2F;1&#x2F;_update?pretty</span><br><span class="line">&#123;</span><br><span class="line">  &quot;doc&quot;: &#123; &quot;name&quot;: &quot;changwu&quot; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>POST+_update进行更新的动作依然需要执行id, 但是它相对于PUT来说,当使用POST进行更新时,id不存在的话会报错,而PUT则会认为这是在新增</strong></p>
</blockquote>
<p>此外: 针对这种更新操作,ES会先删除原来的doc,然后插入这个新的doc</p>
<hr>
<h2 id="document-api"><a href="#document-api" class="headerlink" title="document api"></a>document api</h2><h3 id="multi-index-amp-multi-type"><a href="#multi-index-amp-multi-type" class="headerlink" title="multi-index &amp; multi-type"></a>multi-index &amp; multi-type</h3><ul>
<li>检索所有索引下面的所有数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;_search</span><br></pre></td></tr></table></figure>

<ul>
<li>搜索指定索引下的所有数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;index&#x2F;_search</span><br></pre></td></tr></table></figure>

<ul>
<li>更多模式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;index1&#x2F;index2&#x2F;_search</span><br><span class="line">&#x2F;*1&#x2F;*2&#x2F;_search</span><br><span class="line">&#x2F;index1&#x2F;index2&#x2F;type1&#x2F;type2&#x2F;_search</span><br><span class="line">&#x2F;_all&#x2F;type1&#x2F;type2&#x2F;_search</span><br></pre></td></tr></table></figure>

<h3 id="mget-api-批量查询"><a href="#mget-api-批量查询" class="headerlink" title="_mget api 批量查询"></a>_mget api 批量查询</h3><ul>
<li>在docs中指定_index,_type,_id</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_mget</span><br><span class="line">&#123;</span><br><span class="line">    &quot;docs&quot; : [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_index&quot; : &quot;test&quot;,</span><br><span class="line">            &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">            &quot;_id&quot; : &quot;1&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_index&quot; : &quot;test&quot;,</span><br><span class="line">            &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">            &quot;_id&quot; : &quot;2&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>在URL中指定index</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;test&#x2F;_mget</span><br><span class="line">&#123;</span><br><span class="line">    &quot;docs&quot; : [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">            &quot;_id&quot; : &quot;1&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">            &quot;_id&quot; : &quot;2&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>在URL中指定 index和type</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;test&#x2F;type&#x2F;_mget</span><br><span class="line">&#123;</span><br><span class="line">    &quot;docs&quot; : [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_id&quot; : &quot;1&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_id&quot; : &quot;2&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>在URL中指定index和type,并使用ids指定id范围</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;test&#x2F;type&#x2F;_mget</span><br><span class="line">&#123;</span><br><span class="line">    &quot;ids&quot; : [&quot;1&quot;, &quot;2&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>为不同的doc指定不同的过滤规则</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_mget</span><br><span class="line">&#123;</span><br><span class="line">    &quot;docs&quot; : [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_index&quot; : &quot;test&quot;,</span><br><span class="line">            &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">            &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">            &quot;_source&quot; : false</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_index&quot; : &quot;test&quot;,</span><br><span class="line">            &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">            &quot;_id&quot; : &quot;2&quot;,</span><br><span class="line">            &quot;_source&quot; : [&quot;field3&quot;, &quot;field4&quot;]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_index&quot; : &quot;test&quot;,</span><br><span class="line">            &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">            &quot;_id&quot; : &quot;3&quot;,</span><br><span class="line">            &quot;_source&quot; : &#123;</span><br><span class="line">                &quot;include&quot;: [&quot;user&quot;],</span><br><span class="line">                &quot;exclude&quot;: [&quot;user.location&quot;]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="bulk-api-批量增删改"><a href="#bulk-api-批量增删改" class="headerlink" title="_bulk api 批量增删改"></a>_bulk api 批量增删改</h3><h4 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&quot;action&quot;:&#123;&quot;metadata&quot;&#125;&#125;\n</span><br><span class="line">&#123;&quot;data&quot;&#125;\n</span><br></pre></td></tr></table></figure>
<p>存在哪些类型的操作可以执行呢?</p>
<ul>
<li>delete: 删除文档</li>
<li>create: _create 强制创建</li>
<li>index: 表示普通的put操作,可以是创建文档也可以是全量替换文档</li>
<li>update: 局部替换</li>
</ul>
<p><strong>上面的语法中并不是人们习惯阅读的json格式,但是这种单行形式的json更具备高效的优势</strong></p>
<p>ES如何处理普通的json如下:</p>
<ul>
<li>将json数组转换为JSONArray对象,这就意味着内存中会出现一份一模一样的拷贝,一份是json文本,一份是JSONArray对象</li>
</ul>
<p>但是如果上面的单行JSON,ES直接进行切割使用,不会在内存中整一个数据拷贝出来</p>
<h4 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h4><p>delete比较好看仅仅需要一行json就ok</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;</span><br></pre></td></tr></table></figure>

<h4 id="create"><a href="#create" class="headerlink" title="create"></a>create</h4><p>两行json,第一行指明我们要创建的json的index,type以及id<br>第二行指明我们要创建的doc的数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;field1&quot; : &quot;value3&quot; &#125;</span><br></pre></td></tr></table></figure>

<h4 id="index"><a href="#index" class="headerlink" title="index"></a>index</h4><p>相当于是PUT,可以实现新建或者是全量替换,同样是两行json<br>第一行表示将要新建或者是全量替换的json的index type 以及 id<br>第二行是具体的数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;field1&quot; : &quot;value1&quot; &#125;</span><br></pre></td></tr></table></figure>

<h4 id="update"><a href="#update" class="headerlink" title="update"></a>update</h4><p>表示 parcial update,局部替换<br>他可以指定一个<code>retry_on_conflict</code>的特性,表示可以重试3次</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST _bulk</span><br><span class="line">&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_index&quot; : &quot;index1&quot;, &quot;retry_on_conflict&quot; : 3&#125; &#125;</span><br><span class="line">&#123; &quot;doc&quot; : &#123;&quot;field&quot; : &quot;value&quot;&#125; &#125;</span><br><span class="line">&#123; &quot;update&quot; : &#123; &quot;_id&quot; : &quot;0&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_index&quot; : &quot;index1&quot;, &quot;retry_on_conflict&quot; : 3&#125; &#125;</span><br><span class="line">&#123; &quot;script&quot; : &#123; &quot;source&quot;: &quot;ctx._source.counter +&#x3D; params.param1&quot;, &quot;lang&quot; : &quot;painless&quot;, &quot;params&quot; : &#123;&quot;param1&quot; : 1&#125;&#125;, &quot;upsert&quot; : &#123;&quot;counter&quot; : 1&#125;&#125;</span><br><span class="line">&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;2&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_index&quot; : &quot;index1&quot;, &quot;retry_on_conflict&quot; : 3&#125; &#125;</span><br><span class="line">&#123; &quot;doc&quot; : &#123;&quot;field&quot; : &quot;value&quot;&#125;, &quot;doc_as_upsert&quot; : true &#125;</span><br><span class="line">&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;3&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_index&quot; : &quot;index1&quot;, &quot;_source&quot; : true&#125; &#125;</span><br><span class="line">&#123; &quot;doc&quot; : &#123;&quot;field&quot; : &quot;value&quot;&#125; &#125;</span><br><span class="line">&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;4&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_index&quot; : &quot;index1&quot;&#125; &#125;</span><br><span class="line">&#123; &quot;doc&quot; : &#123;&quot;field&quot; : &quot;value&quot;&#125;, &quot;_source&quot;: true&#125;</span><br></pre></td></tr></table></figure>

<h3 id="滚动查询技术"><a href="#滚动查询技术" class="headerlink" title="滚动查询技术"></a>滚动查询技术</h3><p>滚动查询技术和分页技术在使用场景方面还是存在出入的,这里的滚动查询技术同样适用于系统在海量数据中进行检索,比如过一次性存在10条数据被命中可以被检索出来,那么性能一定会很差,这时可以选择使用滚动查询技术,一批一批的查询,直到所有的数据被查询完成他可以先搜索一批数据再搜索一批数据</p>
<p>采用基于_doc的排序方式会获得较高的性能</p>
<p>每次发送scroll请求,我们还需要指定一个scroll参数,指定一个时间窗口,每次搜索只要在这个时间窗口内完成就ok</p>
<p>示例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;index&#x2F;type&#x2F;_search?scroll&#x3D;1m</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;:&#123;</span><br><span class="line">        &quot;match_all&quot;:&#123;&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;sort&quot;:[&quot;_doc&quot;],</span><br><span class="line">    &quot;size&quot;:3</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAACNFlJmWHZLTkFhU0plbzlHX01LU2VzUXcAAAAAAAAAkRZSZlh2S05BYVNKZW85R19NS1Nlc1F3AAAAAAAAAI8WUmZYdktOQWFTSmVvOUdfTUtTZXNRdwAAAAAAAACQFlJmWHZLTkFhU0plbzlHX01LU2VzUXcAAAAAAAAAjhZSZlh2S05BYVNKZW85R19NS1Nlc1F3&quot;,</span><br><span class="line">  &quot;took&quot;: 9,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 5,</span><br><span class="line">    &quot;successful&quot;: 5,</span><br><span class="line">    &quot;skipped&quot;: 0,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 2,</span><br><span class="line">    &quot;max_score&quot;: null,</span><br><span class="line">    &quot;hits&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my_index&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;_doc&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;2&quot;,</span><br><span class="line">        &quot;_score&quot;: null,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;title&quot;: &quot;This is another document&quot;,</span><br><span class="line">          &quot;body&quot;: &quot;This document has a body&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;sort&quot;: [</span><br><span class="line">          0</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my_index&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;_doc&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;1&quot;,</span><br><span class="line">        &quot;_score&quot;: null,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;title&quot;: &quot;This is a document&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;sort&quot;: [</span><br><span class="line">          0</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>再次滚动查询</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;_search&#x2F;scroll</span><br><span class="line">&#123;</span><br><span class="line">    &quot;scroll&quot;:&quot;1m&quot;,</span><br><span class="line">    &quot;_scroll_id&quot;: &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAACNFlJmWHZLTkFhU0plbzlHX01LU2VzUXcAAAAAAAAAkRZSZlh2S05BYVNKZW85R19NS1Nlc1F3AAAAAAAAAI8WUmZYdktOQWFTSmVvOUdfTUtTZXNRdwAAAAAAAACQFlJmWHZLTkFhU0plbzlHX01LU2VzUXcAAAAAAAAAjhZSZlh2S05BYVNKZW85R19NS1Nlc1F3&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="search-api-搜索api"><a href="#search-api-搜索api" class="headerlink" title="_search api 搜索api"></a>_search api 搜索api</h2><h3 id="query-string-search"><a href="#query-string-search" class="headerlink" title="query string search"></a>query string search</h3><p><code>_search</code>API + 将请求写在URI中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;bank&#x2F;_search?q&#x3D;*&amp;sort&#x3D;account_number:asc&amp;pretty</span><br></pre></td></tr></table></figure>
<p>同样使用的是RestfulAPI, <code>q=*</code> ,表示匹配index=bank的下的所有doc,<code>sort=account_number:asc</code>表示告诉ES,结果按照account_number字段升序排序,<code>pretty</code>是告诉ES,返回一个漂亮的json格式的数据</p>
<p>上面的q还可以写成下面这样</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;bank&#x2F;_search?q&#x3D;自定义field:期望的值</span><br><span class="line">GET &#x2F;bank&#x2F;_search?q&#x3D;+自定义field:期望的值</span><br><span class="line">GET &#x2F;bank&#x2F;_search?q&#x3D;-自定义field:期望的值</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;took&quot; : 63,    &#x2F;&#x2F; 耗费的时间</span><br><span class="line">  &quot;timed_out&quot; : false,  &#x2F;&#x2F; 是否超时了</span><br><span class="line">  &quot;_shards&quot; : &#123;   &#x2F;&#x2F; 分片信息</span><br><span class="line">    &quot;total&quot; : 5, &#x2F;&#x2F; 总共5个分片,它的搜索请求会被打到5个分片上去,并且都成功了</span><br><span class="line">    &quot;successful&quot; : 5,  &#x2F;&#x2F; </span><br><span class="line">    &quot;skipped&quot; : 0, &#x2F;&#x2F; 跳过了0个</span><br><span class="line">    &quot;failed&quot; : 0 &#x2F;&#x2F; 失败了0个</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot; : &#123;  &#x2F;&#x2F;命中的情况</span><br><span class="line">    &quot;total&quot; : 1000,  &#x2F;&#x2F; 命中率 1000个</span><br><span class="line">    &quot;max_score&quot; : null,  &#x2F;&#x2F; 相关性得分,越相关就越匹配</span><br><span class="line">    &quot;hits&quot; : [ &#123;   </span><br><span class="line">      &quot;_index&quot; : &quot;bank&quot;,  &#x2F;&#x2F; 索引</span><br><span class="line">      &quot;_type&quot; : &quot;_doc&quot;,   &#x2F;&#x2F; type</span><br><span class="line">      &quot;_id&quot; : &quot;0&quot;,  &#x2F;&#x2F; id </span><br><span class="line">      &quot;sort&quot;: [0], </span><br><span class="line">      &quot;_score&quot; : null, &#x2F;&#x2F; 相关性得分</span><br><span class="line">                    &#x2F;&#x2F; _source里面存放的是数据</span><br><span class="line">      &quot;_source&quot; : &#123;&quot;account_number&quot;:0,&quot;balance&quot;:16623,&quot;firstname&quot;:&quot;Bradshaw&quot;,&quot;lastname&quot;:&quot;Mckenzie&quot;,&quot;age&quot;:29,&quot;gender&quot;:&quot;F&quot;,&quot;address&quot;:&quot;244 Columbus Place&quot;,&quot;employer&quot;:&quot;Euron&quot;,&quot;email&quot;:&quot;bradshawmckenzie@euron.com&quot;,&quot;city&quot;:&quot;Hobucken&quot;,&quot;state&quot;:&quot;CO&quot;&#125;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">      &quot;_index&quot; : &quot;bank&quot;,</span><br><span class="line">      &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">      &quot;_id&quot; : &quot;1&quot;,</span><br><span class="line">      &quot;sort&quot;: [1],</span><br><span class="line">      &quot;_score&quot; : null,</span><br><span class="line">      &quot;_source&quot; : &#123;&quot;account_number&quot;:1,&quot;balance&quot;:39225,&quot;firstname&quot;:&quot;Amber&quot;,&quot;lastname&quot;:&quot;Duke&quot;,&quot;age&quot;:32,&quot;gender&quot;:&quot;M&quot;,&quot;address&quot;:&quot;880 Holmes Lane&quot;,&quot;employer&quot;:&quot;Pyrami&quot;,&quot;email&quot;:&quot;amberduke@pyrami.com&quot;,&quot;city&quot;:&quot;Brogan&quot;,&quot;state&quot;:&quot;IL&quot;&#125;</span><br><span class="line">    &#125;, ...</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>指定超时时间: <code>GET /_search?timeout=10ms</code> 在进行优化时,可以考虑使用timeout, 比如: 正常来说我们可以在10s内获取2000条数据,但是指定了timeout,发生超时后我们可以获取10ms中获取到的 100条数据</p>
<h3 id="query-dsl-domain-specified-language"><a href="#query-dsl-domain-specified-language" class="headerlink" title="query dsl (domain specified language)"></a>query dsl (domain specified language)</h3><p>参见官网 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html">点击进入官网</a><br><code>_search</code>API +将请求写在请求体中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;bank&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, # 查询全部</span><br><span class="line">  &quot;query&quot;: &#123; &quot;match&quot;: &#123;&quot;name&quot;:&quot;changwu zhu&quot;&#125; &#125;, # 全文检索,户将输入的字符串拆解开,去倒排索引中一一匹配, 哪怕匹配上了一个也会将结果返回</span><br><span class="line">  # 实际上,上面的操作会被ES转换成下面的格式</span><br><span class="line">  #</span><br><span class="line">  # &#123;</span><br><span class="line">  #    &quot;bool&quot;:&#123;</span><br><span class="line">  #        &quot;should&quot;:[</span><br><span class="line">  #         &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;changwu&quot;&#125;&#125;,</span><br><span class="line">  #         &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;zhu&quot;&#125;&#125;</span><br><span class="line">  #     ]</span><br><span class="line">  #  &#125;</span><br><span class="line">  # &#125;</span><br><span class="line">  #</span><br><span class="line">   &quot;query&quot;: &#123; </span><br><span class="line">     &quot;match&quot;: &#123; # 手动控制全文检索的精度,</span><br><span class="line">        &quot;name&quot;:&#123;</span><br><span class="line">            &quot;query&quot;:&quot;changwu zhu&quot;,</span><br><span class="line">            &quot;operator&quot;:&quot;and&quot;,  # and表示,只有同时出现changwu zhu 两个词的doc才会被命中</span><br><span class="line">            &quot;minimum_should_match&quot;:&quot;75%&quot; # 去长尾,控制至少命中3&#x2F;4才算是真正命中</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;, # 全文检索,operator 表示</span><br><span class="line">  # 添加上operator 操作会被ES转换成下面的格式,将上面的should转换成must</span><br><span class="line">  #</span><br><span class="line">  # &#123;</span><br><span class="line">  #    &quot;bool&quot;:&#123;</span><br><span class="line">  #        &quot;must&quot;:[</span><br><span class="line">  #         &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;changwu&quot;&#125;&#125;,</span><br><span class="line">  #         &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;zhu&quot;&#125;&#125;</span><br><span class="line">  #     ]</span><br><span class="line">  #  &#125;</span><br><span class="line">  # &#125;</span><br><span class="line">  #</span><br><span class="line">  # 添加上 minimum_should_match 操作会被ES转换成下面的格式 </span><br><span class="line">  #</span><br><span class="line">  # &#123;</span><br><span class="line">  #    &quot;bool&quot;:&#123;</span><br><span class="line">  #        &quot;should&quot;:[</span><br><span class="line">  #         &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;changwu&quot;&#125;&#125;,</span><br><span class="line">  #         &#123;&quot;term&quot;:&#123;&quot;title&quot;:&quot;zhu&quot;&#125;&#125;</span><br><span class="line">  #     ],</span><br><span class="line">  #       &quot;minimum_should_match&quot;:3</span><br><span class="line">  #  &#125;</span><br><span class="line">  # &#125;</span><br><span class="line">  #  </span><br><span class="line">   &quot;query&quot;: &#123; </span><br><span class="line">     &quot;match&quot;: &#123; #控制权重, </span><br><span class="line">        &quot;name&quot;:&#123;</span><br><span class="line">            &quot;query&quot;:&quot;changwu zhu&quot;,</span><br><span class="line">            &quot;boost&quot;:3  # 将name字段的权重提升成3,默认情况下,所有字段的权重都是样的,都是1</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">   &quot;query&quot;: &#123; </span><br><span class="line">   # 这种用法不容忽略</span><br><span class="line">     &quot;dis_max&quot;: &#123; # 直接取下面多个query中得分最高的query当成最终得分</span><br><span class="line">        &quot;queries&quot;:[</span><br><span class="line">           &#123;&quot;match&quot;:&#123;&quot;name&quot;:&quot;changwu zhu&quot;&#125;&#125;,</span><br><span class="line">           &#123;&quot;match&quot;:&#123;&quot;content&quot;:&quot;changwu&quot;&#125;&#125;</span><br><span class="line">        ]</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    </span><br><span class="line">    # best field策略</span><br><span class="line">    &quot;query&quot;: &#123; # 基于 tie_breaker 优化dis_max</span><br><span class="line">    # tie_breaker可以使dis_max考虑其他field的得分影响</span><br><span class="line">       &quot;multi_match&quot;:&#123;</span><br><span class="line">           &quot;query&quot;:&quot;用于去匹配的字段&quot;,</span><br><span class="line">           &quot;type&quot;:&quot;most_fields&quot;,# 指定检索的策略most_fields</span><br><span class="line">           &quot;fields&quot;:[&quot;field1&quot;,&quot;field2&quot;,&quot;field3&quot;]</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    </span><br><span class="line">    # most field 策略, 优先返回命中更多关键词的doc, (忽略从哪个,从多少个field中命中的,只要命中就行)</span><br><span class="line">    &quot;query&quot;: &#123; # 基于 tie_breaker 优化dis_max</span><br><span class="line">    # tie_breaker可以使dis_max考虑其他field的得分影响</span><br><span class="line">     &quot;dis_max&quot;: &#123; # 直接取下面多个query中得分最高的query当成最终得分, 这也是best field策略</span><br><span class="line">        &quot;queries&quot;:[</span><br><span class="line">           &#123;&quot;match&quot;:&#123;&quot;name&quot;:&quot;changwu zhu&quot;&#125;&#125;,</span><br><span class="line">           &#123;&quot;match&quot;:&#123;&quot;content&quot;:&quot;changwu&quot;&#125;&#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;tie_breaker&quot;:0.4</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">  &quot;query&quot;: &#123; &quot;match_none&quot;: &#123;&#125; &#125;</span><br><span class="line">  &quot;query&quot;: &#123; &quot;term&quot;: &#123;&quot;test_field&quot;:&quot;指定值&quot;&#125; &#125; # 精确匹配</span><br><span class="line">  &quot;query&quot;: &#123; &quot;exits&quot;: &#123;&quot;field&quot;:&quot;title&quot;&#125; &#125; # title不为空(但是这时ES2.0中的用法,现在不再提供了)</span><br><span class="line">  &quot;query&quot;: &#123;  # 短语检索 </span><br><span class="line">              # 顺序的保证是通过 term position来保证的</span><br><span class="line">              # 精准度很高,但是召回率低</span><br><span class="line">         &quot;match_phrase&quot;: &#123; # 只有address字段中包含了完整的 mill lane (相连,顺序也不能变) 时,这个doc才算命中</span><br><span class="line">             &quot;address&quot;: &quot;mill lane&quot;</span><br><span class="line">             &#125; </span><br><span class="line">  &#125;,</span><br><span class="line">   &quot;query&quot;: &#123;  # 短语检索 </span><br><span class="line">         &quot;match_phrase&quot;: &#123; </span><br><span class="line">             &quot;address&quot;: &quot;mill lane&quot;,</span><br><span class="line">             # 指定了slop就不再要求搜索term之间必须相邻,而是可以最多间隔slop距离</span><br><span class="line">             # 在指定了slop参数的情况下,离关键词越近,移动的次数越少, relevance score 越高</span><br><span class="line">             # match_phrase +  slop 和 proximity match 近似匹配作用类似</span><br><span class="line">             # 平衡精准度和召回率</span><br><span class="line">             &quot;slop&quot;:1 # 指定搜索文本中的几个term经过几次移动后可以匹配到一个doc</span><br><span class="line">             &#125; </span><br><span class="line">  &#125;,</span><br><span class="line">  </span><br><span class="line">  # 混合使用match和match_phrase 平衡精准度和召回率</span><br><span class="line">   &quot;query&quot;: &#123; </span><br><span class="line">      &quot;bool&quot;: &#123;  </span><br><span class="line">      &quot;must&quot;:  &#123;</span><br><span class="line">          # 全文检索虽然可以匹配到大量的文档,但是它不能控制词条之间的距离</span><br><span class="line">          # 可能java elasticsearch在Adoc中距离很近,但是它却被ES排在结果集的后面</span><br><span class="line">          # 它的性能比match_phrase高10倍,比proximity高20倍</span><br><span class="line">         &quot;match&quot;: &#123;</span><br><span class="line">            &quot;address&quot;: &quot;java elasticsearch&quot; </span><br><span class="line">            &#125; </span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;should&quot;: &#123;</span><br><span class="line">         # 借助match_phrase+slop可以感知term position的功能,为距离相近的doc贡献分数,让它们靠前排列</span><br><span class="line">          &quot;match_phrase&quot;:&#123;</span><br><span class="line">              &quot;title&quot;:&#123;</span><br><span class="line">                  &quot;query&quot;:&quot;java elasticsearch&quot;,</span><br><span class="line">                  &quot;slop&quot;:50</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  </span><br><span class="line">  # 重打分机制</span><br><span class="line">   &quot;query&quot;: &#123; </span><br><span class="line">       &quot;match&quot;:&#123;</span><br><span class="line">           &quot;title&quot;:&#123;</span><br><span class="line">               &quot;query&quot;:&quot;java elasticsearch&quot;,</span><br><span class="line">               &quot;minimum_should_match&quot;:&quot;50%&quot;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;,</span><br><span class="line">       &quot;rescore&quot;:&#123; # 对全文检索的结果进行重新打分</span><br><span class="line">           &quot;window_size&quot;:50,  # 对全文检索的前50条进行重新打分</span><br><span class="line">           &quot;query&quot;: &#123; </span><br><span class="line">               &quot;rescore_query&quot;:&#123; # 关键字</span><br><span class="line">                    &quot;match_phrase&quot;:&#123; # match_phrase + slop 感知 term persition,贡献分数</span><br><span class="line">                       &quot;title&quot;:&#123;</span><br><span class="line">                           &quot;query&quot;:&quot;java elasticsearch&quot;,</span><br><span class="line">                           &quot;slop&quot;:50</span><br><span class="line">                  &#125;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  </span><br><span class="line">  # 前缀匹配, 相对于全文检索,前缀匹配是不会进行分词的,而且每次匹配都会扫描整个倒排索引,直到扫描完一遍才会停下来</span><br><span class="line">  # 不会计算相关性得分,前缀越短拼配到的越多,性能越不好</span><br><span class="line">  &quot;query&quot;: &#123; # 查询多个, 在下面指定的两个字段中检索含有 &#96;this is a test&#96;的doc</span><br><span class="line">    &quot;multi_match&quot; : &#123;</span><br><span class="line">      &quot;query&quot;:    &quot;this is a test&quot;, </span><br><span class="line">      &quot;fields&quot;: [ &quot;subject&quot;, &quot;message&quot; ] </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;query&quot;: &#123; # 前缀搜索,搜索 user字段以ki开头的 doc</span><br><span class="line">    &quot;prefix&quot; : &#123; &quot;user&quot; : &quot;ki&quot; &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;query&quot;: &#123; # 前缀搜索 + 添加权重</span><br><span class="line">    &quot;prefix&quot; : &#123; &quot;user&quot; :  &#123; &quot;value&quot; : &quot;ki&quot;, &quot;boost&quot; : 2.0 &#125; &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  </span><br><span class="line">  # 通配符搜索</span><br><span class="line">   &quot;query&quot;: &#123;</span><br><span class="line">        &quot;wildcard&quot; : &#123; &quot;user&quot; : &quot;ki*y&quot; &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">   &quot;query&quot;: &#123;</span><br><span class="line">        &quot;wildcard&quot; : &#123; &quot;user&quot; : &#123; &quot;value&quot; : &quot;ki*y&quot;, &quot;boost&quot; : 2.0 &#125; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  # 正则搜索</span><br><span class="line">   &quot;query&quot;: &#123;</span><br><span class="line">        &quot;regexp&quot;:&#123;</span><br><span class="line">            &quot;name.first&quot;: &quot;s.*y&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">   &quot;query&quot;: &#123;# 正则搜索</span><br><span class="line">        &quot;regexp&quot;:&#123;</span><br><span class="line">            &quot;name.first&quot;:&#123;</span><br><span class="line">                &quot;value&quot;:&quot;s.*y&quot;,</span><br><span class="line">                &quot;boost&quot;:1.2</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">  # 搜索推荐, 类似于百度,当用户输入一个词条后,将其他符合条件的词条的选项推送出来</span><br><span class="line">  # 原理和match_pharse相似,但是唯一的区别就是会将最后一个term当作前缀去搜索</span><br><span class="line">  # 下例中: 使用quick brown进行match 使用f进行前缀搜索,使用slop调整term persition,贡献得分</span><br><span class="line">   &quot;query&quot;: &#123;</span><br><span class="line">      &quot;match_phrase_prefix&quot; : &#123;# 前缀匹配</span><br><span class="line">        &quot;message&quot; : &#123;</span><br><span class="line">                &quot;query&quot; : &quot;quick brown f&quot;,</span><br><span class="line">                &quot;max_expansions&quot; : 10, # 指定前缀最多匹配多少个term,超过这个数量就不在倒排索引中检索了,提升性能</span><br><span class="line">                &quot;slop&quot;:10</span><br><span class="line">            &#125;</span><br><span class="line">       &#125; </span><br><span class="line">  &#125;,</span><br><span class="line">  # Function Score Query</span><br><span class="line">  # 用户可以自定义一个function_secore 函数,然后将某个field的值和ES计算出来的分数进行运算</span><br><span class="line">  # 最终实现对自己指定的field进行分数的增强功能</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">        &quot;function_score&quot;: &#123;</span><br><span class="line">            &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;,</span><br><span class="line">            &quot;boost&quot;: &quot;5&quot;,</span><br><span class="line">            &quot;random_score&quot;: &#123;&#125;, </span><br><span class="line">            &quot;boost_mode&quot;:&quot;multiply&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, </span><br><span class="line">  </span><br><span class="line">  # Fuzzy Query 模糊查询会提供容错的处理</span><br><span class="line">   &quot;query&quot;: &#123;</span><br><span class="line">        &quot;fuzzy&quot; : &#123;</span><br><span class="line">            &quot;user&quot; : &#123;</span><br><span class="line">                &quot;value&quot;: &quot;ki&quot;,</span><br><span class="line">                &quot;boost&quot;: 1.0,</span><br><span class="line">                &quot;fuzziness&quot;: 2, # 做大的纠错数量</span><br><span class="line">                &quot;prefix_length&quot;: 0,# 不会被“模糊化”的初始字符数。这有助于减少必须检查的术语的数量。默认值为0。</span><br><span class="line">                &quot;max_expansions&quot;: 100 # 模糊查询将扩展到的最大项数。默认值为50</span><br><span class="line">                transpositions:true # 是否支持模糊变换(ab→ba)。默认的是假的</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;  # 布尔查询, 最终通过将它内置must,should等查询的得分加起来&#x2F;should,must的总数, 得到最终的得分</span><br><span class="line">      &quot;must&quot;: [ # 必须匹配到XXX, 并且会得出相关性得分</span><br><span class="line">        &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, # address中必须包含mill</span><br><span class="line">      ],</span><br><span class="line">      # 在满足must的基础上,should条件不满足也可以,但是如果也匹配上了,相关性得分会增加</span><br><span class="line">      # 如果没有must的话,should中的条件必须满足一个</span><br><span class="line">      &quot;should&quot;: [ # 指定可以包含的值, should是可以影响相关性得分的</span><br><span class="line">        &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;must_not&quot;: [ # 一定不包含谁</span><br><span class="line">        &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;,</span><br><span class="line">      ],</span><br><span class="line">      &quot;filter&quot;: &#123; # 对数据进行过滤</span><br><span class="line">        &quot;range&quot;: &#123; # 按照范围过滤</span><br><span class="line">          &quot;balance&quot;: &#123; # 指定过滤的字段</span><br><span class="line">            &quot;gte&quot;: 20000, # 高于20000</span><br><span class="line">            &quot;lte&quot;: 30000  # 低于30000</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>在上面的组合查询中,每一个子查询都会计算一下他的相关性分数,然后由最外层的bool综合合并一个得分,但是 filter是不会计算分数的</strong></p>
</blockquote>
<blockquote>
<p><strong>默认的排序规则是按照score降序排序,但像上面说的那样,如果全部都是filter的话他就不会计算得分,也就是说所有的得分全是1,这时候就需要定制排序规则,定义的语法我在上面写了</strong></p>
</blockquote>
<h3 id="其他辅助API"><a href="#其他辅助API" class="headerlink" title="其他辅助API"></a>其他辅助API</h3><p>比如下面的高亮,排序,分页,以及<code>_source</code>指定需要的字段都可以进一步作用在<code>query</code>的结果上</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;highlight&quot;:&#123; # 高亮显示</span><br><span class="line">  &quot;fields&quot;:&#123;  # 指定高亮的字段</span><br><span class="line">    &quot;balance&quot;:&#123;&#125;</span><br><span class="line">&#125;,</span><br><span class="line">&quot;sort&quot;: [  # 指定排序条件</span><br><span class="line">  &#123; &quot;account_number&quot;: &quot;asc&quot; &#125; # 按照账户余额降序</span><br><span class="line">],</span><br><span class="line">&quot;from&quot;: 0, # 分页</span><br><span class="line">&quot;size&quot;: 10, # 每页的大小4,通过执行size&#x3D;0,可以实现仅显示聚合结果而不显示命中的信息详情</span><br><span class="line">&quot;_source&quot;: [&quot;account_number&quot;, &quot;balance&quot;], # 默认情况下,ES会返回全文JSON,通过_source可以指定返回的字段</span><br></pre></td></tr></table></figure>
<h3 id="聚合分析"><a href="#聚合分析" class="headerlink" title="聚合分析"></a>聚合分析</h3><p><strong>聚合分析是基于doc value这样一个数据结果进行的,前面有说过,这个doc value 其实就是正排索引, 聚合分析就是根据某一个字段进行分组,要求这个字段是不能被分词的,如果被聚合的字段被分词,按照倒排索引的方式去索引的话,就不得不去扫描整个倒排索引(才可能将被聚合的字段找全,效率很低)</strong><br>三个概念:</p>
<ul>
<li>什么是bucket?<br>bucket就是聚合得到的结果</li>
<li>什么是metric?<br>metric就是对bucket进行分析,如最最大值,最小值,平均值</li>
<li>什么是下钻?<br>下钻就是在现有的分好组的bucket继续分组,比如一个先按性别分组,再按年龄分组</li>
</ul>
<p>聚合的关键字: <code>aggs</code> 和 <code>query</code>地位并列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line">  # 使用聚合时,天然存在一个metric,就是当前bucket的count</span><br><span class="line">  &quot;aggs&quot;: &#123; # 聚合</span><br><span class="line">    &quot;group_by_state&quot;: &#123; # 自定义的名字</span><br><span class="line">      &quot;term&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;balance&quot; # 指定聚合的字段, 意思是 group by balance</span><br><span class="line">      &#125;,</span><br><span class="line">       &quot;terms&quot;: &#123; # terms</span><br><span class="line">        &quot;field&quot;: &#123;&quot;value1&quot;,&quot;value2&quot;,&quot;value3&quot;&#125; # 指定聚合的字段, 意思是 group by balance</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,    </span><br><span class="line">  &quot;aggs&quot;: &#123; # 聚合中嵌套聚合</span><br><span class="line">    &quot;group_by_state&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;field1&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123; # 聚合中嵌套聚合</span><br><span class="line">        &quot;average_balance&quot;: &#123;</span><br><span class="line">          &quot;avg&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;field2&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">   &quot;aggs&quot;: &#123; #嵌套聚合,并且使用内部聚合的结果集</span><br><span class="line">    &quot;group_by_state&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;state.keyword&quot;,</span><br><span class="line">        &quot;order&quot;: &#123;</span><br><span class="line">          &quot;average_balance&quot;: &quot;desc&quot; # 使用的下面聚合的结果集</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;average_balance&quot;: &#123;</span><br><span class="line">          &quot;avg&quot;: &#123;  # avg 求平均值  metric</span><br><span class="line">            &quot;field&quot;: &quot;balance&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">         &quot;min_price&quot;: &#123;</span><br><span class="line">          &quot;min&quot;: &#123;  # metric 求最小值</span><br><span class="line">            &quot;field&quot;: &quot;price&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">         &quot;max_price&quot;: &#123;</span><br><span class="line">          &quot;max&quot;: &#123;  # metric 求最大值</span><br><span class="line">            &quot;field&quot;: &quot;price&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">         &quot;sum_price&quot;: &#123;</span><br><span class="line">          &quot;sum&quot;: &#123;  #  metric 计算总和</span><br><span class="line">            &quot;field&quot;: &quot;price&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">   &quot;aggs&quot;: &#123; # 先按照年龄分组,在按照性别分组,再按照平均工资聚合</span><br><span class="line">             # 最终的结果就得到了每个年龄段,每个性别的平均账户余额</span><br><span class="line">    &quot;group_by_age&quot;: &#123;</span><br><span class="line">      &quot;range&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;age&quot;,</span><br><span class="line">        &quot;ranges&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;from&quot;: 20,</span><br><span class="line">            &quot;to&quot;: 30</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;group_by_gender&quot;: &#123;</span><br><span class="line">          &quot;terms&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;gender.keyword&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;aggs&quot;: &#123;</span><br><span class="line">            &quot;average_balance&quot;: &#123;</span><br><span class="line">              &quot;avg&quot;: &#123;</span><br><span class="line">                &quot;field&quot;: &quot;balance&quot;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      # histogram,类似于terms, 同样会进行bucket分组操作,接受一个field,按照这个field的值的各个范围区间进行分组操作</span><br><span class="line">      # 比如我们指定为2000, 它会划分成这样 0-2000  2000-4000  4000-6000 ...</span><br><span class="line">      &quot;aggs&quot;: &#123; # 聚合中嵌套聚合  </span><br><span class="line">         &quot;group_by_price&quot;: &#123;</span><br><span class="line">              &quot;histogram&quot;: &#123;</span><br><span class="line">                 &quot;field&quot;: &quot;price&quot;,</span><br><span class="line">                 &quot;interval&quot;:2000</span><br><span class="line">             &#125;,</span><br><span class="line">         &quot;aggs&quot;: &#123; # 聚合中嵌套聚合</span><br><span class="line">             &quot;average_price&quot;: &#123;</span><br><span class="line">               &quot;avg&quot;: &#123;</span><br><span class="line">                  &quot;field&quot;: &quot;price&quot;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;sales_over_time&quot; : &#123; # 根据日期进行聚合</span><br><span class="line">            &quot;date_histogram&quot; : &#123;</span><br><span class="line">                &quot;field&quot; : &quot;date&quot;,</span><br><span class="line">                &quot;interval&quot; : &quot;1M&quot;,# 一个月为一个跨度</span><br><span class="line">                &quot;format&quot; : &quot;yyyy-MM-dd&quot;,</span><br><span class="line">                &quot;min_doc_count&quot;:0 #即使这个区间中一条数据都没有,这个区间也要返回</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="filter-aggregate"><a href="#filter-aggregate" class="headerlink" title="filter aggregate"></a>filter aggregate</h4><p>过滤加聚合,统计type=t-shirt的平均价格</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;sales&#x2F;_search?size&#x3D;0</span><br><span class="line">&#123;</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;t_shirts&quot; : &#123;</span><br><span class="line">            &quot;filter&quot; : &#123; &quot;term&quot;: &#123; &quot;type&quot;: &quot;t-shirt&quot; &#125; &#125;,</span><br><span class="line">            &quot;aggs&quot; : &#123;</span><br><span class="line">                &quot;avg_price&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="嵌套聚合-广度优先"><a href="#嵌套聚合-广度优先" class="headerlink" title="嵌套聚合-广度优先"></a>嵌套聚合-广度优先</h4><p>说一个应用于场景: 我们检索电影的评论, 但是我们先按照演员分组聚合,在按照评论的数量进行聚合</p>
<p>分析: 如果我们选择深度优先的话, ES在构建演员电影相关信息时,会顺道计算出电影下面评论数的信息,假如说有10万个演员的话, 10万*10=100万个电影 每个电影下又有很多影评,接着处理影评, 就这样内存中可能会存在几百万条数据,但是我们最终就需要50条,这种开销是很大的</p>
<p>广度优先的话,是我们先处理电影数,而不管电影的评论数的聚合情况,先从10万演员中干掉99990条数据,剩下10个演员再聚合</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;aggs&quot;:&#123;</span><br><span class="line">    &quot;target_actors&quot;:&#123;</span><br><span class="line">        &quot;terms&quot;:&#123;</span><br><span class="line">            &quot;field&quot;:&quot;actors&quot;,</span><br><span class="line">            &quot;size&quot;:10,</span><br><span class="line">            &quot;collect_mode&quot;:&quot;breadth_first&quot; # 广度优先</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="global-aggregation"><a href="#global-aggregation" class="headerlink" title="global aggregation"></a>global aggregation</h4><p>全局聚合,下面先使用query进行全文检索,然后进行聚合, 下面的聚合实际上是针对两个不同的结果进行聚合,第一个聚合添加了global关键字,意思是ES中存在的所有doc进行聚合计算得出t-shirt的平均价格</p>
<p>第二个聚合针对全文检索的结果进行聚合</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;sales&#x2F;_search?size&#x3D;0</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot; : &#123;</span><br><span class="line">        &quot;match&quot; : &#123; &quot;type&quot; : &quot;t-shirt&quot; &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;all_products&quot; : &#123;</span><br><span class="line">            &quot;global&quot; : &#123;&#125;, </span><br><span class="line">            &quot;aggs&quot; : &#123; </span><br><span class="line">                &quot;avg_price&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;t_shirts&quot;: &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Cardinality-Aggregate-基数聚合"><a href="#Cardinality-Aggregate-基数聚合" class="headerlink" title="Cardinality Aggregate 基数聚合"></a>Cardinality Aggregate 基数聚合</h4><p>作用类似于<code>count(distcint)</code>,会对每一个bucket中指定的field进行去重,然后取去重后的count</p>
<p>虽然她会存在5%左右的错误率,但是性能特别好</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;sales&#x2F;_search?size&#x3D;0</span><br><span class="line">&#123;</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;type_count&quot; : &#123;</span><br><span class="line">            &quot;cardinality&quot; : &#123; # 关键字</span><br><span class="line">                &quot;field&quot; : &quot;type&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对Cardinality Aggregate的性能优化, 添加 <code>precision_threshold</code> 优化准确率和内存的开销</p>
<p>下面的示例中将<code>precision_threshold</code>的值调整到100意思是当 type的类型小于100时,去重的精准度为100%, 此时内存的占用情况为 100*8=800字节</p>
<p>加入我们将这个值调整为1000,意思是当type的种类在1000个以内时,去重的精准度100%,内存的占用率为1000*8=80KB</p>
<p>官方给出的指标是, 当将<code>precision_threshold</code>设置为5时,错误率会被控制在5%以内</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">POST &#x2F;sales&#x2F;_search?size&#x3D;0</span><br><span class="line">&#123;</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;type_count&quot; : &#123;</span><br><span class="line">            &quot;cardinality&quot; : &#123; # 关键字</span><br><span class="line">                &quot;field&quot; : &quot;type&quot;,</span><br><span class="line">                &quot;precision_threshold&quot;:100</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>进一步优化,Cardinality底层使用的算法是 HyperLogLog++, 可以针对这个算法的特性进行进一步的优化,因为这个算法的底层会对所有的 unique value取hash值,利用这个hash值去近似的求distcint count, 因此我们可以在创建mapping时,将这个hash的求法设置好,添加doc时,一并计算出这个hash值,这样 HyperLogLog++ 就无需再计算hash值,而是直接使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">PUT &#x2F;index&#x2F;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;mappings&quot;:&#123;</span><br><span class="line">        &quot;my_type&quot;:&#123;</span><br><span class="line">            &quot;properties&quot;:&#123;</span><br><span class="line">                &quot;my_field&quot;:&#123;</span><br><span class="line">                    &quot;type&quot;:&quot;text&quot;,</span><br><span class="line">                    &quot;fields&quot;:&#123;</span><br><span class="line">                        &quot;hash&quot;:&#123;</span><br><span class="line">                            &quot;type&quot;:&quot;murmu3&quot;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="控制聚合的升降序"><a href="#控制聚合的升降序" class="headerlink" title="控制聚合的升降序"></a>控制聚合的升降序</h4><p>先按照颜色聚合,在聚合的结果上,再根据价格进行聚合, 最终的结果中,按照价格聚合的分组中升序排序, 这算是个在下转分析时的排序技巧</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET &#x2F;index&#x2F;type&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;:0,</span><br><span class="line">     &quot;aggs&quot;:&#123;</span><br><span class="line">         &quot;group_by_color&quot;:&#123;</span><br><span class="line">             &quot;term&quot;:&#123;</span><br><span class="line">                 &quot;field&quot;:&quot;color&quot;,</span><br><span class="line">                 &quot;order&quot;:&#123; # </span><br><span class="line">                     &quot;avg_price&quot;:&quot;asc&quot;</span><br><span class="line">                 &#125;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;,</span><br><span class="line">         &quot;aggs&quot;:&#123;</span><br><span class="line">             &quot;avg_price&quot;:&#123;</span><br><span class="line">                 &quot;avg&quot;:&#123;</span><br><span class="line">                     &quot;field&quot;:&quot;price&quot;</span><br><span class="line">                 &#125;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Percentiles-Aggregation"><a href="#Percentiles-Aggregation" class="headerlink" title="Percentiles Aggregation"></a>Percentiles Aggregation</h4><p>计算百分比, <strong>常用它计算如,在200ms内成功访问网站的比率,在500ms内成功访问网站的比例,在1000ms内成功访问网站的比例, 或者是销售价为1000元的商品,占总销售量的比例, 销售价为2000元的商品占总销售量的比例等等</strong></p>
<p>示例: 针对doc中的 load_time字段, 计算出在不同百分比下面的 load_time_outliner情况</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET latency&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;load_time_outlier&quot; : &#123;</span><br><span class="line">            &quot;percentiles&quot; : &#123;</span><br><span class="line">                &quot;field&quot; : &quot;load_time&quot; </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 解读: 在百分之50的加载请求中,平均load_time的时间是在445.0, 在99%的请求中,平均加载时间980.1</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">   &quot;aggregations&quot;: &#123;</span><br><span class="line">      &quot;load_time_outlier&quot;: &#123;</span><br><span class="line">         &quot;values&quot; : &#123;</span><br><span class="line">            &quot;1.0&quot;: 9.9,</span><br><span class="line">            &quot;5.0&quot;: 29.500000000000004,</span><br><span class="line">            &quot;25.0&quot;: 167.5,</span><br><span class="line">            &quot;50.0&quot;: 445.0,</span><br><span class="line">            &quot;75.0&quot;: 722.5,</span><br><span class="line">            &quot;95.0&quot;: 940.5,</span><br><span class="line">            &quot;99.0&quot;: 980.1000000000001</span><br><span class="line">         &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 还可以自己指定百分比跨度间隔</span><br><span class="line"></span><br><span class="line">GET latency&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;load_time_outlier&quot; : &#123;</span><br><span class="line">            &quot;percentiles&quot; : &#123;</span><br><span class="line">                &quot;field&quot; : &quot;load_time&quot;,</span><br><span class="line">                &quot;percents&quot; : [95, 99, 99.9] </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>优化: percentile底层使用的是 TDigest算法,用很多个节点执行百分比计算,近似估计,有误差,节点越多,越精准</p>
<p>可以设置<code>compression</code>的值, 默认是100 , ES限制节点的最多是 <code>compression</code>*20 =2000个node去计算 , 因为节点越多,性能就越差</p>
<p>一个节点占用 32字节, 1002032 = 64KB</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET latency&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,</span><br><span class="line">    &quot;aggs&quot; : &#123;</span><br><span class="line">        &quot;load_time_outlier&quot; : &#123;</span><br><span class="line">            &quot;percentiles&quot; : &#123;</span><br><span class="line">                &quot;field&quot; : &quot;load_time&quot;,</span><br><span class="line">                &quot;percents&quot; : [95, 99, 99.9],</span><br><span class="line">                &quot;compression&quot;:100 # 默认值100</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="优化相关性得分"><a href="#优化相关性得分" class="headerlink" title="优化相关性得分"></a>优化相关性得分</h2><ul>
<li>第一种方式:</li>
</ul>
<p>在content字段中全文检索 <code>java elasticsearch</code>时,给title中同时出现<code>java elasticsearch</code>的doc的权重加倍</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;query&quot;: &#123;</span><br><span class="line">      &quot;bool&quot; : &#123;# 前缀匹配</span><br><span class="line">         &quot;match&quot;:&#123;</span><br><span class="line">            &quot;content&quot;:&#123;</span><br><span class="line">                 &quot;query&quot;:&quot;java elasticsearch&quot;</span><br><span class="line">            &#125;</span><br><span class="line">         &#125;,</span><br><span class="line">         &quot;should&quot;:[</span><br><span class="line">             &quot;match&quot;:&#123;</span><br><span class="line">                 &quot;title&quot;:&#123;</span><br><span class="line">                     &quot;query&quot;:&quot;java elasticsearch&quot;,</span><br><span class="line">                     &quot;boost&quot;:2</span><br><span class="line">                 &#125;</span><br><span class="line">             &#125;</span><br><span class="line">         ]</span><br><span class="line">       &#125; </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>第二种: 更换写法,改变占用的权重比例</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET my_index&#x2F;_doc&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">     &quot;should&quot;:[</span><br><span class="line">      &#123; &quot;match&quot;:&#123;&quot;title&quot;:&quot;this is&quot;&#125;&#125;,  # 1&#x2F;3</span><br><span class="line">      &#123; &quot;match&quot;:&#123;&quot;title&quot;:&quot;this is&quot;&#125;&#125;,  # 1&#x2F;3</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;bool&quot;:&#123;</span><br><span class="line">         &quot;should&quot;:[</span><br><span class="line">           &#123;&quot;match&quot;:&#123;&quot;title&quot;:&quot;this is&quot;&#125;&#125;, # 1&#x2F;6</span><br><span class="line">           &#123;&quot;match&quot;:&#123;&quot;title&quot;:&quot;this is&quot;&#125;&#125;  # 1&#x2F;6</span><br><span class="line">           ]</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   ] </span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>第三种: 如果不希望使用相关性得分,使用下面的语法</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET my_index&#x2F;_doc&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;constant_score&quot; : &#123;</span><br><span class="line">            &quot;filter&quot; : &#123;</span><br><span class="line">              &quot;term&quot; : &#123; &quot;title&quot; : &quot;this&quot;&#125; #</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;boost&quot; : 1.2</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>第四种: 灵活的查询</li>
</ul>
<p>查询必须包含XXX,必须不包含YYY的doc</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET my_index&#x2F;_doc&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;must&quot;:&#123;</span><br><span class="line">        &quot;match&quot;:&#123;</span><br><span class="line">          &quot;title&quot;:&quot;this is a &quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;must_not&quot;:&#123;</span><br><span class="line">        &quot;match&quot;:&#123;</span><br><span class="line">           &quot;title&quot;:&quot;another&quot;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>第五种: 查询必须包含XXX,可以包含YYY,但是包含了YYY后它的权重就会减少指定的值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">GET my_index&#x2F;_doc&#x2F;_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;:&#123;</span><br><span class="line">    &quot;boosting&quot;: &#123;</span><br><span class="line">      &quot;positive&quot;:&#123;</span><br><span class="line">        &quot;match&quot;:&#123;</span><br><span class="line">          &quot;title&quot;:&quot;this is a &quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;negative&quot;:&#123;</span><br><span class="line">        &quot;match&quot;:&#123;</span><br><span class="line">           &quot;title&quot;:&quot;another&quot;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;,</span><br><span class="line">       &quot;negative_boost&quot;: 0.2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>第六种: 重打分机制</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;query&quot;: &#123; </span><br><span class="line">   &quot;match&quot;:&#123;</span><br><span class="line">       &quot;title&quot;:&#123;</span><br><span class="line">           &quot;query&quot;:&quot;java elasticsearch&quot;,</span><br><span class="line">           &quot;minimum_should_match&quot;:&quot;50%&quot;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;,</span><br><span class="line">   &quot;rescore&quot;:&#123; # 对全文检索的结果进行重新打分</span><br><span class="line">       &quot;window_size&quot;:50,  # 对全文检索的前50条进行重新打分</span><br><span class="line">       &quot;query&quot;: &#123; </span><br><span class="line">           &quot;rescore_query&quot;:&#123; # 关键字</span><br><span class="line">                &quot;match_phrase&quot;:&#123; # match_phrase + slop 感知 term persition,贡献分数</span><br><span class="line">                   &quot;title&quot;:&#123;</span><br><span class="line">                       &quot;query&quot;:&quot;java elasticsearch&quot;,</span><br><span class="line">                       &quot;slop&quot;:50</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>第七种: 混用match和match_phrase提高召回率</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;query&quot;: &#123; </span><br><span class="line">  &quot;bool&quot;: &#123;  </span><br><span class="line">  &quot;must&quot;:  &#123;</span><br><span class="line">      # 全文检索虽然可以匹配到大量的文档,但是它不能控制词条之间的距离</span><br><span class="line">      # 可能java elasticsearch在Adoc中距离很近,但是它却被ES排在结果集的后面</span><br><span class="line">      # 它的性能比match_phrase高10倍,比proximity高20倍</span><br><span class="line">     &quot;match&quot;: &#123;</span><br><span class="line">        &quot;address&quot;: &quot;java elasticsearch&quot; </span><br><span class="line">        &#125; </span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;should&quot;: &#123;</span><br><span class="line">     # 借助match_phrase+slop可以感知term position的功能,为距离相近的doc贡献分数,让它们靠前排列</span><br><span class="line">      &quot;match_phrase&quot;:&#123;</span><br><span class="line">          &quot;title&quot;:&#123;</span><br><span class="line">              &quot;query&quot;:&quot;java elasticsearch&quot;,</span><br><span class="line">              &quot;slop&quot;:50</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink系统配置参数一览</title>
    <url>/2020/12/14/Flink%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E4%B8%80%E8%A7%88/</url>
    <content><![CDATA[<blockquote>
<p>慢慢肝,整理下Flink的系统配置信息,不同于环境配置<a href="https://jxeditor.github.io/2020/04/21/Flink%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E4%B8%AD%E7%9A%84%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">Flink代码编写中的环境配置</a></p>
</blockquote>
<span id="more"></span>

<h2 id="AkkaOptions-Akka配置参数"><a href="#AkkaOptions-Akka配置参数" class="headerlink" title="AkkaOptions(Akka配置参数)"></a>AkkaOptions(Akka配置参数)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">akka.ask.callstack</span><br><span class="line">默认值:true</span><br><span class="line">捕获异步请求的调用堆栈,当ASK失败时,得到一个适当的异常,描述原始方法调度.</span><br><span class="line"></span><br><span class="line">akka.ask.timeout</span><br><span class="line">默认值:10 s</span><br><span class="line">Akka超时时间,Flink出现超时失败,可以增加该值.</span><br><span class="line"></span><br><span class="line">akka.tcp.timeout</span><br><span class="line">默认值:20 s</span><br><span class="line">TCP超时时间,由于网络问题导致Flink失败,可以增加该值.</span><br><span class="line"></span><br><span class="line">akka.startup-timeout</span><br><span class="line">默认值:无</span><br><span class="line">Akka启动超时时间.</span><br><span class="line"></span><br><span class="line">akka.transport.heartbeat.interval</span><br><span class="line">默认值:1000 s</span><br><span class="line">Akka传输故障探测器的心跳间隔.Flink使用TCP,所以不需要检测器,可以设置一个极大值来禁用检测器.</span><br><span class="line"></span><br><span class="line">akka.transport.heartbeat.pause</span><br><span class="line">默认值:6000 s</span><br><span class="line">Akka传输故障探测器可接受的心跳暂停时间间隔,同上.</span><br><span class="line"></span><br><span class="line">akka.transport.threshold</span><br><span class="line">默认值:300.0</span><br><span class="line">Akka传输故障探测器的阈值,同上.</span><br><span class="line"></span><br><span class="line">akka.ssl.enabled</span><br><span class="line">默认值:true</span><br><span class="line">Akka远程通信是否打开SSL.仅适用于全局SSL标志security.ssl设置为true.</span><br><span class="line"></span><br><span class="line">akka.framesize</span><br><span class="line">默认值:10485760b</span><br><span class="line">在JM和TM之间发送的消息最大大小.如果Flink失败是因为消息超过了这个限制,则加大它.</span><br><span class="line"></span><br><span class="line">akka.throughput</span><br><span class="line">默认值:15</span><br><span class="line">将线程返回池之前批处理的消息数.低值表示公平调度,高值可以以不公平为代价提高性能.</span><br><span class="line"></span><br><span class="line">akka.log.lifecycle.events</span><br><span class="line">默认值:false</span><br><span class="line">打开Akka的远程事件日志记录.调试的时候此值设为true.</span><br><span class="line"></span><br><span class="line">akka.lookup.timeout</span><br><span class="line">默认值:10 s</span><br><span class="line">用于查找TM的超时时间.</span><br><span class="line"></span><br><span class="line">akka.client.timeout</span><br><span class="line">默认值:60 s</span><br><span class="line">不推荐使用.使用client.timeout替代.</span><br><span class="line"></span><br><span class="line">akka.jvm-exit-on-fatal-error</span><br><span class="line">默认值:true</span><br><span class="line">出现致命的Akka错误时退出JVM.</span><br><span class="line"></span><br><span class="line">akka.retry-gate-closed-for</span><br><span class="line">默认值:50L</span><br><span class="line">断开远程连接后,Gate应该在时间范围关闭(毫秒).</span><br><span class="line"></span><br><span class="line">akka.fork-join-executor.parallelism-factor</span><br><span class="line">默认值:2.0</span><br><span class="line">并行系数用于使用以下公式确定线程池大小(ceil(可用处理器*并行系数)),结果大小由akka.fork-join-executor.parallelism-min和akka.fork-join-executor.parallelism-max控制.</span><br><span class="line"></span><br><span class="line">akka.fork-join-executor.parallelism-min</span><br><span class="line">默认值:8</span><br><span class="line">基于并行数的上限因子的最小线程数.</span><br><span class="line"></span><br><span class="line">akka.fork-join-executor.parallelism-max</span><br><span class="line">默认值:64</span><br><span class="line">基于并行数的上限因子的最大线程数.</span><br><span class="line"></span><br><span class="line">akka.client-socket-worker-pool.pool-size-min</span><br><span class="line">默认值:1</span><br><span class="line">基于数量因子的最小线程数.</span><br><span class="line"></span><br><span class="line">akka.client-socket-worker-pool.pool-size-max</span><br><span class="line">默认值:2</span><br><span class="line">基于数量因子的最大线程数.</span><br><span class="line"></span><br><span class="line">akka.client-socket-worker-pool.pool-size-factor</span><br><span class="line">默认值:1.0</span><br><span class="line">线程池大小用于使用以下公式确定线程池大小(ceil(可用处理器*因子)),结果大小由akka.client-socket-worker-pool.pool-size-min和akka.client-socket-worker-pool.pool-size-max控制.</span><br><span class="line"></span><br><span class="line">akka.server-socket-worker-pool.pool-size-min</span><br><span class="line">默认值:1</span><br><span class="line">基于数量因子的最小线程数.</span><br><span class="line"></span><br><span class="line">akka.server-socket-worker-pool.pool-size-max</span><br><span class="line">默认值:2</span><br><span class="line">基于数量因子的最大线程数.</span><br><span class="line"></span><br><span class="line">akka.server-socket-worker-pool.pool-size-factor</span><br><span class="line">默认值:1.0</span><br><span class="line">线程池大小用于使用以下公式确定线程池大小(ceil(可用处理器*因子)),结果大小由akka.server-socket-worker-pool.pool-size-min和akka.server-socket-worker-pool.pool-size-max控制.</span><br><span class="line"></span><br><span class="line">--- 过时的配置,对Flink没有影响</span><br><span class="line">akka.watch.heartbeat.interval</span><br><span class="line">akka.watch.heartbeat.pause</span><br><span class="line">akka.watch.threshold</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="AlgorithmOptions-Join-Sort算法的配置参数"><a href="#AlgorithmOptions-Join-Sort算法的配置参数" class="headerlink" title="AlgorithmOptions(Join/Sort算法的配置参数)"></a>AlgorithmOptions(Join/Sort算法的配置参数)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">taskmanager.runtime.hashjoin-bloom-filters</span><br><span class="line">默认值:false</span><br><span class="line">在HybridHashJoin实现中激活&#x2F;停用bloom过滤器的标志.在HashJoin需要溢写到磁盘时,这些bloom过滤器可以极大的减少溢写记录的数量,牺牲CPU性能.</span><br><span class="line"></span><br><span class="line">taskmanager.runtime.max-fan</span><br><span class="line">默认值:128</span><br><span class="line">外部合并Join的最大扇入和溢写Hash Table的扇出.限制每个运算符的文件句柄数,设置过小会导致中间合并或分区.</span><br><span class="line"></span><br><span class="line">taskmanager.runtime.sort-spilling-threshold</span><br><span class="line">默认值:0.8f</span><br><span class="line">当内存预算的这一部分已满时,排序操作开始溢写.</span><br><span class="line"></span><br><span class="line">taskmanager.runtime.large-record-handler</span><br><span class="line">默认值:false</span><br><span class="line">溢写时是否使用LargeRecordHandler.如果一个记录不能放入排序缓冲区.记录将溢写到磁盘上,并且只使用key继续排序.合并后读取记录本身.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="BlobServerOptions-BlobServer和BlobCache参数配置"><a href="#BlobServerOptions-BlobServer和BlobCache参数配置" class="headerlink" title="BlobServerOptions(BlobServer和BlobCache参数配置)"></a>BlobServerOptions(BlobServer和BlobCache参数配置)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blob.storage.directory</span><br><span class="line">默认值:无</span><br><span class="line">定义BlobServer要是用的存储目录.</span><br><span class="line"></span><br><span class="line">blob.fetch.retries</span><br><span class="line">默认值:5</span><br><span class="line">定义失败Blob获取的失效次数.</span><br><span class="line"></span><br><span class="line">blob.fetch.num-concurrent</span><br><span class="line">默认值:50</span><br><span class="line">定义JM的最大并发Blob获取数.</span><br><span class="line"></span><br><span class="line">blob.fetch.backlog</span><br><span class="line">默认值:1000</span><br><span class="line">在JM上定义所需的Blob获取backlog的参数.操作系统通常会根据SOMAXCONN设置对backlog大小实施一个上限.</span><br><span class="line"></span><br><span class="line">blob.server.port</span><br><span class="line">默认值:0</span><br><span class="line">BlobServer的服务器端口.</span><br><span class="line"></span><br><span class="line">blob.service.ssl.enabled</span><br><span class="line">默认值:true</span><br><span class="line">覆盖BlobServer传输的SSL支持标志.</span><br><span class="line"></span><br><span class="line">blob.service.cleanup.interval</span><br><span class="line">默认值:3_600L</span><br><span class="line">TM中BlobCache清理时间间隔(秒).</span><br><span class="line"></span><br><span class="line">blob.offload.minsize</span><br><span class="line">默认值:1_024 * 1_024</span><br><span class="line">Offload到BlobServer的消息最小大小.</span><br><span class="line"></span><br><span class="line">blob.client.socket.timeout</span><br><span class="line">默认值:300_000</span><br><span class="line">Blob客户端Socket超时时间间隔(毫秒).</span><br><span class="line"></span><br><span class="line">blob.client.connect.timeout</span><br><span class="line">默认值:0</span><br><span class="line">Blob客户端连接超时时间间隔(毫秒).</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="CheckpointingOptions-CK和SP的配置参数"><a href="#CheckpointingOptions-CK和SP的配置参数" class="headerlink" title="CheckpointingOptions(CK和SP的配置参数)"></a>CheckpointingOptions(CK和SP的配置参数)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">state.backend</span><br><span class="line">默认值:无</span><br><span class="line">用于存储CK状态的状态后端.</span><br><span class="line"></span><br><span class="line">state.checkpoints.num-retained</span><br><span class="line">默认值:1</span><br><span class="line">要保留的已完成CK的最大数目.</span><br><span class="line"></span><br><span class="line">state.backend.async</span><br><span class="line">默认值:true</span><br><span class="line">状态后端是否使用异步快照,某些状态后端不支持异步快照或只支持异步快照.</span><br><span class="line"></span><br><span class="line">state.backend.incremental</span><br><span class="line">默认值:false</span><br><span class="line">状态后端是否创建增量CK.对于增量CK,只存储上一个CK的差异,并不是完整的CK状态.</span><br><span class="line">启用后,WebUI显示或从RestAPI获取的状态大小仅表示增量CK的大小,不是完整的CK大小.</span><br><span class="line"></span><br><span class="line">state.backend.local-recovery</span><br><span class="line">默认值:false</span><br><span class="line">是否启动状态后端的本地恢复.本地恢复目前只支持KeyedStateBackend,MemoryStateBackend不支持本地恢复.</span><br><span class="line"></span><br><span class="line">taskmanager.state.local.root-dirs</span><br><span class="line">默认值:无</span><br><span class="line">用于存储本地恢复状态的目录.</span><br><span class="line"></span><br><span class="line">state.savepoints.dir</span><br><span class="line">默认值:无</span><br><span class="line">SP的默认目录.用于将SP写入文件系统的状态后端(MemoryStateBackend,FsStateBackend,RocksDBStateBackend).</span><br><span class="line"></span><br><span class="line">state.checkpoints.dir</span><br><span class="line">默认值:无</span><br><span class="line">Flink支持的文件系统中存储CK的数据文件和元数据目录.路径必须可让所有参与者(TM&#x2F;JM)访问.</span><br><span class="line"></span><br><span class="line">state.backend.fs.memory-threshold</span><br><span class="line">默认值:20kb</span><br><span class="line">状态数据文件的最小大小.所有小于该值的状态块都以内联方式存储在根检查点元数据文件中,此配置的最大内存阈值为1MB。</span><br><span class="line"></span><br><span class="line">state.backend.fs.write-buffer-size</span><br><span class="line">默认值:4 * 1024</span><br><span class="line">写入文件系统的检查点流的默认写入缓冲区大小.实际写入缓冲区大小被确定为此选项和选项state.backend.fs.memory-threshold的最大值。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ClusterOptions-控制集群行为的配置"><a href="#ClusterOptions-控制集群行为的配置" class="headerlink" title="ClusterOptions(控制集群行为的配置)"></a>ClusterOptions(控制集群行为的配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">cluster.registration.initial-timeout</span><br><span class="line">默认值:100L</span><br><span class="line">群集组件之间的初始注册超时(毫秒).</span><br><span class="line"></span><br><span class="line">cluster.registration.max-timeout</span><br><span class="line">默认值:30000L</span><br><span class="line">群集组件之间的最大注册超时(毫秒).</span><br><span class="line"></span><br><span class="line">cluster.registration.error-delay</span><br><span class="line">默认值:10000L</span><br><span class="line">注册尝试后进行的暂停导致了毫秒内的异常(超时除外).</span><br><span class="line"></span><br><span class="line">cluster.registration.refused-registration-delay</span><br><span class="line">默认值:30000L</span><br><span class="line">注册尝试被拒绝后暂停时间间隔(毫秒).</span><br><span class="line"></span><br><span class="line">cluster.services.shutdown-timeout</span><br><span class="line">默认值:30000L</span><br><span class="line">Executors等待群集服务的关闭超时(毫秒).</span><br><span class="line"></span><br><span class="line">cluster.io-pool.size</span><br><span class="line">默认值:无</span><br><span class="line">集群用于执行阻塞IO操作(主进程和TaskManager进程)的IO执行器池的大小.</span><br><span class="line">默认情况下,它将使用4*集群进程可以访问的CPU cores数量.增加池大小允许同时运行更多IO操作.</span><br><span class="line"></span><br><span class="line">cluster.evenly-spread-out-slots</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">启用slot展开分配策略.此策略尝试在所有可用的TaskExecutors上均匀分布slot。</span><br><span class="line"></span><br><span class="line">cluster.processes.halt-on-fatal-error</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">进程是否应在出现致命错误时停止,而不是执行正常关闭.在某些环境中(例如带有G1垃圾收集器的java8),正常的关闭可能会导致JVM死锁.</span><br><span class="line"></span><br><span class="line">cluster.declarative-resource-management.enabled</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">定义群集是否使用声明性资源管理.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="CoreOptions-核心配置"><a href="#CoreOptions-核心配置" class="headerlink" title="CoreOptions(核心配置)"></a>CoreOptions(核心配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">classloader.resolve-order</span><br><span class="line">默认值:child-first</span><br><span class="line">定义从用户代码加载类时的类解析策略.</span><br><span class="line">child-first:检查用户代码jar.</span><br><span class="line">parent_first:应用程序路径.</span><br><span class="line"></span><br><span class="line">classloader.parent-first-patterns.default</span><br><span class="line">默认值:java.;scala.;org.apache.flink.;com.esotericsoftware.kryo;org.apache.hadoop.;javax.annotation.;org.slf4j;org.apache.log4j;org.apache.logging;org.apache.commons.logging;ch.qos.logback;org.xml;javax.xml;org.apache.xerces;org.w3c</span><br><span class="line">一个分号分割的正则列表,指定哪些类总是首先通过父类加载器解析.不建议修改,要添加另一个模式,建议使用classloader.parent-first-patterns.additional替代.</span><br><span class="line"></span><br><span class="line">classloader.parent-first-patterns.additional</span><br><span class="line">默认值:空字符串</span><br><span class="line">一个分号分割的正则列表,指定哪些类总是首先通过父类加载器解析.</span><br><span class="line"></span><br><span class="line">classloader.fail-on-metaspace-oom-error</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">如果在尝试加载用户代码类时抛出<span class="string">&quot;OutOfMemoryError:Metaspace&quot;</span>,则Flink JVM进程失败.</span><br><span class="line"></span><br><span class="line">classloader.check-leaked-classloader</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">如果作业的用户类加载器在作业终止后使用,则尝试加载类失败.</span><br><span class="line">这通常是由于延迟线程或行为不当的库泄漏了类加载器,这也可能导致类加载器被其他作业使用.只有当这种泄漏阻止进一步的作业运行时,才应禁用此检查.</span><br><span class="line"></span><br><span class="line">plugin.classloader.parent-first-patterns.default</span><br><span class="line">默认值:java.;scala.;org.apache.flink.;javax.annotation.;org.slf4j;org.apache.log4j;org.apache.logging;org.apache.commons.logging;ch.qos.logback</span><br><span class="line">插件父类加载器</span><br><span class="line"></span><br><span class="line">plugin.classloader.parent-first-patterns.additional</span><br><span class="line">默认值:空字符串</span><br><span class="line">插件父类加载器</span><br><span class="line"></span><br><span class="line">env.java.opts</span><br><span class="line">默认值:空字符串</span><br><span class="line">Java选项启动所有Flink进程的JVM.</span><br><span class="line"></span><br><span class="line">env.java.opts.jobmanager</span><br><span class="line">默认值:空字符串</span><br><span class="line">用于启动JobManager的JVM的Java选项.</span><br><span class="line"></span><br><span class="line">env.java.opts.taskmanager</span><br><span class="line">默认值:空字符串</span><br><span class="line">用于启动TaskManager的JVM的Java选项.</span><br><span class="line"></span><br><span class="line">env.java.opts.historyserver</span><br><span class="line">默认值:空字符串</span><br><span class="line">用于启动HistoryServer的JVM的Java选项.</span><br><span class="line"></span><br><span class="line">env.java.opts.client</span><br><span class="line">默认值:空字符串</span><br><span class="line">启动Flink Client的JVM的Java选项.</span><br><span class="line"></span><br><span class="line">env.log.dir</span><br><span class="line">默认值:无</span><br><span class="line">定义保存Flink日志的目录.它必须是一条绝对路径.(默认为Flink根目录下的<span class="built_in">log</span>目录)</span><br><span class="line"></span><br><span class="line">env.pid.dir</span><br><span class="line">默认值:/tmp</span><br><span class="line">定义保存flink-&lt;host&gt;-&lt;process&gt;.pid文件的目录.</span><br><span class="line"></span><br><span class="line">env.log.max</span><br><span class="line">默认值:5</span><br><span class="line">要保留的最大旧日志文件数.</span><br><span class="line"></span><br><span class="line">env.ssh.opts</span><br><span class="line">默认值:无</span><br><span class="line">启动或停止JobManager,TaskManager和Zookeeper服务时传递给SSH客户端的其他命令行选项(start-cluster.sh,stop-cluster.sh,start-zookeeper-quorum.sh,stop-zookeeper-quorum.sh).</span><br><span class="line"></span><br><span class="line">env.hadoop.conf.dir</span><br><span class="line">默认值:无</span><br><span class="line">Hadoop配置目录的路径.需要读取HDFS和Yarn配置.也可以通过环境变量进行设置.</span><br><span class="line"></span><br><span class="line">env.yarn.conf.dir</span><br><span class="line">默认值:无</span><br><span class="line">Yarn配置目录的路径.Flink on Yarn时是必要的.也可以通过环境变量进行设置.</span><br><span class="line"></span><br><span class="line">env.hbase.conf.dir</span><br><span class="line">默认值:无</span><br><span class="line">HBase配置目录的路径.需要读取HBase配置.也可以通过环境变量进行设置.</span><br><span class="line"></span><br><span class="line">io.tmp.dirs</span><br><span class="line">默认值:System.getProperty(<span class="string">&quot;java.io.tmpdir&quot;</span>)</span><br><span class="line">临时文件目录.</span><br><span class="line"></span><br><span class="line">parallelism.default</span><br><span class="line">默认值:1</span><br><span class="line">任务默认并行度.</span><br><span class="line"></span><br><span class="line">fs.default-scheme</span><br><span class="line">默认值:无</span><br><span class="line">默认文件系统Schema.</span><br><span class="line"></span><br><span class="line">fs.allowed-fallback-filesystems</span><br><span class="line">默认值:空字符串</span><br><span class="line">文件允许的Schema列表,可以使用Hadoop代替合适的Flink插件(S3,Wasb).</span><br><span class="line"></span><br><span class="line">fs.overwrite-files</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">指定默认情况下文件输出写入程序是否应覆盖现有文件.</span><br><span class="line"></span><br><span class="line">fs.output.always-create-directory</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">以大于1的并行度运行的文件编写器为输出文件路径创建一个目录,并将不同的结果文件(每个并行编写器任务一个)放入该目录.</span><br><span class="line">如果此选项设置为<span class="string">&quot;true&quot;</span>,则并行度为1的写入程序还将创建一个目录并将单个结果文件放入其中.</span><br><span class="line">如果该选项设置为<span class="string">&quot;false&quot;</span>,写入程序将直接在输出路径中创建文件,而不创建包含目录.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="DeploymentOptions-Executor配置"><a href="#DeploymentOptions-Executor配置" class="headerlink" title="DeploymentOptions(Executor配置)"></a>DeploymentOptions(Executor配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">execution.target</span><br><span class="line">默认值:无</span><br><span class="line">执行的部署目标.这可以采用以下值之一(remote,<span class="built_in">local</span>,yarn-per-job,yarn-session,kubernetes-session)</span><br><span class="line"></span><br><span class="line">execution.attached</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">指定Pipeline是以attached模式还是detached模式提交.</span><br><span class="line"></span><br><span class="line">execution.shutdown-on-attached-exit</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">如果作业是在attached模式下提交的,在CLI突然终止时执行群集关闭,例如响应用户中断,例如键入Ctrl+C.</span><br><span class="line"></span><br><span class="line">execution.job-listeners</span><br><span class="line">默认值:无</span><br><span class="line">要在执行环境中注册的自定义JobListener.注册的侦听器不能有带参数的构造函数.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ExecutionOptions-特定程序的单个Executor配置"><a href="#ExecutionOptions-特定程序的单个Executor配置" class="headerlink" title="ExecutionOptions(特定程序的单个Executor配置)"></a>ExecutionOptions(特定程序的单个Executor配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">execution.runtime-mode</span><br><span class="line">默认值:RuntimeExecutionMode.STREAMING</span><br><span class="line">DataStream运行时执行模式.除此之外,它还控制任务调度,网络Shuffle行为和时间语义(STREAMING,BATCH,AUTOMATIC).</span><br><span class="line"></span><br><span class="line">execution.checkpointing.snapshot-compression</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">是否应该对状态快照数据使用压缩.</span><br><span class="line"></span><br><span class="line">execution.buffer-timeout</span><br><span class="line">默认值:Duration.ofMillis(100)</span><br><span class="line">刷新缓冲区的最大时间(毫秒).默认情况下,帮助开发人员平滑输出缓冲区.</span><br><span class="line">设置参数会导致三种逻辑模式:</span><br><span class="line">    正值按该间隔周期性地触发刷新</span><br><span class="line">    0 在每个记录之后触发刷新,从而最小化延迟</span><br><span class="line">    -1 仅在输出缓冲区已满时触发刷新,从而最大限度地提高吞吐量.</span><br><span class="line"></span><br><span class="line">execution.sorted-inputs.enabled</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">启用或禁用键控运算符的排序输入的标志.只在Batch模式下生效.</span><br><span class="line"></span><br><span class="line">execution.batch-state-backend.enabled</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">为键控运算符启用或禁用批处理运行时特定状态后端和计时器服务的标志.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ExternalResourceOptions-外部资源配置"><a href="#ExternalResourceOptions-外部资源配置" class="headerlink" title="ExternalResourceOptions(外部资源配置)"></a>ExternalResourceOptions(外部资源配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">external-resources</span><br><span class="line">默认值:无</span><br><span class="line">所有外部资源的&lt;resource_name&gt;列表,分号分割,例如:gpu;fpga.</span><br><span class="line"></span><br><span class="line">external-resource.gpu.driver-factory.class</span><br><span class="line">默认值:无</span><br><span class="line">定义由&lt;resource_name&gt;标识的外部资源的工厂类名.</span><br><span class="line">工厂将用于在TaskExecutor端实例化ExternalResourceDriver.</span><br><span class="line">例如:org.apache.flink.externalresource.gpu.GPUDriverFactory.</span><br><span class="line"></span><br><span class="line">external-resource.gpu.amount</span><br><span class="line">默认值:无</span><br><span class="line">为每个TaskExecutor指定的外部资源量,例如:2.</span><br><span class="line"></span><br><span class="line">external-resource.gpu.param.type</span><br><span class="line">默认值:无</span><br><span class="line">由&lt;resource_name&gt;指定的外部资源的自定义配置选项的命名模式.只有遵循此模式的配置才会传递到该外部资源的驱动程序工厂.</span><br><span class="line">例如:nvidia.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HeartbeatManagerOptions-心跳管理配置"><a href="#HeartbeatManagerOptions-心跳管理配置" class="headerlink" title="HeartbeatManagerOptions(心跳管理配置)"></a>HeartbeatManagerOptions(心跳管理配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">heartbeat.interval</span><br><span class="line">默认值:10000L</span><br><span class="line">从发送方请求心跳信号的时间间隔.</span><br><span class="line"></span><br><span class="line">heartbeat.timeout</span><br><span class="line">默认值:50000L</span><br><span class="line">发送方和接收方请求和接收心跳超时.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HighAvailabilityOptions-高可用配置"><a href="#HighAvailabilityOptions-高可用配置" class="headerlink" title="HighAvailabilityOptions(高可用配置)"></a>HighAvailabilityOptions(高可用配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">high-availability</span><br><span class="line">默认值:NONE</span><br><span class="line">定义用于集群执行的高可用性模式.要启用高可用性,请将此模式设置为<span class="string">&quot;ZOOKEEPER&quot;</span>或指定工厂类的FQN.</span><br><span class="line"></span><br><span class="line">high-availability.cluster-id</span><br><span class="line">默认值:/default</span><br><span class="line">Flink群集的ID,用于将多个Flink群集彼此分离.</span><br><span class="line">需要为standalone cluster设置，但会在YARN和Mesos中自动推断.</span><br><span class="line"></span><br><span class="line">high-availability.storageDir</span><br><span class="line">默认值:无</span><br><span class="line">文件系统路径(URI),Flink将元数据保存在高可用性设置中.</span><br><span class="line"></span><br><span class="line">high-availability.jobmanager.port</span><br><span class="line">默认值:0</span><br><span class="line">Flink主机在高可用设置中用于其RPC连接的端口(范围).在高可用性设置中,使用此值而不是JM的端口.</span><br><span class="line">值<span class="string">&quot;0&quot;</span>表示选择了随机自由端口.TaskManagers通过高可用性服务(leader election)发现此端口,因此随机端口或端口范围可以工作,而不需要任何额外的服务发现方法.</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.quorum</span><br><span class="line">默认值:无</span><br><span class="line">使用ZooKeeper以高可用性模式运行Flink时要使用的ZooKeeper队列.</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.root</span><br><span class="line">默认值:/flink</span><br><span class="line">Flink在ZooKeeper中存储实体的根路径.</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.latch</span><br><span class="line">默认值:/leaderlatch</span><br><span class="line">定义用于选举leader的leader latch的znode.</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.jobgraphs</span><br><span class="line">默认值:/jobgraphs</span><br><span class="line">作业图的ZooKeeper根路径(ZNode).</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.leader</span><br><span class="line">默认值:/leader</span><br><span class="line">定义Leader的znode,其中包含指向Leader的URL和当前Leader会话ID.</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.checkpoints</span><br><span class="line">默认值:/checkpoints</span><br><span class="line">已完成检查点的ZooKeeper根路径(ZNode).</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.checkpoint-counter</span><br><span class="line">默认值:/checkpoint-counter</span><br><span class="line">检查点计数器的ZooKeeper根路径(ZNode).</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.mesos-workers</span><br><span class="line">默认值:/mesos-workers</span><br><span class="line">用于持久化Mesos工作进程信息的ZooKeeper根路径.</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.client.session-timeout</span><br><span class="line">默认值:60000</span><br><span class="line">定义ZooKeeper会话的会话超时(毫秒).</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.client.connection-timeout</span><br><span class="line">默认值:15000</span><br><span class="line">定义ZooKeeper的连接超时(毫秒).</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.client.retry-wait</span><br><span class="line">默认值:5000</span><br><span class="line">定义连续重试之间的暂停(毫秒).</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.client.max-retry-attempts</span><br><span class="line">默认值:3</span><br><span class="line">定义客户端放弃之前的连接重试次数.</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.path.running-registry</span><br><span class="line">默认值:/running_job_registry/</span><br><span class="line"></span><br><span class="line">high-availability.zookeeper.client.acl</span><br><span class="line">默认值:open</span><br><span class="line">定义要在ZK节点上配置的ACL(open|creator).</span><br><span class="line">如果ZooKeeper服务器配置的<span class="string">&quot;authProvider&quot;</span>属性映射为使用SASLAuthenticationProvider,并且集群配置为在安全模式(Kerberos)下运行,则可以将配置值设置为<span class="string">&quot;creator&quot;</span>.</span><br><span class="line"></span><br><span class="line">high-availability.job.delay</span><br><span class="line">默认值:无</span><br><span class="line">故障转移后作业管理器恢复当前作业之前的时间.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="HistoryServerOptions-HistoryServer配置"><a href="#HistoryServerOptions-HistoryServer配置" class="headerlink" title="HistoryServerOptions(HistoryServer配置)"></a>HistoryServerOptions(HistoryServer配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">historyserver.archive.fs.refresh-interval</span><br><span class="line">默认值:10000L</span><br><span class="line">刷新存档作业目录的间隔(毫秒).</span><br><span class="line"></span><br><span class="line">historyserver.archive.fs.dir</span><br><span class="line">默认值:无</span><br><span class="line">要从中提取存档作业的目录的逗号分隔列表.</span><br><span class="line">historyserver将监视这些目录中的存档作业.</span><br><span class="line">您可以通过配置JobManager将作业存档到目录<span class="string">&quot;historyserver.archive.fs.dir&quot;</span>.</span><br><span class="line"></span><br><span class="line">historyserver.archive.clean-expired-jobs</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">HistoryServer是否应清理不再存在的作业.</span><br><span class="line"></span><br><span class="line">historyserver.web.tmpdir</span><br><span class="line">默认值:无</span><br><span class="line">此配置参数允许定义historyserver web界面要使用的Flink web目录.web界面将其静态文件复制到目录中.</span><br><span class="line"></span><br><span class="line">historyserver.web.address</span><br><span class="line">默认值:无</span><br><span class="line">HistoryServer的web界面的地址.</span><br><span class="line"></span><br><span class="line">historyserver.web.port</span><br><span class="line">默认值:8082</span><br><span class="line">HistoryServer的web界面的端口.</span><br><span class="line"></span><br><span class="line">historyserver.web.refresh-interval</span><br><span class="line">默认值:10000L</span><br><span class="line">HistoryServer的web界面的刷新间隔.</span><br><span class="line"></span><br><span class="line">historyserver.web.ssl.enabled</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">启用对HistoryServer web前端的HTTPs访问.仅当全局SSL标志security.ssl.enabled已启用设置为<span class="literal">true</span>.</span><br><span class="line"></span><br><span class="line">historyserver.archive.retained-jobs</span><br><span class="line">默认值:-1</span><br><span class="line">由定义的每个存档目录中要保留的最大作业数.</span><br><span class="line">如果设置为<span class="string">&quot;-1&quot;</span>(默认),则对存档的数量没有限制.</span><br><span class="line">如果设置<span class="string">&quot;0&quot;</span>或小于<span class="string">&quot;-1&quot;</span>,HistoryServer将报错.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="JMXServerOptions-JMX配置"><a href="#JMXServerOptions-JMX配置" class="headerlink" title="JMXServerOptions(JMX配置)"></a>JMXServerOptions(JMX配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">jmx.server.port</span><br><span class="line">默认值:无</span><br><span class="line">JMX服务器启动注册表的端口范围.</span><br><span class="line">端口配置可以是单个端口:<span class="string">&quot;9123&quot;</span>,端口范围:<span class="string">&quot;50100-50200&quot;</span>,或范围和端口列表:<span class="string">&quot;50100-50200,50300-50400,51234&quot;</span>.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="JobManagerOptions-JM配置"><a href="#JobManagerOptions-JM配置" class="headerlink" title="JobManagerOptions(JM配置)"></a>JobManagerOptions(JM配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">jobmanager.rpc.address</span><br><span class="line">默认值:无</span><br><span class="line">配置参数,用于定义要连接以与作业管理器通信的网络地址.</span><br><span class="line">此值仅在存在具有静态名称或地址的单个JobManager的设置(简单的独立设置或具有动态服务名称解析的容器设置)中解释.</span><br><span class="line">在许多高可用性设置中,当使用领导选举服务(如ZooKeeper)从潜在的多个备用JobManagers中选择和发现JobManager领导时,不使用它.</span><br><span class="line"></span><br><span class="line">jobmanager.bind-host</span><br><span class="line">默认值:无</span><br><span class="line">JM绑定到的网络接口的本地地址.如果未配置,将使用<span class="string">&quot;0.0.0.0&quot;</span>.</span><br><span class="line"></span><br><span class="line">jobmanager.rpc.port</span><br><span class="line">默认值:6123</span><br><span class="line">配置参数,用于定义要连接以与JM通信的网络端口.</span><br><span class="line"></span><br><span class="line">jobmanager.rpc.bind-port</span><br><span class="line">默认值:无</span><br><span class="line">JM绑定到的本地RPC端口.如果未配置,则外部端口(jobmanager.rpc.port)将被使用.</span><br><span class="line"></span><br><span class="line">jobmanager.heap.size</span><br><span class="line">默认值:无</span><br><span class="line">JM的JVM堆大小.</span><br><span class="line"></span><br><span class="line">jobmanager.heap.mb</span><br><span class="line">默认值:无</span><br><span class="line">JM的JVM堆大小(MB).</span><br><span class="line"></span><br><span class="line">jobmanager.memory.process.size</span><br><span class="line">默认值:无</span><br><span class="line">JM的总进程内存大小.这包括JM JVM进程消耗的所有内存,包括总Flink内存,JVM元空间和JVM开销.</span><br><span class="line">在容器化设置中,这应该设置为容器内存.</span><br><span class="line">另见<span class="string">&#x27;jobmanager.memory.flink.size&#x27;</span>表示总Flink内存大小配置.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.flink.size</span><br><span class="line">默认值:无</span><br><span class="line">作业管理器的总Flink内存大小.</span><br><span class="line">这包括JM消耗的所有内存,除了JVM元空间和JVM开销.它由JVM堆内存和堆外内存组成.</span><br><span class="line">有关总进程内存大小配置,请参见<span class="string">&#x27;jobmanager.memory.process.size&#x27;</span>.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.heap.size</span><br><span class="line">默认值:无</span><br><span class="line">JM的JVM堆内存大小.建议的最小JVM堆大小为128M.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.off-heap.size</span><br><span class="line">默认值:MemorySize.ofMebiBytes(128)</span><br><span class="line">JM的ff-heap内存大小.此选项涵盖所有堆外内存使用,包括直接和本机内存分配.</span><br><span class="line">如果由启用了限制,则JobManager进程的JVM直接内存限制(-XX:MaxDirectMemorySize)将设置为此值<span class="string">&#x27;jobmanager.memory.enable-jvm-direct-memory-limit&#x27;</span>.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.enable-jvm-direct-memory-limit</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">是否启用JobManager进程的JVM直接内存限制(-XX:MaxDirectMemorySize).</span><br><span class="line">限制将设置为<span class="string">&quot;jobmanager.memory.off-heap.size&quot;</span>选项的值.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.jvm-metaspace.size</span><br><span class="line">默认值:MemorySize.ofMebiBytes(256)</span><br><span class="line">JobManager的JVM元空间大小.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.jvm-overhead.min</span><br><span class="line">默认值:MemorySize.ofMebiBytes(192)</span><br><span class="line">JobManager的最小JVM开销大小.</span><br><span class="line">这是为JVM开销(如线程堆栈空间、编译缓存等)保留的堆外内存.这包括本机内存,但不包括直接内存,并且在Flink计算JVM最大直接内存大小参数时不会计算在内.</span><br><span class="line">JVM开销的大小是用来构成总进程内存的配置部分的.如果派生大小小于或大于配置的最小或最大大小,则将使用最小或最大大小.</span><br><span class="line">通过将最小和最大大小设置为相同的值,可以显式指定JVM开销的确切大小.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.jvm-overhead.max</span><br><span class="line">默认值:1g</span><br><span class="line">JobManager的最大JVM开销大小.</span><br><span class="line"></span><br><span class="line">jobmanager.memory.jvm-overhead.fraction</span><br><span class="line">默认值:0.1f</span><br><span class="line">为JVM开销保留的总进程内存的一小部分.</span><br><span class="line"></span><br><span class="line">jobmanager.execution.attempts-history-size</span><br><span class="line">默认值:16</span><br><span class="line">历史记录中保留的先前执行尝试的最大数目.</span><br><span class="line"></span><br><span class="line">jobmanager.execution.failover-strategy</span><br><span class="line">默认值:region</span><br><span class="line">此选项指定作业计算如何从任务失败中恢复.</span><br><span class="line"><span class="string">&#x27;full&#x27;</span>:重新启动所有任务以恢复作业.</span><br><span class="line"><span class="string">&#x27;region&#x27;</span>:重新启动可能受任务影响的所有任务失败.</span><br><span class="line"></span><br><span class="line">jobmanager.archive.fs.dir</span><br><span class="line">默认值:无</span><br><span class="line">JM用于存储已完成作业的存档目录.</span><br><span class="line"></span><br><span class="line">jobstore.cache-size</span><br><span class="line">默认值:50L * 1024L * 1024L</span><br><span class="line">作业存储缓存大小(字节),用于将已完成的作业保留在内存中.</span><br><span class="line"></span><br><span class="line">jobstore.expiration-time</span><br><span class="line">默认值:60L * 60L</span><br><span class="line">完成的作业过期并从作业存储中清除的时间(以秒为单位).</span><br><span class="line"></span><br><span class="line">jobstore.max-capacity</span><br><span class="line">默认值:Integer.MAX_VALUE</span><br><span class="line">作业存储中可以保留的最大已完成作业数.</span><br><span class="line"></span><br><span class="line">jobmanager.retrieve-taskmanager-hostname</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">指示JobManager是否在注册期间检索TaskManager的规范主机名的标志.</span><br><span class="line">如果该选项设置为<span class="string">&quot;false&quot;</span>,则TaskManager向JobManager注册可能会更快,因为不会执行反向DNS查找.</span><br><span class="line">但是,本地input split分配(例如HDFS文件)可能会受到影响.</span><br><span class="line"></span><br><span class="line">slot.request.timeout</span><br><span class="line">默认值:5L * 60L * 1000L</span><br><span class="line">从Slot池请求Slot的超时(以毫秒为单位).</span><br><span class="line"></span><br><span class="line">slot.idle.timeout</span><br><span class="line">默认值:50000L 复用heartbeat.timeout取值</span><br><span class="line">Slot池中空闲Slot的超时(毫秒).</span><br><span class="line"></span><br><span class="line">jobmanager.scheduler</span><br><span class="line">默认值:ng</span><br><span class="line">确定用于计划任务的计划程序实现.可接受的值为:ng 新一代调度程序.</span><br><span class="line"></span><br><span class="line">jobmanager.partition.release-during-job-execution</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">控制在作业执行期间是否应该释放分区.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="MetricOptions-指标配置"><a href="#MetricOptions-指标配置" class="headerlink" title="MetricOptions(指标配置)"></a>MetricOptions(指标配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">metrics.reporters</span><br><span class="line">默认值:无</span><br><span class="line">Reporter的可选列表.</span><br><span class="line">如果已配置,则只会启动名称与列表中任何名称匹配的报告器.</span><br><span class="line">否则,将启动配置中可以找到的所有报告程序.</span><br><span class="line">例如:</span><br><span class="line">metrics.reporters = foo,bar</span><br><span class="line">metrics.reporter.foo.class = org.apache.flink.metrics.reporter.JMXReporter</span><br><span class="line">metrics.reporter.foo.interval = 10</span><br><span class="line">metrics.reporter.bar.class = org.apache.flink.metrics.graphite.GraphiteReporter</span><br><span class="line">metrics.reporter.bar.port = 1337</span><br><span class="line"></span><br><span class="line">metrics.reporter.&lt;name&gt;.class</span><br><span class="line">默认值:无</span><br><span class="line">用于名为&lt;name&gt;的Reporter类.</span><br><span class="line"></span><br><span class="line">metrics.reporter.&lt;name&gt;.interval</span><br><span class="line">默认值:Duration.ofSeconds(10)</span><br><span class="line">Reporter的发送报告间隔.</span><br><span class="line"></span><br><span class="line">metrics.reporter.&lt;name&gt;.&lt;parameter&gt;</span><br><span class="line">默认值:无</span><br><span class="line">Reporter的配置参数.</span><br><span class="line"></span><br><span class="line">metrics.scope.delimiter</span><br><span class="line">默认值:.</span><br><span class="line">用于组合度量标识符的分隔符.</span><br><span class="line"></span><br><span class="line">metrics.scope.jm</span><br><span class="line">默认值:&lt;host&gt;.jobmanager</span><br><span class="line">定义应用于JobManager范围内的所有度量的范围格式字符串.</span><br><span class="line"></span><br><span class="line">metrics.scope.tm</span><br><span class="line">默认值:&lt;host&gt;.taskmanager.&lt;tm_id&gt;</span><br><span class="line">定义应用于TaskManager范围内的所有度量的范围格式字符串.</span><br><span class="line"></span><br><span class="line">metrics.scope.jm.job</span><br><span class="line">默认值:&lt;host&gt;.jobmanager.&lt;job_name&gt;</span><br><span class="line">定义作用域格式字符串,该字符串应用于JobManager上作用域为作业的所有度量.</span><br><span class="line"></span><br><span class="line">metrics.scope.tm.job</span><br><span class="line">默认值:&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;</span><br><span class="line">定义作用域格式字符串,该字符串应用于TaskManager上作用域为作业的所有度量.</span><br><span class="line"></span><br><span class="line">metrics.scope.task</span><br><span class="line">默认值:&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;task_name&gt;.&lt;subtask_index&gt;</span><br><span class="line">定义作用域格式字符串,该字符串应用于作用域为Task的所有度量.</span><br><span class="line"></span><br><span class="line">metrics.scope.operator</span><br><span class="line">默认值:&lt;host&gt;.taskmanager.&lt;tm_id&gt;.&lt;job_name&gt;.&lt;operator_name&gt;.&lt;subtask_index&gt;</span><br><span class="line">定义作用域格式字符串,该字符串应用于作用域为Operator的所有度量.</span><br><span class="line"></span><br><span class="line">metrics.latency.interval</span><br><span class="line">默认值:0L</span><br><span class="line">定义从Source发出延迟跟踪标记的间隔.</span><br><span class="line">如果设置为0或负值,则禁用延迟跟踪.启用此功能可以显著影响集群的性能.</span><br><span class="line"></span><br><span class="line">metrics.latency.granularity</span><br><span class="line">默认值:operator</span><br><span class="line">定义延迟度量的粒度.可接受的值为:</span><br><span class="line">single - 跟踪延迟不区分Source和SubTask</span><br><span class="line">operator - 跟踪延迟,同时区分Source,而不是SubTask</span><br><span class="line">subtask - 跟踪延迟,同时区分Source和SubTask.</span><br><span class="line"></span><br><span class="line">metrics.latency.history-size</span><br><span class="line">默认值:128</span><br><span class="line">定义要在每个Operator上保持的测量延迟数.</span><br><span class="line"></span><br><span class="line">metrics.system-resource</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">指示Flink是否应报告系统资源指标(如计算机的CPU,内存或网络使用情况)的标志.</span><br><span class="line"></span><br><span class="line">metrics.system-resource-probing-interval</span><br><span class="line">默认值:5000L</span><br><span class="line">探测指定的系统资源度量之间的间隔(毫秒).</span><br><span class="line"></span><br><span class="line">metrics.internal.query-service.port</span><br><span class="line">默认值:0</span><br><span class="line">用于Flink的内部度量查询服务的端口范围.</span><br><span class="line">接受端口列表(<span class="string">&quot;50100,50101&quot;</span>),范围(<span class="string">&quot;50100-50200&quot;</span>)或两者的组合.</span><br><span class="line">建议设置端口范围,以避免多个Flink组件在同一台机器上运行时发生冲突.</span><br><span class="line">默认情况下,Flink将随机选择一个端口.</span><br><span class="line"></span><br><span class="line">metrics.internal.query-service.thread-priority</span><br><span class="line">默认值:1</span><br><span class="line">用于Flink的内部度量查询服务的线程优先级.</span><br><span class="line">线程是由Akka的线程池执行器创建的.</span><br><span class="line">优先级的范围是从1(最小优先级)到10(最大优先级).</span><br><span class="line">警告,增加此值可能会降低主要Flink组件.</span><br><span class="line"></span><br><span class="line">metrics.fetcher.update-interval</span><br><span class="line">默认值:10000L</span><br><span class="line">WEB UI使用的度量获取程序的更新间隔(毫秒).</span><br><span class="line">减小此值以加快更新度量.如果度量获取程序导致过多负载,请增加此值.将此值设置为0将完全禁用度量获取.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="NettyShuffleEnvironmentOptions-网络堆栈配置"><a href="#NettyShuffleEnvironmentOptions-网络堆栈配置" class="headerlink" title="NettyShuffleEnvironmentOptions(网络堆栈配置)"></a>NettyShuffleEnvironmentOptions(网络堆栈配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">taskmanager.data.port</span><br><span class="line">默认值:0</span><br><span class="line">用于数据交换操作的TM的外部端口.</span><br><span class="line"></span><br><span class="line">taskmanager.data.bind-port</span><br><span class="line">默认值:无</span><br><span class="line">用于数据交换操作的TM的绑定端口.</span><br><span class="line"></span><br><span class="line">taskmanager.data.ssl.enabled</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">为TM数据传输启用SSL支持.仅当内部SSL的全局标志<span class="string">&quot;security.ssl.internal.enabled&quot;</span>启用时设置为<span class="literal">true</span>.</span><br><span class="line"></span><br><span class="line">taskmanager.network.blocking-shuffle.compression.enabled</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">指示是否将压缩shuffle数据以阻止shuffle模式.</span><br><span class="line"></span><br><span class="line">taskmanager.network.compression.codec</span><br><span class="line">默认值:LZ4</span><br><span class="line">压缩Shuffle数据时要使用的编解码器.</span><br><span class="line"></span><br><span class="line">taskmanager.network.detailed-metrics</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">用于启用/禁用有关入站/出站网络队列长度的更详细度量.</span><br><span class="line"></span><br><span class="line">taskmanager.network.numberOfBuffers</span><br><span class="line">默认值:2048</span><br><span class="line">网络堆栈中使用的缓冲区数.</span><br><span class="line"></span><br><span class="line">taskmanager.network.memory.fraction</span><br><span class="line">默认值:0.1f</span><br><span class="line">JVM内存的一小部分用于网络缓冲区.</span><br><span class="line">被taskmanager.memory.network.fraction替代.</span><br><span class="line"></span><br><span class="line">taskmanager.network.memory.min</span><br><span class="line">默认值:64mb</span><br><span class="line">网络缓冲最小内存.</span><br><span class="line">被taskmanager.memory.network.min替代.</span><br><span class="line"></span><br><span class="line">taskmanager.network.memory.max</span><br><span class="line">默认值:1gb</span><br><span class="line">网络缓冲最大内存.</span><br><span class="line">被taskmanager.memory.network.max替代.</span><br><span class="line"></span><br><span class="line">taskmanager.network.memory.buffers-per-channel</span><br><span class="line">默认值:2</span><br><span class="line">在credit-based的流控制模型中,用于每个传出/传入通道(subpartition/inputchannel)的独占网络缓冲区数.</span><br><span class="line">为获得良好的性能,应至少配置2个.</span><br><span class="line">1个缓冲区用于接收subpartition中的in-fight数据,</span><br><span class="line">1个缓冲区用于并行序列化.</span><br><span class="line"></span><br><span class="line">taskmanager.network.memory.floating-buffers-per-gate</span><br><span class="line">默认值:8</span><br><span class="line">为每个输出/输入gate(resultpartition/inputgate)使用的额外网络缓冲区数.</span><br><span class="line">在credit-based的流控制模式中,这表示所有inputchannel之间共享多少floating credit.</span><br><span class="line">Floating缓冲区基于backlog(subpartition中的实时输出缓冲区)反馈进行分配,有助于缓解subpartition间数据分布不平衡造成的反压.</span><br><span class="line">如果节点之间的往返时间较长或者群集中的机器数量较多,则应增加此值.</span><br><span class="line"></span><br><span class="line">taskmanager.network.sort-shuffle.min-buffers</span><br><span class="line">默认值:64</span><br><span class="line">每个sort-merge blocking结果分区所需的最小网络缓冲区数.</span><br><span class="line">对于大规模批量作业,建议增加此配置值以提高压缩比并减少小的网络数据包.</span><br><span class="line">注意:要增加此配置值,您可能还需要增加总网络内存的大小,以避免<span class="string">&quot;网络缓冲区数量不足&quot;</span>错误.</span><br><span class="line"></span><br><span class="line">taskmanager.network.sort-shuffle.min-parallelism</span><br><span class="line">默认值:Integer.MAX_VALUE</span><br><span class="line">并行度阈值,用于在sort-merge blocking shuffle和默认的基于哈希的blocking shuffle之间切换</span><br><span class="line">这意味着对于较小的并行度,将使用基于哈希的blocking shuffle,对于较大的并行度,将使用sort-merge blocking shuffle.</span><br><span class="line">注意:sort merge blocking shuffle使用unmanaged direct内存进行数据写入和读取,因此如果发生直接内存错误,只需增加直接内存的大小.</span><br><span class="line"></span><br><span class="line">taskmanager.network.memory.max-buffers-per-channel</span><br><span class="line">默认值:10</span><br><span class="line">可用于每个channel的最大缓冲区数.</span><br><span class="line">如果一个channel超过了最大缓冲区的数目,它将使任务变得不可用,导致背压并阻塞数据处理.</span><br><span class="line">这可能会加快检查点对齐,因为在数据倾斜和配置了大量Float缓冲区的情况下,可以防止缓冲的in-flight数据的过度增长.</span><br><span class="line">这个限制没有严格的保证,可以被flatMap操作符,跨越多个缓冲区的记录或产生大量数据的单个计时器忽略.</span><br><span class="line"></span><br><span class="line">taskmanager.network.memory.exclusive-buffers-request-timeout-ms</span><br><span class="line">默认值:30000L</span><br><span class="line">为每个channel请求独占缓冲区的超时.</span><br><span class="line">由于本地缓冲池的最大缓冲区数和所需缓冲区数不同,因此可能存在上游任务已占用所有缓冲区而下游任务正在等待独占缓冲区的死锁情况.</span><br><span class="line">超时使独占缓冲区请求失败,并要求用户增加缓冲区总数,从而打破了这种关系.</span><br><span class="line"></span><br><span class="line">taskmanager.network.blocking-shuffle.type</span><br><span class="line">默认值:file</span><br><span class="line">blocking shuffle类型,可以是<span class="string">&quot;mmap&quot;</span>或<span class="string">&quot;file&quot;</span>.</span><br><span class="line"><span class="string">&quot;auto&quot;</span>表示根据系统内存结构自动选择属性类型(mmap为64位,file为32位).</span><br><span class="line">请注意,mmap的内存使用情况不受配置的内存限制的影响,但是一些资源框架(如yarn)会跟踪内存使用情况,一旦内存超过某个阈值,就会终止容器.</span><br><span class="line">另外请注意,此选项是实验性的,将来可能会更改.</span><br><span class="line"></span><br><span class="line">taskmanager.network.netty.num-arenas</span><br><span class="line">默认值:-1</span><br><span class="line">Netty arenas的数量.</span><br><span class="line"></span><br><span class="line">taskmanager.network.netty.server.numThreads</span><br><span class="line">默认值:-1</span><br><span class="line">Netty服务器的线程数量.</span><br><span class="line"></span><br><span class="line">taskmanager.network.netty.client.numThreads</span><br><span class="line">默认值:-1</span><br><span class="line">Netty客户端的线程数量.</span><br><span class="line"></span><br><span class="line">taskmanager.network.netty.server.backlog</span><br><span class="line">默认值:0</span><br><span class="line">Netty服务器连接积压.</span><br><span class="line"></span><br><span class="line">taskmanager.network.netty.client.connectTimeoutSec</span><br><span class="line">默认值:120</span><br><span class="line">Netty客户端连接超时.</span><br><span class="line"></span><br><span class="line">taskmanager.network.retries</span><br><span class="line">默认值:0</span><br><span class="line">网络通信的重试次数.</span><br><span class="line">目前它只用于建立input/output channel连接.</span><br><span class="line"></span><br><span class="line">taskmanager.network.netty.sendReceiveBufferSize</span><br><span class="line">默认值:0</span><br><span class="line">Netty发送和接收缓冲区大小.</span><br><span class="line">这默认为系统缓冲区大小(cat /proc/sys/net/ipv4/tcp_[rw]mem),在现代Linux中是4mib.</span><br><span class="line"></span><br><span class="line">taskmanager.network.netty.transport</span><br><span class="line">默认值:auto</span><br><span class="line">Netty传输类型,可以是<span class="string">&quot;nio&quot;</span>或<span class="string">&quot;epoll&quot;</span>.</span><br><span class="line"><span class="string">&quot;auto&quot;</span>是指根据平台自动选择属性模式.</span><br><span class="line">请注意,<span class="string">&quot;epoll&quot;</span>模式可以获得更好的性能,更少的GC,并且具有更高级的特性,这些特性仅在现代Linux上可用.</span><br><span class="line"></span><br><span class="line">taskmanager.network.request-backoff.initial</span><br><span class="line">默认值:100</span><br><span class="line">输入通道分区请求的最小回退(毫秒).</span><br><span class="line"></span><br><span class="line">taskmanager.network.request-backoff.max</span><br><span class="line">默认值:10000</span><br><span class="line">输入通道分区请求的最大回退(毫秒).</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="OptimizerOptions-优化器配置"><a href="#OptimizerOptions-优化器配置" class="headerlink" title="OptimizerOptions(优化器配置)"></a>OptimizerOptions(优化器配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">compiler.delimited-informat.max-line-samples</span><br><span class="line">默认值:10</span><br><span class="line">编译器为分隔输入获取的最大行样本数.样本用于估计记录的数量.可以使用输入格式的参数覆盖特定输入的此值.</span><br><span class="line"></span><br><span class="line">compiler.delimited-informat.min-line-samples</span><br><span class="line">默认值:2</span><br><span class="line">编译器为分隔输入获取的最小行样本数.样本用于估计记录的数量.可以使用输入格式的参数覆盖特定输入的此值.</span><br><span class="line"></span><br><span class="line">compiler.delimited-informat.max-sample-len</span><br><span class="line">默认值:2097152</span><br><span class="line">编译器对分隔输入所采用的行样本的最大长度.如果单个样本的长度超过此值(可能是因为解析器配置错误),则采样将中止.</span><br><span class="line">可以使用输入格式的参数覆盖特定输入的此值.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="PipelineOptions-作业执行配置"><a href="#PipelineOptions-作业执行配置" class="headerlink" title="PipelineOptions(作业执行配置)"></a>PipelineOptions(作业执行配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pipeline.name</span><br><span class="line">默认值:无</span><br><span class="line">用于打印和记录的作业名称.</span><br><span class="line"></span><br><span class="line">pipeline.jars</span><br><span class="line">默认值:无</span><br><span class="line">要打包的jar和要发送到集群的作业jar的分号分隔列表.这些必须是有效的路径.</span><br><span class="line"></span><br><span class="line">pipeline.classpaths</span><br><span class="line">默认值:无</span><br><span class="line">要打包的类路径的分号列表,其中包含要发送到集群的作业jar.这些必须是有效的URL.</span><br><span class="line"></span><br><span class="line">pipeline.auto-generate-uids</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">禁用自动生成的UID时,用户将被迫在DataStream应用程序上手动指定UID.</span><br><span class="line"></span><br><span class="line">pipeline.auto-type-registration</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">控制Flink是否自动向Kryo注册用户程序中的所有类型.</span><br><span class="line"></span><br><span class="line">pipeline.auto-watermark-interval</span><br><span class="line">默认值:Duration.ZERO</span><br><span class="line">自动水印发射的间隔.整个流系统都使用水印来跟踪时间的进程.例如,它们用于基于时间的窗口.</span><br><span class="line"></span><br><span class="line">pipeline.closure-cleaner-level</span><br><span class="line">默认值:ClosureCleanerLevel.RECURSIVE</span><br><span class="line">配置闭包清理器的工作模式.</span><br><span class="line">ClosureCleanerLevel.NONE - 完全禁用闭包清理器</span><br><span class="line">ClosureCleanerLevel.TOP_LEVEL - 只清理顶级类而不递归到字段中</span><br><span class="line">ClosureCleanerLevel.RECURSIVE - 递归地清除所有字段.</span><br><span class="line"></span><br><span class="line">pipeline.force-avro</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">强制Flink对pojo使用apache avro序列化程序.</span><br><span class="line">重要提示:请确保包含flink-avro模块.</span><br><span class="line"></span><br><span class="line">pipeline.force-kryo</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">如果启用,则强制TypeExtractor对POJO使用Kryo序列化程序,即使我们可以作为POJO进行分析.</span><br><span class="line">在某些情况下,这可能更可取.例如,当使用子类不能作为POJO分析的接口时.</span><br><span class="line"></span><br><span class="line">pipeline.generic-types</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">如果禁用泛型类型的使用,Flink将在遇到要通过Kryo进行序列化的数据类型时抛出UnsupportedOperationException.</span><br><span class="line"></span><br><span class="line">pipeline.global-job-parameters</span><br><span class="line">默认值:无</span><br><span class="line">注册自定义的可序列化用户配置对象.可以在Operator中访问配置.</span><br><span class="line"></span><br><span class="line">pipeline.max-parallelism</span><br><span class="line">默认值:-1</span><br><span class="line">用于尚未指定最大并行度的运算符的程序范围内的最大并行度.最大并行度指定动态缩放的上限和用于分区状态的键组数.</span><br><span class="line"></span><br><span class="line">pipeline.object-reuse</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">启用时,Flink内部用于反序列化和将数据传递给用户代码函数的对象将被重用.</span><br><span class="line">请记住,当操作的用户代码函数不知道这种行为时,这可能会导致错误.</span><br><span class="line"></span><br><span class="line">pipeline.default-kryo-serializers</span><br><span class="line">默认值:无</span><br><span class="line">以分号分隔的类名和Kryo序列化程序对的列表要用作Kryo默认序列化程序的类名.</span><br><span class="line"></span><br><span class="line">pipeline.registered-kryo-types</span><br><span class="line">默认值:无</span><br><span class="line">要在序列化堆栈中注册的类型的分号分隔列表.</span><br><span class="line">如果该类型最终被序列化为POJO,则该类型将向POJO序列化程序注册.</span><br><span class="line">如果类型最终被Kryo序列化,那么它将在Kryo注册,以确保只写入标记.</span><br><span class="line"></span><br><span class="line">pipeline.registered-pojo-types</span><br><span class="line">默认值:无</span><br><span class="line">要在序列化堆栈中注册的类型的分号分隔列表.</span><br><span class="line"></span><br><span class="line">pipeline.operator-chaining</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">Operator Chain允许将non-shuffle操作放在同一线程中,从而完全避免序列化和反序列化.</span><br><span class="line"></span><br><span class="line">pipeline.cached-files</span><br><span class="line">默认值:无</span><br><span class="line">要以给定名称在分布式缓存中注册的文件.</span><br><span class="line">这些文件可以通过本地路径从(分布式)运行时中的任何用户定义函数访问.</span><br><span class="line">文件可以是本地文件(将通过BlobServer分发),也可以是分布式文件系统中的文件.</span><br><span class="line">如果需要,运行时会将文件临时复制到本地缓存中.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="QueryableStateOptions-查询状态配置"><a href="#QueryableStateOptions-查询状态配置" class="headerlink" title="QueryableStateOptions(查询状态配置)"></a>QueryableStateOptions(查询状态配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">queryable-state.proxy.ports</span><br><span class="line">默认值:9069</span><br><span class="line">可查询状态代理的端口范围.</span><br><span class="line"></span><br><span class="line">queryable-state.proxy.network-threads</span><br><span class="line">默认值:0</span><br><span class="line">可查询状态代理的网络(Netty的事件循环)线程数.</span><br><span class="line"></span><br><span class="line">queryable-state.proxy.query-threads</span><br><span class="line">默认值:0</span><br><span class="line">可查询状态代理的查询线程数.</span><br><span class="line">如果设置为0,则使用Slot数.</span><br><span class="line"></span><br><span class="line">queryable-state.server.ports</span><br><span class="line">默认值:9067</span><br><span class="line">可查询状态服务器的端口范围.</span><br><span class="line"></span><br><span class="line">queryable-state.server.network-threads</span><br><span class="line">默认值:0</span><br><span class="line">可查询状态服务器的网络(Netty的事件循环)线程数.</span><br><span class="line"></span><br><span class="line">queryable-state.server.query-threads</span><br><span class="line">默认值:0</span><br><span class="line">可查询状态服务器的查询线程数.如果设置为0,则使用Slot数.</span><br><span class="line"></span><br><span class="line">queryable-state.enable</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">是否启用可查询状态代理和服务器.</span><br><span class="line"></span><br><span class="line">queryable-state.client.network-threads</span><br><span class="line">默认值:0</span><br><span class="line">可查询状态客户端的网络(Netty的事件循环)线程数.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="ResourceManagerOptions-RM配置"><a href="#ResourceManagerOptions-RM配置" class="headerlink" title="ResourceManagerOptions(RM配置)"></a>ResourceManagerOptions(RM配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">resourcemanager.job.timeout</span><br><span class="line">默认值:5 minutes</span><br><span class="line">没有指派JM作为leader的作业超时时间.</span><br><span class="line"></span><br><span class="line">local.number-resourcemanager</span><br><span class="line">默认值:1</span><br><span class="line">启动RM的数量.</span><br><span class="line"></span><br><span class="line">resourcemanager.rpc.port</span><br><span class="line">默认值:0</span><br><span class="line">定义与资源管理器通信时要连接的网络端口.</span><br><span class="line">默认情况下,作业管理器的端口,因为使用的是同一ActorSystem.</span><br><span class="line">无法使用此配置键定义端口范围.</span><br><span class="line"></span><br><span class="line">slotmanager.number-of-slots.max</span><br><span class="line">默认值:Integer.MAX_VALUE</span><br><span class="line">定义Flink群集分配的最大Slot数.</span><br><span class="line">此配置选项用于限制批处理工作负载的资源消耗.</span><br><span class="line">不建议为流式工作负载配置此选项,如果没有足够的插槽,流式工作负载可能会失败.</span><br><span class="line">请注意,此配置选项对standalone集群不起作用,其中分配的插槽数量不受Flink控制.</span><br><span class="line"></span><br><span class="line">slotmanager.redundant-taskmanager-num</span><br><span class="line">默认值:0</span><br><span class="line">冗余任务管理器的数量.冗余任务管理器是由Flink启动的额外任务管理器,目的是在由于任务管理器丢失而导致失败时加快作业恢复.</span><br><span class="line">请注意,此功能仅适用于Active部署(Native K8s,Yarn和Mesos).</span><br><span class="line"></span><br><span class="line">slotmanager.request-timeout</span><br><span class="line">默认值:-1L</span><br><span class="line">丢弃Slot请求的超时时间.</span><br><span class="line"></span><br><span class="line">resourcemanager.standalone.start-up-time</span><br><span class="line">默认值:-1L</span><br><span class="line">standalone群集启动期间的时间(毫秒).</span><br><span class="line">在此期间,standalone集群的资源管理器期望注册新的任务执行器,并且不会使任何当前注册的Slot都无法满足的Slot请求失败.</span><br><span class="line">在这段时间之后,它将立即失败挂起的和新来的请求.这些请求不能被注册的Slot满足.</span><br><span class="line">如果不设置,将使用<span class="string">&#x27;slotmanager.request-timeout&#x27;</span>.</span><br><span class="line"></span><br><span class="line">slotmanager.taskmanager-timeout</span><br><span class="line">默认值:30000L</span><br><span class="line">释放空闲任务管理器的超时.</span><br><span class="line"></span><br><span class="line">resourcemanager.taskmanager-timeout</span><br><span class="line">默认值:30000L</span><br><span class="line">释放空闲任务管理器的超时.</span><br><span class="line"></span><br><span class="line">resourcemanager.taskmanager-release.wait.result.consumed</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">仅当每个生成的结果分区被占用或失败时才释放任务执行器.</span><br><span class="line"><span class="string">&#x27;True&#x27;</span>是默认值,<span class="string">&#x27;False&#x27;</span>表示空闲任务执行器释放未被确认结果分区消耗的接收方阻止,并且可以在<span class="string">&#x27;resourcemanager.taskmanager-timeout&#x27;</span>.</span><br><span class="line">将此选项设置为<span class="string">&#x27;false&#x27;</span>可以加快任务执行器的释放速度,但如果使用结束时间比<span class="string">&#x27;false&#x27;</span>慢,则可能导致意外失败<span class="string">&#x27;resourcemanager.taskmanager-timeout&#x27;</span>。</span><br><span class="line"></span><br><span class="line">resourcemanager.taskmanager-registration.timeout</span><br><span class="line">默认值:5 min</span><br><span class="line">TaskManager在活动资源管理器上注册超时.如果超出,活动资源管理器将释放并尝试重新请求辅助进程的资源.</span><br><span class="line">如果未配置,则回退到<span class="string">&#x27;taskmanager.registration.timeout&#x27;</span>.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="RestartStrategyOptions-重启策略配置"><a href="#RestartStrategyOptions-重启策略配置" class="headerlink" title="RestartStrategyOptions(重启策略配置)"></a>RestartStrategyOptions(重启策略配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">restart-strategy</span><br><span class="line">默认值:无</span><br><span class="line">定义在作业失败时使用的重新启动策略</span><br><span class="line">无重启策略:none,off,<span class="built_in">disable</span></span><br><span class="line">固定延迟重启策略:fixeddelay,fixed-delay</span><br><span class="line">故障率重启策略:failurerate,failure-rate</span><br><span class="line">指数延迟重启策略:exponentialdelay,exponential-delay</span><br><span class="line">如果CK关闭时,默认值为node,CK开启时,默认值为fixeddelay,重启次数Integer.MAX_VALUE,延迟1秒.</span><br><span class="line"></span><br><span class="line">restart-strategy.fixed-delay.attempts</span><br><span class="line">默认值:1</span><br><span class="line">如果restart-strategy被设置为fixeddelay,该参数表示作业失败前重试次数.</span><br><span class="line"></span><br><span class="line">restart-strategy.fixed-delay.delay</span><br><span class="line">默认值:1 s</span><br><span class="line">如果restart-strategy被设置为fixeddelay,该参数表示两次重启之间的时间间隔.</span><br><span class="line"></span><br><span class="line">restart-strategy.failure-rate.max-failures-per-interval</span><br><span class="line">默认值:1</span><br><span class="line">如果restart-strategy被设置为failure-rate,该参数表示作业失败前在给定时间间隔内重新启动的最大次数.</span><br><span class="line"></span><br><span class="line">restart-strategy.failure-rate.failure-rate-interval</span><br><span class="line">默认值:1 min</span><br><span class="line">如果restart-strategy被设置为failure-rate,该参数表示测试故障率的时间间隔.</span><br><span class="line"></span><br><span class="line">restart-strategy.failure-rate.delay</span><br><span class="line">默认值:1 s</span><br><span class="line">两次重启之间的时间间隔.</span><br><span class="line"></span><br><span class="line">restart-strategy.exponential-delay.initial-backoff</span><br><span class="line">默认值:1 s</span><br><span class="line">重新启动之间的起始持续时间.</span><br><span class="line"></span><br><span class="line">restart-strategy.exponential-delay.max-backoff</span><br><span class="line">默认值:5 min</span><br><span class="line">重新启动之间可能的最长持续时间.</span><br><span class="line"></span><br><span class="line">restart-strategy.exponential-delay.backoff-multiplier</span><br><span class="line">默认值:2.0</span><br><span class="line">回退值在每次失败后乘以此值,直到达到最大回退值</span><br><span class="line"></span><br><span class="line">restart-strategy.exponential-delay.reset-backoff-threshold</span><br><span class="line">默认值:1 h</span><br><span class="line">回退值重置为初始值时的阈值.</span><br><span class="line"></span><br><span class="line">restart-strategy.exponential-delay.jitter-factor</span><br><span class="line">默认值:0.1</span><br><span class="line">向回退值添加随机值,避免同一时间重启多个Job.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="RestOptions-REST通信配置"><a href="#RestOptions-REST通信配置" class="headerlink" title="RestOptions(REST通信配置)"></a>RestOptions(REST通信配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">rest.bind-address</span><br><span class="line">默认值:无</span><br><span class="line">服务器绑定自身的地址.</span><br><span class="line"></span><br><span class="line">rest.bind-port</span><br><span class="line">默认值:8081</span><br><span class="line">服务器绑定自身的端口.</span><br><span class="line"></span><br><span class="line">rest.addres</span><br><span class="line">默认值:无</span><br><span class="line">客户端连接到服务器时应使用的地址.</span><br><span class="line">注意:只有在高可用性配置为<span class="string">&quot;NONE&quot;</span>时才考虑此选项.</span><br><span class="line"></span><br><span class="line">rest.post</span><br><span class="line">默认值:8081</span><br><span class="line">客户端连接到服务器时应使用的端口.</span><br><span class="line"></span><br><span class="line">rest.await-leader-timeout</span><br><span class="line">默认值:30_000L</span><br><span class="line">客户端等待leader地址的时间(毫秒).</span><br><span class="line"></span><br><span class="line">rest.retry.max-attempts</span><br><span class="line">默认值:20</span><br><span class="line">客户端重试次数最大值.</span><br><span class="line"></span><br><span class="line">rest.retry.delay</span><br><span class="line">默认值:3_000L</span><br><span class="line">客户端重试延迟时间(毫秒).</span><br><span class="line"></span><br><span class="line">rest.connection-timeout</span><br><span class="line">默认值:15_000L</span><br><span class="line">客户端请求连接超时时间(毫秒).</span><br><span class="line"></span><br><span class="line">rest.idleness-timeout</span><br><span class="line">默认值:5L * 60L * 1_000L</span><br><span class="line">失败前连接保持空闲的最长时间(毫秒).</span><br><span class="line"></span><br><span class="line">rest.server.max-content-length</span><br><span class="line">默认值:104_857_600</span><br><span class="line">服务器将处理的最大内容长度(字节).</span><br><span class="line"></span><br><span class="line">rest.client.max-content-length</span><br><span class="line">默认值:104_857_600</span><br><span class="line">客户端将处理的最大内容长度(字节).</span><br><span class="line"></span><br><span class="line">rest.server.numThreads</span><br><span class="line">默认值:4</span><br><span class="line">异步处理请求的线程数</span><br><span class="line"></span><br><span class="line">rest.server.thread-priority</span><br><span class="line">默认值:5</span><br><span class="line">REST服务器执行器处理异步请求的线程优先级.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="SecurityOptions-安全性配置"><a href="#SecurityOptions-安全性配置" class="headerlink" title="SecurityOptions(安全性配置)"></a>SecurityOptions(安全性配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">security.context.factory.classes</span><br><span class="line">默认值:</span><br><span class="line">    org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory</span><br><span class="line">    org.apache.flink.runtime.security.contexts.NoOpSecurityContextFactory</span><br><span class="line">应用于实例化安全上下文的工厂列表.</span><br><span class="line"></span><br><span class="line">security.module.factory.classes</span><br><span class="line">默认值:</span><br><span class="line">    org.apache.flink.runtime.security.modules.HadoopModuleFactory</span><br><span class="line">    org.apache.flink.runtime.security.modules.JaasModuleFactory</span><br><span class="line">    org.apache.flink.runtime.security.modules.ZookeeperModuleFactory</span><br><span class="line">应用于实例化安全模块的工厂列表.</span><br><span class="line"></span><br><span class="line">security.kerberos.login.principal</span><br><span class="line">默认值:无</span><br><span class="line">与keytab关联的Kerberos主体名称.</span><br><span class="line"></span><br><span class="line">security.kerberos.login.keytab</span><br><span class="line">默认值:无</span><br><span class="line">包含用户凭据的Kerberos keytab文件的绝对路径.</span><br><span class="line"></span><br><span class="line">security.kerberos.krb5-conf.path</span><br><span class="line">默认值:无</span><br><span class="line">指定krb5.conf文件的本地位置.</span><br><span class="line">如果定义了,这个conf将安装在JobManager和TaskManager containers/pods上,用于Kubernetes,Yarn和Mesos.</span><br><span class="line">注意:定义的KDC需要从容器内部可见.</span><br><span class="line"></span><br><span class="line">security.kerberos.login.use-ticket-cache</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">指示是否从Kerberos ticket缓存中读取.</span><br><span class="line"></span><br><span class="line">security.kerberos.login.contexts</span><br><span class="line">默认值:无</span><br><span class="line">以逗号分隔的登录上下文列表,用于提供Kerberos凭据.</span><br><span class="line"></span><br><span class="line">zookeeper.sasl.disable</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line"></span><br><span class="line">zookeeper.sasl.service-name</span><br><span class="line">默认值:zookeeper</span><br><span class="line"></span><br><span class="line">zookeeper.sasl.login-context-name</span><br><span class="line">默认值:Client</span><br><span class="line"></span><br><span class="line">security.ssl.enabled</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">为内部和外部网络通信打开SSL.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.enabled</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">为内部网络通信打开SSL.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.enabled</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">打开SSL以便通过REST端点进行外部通信.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.authentication-enabled</span><br><span class="line">默认值:<span class="literal">false</span></span><br><span class="line">为通过REST端点的外部通信启用相互SSL身份验证</span><br><span class="line"></span><br><span class="line">security.ssl.keystore</span><br><span class="line">默认值:无</span><br><span class="line">Flink端点用于其SSL密钥和证书的Java密钥库文件.</span><br><span class="line"></span><br><span class="line">security.ssl.keystore-password</span><br><span class="line">默认值:无</span><br><span class="line">解密密钥库文件的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.key-password</span><br><span class="line">默认值:无</span><br><span class="line">解密密钥库中服务器密钥的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.truststore</span><br><span class="line">默认值:无</span><br><span class="line">包含Flink端点用于验证对等方证书的公共CA证书的信任库文件.</span><br><span class="line"></span><br><span class="line">security.ssl.truststore-password</span><br><span class="line">默认值:无</span><br><span class="line">解密信任库的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.keystore</span><br><span class="line">默认值:无</span><br><span class="line">带有SSL密钥和证书的Java keystore文件,用于Flink的内部端点(rpc,数据传输,blob服务器).</span><br><span class="line"></span><br><span class="line">security.ssl.internal.keystore-password</span><br><span class="line">默认值:无</span><br><span class="line">解密密钥库文件的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.key-password</span><br><span class="line">默认值:无</span><br><span class="line">解密密钥库中密钥的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.truststore</span><br><span class="line">默认值:无</span><br><span class="line">包含Flink端点用于验证对等方证书的公共CA证书的信任库文件.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.truststore-password</span><br><span class="line">默认值:无</span><br><span class="line">解密信任库的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.cert.fingerprint</span><br><span class="line">默认值:无</span><br><span class="line">内部证书的sha1指纹.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.keystore</span><br><span class="line">默认值:无</span><br><span class="line">带有SSL密钥和证书的Javakeystore文件,用于Flink的外部REST端点.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.keystore-password</span><br><span class="line">默认值:无</span><br><span class="line">解密密钥库文件的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.key-password</span><br><span class="line">默认值:无</span><br><span class="line">解密密钥库中密钥的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.truststore</span><br><span class="line">默认值:无</span><br><span class="line">包含用于验证对等方证书的公共CA证书的信任库文件.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.truststore-password</span><br><span class="line">默认值:无</span><br><span class="line">解密信任库的密码.</span><br><span class="line"></span><br><span class="line">security.ssl.rest.cert.fingerprint</span><br><span class="line">默认值:无</span><br><span class="line">内部证书的sha1指纹.</span><br><span class="line"></span><br><span class="line">security.ssl.protocol</span><br><span class="line">默认值:TLSv1.2</span><br><span class="line">SSL传输支持的SSL协议版本.</span><br><span class="line"></span><br><span class="line">security.ssl.algorithms</span><br><span class="line">默认值:TLS_RSA_WITH_AES_128_CBC_SHA</span><br><span class="line">要支持的标准SSL算法的逗号分隔列表.</span><br><span class="line"></span><br><span class="line">security.ssl.verify-hostname</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">在SSL握手期间启用对等主机名验证的标志.</span><br><span class="line"></span><br><span class="line">security.ssl.provider</span><br><span class="line">默认值:JDK</span><br><span class="line">用于SSL传输的SSL引擎提供程序:</span><br><span class="line">JDK</span><br><span class="line">OPENSSL</span><br><span class="line"></span><br><span class="line">security.ssl.internal.session-cache-size</span><br><span class="line">默认值:-1</span><br><span class="line">用于存储SSL会话对象的缓存的大小.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.session-timeout</span><br><span class="line">默认值:缓存的SSL会话对象的超时(毫秒),-1表示用系统默认值.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.handshake-timeout</span><br><span class="line">默认值:-1</span><br><span class="line">SSL握手期间的超时.</span><br><span class="line"></span><br><span class="line">security.ssl.internal.close-notify-flush-timeout</span><br><span class="line">默认值:-1</span><br><span class="line">刷新由关闭通道触发的<span class="string">&quot;close_notify&quot;</span>的超时.</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="TaskManagerOptions-TM配置"><a href="#TaskManagerOptions-TM配置" class="headerlink" title="TaskManagerOptions(TM配置)"></a>TaskManagerOptions(TM配置)</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="WebOptions"><a href="#WebOptions" class="headerlink" title="WebOptions"></a>WebOptions</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">web.port</span><br><span class="line">默认值:8081</span><br><span class="line"></span><br><span class="line">web.access-control-allow-origin</span><br><span class="line">默认值:*</span><br><span class="line">访问控制允许来自web前端的所有响应的源站标头.</span><br><span class="line"></span><br><span class="line">web.refresh-interval</span><br><span class="line">默认值:3000L</span><br><span class="line">Web页面的刷新间隔.</span><br><span class="line"></span><br><span class="line">web.ssl.enabled</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">指示是否覆盖JobManager Web UI的SSL支持的标志.</span><br><span class="line"></span><br><span class="line">web.tmpdir</span><br><span class="line">默认值:java.io.tmpdir</span><br><span class="line">由WebMonitor使用的Flink Web目录.</span><br><span class="line"></span><br><span class="line">web.upload.dir</span><br><span class="line">默认值:无</span><br><span class="line">上载作业jar的目录.</span><br><span class="line">如果未指定动态目录,则将在JOB_MANAGER_WEB_TMPDIR_KEY指定的目录下使用.</span><br><span class="line"></span><br><span class="line">web.history</span><br><span class="line">默认值:5</span><br><span class="line">JobManager的存档作业数.</span><br><span class="line"></span><br><span class="line">web.log.path</span><br><span class="line">默认值:无</span><br><span class="line">日志文件的路径.</span><br><span class="line"></span><br><span class="line">web.submit.enable</span><br><span class="line">默认值:<span class="literal">true</span></span><br><span class="line">是否可以提交作业.</span><br><span class="line"></span><br><span class="line">web.checkpoints.history</span><br><span class="line">默认值:10</span><br><span class="line">要记住的最近历史记录的检查点数.</span><br><span class="line"></span><br><span class="line">web.timeout</span><br><span class="line">默认值:10 * 60 * 1000L</span><br><span class="line">Web监视器异步操作超时(毫秒).</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之一Producer发送者</title>
    <url>/2020/05/06/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E4%B8%80Producer%E5%8F%91%E9%80%81%E8%80%85/</url>
    <content><![CDATA[<blockquote>
<p>关注了大佬的Blog,对于自己深受打击,kafka-clients-2.1.0-CDH-6.2.0版本</p>
</blockquote>
<span id="more"></span>

<h2 id="Producer简单使用"><a href="#Producer简单使用" class="headerlink" title="Producer简单使用"></a>Producer简单使用</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.配置Producer需要配置</span></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;127.0.0.1:9092,127.0.0.2:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">topicName = <span class="string">&quot;test&quot;</span>;</span><br><span class="line">msgNum = <span class="number">10</span>; <span class="comment">// 发送的消息数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.初始化Producer实例</span></span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; msgNum; i++) &#123;</span><br><span class="line">    String msg = i + <span class="string">&quot; test&quot;</span>;</span><br><span class="line">    <span class="comment">// 3.调用send接口进行数据发送</span></span><br><span class="line">    producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topicName, msg));</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Producer发送流程"><a href="#Producer发送流程" class="headerlink" title="Producer发送流程"></a>Producer发送流程</h2><h3 id="send实现"><a href="#send实现" class="headerlink" title="send实现"></a>send实现</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> send(record, <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以知道当发送确认后,可以调用回调函数</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 拦截可能被修改的记录;此方法不引发异常</span></span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = <span class="keyword">this</span>.interceptors.onSend(record);</span><br><span class="line">    <span class="comment">// 最终使用doSend方法</span></span><br><span class="line">    <span class="keyword">return</span> doSend(interceptedRecord, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="doSend实现"><a href="#doSend实现" class="headerlink" title="doSend实现"></a>doSend实现</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">throwIfProducerClosed();<span class="comment">// 如果Producer已经关闭跑出异常</span></span><br><span class="line"><span class="comment">// 1.首先确保topic的metadata可用</span></span><br><span class="line">ClusterAndWaitTime clusterAndWaitTime;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">&#125; <span class="keyword">catch</span> (KafkaException e) &#123;</span><br><span class="line">    <span class="comment">// matadata关闭抛出异常</span></span><br><span class="line">    <span class="keyword">if</span> (metadata.isClosed())</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">&quot;Producer closed while send in progress&quot;</span>, e);</span><br><span class="line">    <span class="keyword">throw</span> e;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">long</span> remainingWaitMs = Math.max(<span class="number">0</span>, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line"><span class="comment">// 2.序列化record的topic,header,Key,Value</span></span><br><span class="line"><span class="keyword">byte</span>[] serializedKey;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert key of class &quot;</span> + record.key().getClass().getName() +</span><br><span class="line">            <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">            <span class="string">&quot; specified in key.serializer&quot;</span>, cce);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">byte</span>[] serializedValue;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">&#125; <span class="keyword">catch</span> (ClassCastException cce) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">&quot;Can&#x27;t convert value of class &quot;</span> + record.value().getClass().getName() +</span><br><span class="line">            <span class="string">&quot; to class &quot;</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">            <span class="string">&quot; specified in value.serializer&quot;</span>, cce);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 3.获取record的partition的值(可以在Record指定,也可以根据算法生成)</span></span><br><span class="line"><span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line"></span><br><span class="line">setReadOnly(record.headers());</span><br><span class="line">Header[] headers = record.headers().toArray();</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),</span><br><span class="line">        compressionType, serializedKey, serializedValue, headers);</span><br><span class="line">ensureValidRecordSize(serializedSize); <span class="comment">// 如果record的字节超出限制或大于内存限制,会抛出异常</span></span><br><span class="line"><span class="keyword">long</span> timestamp = record.timestamp() == <span class="keyword">null</span> ? time.milliseconds() : record.timestamp();</span><br><span class="line">log.trace(<span class="string">&quot;Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;&quot;</span>, record, callback, record.topic(), partition);</span><br><span class="line"><span class="comment">// producer callback will make sure to call both &#x27;callback&#x27; and interceptor callback</span></span><br><span class="line">Callback interceptCallback = <span class="keyword">new</span> InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">    transactionManager.maybeAddPartitionToTransaction(tp);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4.向accumulator追加数据</span></span><br><span class="line">RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">        serializedValue, headers, interceptCallback, remainingWaitMs);</span><br><span class="line"><span class="comment">// 5.如果batch满了,唤醒sender线程发送数据</span></span><br><span class="line"><span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">    log.trace(<span class="string">&quot;Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch&quot;</span>, record.topic(), partition);</span><br><span class="line">    <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result.future;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="发送过程详解"><a href="#发送过程详解" class="headerlink" title="发送过程详解"></a>发送过程详解</h2><h3 id="获取topic的metadata信息"><a href="#获取topic的metadata信息" class="headerlink" title="获取topic的metadata信息"></a>获取topic的metadata信息</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 等待Metadata的更新</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> ClusterAndWaitTime <span class="title">waitOnMetadata</span><span class="params">(String topic, Integer partition, <span class="keyword">long</span> maxWaitMs)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// 将topic添加到元数据topic列表（如果尚未存在），并重置过期时间</span></span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cluster.invalidTopics().contains(topic))</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> InvalidTopicException(topic);</span><br><span class="line"></span><br><span class="line">    metadata.add(topic);</span><br><span class="line"></span><br><span class="line">    Integer partitionsCount = cluster.partitionCountForTopic(topic); <span class="comment">// 如果topic已经存在meta中,则返回该topic的partition数,否则返回null</span></span><br><span class="line">    <span class="comment">// 如果有缓存的元数据,并且记录的分区未定义或在已知分区范围内,则返回该元数据</span></span><br><span class="line">    <span class="keyword">if</span> (partitionsCount != <span class="keyword">null</span> &amp;&amp; (partition == <span class="keyword">null</span> || partition &lt; partitionsCount))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ClusterAndWaitTime(cluster, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> begin = time.milliseconds();</span><br><span class="line">    <span class="keyword">long</span> remainingWaitMs = maxWaitMs;</span><br><span class="line">    <span class="keyword">long</span> elapsed;</span><br><span class="line">    <span class="comment">// 发送metadata请求,直到获取这个topic的metadata或者请求超时</span></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        log.trace(<span class="string">&quot;Requesting metadata update for topic &#123;&#125;.&quot;</span>, topic);</span><br><span class="line">        metadata.add(topic);</span><br><span class="line">        <span class="keyword">int</span> version = metadata.requestUpdate(); <span class="comment">// 返回当前版本号,初始值为0,每次更新时会自增,并将needUpdate设置为true</span></span><br><span class="line">        sender.wakeup(); <span class="comment">// 唤起sender,发送metadata请求</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            metadata.awaitUpdate(version, remainingWaitMs); <span class="comment">// 等待metadata的更新</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (TimeoutException ex) &#123;</span><br><span class="line">            <span class="comment">// Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">&quot;Failed to update metadata after &quot;</span> + maxWaitMs + <span class="string">&quot; ms.&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        cluster = metadata.fetch();</span><br><span class="line">        elapsed = time.milliseconds() - begin;</span><br><span class="line">        <span class="keyword">if</span> (elapsed &gt;= maxWaitMs)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">&quot;Failed to update metadata after &quot;</span> + maxWaitMs + <span class="string">&quot; ms.&quot;</span>); <span class="comment">// 超时</span></span><br><span class="line">        <span class="keyword">if</span> (cluster.unauthorizedTopics().contains(topic))</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TopicAuthorizationException(topic); <span class="comment">// 认证失败</span></span><br><span class="line">        <span class="keyword">if</span> (cluster.invalidTopics().contains(topic))</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> InvalidTopicException(topic);</span><br><span class="line">        remainingWaitMs = maxWaitMs - elapsed;</span><br><span class="line">        partitionsCount = cluster.partitionCountForTopic(topic);</span><br><span class="line">    &#125; <span class="keyword">while</span> (partitionsCount == <span class="keyword">null</span>); <span class="comment">// 不停循环,直到partitionsCount不为null(直到metadata出现这个topic的相关信息)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (partition != <span class="keyword">null</span> &amp;&amp; partition &gt;= partitionsCount) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(</span><br><span class="line">                String.format(<span class="string">&quot;Invalid partition given with record: %d is not in the range [0...%d).&quot;</span>, partition, partitionsCount));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ClusterAndWaitTime(cluster, elapsed);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Metadata更新操作"><a href="#Metadata更新操作" class="headerlink" title="Metadata更新操作"></a>Metadata更新操作</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 更新metadata信息(根据version值判断)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">awaitUpdate</span><span class="params">(<span class="keyword">final</span> <span class="keyword">int</span> lastVersion, <span class="keyword">final</span> <span class="keyword">long</span> maxWaitMs)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (maxWaitMs &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">&quot;Max time to wait for metadata updates should not be &lt; 0 milliseconds&quot;</span>);</span><br><span class="line">    <span class="keyword">long</span> begin = System.currentTimeMillis();</span><br><span class="line">    <span class="keyword">long</span> remainingWaitMs = maxWaitMs;</span><br><span class="line">    <span class="keyword">while</span> ((<span class="keyword">this</span>.version &lt;= lastVersion) &amp;&amp; !isClosed()) &#123; <span class="comment">// 不断循环,直到metadata更新成功,version自增</span></span><br><span class="line">        AuthenticationException ex = getAndClearAuthenticationException();</span><br><span class="line">        <span class="keyword">if</span> (ex != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> ex;</span><br><span class="line">        <span class="keyword">if</span> (remainingWaitMs != <span class="number">0</span>)</span><br><span class="line">            wait(remainingWaitMs); <span class="comment">// 阻塞线程,等待metadata更新</span></span><br><span class="line">        <span class="keyword">long</span> elapsed = System.currentTimeMillis() - begin;</span><br><span class="line">        <span class="keyword">if</span> (elapsed &gt;= maxWaitMs) <span class="comment">// 超时</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">&quot;Failed to update metadata after &quot;</span> + maxWaitMs + <span class="string">&quot; ms.&quot;</span>);</span><br><span class="line">        remainingWaitMs = maxWaitMs - elapsed;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (isClosed())</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">&quot;Requested metadata update after close&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至此,Producer线程会阻塞在两个while循环中,直到metadata更新.metadata更新主要通过sender.wakeup()来唤醒sender线程,间接唤醒NetworkClient线程,NetworkClient线程来负责发送Metadata请求,并处理Server端的响应.在唤醒NetworkClient后会调用poll方法进行实际操作,如下:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;ClientResponse&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!abortedSends.isEmpty()) &#123;</span><br><span class="line">        <span class="comment">// If there are aborted sends because of unsupported version exceptions or disconnects,</span></span><br><span class="line">        <span class="comment">// handle them immediately without waiting for Selector#poll.</span></span><br><span class="line">        List&lt;ClientResponse&gt; responses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        handleAbortedSends(responses);</span><br><span class="line">        completeResponses(responses);</span><br><span class="line">        <span class="keyword">return</span> responses;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> metadataTimeout = metadataUpdater.maybeUpdate(now); <span class="comment">// 判断是否需要更新meta,如果需要就更新(请求更新metadata的地方)</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">this</span>.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        log.error(<span class="string">&quot;Unexpected error during I/O&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// process completed actions</span></span><br><span class="line">    <span class="keyword">long</span> updatedNow = <span class="keyword">this</span>.time.milliseconds();</span><br><span class="line">    List&lt;ClientResponse&gt; responses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    handleCompletedSends(responses, updatedNow); <span class="comment">// // 通过selector中获取Server端的response</span></span><br><span class="line">    handleCompletedReceives(responses, updatedNow); <span class="comment">// 在返回的handler中,会处理metadata的更新</span></span><br><span class="line">    handleDisconnections(responses, updatedNow);</span><br><span class="line">    handleConnections();</span><br><span class="line">    handleInitiateApiVersionRequests(updatedNow);</span><br><span class="line">    handleTimedOutRequests(responses, updatedNow);</span><br><span class="line">    completeResponses(responses);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> responses;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>判断Metadata是否需要更新,如果需要更新,先与Broker建立连接,然后发送更新metadata请求</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">maybeUpdate</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// should we update our metadata?</span></span><br><span class="line">    <span class="comment">// metadata是否应该更新</span></span><br><span class="line">    <span class="keyword">long</span> timeToNextMetadataUpdate = metadata.timeToNextUpdate(now); <span class="comment">// metadata下次更新的时间(需要判断是强制更新还是metadata过期更新,前者是立马更新,后者是计算metadata的过期时间)</span></span><br><span class="line">    <span class="comment">// 如果一条metadata的fetch请求还未从server收到回复,那么时间设置为waitForMetadataFetch(默认30s)</span></span><br><span class="line">    <span class="keyword">long</span> waitForMetadataFetch = <span class="keyword">this</span>.metadataFetchInProgress ? defaultRequestTimeoutMs : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> metadataTimeout = Math.max(timeToNextMetadataUpdate, waitForMetadataFetch);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (metadataTimeout &gt; <span class="number">0</span>) &#123; <span class="comment">// 时间未到时,直接返回下次应该更新的时间</span></span><br><span class="line">        <span class="keyword">return</span> metadataTimeout;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 选择一个连接数最小的节点</span></span><br><span class="line">    Node node = leastLoadedNode(now);</span><br><span class="line">    <span class="keyword">if</span> (node == <span class="keyword">null</span>) &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;Give up sending metadata request since no node is available&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> reconnectBackoffMs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> maybeUpdate(now, node); <span class="comment">// 可以发送metadata请求的话,就发送metadata请求</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断是否可以发送请求,可以的话将metadata请求加入到发送列表中</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">maybeUpdate</span><span class="params">(<span class="keyword">long</span> now, Node node)</span> </span>&#123;</span><br><span class="line">    String nodeConnectionId = node.idString();</span><br><span class="line">    <span class="keyword">if</span> (canSendRequest(nodeConnectionId, now)) &#123; <span class="comment">// 通道已经准备好,并且支持发送更多的请求</span></span><br><span class="line">        <span class="keyword">this</span>.metadataFetchInProgress = <span class="keyword">true</span>; <span class="comment">// 准备开始发送数据,将metadataFetchInProgress置为true</span></span><br><span class="line">        MetadataRequest.Builder metadataRequest; <span class="comment">// // 创建metadata请求</span></span><br><span class="line">        <span class="keyword">if</span> (metadata.needMetadataForAllTopics()) <span class="comment">// 强制更新所有topic的metadata(虽然默认不会更新所有topic的 metadata信息,但是每个Broker会保存所有topic的meta信息)</span></span><br><span class="line">            metadataRequest = MetadataRequest.Builder.allTopics();</span><br><span class="line">        <span class="keyword">else</span> <span class="comment">// 只更新metadata中的topics列表(列表中的topics由metadata.add()得到)</span></span><br><span class="line">            metadataRequest = <span class="keyword">new</span> MetadataRequest.Builder(<span class="keyword">new</span> ArrayList&lt;&gt;(metadata.topics()),</span><br><span class="line">                    metadata.allowAutoTopicCreation());</span><br><span class="line">        log.debug(<span class="string">&quot;Sending metadata request &#123;&#125; to node &#123;&#125;&quot;</span>, metadataRequest, node);</span><br><span class="line">        sendInternalMetadataRequest(metadataRequest, nodeConnectionId, now); <span class="comment">// 发送metadata请求</span></span><br><span class="line">        <span class="keyword">return</span> defaultRequestTimeoutMs;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果client正在与任何一个node的连接状态是connecting,那么就进行等待</span></span><br><span class="line">    <span class="keyword">if</span> (isAnyNodeConnecting()) &#123;</span><br><span class="line">        <span class="comment">// Strictly the timeout we should return here is &quot;connect timeout&quot;, but as we don&#x27;t</span></span><br><span class="line">        <span class="comment">// have such application level configuration, using reconnect backoff instead.</span></span><br><span class="line">        <span class="keyword">return</span> reconnectBackoffMs;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果没有连接这个node,那就初始化连接</span></span><br><span class="line">    <span class="keyword">if</span> (connectionStates.canConnect(nodeConnectionId, now)) &#123;</span><br><span class="line">        <span class="comment">// we don&#x27;t have a connection to this node right now, make one</span></span><br><span class="line">        log.debug(<span class="string">&quot;Initialize connection to node &#123;&#125; for sending metadata request&quot;</span>, node);</span><br><span class="line">        initiateConnect(node, now); <span class="comment">// 初始化连接</span></span><br><span class="line">        <span class="keyword">return</span> reconnectBackoffMs;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// connected, but can&#x27;t send more OR connecting</span></span><br><span class="line">    <span class="comment">// In either case, we just need to wait for a network event to let us know the selected</span></span><br><span class="line">    <span class="comment">// connection might be usable again.</span></span><br><span class="line">    <span class="keyword">return</span> Long.MAX_VALUE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送metadata请求</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendInternalMetadataRequest</span><span class="params">(MetadataRequest.Builder builder,String nodeConnectionId, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    ClientRequest clientRequest = newClientRequest(nodeConnectionId, builder, now, <span class="keyword">true</span>);</span><br><span class="line">    doSend(clientRequest, <span class="keyword">true</span>, now);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每次Producer请求更新metadata时的情况</p>
<ul>
<li>如果node可以发送请求,则直接发送请求</li>
<li>如果该node正在建立连接,则直接返回</li>
<li>如果该node还没建立连接,则向broker初始化连接</li>
</ul>
<p>KafkaProducer线程被两个while循环中,知道metadata更新</p>
<ul>
<li>sender线程第一次调用poll,初始化与node的连接</li>
<li>sender线程第二次调用poll,发送Metadata请求</li>
<li>sender线程第三次调用poll,获取metadataResponse,更新metadata</li>
</ul>
<p>当不阻塞之后,Producer才会开始发送信息<br>NetworkClient接收到Server端对Metadata请求的响应后,更新metadata信息</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 处理任何已经完成的接收响应</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleCompletedReceives</span><span class="params">(List&lt;ClientResponse&gt; responses, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (NetworkReceive receive : <span class="keyword">this</span>.selector.completedReceives()) &#123;</span><br><span class="line">        String source = receive.source();</span><br><span class="line">        InFlightRequest req = inFlightRequests.completeNext(source);</span><br><span class="line">        Struct responseStruct = parseStructMaybeUpdateThrottleTimeMetrics(receive.payload(), req.header,</span><br><span class="line">            throttleTimeSensor, now);</span><br><span class="line">        <span class="keyword">if</span> (log.isTraceEnabled()) &#123;</span><br><span class="line">            log.trace(<span class="string">&quot;Completed receive from node &#123;&#125; for &#123;&#125; with correlation id &#123;&#125;, received &#123;&#125;&quot;</span>, req.destination,</span><br><span class="line">                req.header.apiKey(), req.header.correlationId(), responseStruct);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// If the received response includes a throttle delay, throttle the connection.</span></span><br><span class="line">        AbstractResponse body = AbstractResponse.parseResponse(req.header.apiKey(), responseStruct);</span><br><span class="line">        maybeThrottle(body, req.header.apiVersion(), req.destination, now);</span><br><span class="line">        <span class="keyword">if</span> (req.isInternalRequest &amp;&amp; body <span class="keyword">instanceof</span> MetadataResponse) <span class="comment">// 如果是meta响应</span></span><br><span class="line">            metadataUpdater.handleCompletedMetadataResponse(req.header, now, (MetadataResponse) body);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (req.isInternalRequest &amp;&amp; body <span class="keyword">instanceof</span> ApiVersionsResponse)</span><br><span class="line">            handleApiVersionsResponse(responses, req, now, (ApiVersionsResponse) body);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            responses.add(req.completed(body, now));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理Server端对Metadata请求处理后的响应</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleCompletedMetadataResponse</span><span class="params">(RequestHeader requestHeader, <span class="keyword">long</span> now, MetadataResponse response)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.metadataFetchInProgress = <span class="keyword">false</span>;</span><br><span class="line">    Cluster cluster = response.cluster();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If any partition has leader with missing listeners, log a few for diagnosing broker configuration</span></span><br><span class="line">    <span class="comment">// issues. This could be a transient issue if listeners were added dynamically to brokers.</span></span><br><span class="line">    List&lt;TopicPartition&gt; missingListenerPartitions = response.topicMetadata().stream().flatMap(topicMetadata -&gt;</span><br><span class="line">        topicMetadata.partitionMetadata().stream()</span><br><span class="line">            .filter(partitionMetadata -&gt; partitionMetadata.error() == Errors.LISTENER_NOT_FOUND)</span><br><span class="line">            .map(partitionMetadata -&gt; <span class="keyword">new</span> TopicPartition(topicMetadata.topic(), partitionMetadata.partition())))</span><br><span class="line">        .collect(Collectors.toList());</span><br><span class="line">    <span class="keyword">if</span> (!missingListenerPartitions.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">int</span> count = missingListenerPartitions.size();</span><br><span class="line">        log.warn(<span class="string">&quot;&#123;&#125; partitions have leader brokers without a matching listener, including &#123;&#125;&quot;</span>,</span><br><span class="line">                count, missingListenerPartitions.subList(<span class="number">0</span>, Math.min(<span class="number">10</span>, count)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check if any topics metadata failed to get updated</span></span><br><span class="line">    Map&lt;String, Errors&gt; errors = response.errors();</span><br><span class="line">    <span class="keyword">if</span> (!errors.isEmpty())</span><br><span class="line">        log.warn(<span class="string">&quot;Error while fetching metadata with correlation id &#123;&#125; : &#123;&#125;&quot;</span>, requestHeader.correlationId(), errors);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// don&#x27;t update the cluster if there are no valid nodes...the topic we want may still be in the process of being</span></span><br><span class="line">    <span class="comment">// created which means we will get errors and no nodes until it exists</span></span><br><span class="line">    <span class="keyword">if</span> (cluster.nodes().size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 更新meta信息</span></span><br><span class="line">        <span class="keyword">this</span>.metadata.update(cluster, response.unavailableTopics(), now);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果metadata中node信息无效,则不更新信息</span></span><br><span class="line">        log.trace(<span class="string">&quot;Ignoring empty metadata response with correlation id &#123;&#125;.&quot;</span>, requestHeader.correlationId());</span><br><span class="line">        <span class="keyword">this</span>.metadata.failedUpdate(now, <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Metadata更新策略"><a href="#Metadata更新策略" class="headerlink" title="Metadata更新策略"></a>Metadata更新策略</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">KafkaProducer第一次发送信息时强制更新,其他时间周期性更新,通过lastRefreshMs,lastSuccessfulRefreshMs两个字段实现</span><br><span class="line">强制更新:调用Metadata.requestUpdate()将needUpdate置为true</span><br><span class="line"></span><br><span class="line">强制更新触发:</span><br><span class="line">    initConnect()初始化连接</span><br><span class="line">    poll()对handleDisconnections()处理连接断开情况</span><br><span class="line">    poll()对handleTimedOutRequests()处理请求超时</span><br><span class="line">    发送信息时找不到partition的leader</span><br><span class="line">    处理Producer响应(handleProduceResponse),如果返回关于metadata过期的异常</span><br><span class="line"></span><br><span class="line">强制更新主要用于处理各种异常情况</span><br></pre></td></tr></table></figure>
<h3 id="key和value序列化"><a href="#key和value序列化" class="headerlink" title="key和value序列化"></a>key和value序列化</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 内部提供了一些序列化方法</span></span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当然,也可以自定义序列化的具体实现</span></span><br></pre></td></tr></table></figure>
<h3 id="获取Partition值"><a href="#获取Partition值" class="headerlink" title="获取Partition值"></a>获取Partition值</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 1.指明partition的情况下,直接将指明的值作为partition值</span></span><br><span class="line"><span class="comment">// 2.没有指明partition值但有key的情况下,将key的hash值与topic的partition数进行取余得到partition值</span></span><br><span class="line"><span class="comment">// 3.既没有partition值又没有key值,第一次调用时随机生成一个整数(后面每次调用在这个整数上自增),将这个值与topic的partition数进行取余得到partition值(Round-robin算法)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// record有partition值时直接返回,不然调用partitioner的partition方法去计算</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">            partition :</span><br><span class="line">            partitioner.partition(</span><br><span class="line">                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Producer默认使用DefaultPartitioner,可以自定义partition策略</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">    List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">    <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">    <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123; <span class="comment">// 没有Key的情况下</span></span><br><span class="line">        <span class="keyword">int</span> nextValue = nextValue(topic); <span class="comment">// 第一次生成随机整数,后面每次调用都自增</span></span><br><span class="line">        List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">        <span class="comment">// leader不为null,即为可用的partition</span></span><br><span class="line">        <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">            <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// 有Key的情况,使用key的hash值进行计算</span></span><br><span class="line">        <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">    AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123; <span class="comment">// 第一次调用,随机整数</span></span><br><span class="line">        counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">        AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">        <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">            counter = currentCounter;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> counter.getAndIncrement(); <span class="comment">// 自增</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="向Accumulator写数据"><a href="#向Accumulator写数据" class="headerlink" title="向Accumulator写数据"></a>向Accumulator写数据</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Producer先将record写入到buffer中,当达到一个batch.size的大小时,唤起sender线程取发送ProducerBatch</span></span><br><span class="line"><span class="comment">// Producer通过RecordAccumulator实例追加数据,主要变量为ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches</span></span><br><span class="line"><span class="comment">// 每个TopicPartition都对应一个Deque&lt;ProducerBatch&gt;</span></span><br><span class="line"><span class="comment">// 当添加数据时,会向其topic-partition对应的这个queue最新创建的一个ProducerBatch中添加record</span></span><br><span class="line"><span class="comment">// 而发送数据时,则会先从queue中最老的那个RecordBatch开始发送</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 向accumulator添加一条record,并返回添加后的结果(结果包含,future metadata,batch是否满的标志以及新batch是否创建)</span></span><br><span class="line"><span class="comment">// 其中,maxTimeToBlock是buffer.memory的block的最大时间</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     Header[] headers,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">long</span> maxTimeToBlock)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// We keep track of the number of appending thread to make sure we do not miss batches in</span></span><br><span class="line">    <span class="comment">// abortIncompleteBatches().</span></span><br><span class="line">    appendsInProgress.incrementAndGet();</span><br><span class="line">    ByteBuffer buffer = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (headers == <span class="keyword">null</span>) headers = Record.EMPTY_HEADERS;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// check if we have an in-progress batch</span></span><br><span class="line">        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp); <span class="comment">// 每个topicPartition对应一个queue</span></span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123; <span class="comment">// 在对一个queue进行操作时,会保证线程安全</span></span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">&quot;Producer closed while send in progress&quot;</span>);</span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq); <span class="comment">// 追加数据</span></span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> appendResult; <span class="comment">// 这个topic-partition已经有记录了</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we don&#x27;t have an in-progress record batch try to allocate a new batch</span></span><br><span class="line">        <span class="comment">// 为topic-partition创建一个新的ProducerBatch,需要初始化相应的ProducerBatch,要为其分配的大小是:max(batch.size,加上头文件的本条消息的大小)</span></span><br><span class="line">        <span class="keyword">byte</span> maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">        <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">        log.trace(<span class="string">&quot;Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;&quot;</span>, size, tp.topic(), tp.partition());</span><br><span class="line">        buffer = free.allocate(size, maxTimeToBlock); <span class="comment">// 给这个ProducerBatch初始化一个buffer</span></span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="comment">// Need to check if producer is closed again after grabbing the dequeue lock.</span></span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">&quot;Producer closed while send in progress&quot;</span>);</span><br><span class="line"></span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) &#123; <span class="comment">// 如果突然发现这个queue已经存在,直接返回</span></span><br><span class="line">                <span class="comment">// Somebody else found us a batch, return the one we waited for! Hopefully this doesn&#x27;t happen often...</span></span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 给topic-partition创建一个ProducerBatch</span></span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">            ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">            <span class="comment">// 向新的ProducerBatch中追加数据</span></span><br><span class="line">            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将RecordBatch添加到对应的queue中</span></span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            <span class="comment">// 向未ack的batch集合添加这个batch</span></span><br><span class="line">            incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Don&#x27;t deallocate this buffer in the finally block as it&#x27;s being used in the record batch</span></span><br><span class="line">            buffer = <span class="keyword">null</span>;</span><br><span class="line">            <span class="comment">// 如果dp.size()&gt;1就证明这个queue有一个batch是可以发送了</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="keyword">null</span>)</span><br><span class="line">            free.deallocate(buffer);</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="发送ProducerBatch"><a href="#发送ProducerBatch" class="headerlink" title="发送ProducerBatch"></a>发送ProducerBatch</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 当record写入成功后,如果发现ProducerBatch满足发送的条件(通常是queue中有多个Batch,那么最先添加的batch肯定是可以发送的)</span></span><br><span class="line"><span class="comment">// 那么就会唤醒sender线程,发送ProducerBatch</span></span><br><span class="line"><span class="comment">// sender线程对ProducerBatch的处理是在run()方法中进行的</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (transactionManager.shouldResetProducerStateAfterResolvingSequences())</span><br><span class="line">                <span class="comment">// Check if the previous run expired batches which requires a reset of the producer state.</span></span><br><span class="line">                transactionManager.resetProducerId();</span><br><span class="line">            <span class="keyword">if</span> (!transactionManager.isTransactional()) &#123;</span><br><span class="line">                <span class="comment">// this is an idempotent producer, so make sure we have a producer id</span></span><br><span class="line">                maybeWaitForProducerId();</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transactionManager.hasUnresolvedSequences() &amp;&amp; !transactionManager.hasFatalError()) &#123;</span><br><span class="line">                transactionManager.transitionToFatalError(</span><br><span class="line">                    <span class="keyword">new</span> KafkaException(<span class="string">&quot;The client hasn&#x27;t received acknowledgment for &quot;</span> +</span><br><span class="line">                        <span class="string">&quot;some previously sent messages and can no longer retry them. It isn&#x27;t safe to continue.&quot;</span>));</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transactionManager.hasInFlightTransactionalRequest() || maybeSendTransactionalRequest(now)) &#123;</span><br><span class="line">                <span class="comment">// as long as there are outstanding transactional requests, we simply wait for them to return</span></span><br><span class="line">                client.poll(retryBackoffMs, now);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// do not continue sending if the transaction manager is in a failed state or if there</span></span><br><span class="line">            <span class="comment">// is no producer id (for the idempotent case).</span></span><br><span class="line">            <span class="keyword">if</span> (transactionManager.hasFatalError() || !transactionManager.hasProducerId()) &#123;</span><br><span class="line">                RuntimeException lastError = transactionManager.lastError();</span><br><span class="line">                <span class="keyword">if</span> (lastError != <span class="keyword">null</span>)</span><br><span class="line">                    maybeAbortBatches(lastError);</span><br><span class="line">                client.poll(retryBackoffMs, now);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (transactionManager.hasAbortableError()) &#123;</span><br><span class="line">                accumulator.abortUndrainedBatches(transactionManager.lastError());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (AuthenticationException e) &#123;</span><br><span class="line">            <span class="comment">// This is already logged as error, but propagated here to perform any clean ups.</span></span><br><span class="line">            log.trace(<span class="string">&quot;Authentication exception while processing transactional request: &#123;&#125;&quot;</span>, e);</span><br><span class="line">            transactionManager.authenticationFailed(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发送Producer数据</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = sendProducerData(now);</span><br><span class="line">    client.poll(pollTimeout, now);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">sendProducerData</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line">    <span class="comment">// 获取那些已经可以发送的ProducerBatch对应的nodes</span></span><br><span class="line">    RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果有topic-partition的leader是未知的,就强制更新metadata</span></span><br><span class="line">    <span class="keyword">if</span> (!result.unknownLeaderTopics.isEmpty()) &#123;</span><br><span class="line">        <span class="keyword">for</span> (String topic : result.unknownLeaderTopics)</span><br><span class="line">            <span class="keyword">this</span>.metadata.add(topic);</span><br><span class="line"></span><br><span class="line">        log.debug(<span class="string">&quot;Requesting metadata update due to unknown leader topics from the batched records: &#123;&#125;&quot;</span>,</span><br><span class="line">            result.unknownLeaderTopics);</span><br><span class="line">        <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果与node没有连接(如果可以连接,同时初始化该连接),就证明该node暂时不能发送数据,暂时移除该node</span></span><br><span class="line">    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">    <span class="keyword">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        Node node = iter.next();</span><br><span class="line">        <span class="keyword">if</span> (!<span class="keyword">this</span>.client.ready(node, now)) &#123;</span><br><span class="line">            iter.remove();</span><br><span class="line">            notReadyTimeout = Math.min(notReadyTimeout, <span class="keyword">this</span>.client.pollDelayMs(node, now));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回该node对应的所有可以发送的ProducerBatch组成的batches(key是node.id),并将ProducerBatch从对应的queue中移除</span></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes, <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line">    addToInflightBatches(batches);</span><br><span class="line">    <span class="keyword">if</span> (guaranteeMessageOrder) &#123;</span><br><span class="line">        <span class="comment">// 记录将要发送的ProducerBatch</span></span><br><span class="line">        <span class="keyword">for</span> (List&lt;ProducerBatch&gt; batchList : batches.values()) &#123;</span><br><span class="line">            <span class="keyword">for</span> (ProducerBatch batch : batchList)</span><br><span class="line">                <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    accumulator.resetNextBatchExpiryTime();</span><br><span class="line">    List&lt;ProducerBatch&gt; expiredInflightBatches = getExpiredInflightBatches(now);</span><br><span class="line">    List&lt;ProducerBatch&gt; expiredBatches = <span class="keyword">this</span>.accumulator.expiredBatches(now);</span><br><span class="line">    expiredBatches.addAll(expiredInflightBatches);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!expiredBatches.isEmpty())</span><br><span class="line">        log.trace(<span class="string">&quot;Expired &#123;&#125; batches in accumulator&quot;</span>, expiredBatches.size());</span><br><span class="line">    <span class="comment">// 将由于元数据不可用而导致发送超时的ProducerBatch移除</span></span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch expiredBatch : expiredBatches) &#123;</span><br><span class="line">        String errorMessage = <span class="string">&quot;Expiring &quot;</span> + expiredBatch.recordCount + <span class="string">&quot; record(s) for &quot;</span> + expiredBatch.topicPartition</span><br><span class="line">            + <span class="string">&quot;:&quot;</span> + (now - expiredBatch.createdMs) + <span class="string">&quot; ms has passed since batch creation&quot;</span>;</span><br><span class="line">        failBatch(expiredBatch, -<span class="number">1</span>, NO_TIMESTAMP, <span class="keyword">new</span> TimeoutException(errorMessage), <span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; expiredBatch.inRetry()) &#123;</span><br><span class="line">            transactionManager.markSequenceUnresolved(expiredBatch.topicPartition);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    sensors.updateProduceRequestMetrics(batches);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class="line">    pollTimeout = Math.min(pollTimeout, <span class="keyword">this</span>.accumulator.nextExpiryTimeMs() - now);</span><br><span class="line">    pollTimeout = Math.max(pollTimeout, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (!result.readyNodes.isEmpty()) &#123;</span><br><span class="line">        log.trace(<span class="string">&quot;Nodes with data ready to send: &#123;&#125;&quot;</span>, result.readyNodes);</span><br><span class="line">        pollTimeout = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 发送ProducerBatch</span></span><br><span class="line">    sendProduceRequests(batches, now);</span><br><span class="line">    <span class="keyword">return</span> pollTimeout;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequests</span><span class="params">(Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; collated, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Integer, List&lt;ProducerBatch&gt;&gt; entry : collated.entrySet())</span><br><span class="line">        sendProduceRequest(now, entry.getKey(), acks, requestTimeoutMs, entry.getValue());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 发送哦Produce请求</span></span><br><span class="line"><span class="comment">// 将batches中leader为同一个node的所有ProducerBatch放在一个请求中进行发送</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;ProducerBatch&gt; batches)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (batches.isEmpty())</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, ProducerBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// find the minimum magic version used when creating the record sets</span></span><br><span class="line">    <span class="keyword">byte</span> minUsedMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch batch : batches) &#123;</span><br><span class="line">        <span class="keyword">if</span> (batch.magic() &lt; minUsedMagic)</span><br><span class="line">            minUsedMagic = batch.magic();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch batch : batches) &#123;</span><br><span class="line">        TopicPartition tp = batch.topicPartition;</span><br><span class="line">        MemoryRecords records = batch.records();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// down convert if necessary to the minimum magic used. In general, there can be a delay between the time</span></span><br><span class="line">        <span class="comment">// that the producer starts building the batch and the time that we send the request, and we may have</span></span><br><span class="line">        <span class="comment">// chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use</span></span><br><span class="line">        <span class="comment">// the new message format, but found that the broker didn&#x27;t support it, so we need to down-convert on the</span></span><br><span class="line">        <span class="comment">// client before sending. This is intended to handle edge cases around cluster upgrades where brokers may</span></span><br><span class="line">        <span class="comment">// not all support the same message format version. For example, if a partition migrates from a broker</span></span><br><span class="line">        <span class="comment">// which is supporting the new magic version to one which doesn&#x27;t, then we will need to convert.</span></span><br><span class="line">        <span class="keyword">if</span> (!records.hasMatchingMagic(minUsedMagic))</span><br><span class="line">            records = batch.records().downConvert(minUsedMagic, <span class="number">0</span>, time).records();</span><br><span class="line">        produceRecordsByPartition.put(tp, records);</span><br><span class="line">        recordsByPartition.put(tp, batch);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    String transactionalId = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional()) &#123;</span><br><span class="line">        transactionalId = transactionManager.transactionalId();</span><br><span class="line">    &#125;</span><br><span class="line">    ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout,</span><br><span class="line">            produceRecordsByPartition, transactionalId);</span><br><span class="line">    RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>&#123;</span><br><span class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    String nodeId = Integer.toString(destination);</span><br><span class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>,</span><br><span class="line">            requestTimeoutMs, callback);</span><br><span class="line">    client.send(clientRequest, now);</span><br><span class="line">    log.trace(<span class="string">&quot;Sent produce request to &#123;&#125;: &#123;&#125;&quot;</span>, nodeId, requestBuilder);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka源码系列之十副本管理写入</title>
    <url>/2020/05/08/Kafka%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%81%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86%E5%86%99%E5%85%A5/</url>
    <content><![CDATA[<blockquote>
<p>介绍Server端接收到Produce请求,是如何进行处理的,处理之后分片是如何产生的</p>
</blockquote>
<span id="more"></span>

<h2 id="Client端发送Produce请求"><a href="#Client端发送Produce请求" class="headerlink" title="Client端发送Produce请求"></a>Client端发送Produce请求</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在第一章有详细描述,Producer是如何向Server发送请求的</span><br><span class="line">主要是在Sender.sendProduceRequests()方法中实现</span><br><span class="line">发送List&lt;ProducerBatch&gt; batches</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Server端处理Produce请求"><a href="#Server端处理Produce请求" class="headerlink" title="Server端处理Produce请求"></a>Server端处理Produce请求</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在之前的章节里,可以知道Server端Broker在收到Produce请求后</span><br><span class="line">会有一个KafkaApis进行处理,它是Server端处理所有请求的入口</span><br><span class="line">KafkaApis.PRODUCE -&gt; handleProduceRequest()</span><br></pre></td></tr></table></figure>
<h3 id="handleProduceRequest"><a href="#handleProduceRequest" class="headerlink" title="handleProduceRequest"></a>handleProduceRequest</h3><p>整体为,查看topic是否存在,client是否有相应的Describe权限<br>对于已经有Describe权限的topic查看是否有Write权限<br>调用replicaManager.appendRecords()方法向有Write权限的tp追加相应的record</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleProduceRequest</span></span>(request: <span class="type">RequestChannel</span>.<span class="type">Request</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> produceRequest = request.body[<span class="type">ProduceRequest</span>]</span><br><span class="line">    <span class="keyword">val</span> numBytesAppended = request.header.toStruct.sizeOf + request.sizeOfBodyInBytes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (produceRequest.isTransactional) &#123;</span><br><span class="line">      <span class="comment">// 事务授权失败</span></span><br><span class="line">      <span class="keyword">if</span> (!authorize(request.session, <span class="type">Write</span>, <span class="type">Resource</span>(<span class="type">TransactionalId</span>, produceRequest.transactionalId, <span class="type">LITERAL</span>))) &#123;</span><br><span class="line">        sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">TRANSACTIONAL_ID_AUTHORIZATION_FAILED</span>.exception)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// Note that authorization to a transactionalId implies ProducerId authorization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 集群授权失败</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (produceRequest.isIdempotent &amp;&amp; !authorize(request.session, <span class="type">IdempotentWrite</span>, <span class="type">Resource</span>.<span class="type">ClusterResource</span>)) &#123;</span><br><span class="line">      sendErrorResponseMaybeThrottle(request, <span class="type">Errors</span>.<span class="type">CLUSTER_AUTHORIZATION_FAILED</span>.exception)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 没有Describe权限</span></span><br><span class="line">    <span class="keyword">val</span> unauthorizedTopicResponses = mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]()</span><br><span class="line">    <span class="comment">// 不存在</span></span><br><span class="line">    <span class="keyword">val</span> nonExistingTopicResponses = mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]()</span><br><span class="line">    <span class="comment">// 有权限</span></span><br><span class="line">    <span class="keyword">val</span> authorizedRequestInfo = mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>]()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 进行筛选,判断有没有Write权限</span></span><br><span class="line">    <span class="keyword">for</span> ((topicPartition, memoryRecords) &lt;- produceRequest.partitionRecordsOrFail.asScala) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!authorize(request.session, <span class="type">Write</span>, <span class="type">Resource</span>(<span class="type">Topic</span>, topicPartition.topic, <span class="type">LITERAL</span>)))</span><br><span class="line">        unauthorizedTopicResponses += topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">TOPIC_AUTHORIZATION_FAILED</span>)</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span> (!metadataCache.contains(topicPartition))</span><br><span class="line">        nonExistingTopicResponses += topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>)</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">        authorizedRequestInfo += (topicPartition -&gt; memoryRecords)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// the callback for sending a produce response</span></span><br><span class="line">    <span class="comment">// 回调函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sendResponseCallback</span></span>(responseStatus: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>]) &#123;</span><br><span class="line">      <span class="keyword">val</span> mergedResponseStatus = responseStatus ++ unauthorizedTopicResponses ++ nonExistingTopicResponses</span><br><span class="line">      <span class="keyword">var</span> errorInResponse = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">      mergedResponseStatus.foreach &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (status.error != <span class="type">Errors</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">          errorInResponse = <span class="literal">true</span></span><br><span class="line">          debug(<span class="string">&quot;Produce request with correlation id %d from client %s on partition %s failed due to %s&quot;</span>.format(</span><br><span class="line">            request.header.correlationId,</span><br><span class="line">            request.header.clientId,</span><br><span class="line">            topicPartition,</span><br><span class="line">            status.error.exceptionName))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// When this callback is triggered, the remote API call has completed</span></span><br><span class="line">      request.apiRemoteCompleteTimeNanos = time.nanoseconds</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Record both bandwidth and request quota-specific values and throttle by muting the channel if any of the quotas</span></span><br><span class="line">      <span class="comment">// have been violated. If both quotas have been violated, use the max throttle time between the two quotas. Note</span></span><br><span class="line">      <span class="comment">// that the request quota is not enforced if acks == 0.</span></span><br><span class="line">      <span class="keyword">val</span> bandwidthThrottleTimeMs = quotas.produce.maybeRecordAndGetThrottleTimeMs(request, numBytesAppended, time.milliseconds())</span><br><span class="line">      <span class="keyword">val</span> requestThrottleTimeMs = <span class="keyword">if</span> (produceRequest.acks == <span class="number">0</span>) <span class="number">0</span> <span class="keyword">else</span> quotas.request.maybeRecordAndGetThrottleTimeMs(request)</span><br><span class="line">      <span class="keyword">val</span> maxThrottleTimeMs = <span class="type">Math</span>.max(bandwidthThrottleTimeMs, requestThrottleTimeMs)</span><br><span class="line">      <span class="keyword">if</span> (maxThrottleTimeMs &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (bandwidthThrottleTimeMs &gt; requestThrottleTimeMs) &#123;</span><br><span class="line">          quotas.produce.throttle(request, bandwidthThrottleTimeMs, sendResponse)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          quotas.request.throttle(request, requestThrottleTimeMs, sendResponse)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 立即发送响应,如果进行限制,则channel已经mute</span></span><br><span class="line">      <span class="keyword">if</span> (produceRequest.acks == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 因为设置的ack=0,相当于client会默认发送成功,如果server在处理过程出现错误,会关闭socket连接来间接通知client</span></span><br><span class="line">        <span class="comment">// client会重新刷新meta,重新建立相应的连接</span></span><br><span class="line">        <span class="keyword">if</span> (errorInResponse) &#123;</span><br><span class="line">          <span class="keyword">val</span> exceptionsSummary = mergedResponseStatus.map &#123; <span class="keyword">case</span> (topicPartition, status) =&gt;</span><br><span class="line">            topicPartition -&gt; status.error.exceptionName</span><br><span class="line">          &#125;.mkString(<span class="string">&quot;, &quot;</span>)</span><br><span class="line">          info(</span><br><span class="line">            <span class="string">s&quot;Closing connection due to error during produce request with correlation id <span class="subst">$&#123;request.header.correlationId&#125;</span> &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;from client id <span class="subst">$&#123;request.header.clientId&#125;</span> with ack=0\n&quot;</span> +</span><br><span class="line">              <span class="string">s&quot;Topic and partition to exceptions: <span class="subst">$exceptionsSummary</span>&quot;</span></span><br><span class="line">          )</span><br><span class="line">          closeConnection(request, <span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava).errorCounts)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// Note that although request throttling is exempt for acks == 0, the channel may be throttled due to</span></span><br><span class="line">          <span class="comment">// bandwidth quota violation.</span></span><br><span class="line">          sendNoOpResponseExemptThrottle(request)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        sendResponse(request, <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ProduceResponse</span>(mergedResponseStatus.asJava, maxThrottleTimeMs)), <span class="type">None</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">processingStatsCallback</span></span>(processingStats: <span class="type">FetchResponseStats</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      processingStats.foreach &#123; <span class="keyword">case</span> (tp, info) =&gt;</span><br><span class="line">        updateRecordConversionStats(request, tp, info)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (authorizedRequestInfo.isEmpty)</span><br><span class="line">      sendResponseCallback(<span class="type">Map</span>.empty)</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> internalTopicsAllowed = request.header.clientId == <span class="type">AdminUtils</span>.<span class="type">AdminClientId</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// call the replica manager to append messages to the replicas</span></span><br><span class="line">      <span class="comment">// 追加Record,写入</span></span><br><span class="line">      replicaManager.appendRecords(</span><br><span class="line">        timeout = produceRequest.timeout.toLong,</span><br><span class="line">        requiredAcks = produceRequest.acks,</span><br><span class="line">        internalTopicsAllowed = internalTopicsAllowed,</span><br><span class="line">        isFromClient = <span class="literal">true</span>,</span><br><span class="line">        entriesPerPartition = authorizedRequestInfo,</span><br><span class="line">        responseCallback = sendResponseCallback,</span><br><span class="line">        recordConversionStatsCallback = processingStatsCallback)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// if the request is put into the purgatory, it will have a held reference and hence cannot be garbage collected;</span></span><br><span class="line">      <span class="comment">// hence we clear its data here in order to let GC reclaim its memory since it is already appended to log</span></span><br><span class="line">      produceRequest.clearPartitionRecords()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="ReplicaManager"><a href="#ReplicaManager" class="headerlink" title="ReplicaManager"></a>ReplicaManager</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">副本管理器,一个副本对应一个Log对象</span><br><span class="line">KafkaServer启动时,会创建ReplicaManager对象</span><br><span class="line">ReplicaManager并不负责具体的日志创建,只是管理Broker上的所有分区</span><br><span class="line">在创建Partition对象时,需要ReplicaManager的LogManager对象</span><br><span class="line">Partition会通过这个LogManager对象为每个Replica创建相应的日志</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 注意replicaManager与logManager的来源</span><br><span class="line">object Partition &#123;</span><br><span class="line">  def apply(topicPartition: TopicPartition,</span><br><span class="line">            time: Time,</span><br><span class="line">            replicaManager: ReplicaManager): Partition &#x3D; &#123;</span><br><span class="line">    new Partition(topicPartition,</span><br><span class="line">      isOffline &#x3D; false,</span><br><span class="line">      replicaLagTimeMaxMs &#x3D; replicaManager.config.replicaLagTimeMaxMs,</span><br><span class="line">      localBrokerId &#x3D; replicaManager.config.brokerId,</span><br><span class="line">      time &#x3D; time,</span><br><span class="line">      replicaManager &#x3D; replicaManager,</span><br><span class="line">      logManager &#x3D; replicaManager.logManager,</span><br><span class="line">      zkClient &#x3D; replicaManager.zkClient)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ReplicaManager与LogManager的对比</span><br><span class="line">LogManager管理Log,由LogSegment组成</span><br><span class="line">ReplicaManager管理Partition,由Replica组成</span><br><span class="line"></span><br><span class="line">replicaManager &#x3D; createReplicaManager(isShuttingDown)</span><br><span class="line">replicaManager.startup()</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 传递了LogManager</span><br><span class="line">protected def createReplicaManager(isShuttingDown: AtomicBoolean): ReplicaManager &#x3D;</span><br><span class="line">    new ReplicaManager(config, metrics, time, zkClient, kafkaScheduler, logManager, isShuttingDown, quotaManagers,</span><br><span class="line">      brokerTopicStats, metadataCache, logDirFailureChannel)</span><br></pre></td></tr></table></figure>
<h4 id="appendRecords"><a href="#appendRecords" class="headerlink" title="appendRecords"></a>appendRecords</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>判断acks设置是否有效(<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>),无效直接返回异常,不再处理</span><br><span class="line"><span class="number">2.</span>有效,调用appendToLocalLog()将records追加到本地对应的<span class="type">Log</span>对象中</span><br><span class="line"><span class="number">3.</span>appendToLocalLog()处理完后,如果发现clients设置的acks=<span class="number">-1</span>,则需要isr的其他副本同步完成才能返回<span class="type">Response</span></span><br><span class="line">    那么就会创建一个<span class="type">DelayedProduce</span>对象,等待isr其他副本进行同步</span><br><span class="line">    否则直接返回追加的结果</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向partition的leader写入数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecords</span></span>(timeout: <span class="type">Long</span>,</span><br><span class="line">                    requiredAcks: <span class="type">Short</span>,</span><br><span class="line">                    internalTopicsAllowed: <span class="type">Boolean</span>,</span><br><span class="line">                    isFromClient: <span class="type">Boolean</span>,</span><br><span class="line">                    entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</span><br><span class="line">                    responseCallback: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>] =&gt; <span class="type">Unit</span>,</span><br><span class="line">                    delayedProduceLock: <span class="type">Option</span>[<span class="type">Lock</span>] = <span class="type">None</span>,</span><br><span class="line">                    recordConversionStatsCallback: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">RecordConversionStats</span>] =&gt; <span class="type">Unit</span> = _ =&gt; ()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isValidRequiredAcks(requiredAcks)) &#123; <span class="comment">// acks设置有效</span></span><br><span class="line">      <span class="keyword">val</span> sTime = time.milliseconds</span><br><span class="line">      <span class="comment">// 向本地的副本log追加数据</span></span><br><span class="line">      <span class="keyword">val</span> localProduceResults = appendToLocalLog(internalTopicsAllowed = internalTopicsAllowed,</span><br><span class="line">        isFromClient = isFromClient, entriesPerPartition, requiredAcks)</span><br><span class="line">      debug(<span class="string">&quot;Produce to local log in %d ms&quot;</span>.format(time.milliseconds - sTime))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> produceStatus = localProduceResults.map &#123; <span class="keyword">case</span> (topicPartition, result) =&gt;</span><br><span class="line">        topicPartition -&gt;</span><br><span class="line">                <span class="type">ProducePartitionStatus</span>(</span><br><span class="line">                  result.info.lastOffset + <span class="number">1</span>, <span class="comment">// required offset</span></span><br><span class="line">                  <span class="keyword">new</span> <span class="type">PartitionResponse</span>(result.error, result.info.firstOffset.getOrElse(<span class="number">-1</span>), result.info.logAppendTime, result.info.logStartOffset)) <span class="comment">// response status</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      recordConversionStatsCallback(localProduceResults.mapValues(_.info.recordConversionStats))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (delayedProduceRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) &#123;</span><br><span class="line">        <span class="comment">// 处理ack=-1的情况,需要等到isr的follower都写入成功,才能返回最后结果</span></span><br><span class="line">        <span class="comment">// create delayed produce operation</span></span><br><span class="line">        <span class="keyword">val</span> produceMetadata = <span class="type">ProduceMetadata</span>(requiredAcks, produceStatus)</span><br><span class="line">        <span class="comment">// 延迟produce请求</span></span><br><span class="line">        <span class="keyword">val</span> delayedProduce = <span class="keyword">new</span> <span class="type">DelayedProduce</span>(timeout, produceMetadata, <span class="keyword">this</span>, responseCallback, delayedProduceLock)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create a list of (topic, partition) pairs to use as keys for this delayed produce operation</span></span><br><span class="line">        <span class="keyword">val</span> producerRequestKeys = entriesPerPartition.keys.map(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(_)).toSeq</span><br><span class="line"></span><br><span class="line">        <span class="comment">// try to complete the request immediately, otherwise put it into the purgatory</span></span><br><span class="line">        <span class="comment">// this is because while the delayed produce operation is being created, new</span></span><br><span class="line">        <span class="comment">// requests may arrive and hence make this operation completable.</span></span><br><span class="line">        delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// we can respond immediately</span></span><br><span class="line">        <span class="keyword">val</span> produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</span><br><span class="line">        <span class="comment">// 通过回调函数直接返回结果</span></span><br><span class="line">        responseCallback(produceResponseStatus)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 返回INVALID_REQUIRED_ACKS错误</span></span><br><span class="line">      <span class="comment">// If required.acks is outside accepted range, something is wrong with the client</span></span><br><span class="line">      <span class="comment">// Just return an error and don&#x27;t handle the request at all</span></span><br><span class="line">      <span class="keyword">val</span> responseStatus = entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, _) =&gt;</span><br><span class="line">        topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(<span class="type">Errors</span>.<span class="type">INVALID_REQUIRED_ACKS</span>,</span><br><span class="line">          <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>.firstOffset.getOrElse(<span class="number">-1</span>), <span class="type">RecordBatch</span>.<span class="type">NO_TIMESTAMP</span>, <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>.logStartOffset)</span><br><span class="line">      &#125;</span><br><span class="line">      responseCallback(responseStatus)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="appendToLocalLog"><a href="#appendToLocalLog" class="headerlink" title="appendToLocalLog()"></a>appendToLocalLog()</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>首先判断要写入的topic是不是kafka内置的topic,内置topic不允许<span class="type">Producer</span>写入</span><br><span class="line"><span class="number">2.</span>查找<span class="type">TP</span>对应的<span class="type">Partition</span>对象,如果在allPartitions中查找到了对应的<span class="type">Partition</span></span><br><span class="line">    直接调用partition.appendRecordsToLeader()追加相应的records</span><br><span class="line">    否则向client抛出异常</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向本地的Replica写入数据</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">appendToLocalLog</span></span>(internalTopicsAllowed: <span class="type">Boolean</span>,</span><br><span class="line">                               isFromClient: <span class="type">Boolean</span>,</span><br><span class="line">                               entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</span><br><span class="line">                               requiredAcks: <span class="type">Short</span>): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">LogAppendResult</span>] = &#123;</span><br><span class="line">    trace(<span class="string">s&quot;Append [<span class="subst">$entriesPerPartition</span>] to local log&quot;</span>)</span><br><span class="line">    entriesPerPartition.map &#123; <span class="keyword">case</span> (topicPartition, records) =&gt;</span><br><span class="line">      <span class="comment">// 遍历要写的所哦呦tp</span></span><br><span class="line">      brokerTopicStats.topicStats(topicPartition.topic).totalProduceRequestRate.mark()</span><br><span class="line">      brokerTopicStats.allTopicsStats.totalProduceRequestRate.mark()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 不允许向kafka内部使用的topic追加数据</span></span><br><span class="line">      <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topicPartition.topic) &amp;&amp; !internalTopicsAllowed) &#123;</span><br><span class="line">        (topicPartition, <span class="type">LogAppendResult</span>(</span><br><span class="line">          <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>,</span><br><span class="line">          <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">InvalidTopicException</span>(<span class="string">s&quot;Cannot append to internal topic <span class="subst">$&#123;topicPartition.topic&#125;</span>&quot;</span>))))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 查找对应的Partition,并向分区对应的副本写入数据文件</span></span><br><span class="line">          <span class="keyword">val</span> partition = getPartitionOrException(topicPartition, expectLeader = <span class="literal">true</span>)</span><br><span class="line">          <span class="comment">// 追加数据</span></span><br><span class="line">          <span class="keyword">val</span> info = partition.appendRecordsToLeader(records, isFromClient, requiredAcks)</span><br><span class="line">          <span class="keyword">val</span> numAppendedMessages = info.numMessages</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 更新Metrics</span></span><br><span class="line">          brokerTopicStats.topicStats(topicPartition.topic).bytesInRate.mark(records.sizeInBytes)</span><br><span class="line">          brokerTopicStats.allTopicsStats.bytesInRate.mark(records.sizeInBytes)</span><br><span class="line">          brokerTopicStats.topicStats(topicPartition.topic).messagesInRate.mark(numAppendedMessages)</span><br><span class="line">          brokerTopicStats.allTopicsStats.messagesInRate.mark(numAppendedMessages)</span><br><span class="line"></span><br><span class="line">          trace(<span class="string">s&quot;<span class="subst">$&#123;records.sizeInBytes&#125;</span> written to log <span class="subst">$topicPartition</span> beginning at offset &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;<span class="subst">$&#123;info.firstOffset.getOrElse(-1)&#125;</span> and ending at offset <span class="subst">$&#123;info.lastOffset&#125;</span>&quot;</span>)</span><br><span class="line">          (topicPartition, <span class="type">LogAppendResult</span>(info))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="comment">// 处理追加过程中的异常</span></span><br><span class="line">          <span class="comment">// <span class="doctag">NOTE:</span> Failed produce requests metric is not incremented for known exceptions</span></span><br><span class="line">          <span class="comment">// it is supposed to indicate un-expected failures of a broker in handling a produce request</span></span><br><span class="line">          <span class="keyword">case</span> e@ (_: <span class="type">UnknownTopicOrPartitionException</span> |</span><br><span class="line">                   _: <span class="type">NotLeaderForPartitionException</span> |</span><br><span class="line">                   _: <span class="type">RecordTooLargeException</span> |</span><br><span class="line">                   _: <span class="type">RecordBatchTooLargeException</span> |</span><br><span class="line">                   _: <span class="type">CorruptRecordException</span> |</span><br><span class="line">                   _: <span class="type">KafkaStorageException</span> |</span><br><span class="line">                   _: <span class="type">InvalidTimestampException</span>) =&gt;</span><br><span class="line">            (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>, <span class="type">Some</span>(e)))</span><br><span class="line">          <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">            <span class="keyword">val</span> logStartOffset = getPartition(topicPartition) <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</span><br><span class="line">                partition.logStartOffset</span><br><span class="line">              <span class="keyword">case</span> _ =&gt;</span><br><span class="line">                <span class="number">-1</span></span><br><span class="line">            &#125;</span><br><span class="line">            brokerTopicStats.topicStats(topicPartition.topic).failedProduceRequestRate.mark()</span><br><span class="line">            brokerTopicStats.allTopicsStats.failedProduceRequestRate.mark()</span><br><span class="line">            error(<span class="string">s&quot;Error processing append operation on partition <span class="subst">$topicPartition</span>&quot;</span>, t)</span><br><span class="line">            (topicPartition, <span class="type">LogAppendResult</span>(<span class="type">LogAppendInfo</span>.unknownLogAppendInfoWithLogStartOffset(logStartOffset), <span class="type">Some</span>(t)))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Partition-appendRecordsToLeader"><a href="#Partition-appendRecordsToLeader" class="headerlink" title="Partition.appendRecordsToLeader()"></a>Partition.appendRecordsToLeader()</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 根据topic的min.isrs配置以及当前这个partition的isr情况判断是否可以写入</span></span><br><span class="line"><span class="comment">// 如果不满足条件,抛出NotEnoughReplicasException</span></span><br><span class="line"><span class="comment">// 满足,调用log.append()向replica追加日志</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecordsToLeader</span></span>(records: <span class="type">MemoryRecords</span>, isFromClient: <span class="type">Boolean</span>, requiredAcks: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogAppendInfo</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) &#123;</span><br><span class="line">      leaderReplicaIfLocal <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</span><br><span class="line">          <span class="comment">// 获取对应的Log对象</span></span><br><span class="line">          <span class="keyword">val</span> log = leaderReplica.log.get</span><br><span class="line">          <span class="keyword">val</span> minIsr = log.config.minInSyncReplicas</span><br><span class="line">          <span class="keyword">val</span> inSyncSize = inSyncReplicas.size</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Avoid writing to leader if there are not enough insync replicas to make it safe</span></span><br><span class="line">          <span class="comment">// 如果ack设置为-1,isr数小于设置的min.isr时,会向producer抛出异常</span></span><br><span class="line">          <span class="keyword">if</span> (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == <span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotEnoughReplicasException</span>(<span class="string">s&quot;The size of the current ISR <span class="subst">$&#123;inSyncReplicas.map(_.brokerId)&#125;</span> &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;is insufficient to satisfy the min.isr requirement of <span class="subst">$minIsr</span> for partition <span class="subst">$topicPartition</span>&quot;</span>)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 向副本对应的log追加相应的数据</span></span><br><span class="line">          <span class="keyword">val</span> info = log.appendAsLeader(records, leaderEpoch = <span class="keyword">this</span>.leaderEpoch, isFromClient)</span><br><span class="line">          <span class="comment">// probably unblock some follower fetch requests since log end offset has been updated</span></span><br><span class="line">          replicaManager.tryCompleteDelayedFetch(<span class="type">TopicPartitionOperationKey</span>(<span class="keyword">this</span>.topic, <span class="keyword">this</span>.partitionId))</span><br><span class="line">          <span class="comment">// we may need to increment high watermark since ISR could be down to 1</span></span><br><span class="line">          <span class="comment">// 判断是否需要增加HW(追加日志后会进行一次判断)</span></span><br><span class="line">          (info, maybeIncrementLeaderHW(leaderReplica))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="comment">// leader不在本台机器上</span></span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">&quot;Leader not local for partition %s on broker %d&quot;</span></span><br><span class="line">            .format(topicPartition, localBrokerId))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// some delayed operations may be unblocked after HW changed</span></span><br><span class="line">    <span class="keyword">if</span> (leaderHWIncremented)</span><br><span class="line">      tryCompleteDelayedRequests()</span><br><span class="line"></span><br><span class="line">    info</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="存储层"><a href="#存储层" class="headerlink" title="存储层"></a>存储层</h2><h3 id="Log对象"><a href="#Log对象" class="headerlink" title="Log对象"></a>Log对象</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">每个<span class="type">Replica</span>都会对应一个<span class="type">Log</span>对象,<span class="type">Log</span>对象是管理当前分区的一个单位</span><br><span class="line">它会包含这个分区的所有<span class="type">Segment</span>文件(包括索引,时间戳索引文件)</span><br><span class="line">提供增删查方法</span><br><span class="line"></span><br><span class="line">nextOffsetMetadata: 下一个偏移量元数据,包括activeSegment的下一条消息的偏移量,该activeSegment的基准偏移量及日志分段的大小</span><br><span class="line">activeSegment: <span class="type">Log</span>管理segments中最新segment,一个<span class="type">Log</span>只会有一个activeSegment,其他的segment已经持久化到磁盘了</span><br><span class="line">logEndOffset: 下一条消息的offset,取自nextOffsetMetadata的offset</span><br><span class="line"></span><br><span class="line"><span class="comment">// 声明为volatile,如果该值被修改,其他使用此变量的线程就可以立刻见到变化后的值,在生产和消费都会使用到这个值</span></span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> nextOffsetMetadata: <span class="type">LogOffsetMetadata</span> = _</span><br><span class="line"><span class="comment">// 下一个偏移量元数据</span></span><br><span class="line"><span class="comment">// 第一个参数: 下一条消息的偏移量</span></span><br><span class="line"><span class="comment">// 第二个参数: 日志分段的基准偏移量</span></span><br><span class="line"><span class="comment">// 第三个参数: 日志分段大小</span></span><br><span class="line">nextOffsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(nextOffset, activeSegment.baseOffset, activeSegment.size)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 只会有一个活动的日志分段</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activeSegment</span> </span>= segments.lastEntry.getValue</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下一条消息的offset,从nextOffsetMetadata获取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logEndOffset</span></span>: <span class="type">Long</span> = nextOffsetMetadata.messageOffset</span><br></pre></td></tr></table></figure>
<h4 id="日志写入"><a href="#日志写入" class="headerlink" title="日志写入"></a>日志写入</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">analyzeAndValidateRecords(): 对这批要写入的消息进行检测,检查消息的大小以及校验</span><br><span class="line">trimInvalidBytes(): 删除无效消息</span><br><span class="line"><span class="type">LogValidator</span>.validateMessagesAndAssignOffsets(): 设置相应的offset和timestrap</span><br><span class="line">maybeRoll(): 判断是否需要新建一个segment,如果当前segment放不下这批消息的话,需要新建</span><br><span class="line">segment.append(): 向segment添加信息</span><br><span class="line">updateLogEndOffset(): 更新<span class="type">LEO</span></span><br><span class="line">flush(): 刷新磁盘</span><br><span class="line"></span><br><span class="line">时间戳记录有两种</span><br><span class="line">    <span class="type">CreateTime</span>: 默认,创建时间</span><br><span class="line">    <span class="type">LogAppendTime</span>: 添加时间</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向activeSegment追加log,必要的情况下,滚动创建新的segment</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, isFromClient: <span class="type">Boolean</span>, assignOffsets: <span class="type">Boolean</span>, leaderEpoch: <span class="type">Int</span>): <span class="type">LogAppendInfo</span> = &#123;</span><br><span class="line">    maybeHandleIOException(<span class="string">s&quot;Error while appending records to <span class="subst">$topicPartition</span> in dir <span class="subst">$&#123;dir.getParent&#125;</span>&quot;</span>) &#123;</span><br><span class="line">      <span class="comment">// 返回这批消息的概要信息,并对其进行校验</span></span><br><span class="line">      <span class="keyword">val</span> appendInfo = analyzeAndValidateRecords(records, isFromClient = isFromClient)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// return if we have no valid messages or if this is a duplicate of the last appended entry</span></span><br><span class="line">      <span class="keyword">if</span> (appendInfo.shallowCount == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> appendInfo</span><br><span class="line"></span><br><span class="line">      <span class="comment">// trim any invalid bytes or partial messages before appending it to the on-disk log</span></span><br><span class="line">      <span class="comment">// 删除无效信息</span></span><br><span class="line">      <span class="keyword">var</span> validRecords = trimInvalidBytes(records, appendInfo)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// they are valid, insert them in the log</span></span><br><span class="line">      lock synchronized &#123;</span><br><span class="line">        checkIfMemoryMappedBufferClosed()</span><br><span class="line">        <span class="keyword">if</span> (assignOffsets) &#123;</span><br><span class="line">          <span class="comment">// assign offsets to the message set</span></span><br><span class="line">          <span class="comment">// 计算这个消息集的起始offset,对offset的操作是一个原子操作</span></span><br><span class="line">          <span class="keyword">val</span> offset = <span class="keyword">new</span> <span class="type">LongRef</span>(nextOffsetMetadata.messageOffset)</span><br><span class="line">          <span class="comment">// 作为消息集的第一个offset</span></span><br><span class="line">          appendInfo.firstOffset = <span class="type">Some</span>(offset.value)</span><br><span class="line">          <span class="comment">// 设置时间戳以server收到的时间戳为准</span></span><br><span class="line">          <span class="keyword">val</span> now = time.milliseconds</span><br><span class="line">          <span class="comment">// 验证消息,并为每条record设置相应的offset和timestrap</span></span><br><span class="line">          <span class="keyword">val</span> validateAndOffsetAssignResult = <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">LogValidator</span>.validateMessagesAndAssignOffsets(validRecords,</span><br><span class="line">              offset,</span><br><span class="line">              time,</span><br><span class="line">              now,</span><br><span class="line">              appendInfo.sourceCodec,</span><br><span class="line">              appendInfo.targetCodec,</span><br><span class="line">              config.compact,</span><br><span class="line">              config.messageFormatVersion.recordVersion.value,</span><br><span class="line">              config.messageTimestampType,</span><br><span class="line">              config.messageTimestampDifferenceMaxMs,</span><br><span class="line">              leaderEpoch,</span><br><span class="line">              isFromClient)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">s&quot;Error validating messages while appending to log <span class="subst">$name</span>&quot;</span>, e)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// 返回已经计算好的offset和timestrap的MemoryRecord</span></span><br><span class="line">          validRecords = validateAndOffsetAssignResult.validatedRecords</span><br><span class="line">          appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</span><br><span class="line">          appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp</span><br><span class="line">          <span class="comment">// 最后一条消息的offset</span></span><br><span class="line">          appendInfo.lastOffset = offset.value - <span class="number">1</span></span><br><span class="line">          appendInfo.recordConversionStats = validateAndOffsetAssignResult.recordConversionStats</span><br><span class="line">          <span class="keyword">if</span> (config.messageTimestampType == <span class="type">TimestampType</span>.<span class="type">LOG_APPEND_TIME</span>)</span><br><span class="line">            appendInfo.logAppendTime = now</span><br><span class="line"></span><br><span class="line">          <span class="comment">// re-validate message sizes if there&#x27;s a possibility that they have changed (due to re-compression or message</span></span><br><span class="line">          <span class="comment">// format conversion)</span></span><br><span class="line">          <span class="comment">// 更新metrics记录</span></span><br><span class="line">          <span class="keyword">if</span> (validateAndOffsetAssignResult.messageSizeMaybeChanged) &#123;</span><br><span class="line">            <span class="keyword">for</span> (batch &lt;- validRecords.batches.asScala) &#123;</span><br><span class="line">              <span class="keyword">if</span> (batch.sizeInBytes &gt; config.maxMessageSize) &#123;</span><br><span class="line">                <span class="comment">// we record the original message set size instead of the trimmed size</span></span><br><span class="line">                <span class="comment">// to be consistent with pre-compression bytesRejectedRate recording</span></span><br><span class="line">                brokerTopicStats.topicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">                brokerTopicStats.allTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordTooLargeException</span>(<span class="string">s&quot;Message batch size is <span class="subst">$&#123;batch.sizeInBytes&#125;</span> bytes in append to&quot;</span> +</span><br><span class="line">                  <span class="string">s&quot;partition <span class="subst">$topicPartition</span> which exceeds the maximum configured size of <span class="subst">$&#123;config.maxMessageSize&#125;</span>.&quot;</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// we are taking the offsets we are given</span></span><br><span class="line">          <span class="keyword">if</span> (!appendInfo.offsetsMonotonic)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetsOutOfOrderException</span>(<span class="string">s&quot;Out of order offsets found in append to <span class="subst">$topicPartition</span>: &quot;</span> +</span><br><span class="line">                                                 records.records.asScala.map(_.offset))</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (appendInfo.firstOrLastOffsetOfFirstBatch &lt; nextOffsetMetadata.messageOffset) &#123;</span><br><span class="line">            <span class="comment">// we may still be able to recover if the log is empty</span></span><br><span class="line">            <span class="comment">// one example: fetching from log start offset on the leader which is not batch aligned,</span></span><br><span class="line">            <span class="comment">// which may happen as a result of AdminClient#deleteRecords()</span></span><br><span class="line">            <span class="keyword">val</span> firstOffset = appendInfo.firstOffset <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> <span class="type">Some</span>(offset) =&gt; offset</span><br><span class="line">              <span class="keyword">case</span> <span class="type">None</span> =&gt; records.batches.asScala.head.baseOffset()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> firstOrLast = <span class="keyword">if</span> (appendInfo.firstOffset.isDefined) <span class="string">&quot;First offset&quot;</span> <span class="keyword">else</span> <span class="string">&quot;Last offset of the first batch&quot;</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnexpectedAppendOffsetException</span>(</span><br><span class="line">              <span class="string">s&quot;Unexpected offset in append to <span class="subst">$topicPartition</span>. <span class="subst">$firstOrLast</span> &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;<span class="subst">$&#123;appendInfo.firstOrLastOffsetOfFirstBatch&#125;</span> is less than the next offset <span class="subst">$&#123;nextOffsetMetadata.messageOffset&#125;</span>. &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;First 10 offsets in append: <span class="subst">$&#123;records.records.asScala.take(10).map(_.offset)&#125;</span>, last offset in&quot;</span> +</span><br><span class="line">              <span class="string">s&quot; append: <span class="subst">$&#123;appendInfo.lastOffset&#125;</span>. Log start offset = <span class="subst">$logStartOffset</span>&quot;</span>,</span><br><span class="line">              firstOffset, appendInfo.lastOffset)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// update the epoch cache with the epoch stamped onto the message by the leader</span></span><br><span class="line">        validRecords.batches.asScala.foreach &#123; batch =&gt;</span><br><span class="line">          <span class="keyword">if</span> (batch.magic &gt;= <span class="type">RecordBatch</span>.<span class="type">MAGIC_VALUE_V2</span>)</span><br><span class="line">            _leaderEpochCache.assign(batch.partitionLeaderEpoch, batch.baseOffset)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// check messages set size may be exceed config.segmentSize</span></span><br><span class="line">        <span class="keyword">if</span> (validRecords.sizeInBytes &gt; config.segmentSize) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordBatchTooLargeException</span>(<span class="string">s&quot;Message batch size is <span class="subst">$&#123;validRecords.sizeInBytes&#125;</span> bytes in append &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;to partition <span class="subst">$topicPartition</span>, which exceeds the maximum configured segment size of <span class="subst">$&#123;config.segmentSize&#125;</span>.&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// now that we have valid records, offsets assigned, and timestamps updated, we need to</span></span><br><span class="line">        <span class="comment">// validate the idempotent/transactional state of the producers and collect some metadata</span></span><br><span class="line">        <span class="keyword">val</span> (updatedProducers, completedTxns, maybeDuplicate) = analyzeAndValidateProducerState(validRecords, isFromClient)</span><br><span class="line">        maybeDuplicate.foreach &#123; duplicate =&gt;</span><br><span class="line">          appendInfo.firstOffset = <span class="type">Some</span>(duplicate.firstOffset)</span><br><span class="line">          appendInfo.lastOffset = duplicate.lastOffset</span><br><span class="line">          appendInfo.logAppendTime = duplicate.timestamp</span><br><span class="line">          appendInfo.logStartOffset = logStartOffset</span><br><span class="line">          <span class="keyword">return</span> appendInfo</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// maybe roll the log if this segment is full</span></span><br><span class="line">        <span class="comment">// 如果当前segment满了,需要重新创建一个segment</span></span><br><span class="line">        <span class="keyword">val</span> segment = maybeRoll(validRecords.sizeInBytes, appendInfo)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> logOffsetMetadata = <span class="type">LogOffsetMetadata</span>(</span><br><span class="line">          messageOffset = appendInfo.firstOrLastOffsetOfFirstBatch,</span><br><span class="line">          segmentBaseOffset = segment.baseOffset,</span><br><span class="line">          relativePositionInSegment = segment.size)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 追加消息到当前segment</span></span><br><span class="line">        segment.append(largestOffset = appendInfo.lastOffset,</span><br><span class="line">          largestTimestamp = appendInfo.maxTimestamp,</span><br><span class="line">          shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</span><br><span class="line">          records = validRecords)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// update the producer state</span></span><br><span class="line">        <span class="keyword">for</span> ((_, producerAppendInfo) &lt;- updatedProducers) &#123;</span><br><span class="line">          producerAppendInfo.maybeCacheTxnFirstOffsetMetadata(logOffsetMetadata)</span><br><span class="line">          producerStateManager.update(producerAppendInfo)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// update the transaction index with the true last stable offset. The last offset visible</span></span><br><span class="line">        <span class="comment">// to consumers using READ_COMMITTED will be limited by this value and the high watermark.</span></span><br><span class="line">        <span class="keyword">for</span> (completedTxn &lt;- completedTxns) &#123;</span><br><span class="line">          <span class="keyword">val</span> lastStableOffset = producerStateManager.completeTxn(completedTxn)</span><br><span class="line">          segment.updateTxnIndex(completedTxn, lastStableOffset)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// always update the last producer id map offset so that the snapshot reflects the current offset</span></span><br><span class="line">        <span class="comment">// even if there isn&#x27;t any idempotent data being written</span></span><br><span class="line">        producerStateManager.updateMapEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// increment the log end offset</span></span><br><span class="line">        <span class="comment">// 修改最新的next_offset</span></span><br><span class="line">        updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// update the first unstable offset (which is used to compute LSO)</span></span><br><span class="line">        updateFirstUnstableOffset()</span><br><span class="line"></span><br><span class="line">        trace(<span class="string">s&quot;Appended message set with last offset: <span class="subst">$&#123;appendInfo.lastOffset&#125;</span>, &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;first offset: <span class="subst">$&#123;appendInfo.firstOffset&#125;</span>, &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;next offset: <span class="subst">$&#123;nextOffsetMetadata.messageOffset&#125;</span>, &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;and messages: <span class="subst">$validRecords</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 满足条件的话,刷新磁盘</span></span><br><span class="line">        <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)</span><br><span class="line">          flush()</span><br><span class="line"></span><br><span class="line">        appendInfo</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="日志分段"><a href="#日志分段" class="headerlink" title="日志分段"></a>日志分段</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 判断是否需要创建日志分段,如果不需要,就返回当前分段,需要则返回新创建的日志分段</span><br><span class="line">private def maybeRoll(messagesSize: Int, appendInfo: LogAppendInfo): LogSegment &#x3D; &#123;</span><br><span class="line">    &#x2F;&#x2F; 最新日志分段(活跃日志分段)</span><br><span class="line">    val segment &#x3D; activeSegment</span><br><span class="line">    val now &#x3D; time.milliseconds</span><br><span class="line"></span><br><span class="line">    val maxTimestampInMessages &#x3D; appendInfo.maxTimestamp</span><br><span class="line">    val maxOffsetInMessages &#x3D; appendInfo.lastOffset</span><br><span class="line"></span><br><span class="line">    if (segment.shouldRoll(RollParams(config, appendInfo, messagesSize, now))) &#123;</span><br><span class="line">      debug(s&quot;Rolling new log segment (log_size &#x3D; $&#123;segment.size&#125;&#x2F;$&#123;config.segmentSize&#125;&#125;, &quot; +</span><br><span class="line">        s&quot;offset_index_size &#x3D; $&#123;segment.offsetIndex.entries&#125;&#x2F;$&#123;segment.offsetIndex.maxEntries&#125;, &quot; +</span><br><span class="line">        s&quot;time_index_size &#x3D; $&#123;segment.timeIndex.entries&#125;&#x2F;$&#123;segment.timeIndex.maxEntries&#125;, &quot; +</span><br><span class="line">        s&quot;inactive_time_ms &#x3D; $&#123;segment.timeWaitedForRoll(now, maxTimestampInMessages)&#125;&#x2F;$&#123;config.segmentMs - segment.rollJitterMs&#125;).&quot;)</span><br><span class="line">      appendInfo.firstOffset match &#123;</span><br><span class="line">        &#x2F;&#x2F; 创建新的日志分段</span><br><span class="line">        case Some(firstOffset) &#x3D;&gt; roll(firstOffset)</span><br><span class="line">        case None &#x3D;&gt; roll(maxOffsetInMessages - Integer.MAX_VALUE)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      &#x2F;&#x2F; 使用当前的日志分段</span><br><span class="line">      segment</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; LogSegment</span><br><span class="line">def shouldRoll(rollParams: RollParams): Boolean &#x3D; &#123;</span><br><span class="line">  &#x2F;&#x2F; 距离上次日志分段是否达到了阈值</span><br><span class="line">  val reachedRollMs &#x3D; timeWaitedForRoll(rollParams.now, rollParams.maxTimestampInMessages) &gt; rollParams.maxSegmentMs - rollJitterMs</span><br><span class="line">  &#x2F;&#x2F; 1.文件满了,不足以放下这么大的messageSet</span><br><span class="line">  &#x2F;&#x2F; 2.文件有数据,到了分段的阈值</span><br><span class="line">  &#x2F;&#x2F; 3.索引文件满了</span><br><span class="line">  &#x2F;&#x2F; 4.时间索引文件满了</span><br><span class="line">  &#x2F;&#x2F; 5.最大offset,其相对偏移量超过了正整数的阈值</span><br><span class="line">  size &gt; rollParams.maxSegmentBytes - rollParams.messagesSize ||</span><br><span class="line">    (size &gt; 0 &amp;&amp; reachedRollMs) ||</span><br><span class="line">    offsetIndex.isFull || timeIndex.isFull || !canConvertToRelativeOffset(rollParams.maxOffsetInMessages)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 滚动创建日志,并添加到日志管理的映射表中</span><br><span class="line">def roll(expectedNextOffset: Long &#x3D; 0): LogSegment &#x3D; &#123;</span><br><span class="line">    maybeHandleIOException(s&quot;Error while rolling log segment for $topicPartition in dir $&#123;dir.getParent&#125;&quot;) &#123;</span><br><span class="line">      val start &#x3D; time.hiResClockMs()</span><br><span class="line">      lock synchronized &#123;</span><br><span class="line">        checkIfMemoryMappedBufferClosed()</span><br><span class="line">        &#x2F;&#x2F; 选择最新的offset作为基准偏移量</span><br><span class="line">        val newOffset &#x3D; math.max(expectedNextOffset, logEndOffset)</span><br><span class="line">        &#x2F;&#x2F; 创建数据文件</span><br><span class="line">        val logFile &#x3D; Log.logFile(dir, newOffset)</span><br><span class="line">        &#x2F;&#x2F; 创建offset索引文件</span><br><span class="line">        val offsetIdxFile &#x3D; offsetIndexFile(dir, newOffset)</span><br><span class="line">        &#x2F;&#x2F; 创建时间索引文件</span><br><span class="line">        val timeIdxFile &#x3D; timeIndexFile(dir, newOffset)</span><br><span class="line">        val txnIdxFile &#x3D; transactionIndexFile(dir, newOffset)</span><br><span class="line">        for (file &lt;- List(logFile, offsetIdxFile, timeIdxFile, txnIdxFile) if file.exists) &#123;</span><br><span class="line">          warn(s&quot;Newly rolled segment file $&#123;file.getAbsolutePath&#125; already exists; deleting it first&quot;)</span><br><span class="line">          Files.delete(file.toPath)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Option(segments.lastEntry).foreach(_.getValue.onBecomeInactiveSegment())</span><br><span class="line"></span><br><span class="line">        producerStateManager.updateMapEndOffset(newOffset)</span><br><span class="line">        producerStateManager.takeSnapshot()</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F; 创建一个segment对象</span><br><span class="line">        val segment &#x3D; LogSegment.open(dir,</span><br><span class="line">          baseOffset &#x3D; newOffset,</span><br><span class="line">          config,</span><br><span class="line">          time &#x3D; time,</span><br><span class="line">          fileAlreadyExists &#x3D; false,</span><br><span class="line">          initFileSize &#x3D; initFileSize,</span><br><span class="line">          preallocate &#x3D; config.preallocate)</span><br><span class="line">        &#x2F;&#x2F; 添加到日志管理中</span><br><span class="line">        val prev &#x3D; addSegment(segment)</span><br><span class="line">        if (prev !&#x3D; null)</span><br><span class="line">          throw new KafkaException(s&quot;Trying to roll a new log segment for topic partition $topicPartition with &quot; +</span><br><span class="line">            s&quot;start offset $newOffset while it already exists.&quot;)</span><br><span class="line">        &#x2F;&#x2F; 更新offset</span><br><span class="line">        updateLogEndOffset(nextOffsetMetadata.messageOffset)</span><br><span class="line">        &#x2F;&#x2F; schedule an asynchronous flush of the old segment</span><br><span class="line">        scheduler.schedule(&quot;flush-log&quot;, () &#x3D;&gt; flush(newOffset), delay &#x3D; 0L)</span><br><span class="line"></span><br><span class="line">        info(s&quot;Rolled new log segment at offset $newOffset in $&#123;time.hiResClockMs() - start&#125; ms.&quot;)</span><br><span class="line"></span><br><span class="line">        segment</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="LogSegment对象"><a href="#LogSegment对象" class="headerlink" title="LogSegment对象"></a>LogSegment对象</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">真正的日志写入,在LogSegment的append()方法中完成</span><br><span class="line">它会跟Kafka最底层的文件通道,mmap打交道</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 在指定offset处追加msgs,需要的情况下追加相应的索引</span><br><span class="line">@nonthreadsafe</span><br><span class="line">def append(largestOffset: Long,</span><br><span class="line">             largestTimestamp: Long,</span><br><span class="line">             shallowOffsetOfMaxTimestamp: Long,</span><br><span class="line">             records: MemoryRecords): Unit &#x3D; &#123;</span><br><span class="line">    if (records.sizeInBytes &gt; 0) &#123;</span><br><span class="line">      trace(s&quot;Inserting $&#123;records.sizeInBytes&#125; bytes at end offset $largestOffset at position $&#123;log.sizeInBytes&#125; &quot; +</span><br><span class="line">            s&quot;with largest timestamp $largestTimestamp at shallow offset $shallowOffsetOfMaxTimestamp&quot;)</span><br><span class="line">      val physicalPosition &#x3D; log.sizeInBytes()</span><br><span class="line">      if (physicalPosition &#x3D;&#x3D; 0)</span><br><span class="line">        rollingBasedTimestamp &#x3D; Some(largestTimestamp)</span><br><span class="line"></span><br><span class="line">      ensureOffsetInRange(largestOffset)</span><br><span class="line"></span><br><span class="line">      &#x2F;&#x2F; 追加到数据文件</span><br><span class="line">      val appendedBytes &#x3D; log.append(records)</span><br><span class="line">      trace(s&quot;Appended $appendedBytes to $&#123;log.file&#125; at end offset $largestOffset&quot;)</span><br><span class="line">      &#x2F;&#x2F; Update the in memory max timestamp and corresponding offset.</span><br><span class="line">      if (largestTimestamp &gt; maxTimestampSoFar) &#123;</span><br><span class="line">        maxTimestampSoFar &#x3D; largestTimestamp</span><br><span class="line">        offsetOfMaxTimestamp &#x3D; shallowOffsetOfMaxTimestamp</span><br><span class="line">      &#125;</span><br><span class="line">      &#x2F;&#x2F; 判断是否需要追加索引(数据每次都会添加到数据文件中,但不是每次都会添加索引的,间隔indexIntervalBytes 大小才会写入一个索引文件)</span><br><span class="line">      if (bytesSinceLastIndexEntry &gt; indexIntervalBytes) &#123;</span><br><span class="line">        &#x2F;&#x2F; 添加索引</span><br><span class="line">        offsetIndex.append(largestOffset, physicalPosition)</span><br><span class="line">        timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</span><br><span class="line">        bytesSinceLastIndexEntry &#x3D; 0</span><br><span class="line">      &#125;</span><br><span class="line">      bytesSinceLastIndexEntry +&#x3D; records.sizeInBytes</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="索引文件"><a href="#索引文件" class="headerlink" title="索引文件"></a>索引文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">offsetIndex以及timeIndex</span><br><span class="line"></span><br><span class="line">offsetIndex</span><br><span class="line">    采用绝对偏移量+相对偏移量的方式进行存储,每个segment最开始的绝对偏移量也是其基准偏移量</span><br><span class="line">    数据文件每隔一定的大小创建一个索引条目,而不是每条消息会创建索引条目,通过index.interval.bytes来配置</span><br><span class="line">        默认4096,4KB</span><br><span class="line"></span><br><span class="line">好处:</span><br><span class="line">    索引条目稀疏</span><br><span class="line">    索引的相对偏移量占据4个字节,绝对偏移量占据8个字节,物理位置4个字节</span><br><span class="line">        使用相对索引可以将每条索引条目的大小从12字节减少到8字节</span><br><span class="line">    因为偏移量有序的,再读取数据时,可以按照二分查找的方式去快速定位偏移量的位置</span><br><span class="line">    这样的稀疏索引是可以完全放到内存中,加快偏移量的查找</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark性能调优与数据倾斜</title>
    <url>/2017/11/13/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</url>
    <content><![CDATA[<blockquote>
<p>当发生Spark程序执行不理想的时候,需要考虑调优的问题</p>
</blockquote>
<span id="more"></span>

<h2 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h2><h3 id="更多的资源"><a href="#更多的资源" class="headerlink" title="更多的资源"></a>更多的资源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.分配更多的资源</span><br><span class="line">Executor的数量</span><br><span class="line">每个Executor所能分配的CPU数量</span><br><span class="line">每个Executor所能分配的内存量</span><br><span class="line">Driver端分配的内存数量</span><br><span class="line"></span><br><span class="line"># b.在哪里分配这些资源</span><br><span class="line">在生产环境中，提交spark作业时，用的spark-submit shell脚本，里面调整对应的参数：</span><br><span class="line">spark-submit \</span><br><span class="line">--class MainClass \ 执行主类</span><br><span class="line">--num-executors 3 \  配置executor的数量</span><br><span class="line">--driver-memory 4G \  配置driver的内存（影响不大）</span><br><span class="line">--executor-memory 8G \  配置每个executor的内存大小</span><br><span class="line">--total-executor-cores 3 \  配置所有executor的cpu core数量</span><br><span class="line">spark.jar \  jar位置</span><br><span class="line"></span><br><span class="line"># c.调节到多大，算是最大</span><br><span class="line">常用的资源调度模式有Spark Standalone和Spark On Yarn。</span><br><span class="line">比如说你的每台机器能够给你使用60G内存，10个cpu core，20台机器。那么executor的数量是20。平均每个executor所能分配60G内存和10个cpu core。</span><br><span class="line"></span><br><span class="line"># d.为什么多分配了这些资源以后，性能会得到提升</span><br><span class="line">Executor:</span><br><span class="line">    如果executor数量比较少，那么，能够并行执行的task数量就比较少，就意味着，我们的Application的并行执行的能力就很弱。</span><br><span class="line">    比如有3个executor，每个executor有2个cpu core，那么同时能够并行执行的task，就是6个。6个执行完以后，再换下一批6个task。</span><br><span class="line">    增加了executor数量以后，那么，就意味着，能够并行执行的task数量，也就变多了。比如原先是6个，现在可能可以并行执行10个，甚至20个，100个。那么并行能力就比之前提升了数倍，数十倍。相应的，性能（执行的速度），也能提升数倍-数十倍。</span><br><span class="line">CPU Core:</span><br><span class="line">    原本20个executor，每个才2个cpu core。能够并行执行的task数量，就是40个task。</span><br><span class="line">    现在每个executor的cpu core，增加到了4个。能够并行执行的task数量，就是100个task。就物理性能来看，执行的速度，提升了2.5倍。</span><br><span class="line">Executor内存量:</span><br><span class="line">    如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，甚至不写入磁盘。减少了磁盘IO。</span><br><span class="line">    对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。</span><br><span class="line">    对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，速度变快了。</span><br></pre></td></tr></table></figure>
<h3 id="调节并行度"><a href="#调节并行度" class="headerlink" title="调节并行度"></a>调节并行度</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.并行度的概念</span><br><span class="line">指的是Spark作业中，各个stage的task数量，代表了Spark作业的在各个阶段（stage）的并行度。</span><br><span class="line"></span><br><span class="line"># b.并行度过低，会怎么样?</span><br><span class="line">比如现在spark-submit脚本里面，给我们的spark作业分配了足够多的资源，比如50个executor，每个executor有10G内存，每个executor有3个cpu core。基本已经达到了集群或者yarn队列的资源上限。</span><br><span class="line">task没有设置，或者设置的很少，比如就设置了100个task</span><br><span class="line">50个executor，每个executor有3个cpu core，也就是说，你的Application任何一个stage运行的时候，都有总数在150个cpu core，可以并行运行。</span><br><span class="line">但是你现在，只有100个task，平均分配一下，每个executor分配到2个task，ok，那么同时在运行的task，只有100个，每个executor只会并行运行2个task。每个executor剩下的一个cpu core， 就浪费掉了。</span><br><span class="line">合理的并行度的设置，应该是要设置的足够大，大到可以完全合理的利用你的集群资源。</span><br><span class="line">比如上面的例子，总共集群有150个cpu core，可以并行运行150个task。那么就应该将你的Application的并行度，至少设置成150，才能完全有效的利用你的集群资源，让150个task，并行执行。</span><br><span class="line">而且task增加到150个以后，即可以同时并行运行，还可以让每个task要处理的数据量变少。</span><br><span class="line">比如总共150G的数据要处理，如果是100个task，每个task计算1.5G的数据，现在增加到150个task，可以并行运行，而且每个task主要处理1G的数据就可以。</span><br><span class="line">很简单的道理，只要合理设置并行度，就可以完全充分利用你的集群计算资源，并且减少每个task要处理的数据量，最终，就是提升你的整个Spark作业的性能和运行速度。</span><br><span class="line"></span><br><span class="line"># c.设置并行度</span><br><span class="line">task数量:</span><br><span class="line">    至少设置成与Spark application的总cpu core数量相同（最理想情况，比如总共150个cpu core，分配了150个task，一起运行，差不多同一时间运行完毕）。</span><br><span class="line">    官方是推荐，task数量，设置成spark application总cpu core数量的2~3倍，比如150个cpu core，基本要设置task数量为300~500。</span><br><span class="line">    实际情况，与理想情况不同的，有些task会运行的快一点，比如50s就完了，有些task，可能会慢一点，要1分半才运行完，所以如果你的task数量，刚好设置的跟cpu core数量相同，可能还是会导致资源的浪费。</span><br><span class="line">    比如150个task，10个先运行完了，剩余140个还在运行，但是这个时候，有10个cpu core就空闲出来了，就导致了浪费。</span><br><span class="line">    那如果task数量设置成cpu core总数的2~3倍，那么一个task运行完了以后，另一个task马上可以补上来，就尽量让cpu core不要空闲，同时也是尽量提升spark作业运行的效率和速度，提升性能。</span><br><span class="line">如何设置:</span><br><span class="line">    spark.default.parallelism</span><br><span class="line">    SparkConf conf &#x3D; new SparkConf().set(&quot;spark.default.parallelism&quot;, &quot;500&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="重构RDD架构以及RDD持久化"><a href="#重构RDD架构以及RDD持久化" class="headerlink" title="重构RDD架构以及RDD持久化"></a>重构RDD架构以及RDD持久化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.RDD架构重构与优化</span><br><span class="line">尽量去复用RDD，差不多的RDD，可以抽取成为一个共同的RDD，供后面的RDD计算时，反复使用。</span><br><span class="line"></span><br><span class="line"># b.公共RDD一定要实现持久化</span><br><span class="line">对于要多次计算和使用的公共RDD，一定要进行持久化。</span><br><span class="line">持久化，就是将RDD的数据缓存到内存中&#x2F;磁盘中（BlockManager）以后无论对这个RDD做多少次计算，那么都是直接取这个RDD的持久化的数据，比如从内存中或者磁盘中，直接提取一份数据。</span><br><span class="line"></span><br><span class="line"># c.持久化，是可以进行序列化的</span><br><span class="line">如果正常将数据持久化在内存中，那么可能会导致内存的占用过大，这样的话，也许，会导致OOM内存溢出。</span><br><span class="line">当纯内存无法支撑公共RDD数据完全存放的时候，就优先考虑使用序列化的方式在纯内存中存储。将RDD的每个partition的数据，序列化成一个大的字节数组，就一个对象。</span><br><span class="line">序列化后，大大减少内存的空间占用。缺点就是，在获取数据的时候，需要反序列化。</span><br><span class="line">如果序列化纯内存方式，还是导致OOM内存溢出，就只能考虑磁盘的方式、内存+磁盘的普通方式（无序列化）、内存+磁盘（序列化）。</span><br><span class="line"></span><br><span class="line"># d.为了数据高可靠性,且内存充足,可以使用双副本机制进行持久化</span><br><span class="line">持久化的双副本机制，持久化后的一个副本，因为机器宕机了，副本丢了，就还是得重新计算一次。</span><br><span class="line">持久化的每个数据单元，存储一份副本，放在其他节点上面。从而进行容错。一个副本丢了，不用重新计算，还可以使用另外一份副本。</span><br><span class="line">这种方式，仅仅针对你的内存资源极度充足的情况。</span><br></pre></td></tr></table></figure>
<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.概念及需求</span><br><span class="line">Spark Application（我们自己写的Spark作业）最开始在Driver端，在我们提交任务的时候，需要传递到各个Executor的Task上运行。</span><br><span class="line">对于一些只读、固定的数据(比如从DB中读出的数据),每次都需要Driver广播到各个Task上，这样效率低下。</span><br><span class="line">广播变量允许将变量值广播（提前广播）给各个Executor。</span><br><span class="line">该Executor上的各个Task再从所在节点的BlockManager获取变量，如果本地没有，那么就从Driver远程拉取变量副本，并保存在本地的BlockManager中。</span><br><span class="line">此后这个executor上的task，都会直接使用本地的BlockManager中的副本。而不是从Driver获取变量，从而提升了效率。</span><br><span class="line">一个Executor只需要在第一个Task启动时，获得一份Broadcast数据，之后的Task都从本节点的BlockManager中获取相关数据。</span><br><span class="line"></span><br><span class="line"># b.使用方法</span><br><span class="line">调用SparkContext.broadcast方法创建一个Broadcast[T]对象。任何序列化的类型都可以这么实现。</span><br><span class="line">通过value属性访问改对象的值(Java之中为value()方法)</span><br><span class="line">变量只会被发送到各个节点一次，应作为只读值处理（修改这个值不会影响到别的节点）</span><br></pre></td></tr></table></figure>
<h3 id="使用Kryo序列化"><a href="#使用Kryo序列化" class="headerlink" title="使用Kryo序列化"></a>使用Kryo序列化</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.概念及需求</span><br><span class="line">默认情况下，Spark内部是使用Java的序列化机制，ObjectOutputStream &#x2F; ObjectInputStream，对象输入输出流机制，来进行序列化。</span><br><span class="line">这种默认序列化机制的好处在于，处理起来比较方便，也不需要我们手动去做什么事情，只是，你在算子里面使用的变量，必须是实现Serializable接口的，可序列化即可。</span><br><span class="line">但是缺点在于，默认的序列化机制的效率不高，序列化的速度比较慢，序列化以后的数据，占用的内存空间相对还是比较大。</span><br><span class="line">Spark支持使用Kryo序列化机制。这种序列化机制，比默认的Java序列化机制速度要快，序列化后的数据更小，大概是Java序列化机制的1&#x2F;10。</span><br><span class="line">所以Kryo序列化优化以后，可以让网络传输的数据变少，在集群中耗费的内存资源大大减少。</span><br><span class="line"></span><br><span class="line"># b.Kryo序列化机制启用以后生效的几个地方</span><br><span class="line">算子函数中使用到的外部变量: 使用Kryo以后，优化网络传输的性能，可以优化集群中内存的占用和消耗</span><br><span class="line">持久化RDD: 优化内存的占用和消耗。持久化RDD占用的内存越少，task执行的时候，创建的对象，就不至于频繁的占满内存，频繁发生GC。</span><br><span class="line">shuffle: 可以优化网络传输的性能</span><br><span class="line"></span><br><span class="line"># c.使用方法</span><br><span class="line">第一步，在SparkConf中设置一个属性，spark.serializer，org.apache.spark.serializer.KryoSerializer类。</span><br><span class="line">第二步，注册你使用的需要通过Kryo序列化的一些自定义类，SparkConf.registerKryoClasses()。</span><br><span class="line">项目中的使用：</span><br><span class="line">    .set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">    .registerKryoClasses(new Class[]&#123;CategorySortKey.class&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="使用fastutil优化数据格式"><a href="#使用fastutil优化数据格式" class="headerlink" title="使用fastutil优化数据格式"></a>使用fastutil优化数据格式</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.fastutil介绍</span><br><span class="line">fastutil是扩展了Java标准集合框架（Map、List、Set。HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue.</span><br><span class="line">fastutil能够提供更小的内存占用，更快的存取速度。我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于fastutil集合类可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度。</span><br><span class="line">fastutil也提供了64位的array、set和list，以及高性能快速的，以及实用的IO类，来处理二进制和文本类型的文件。</span><br><span class="line">fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</span><br><span class="line">fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。</span><br><span class="line">fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（&#x3D;）进行比较的，而不是equals()方法。</span><br><span class="line">fastutil尽量提供了在任何场景下都是速度最快的集合类库。</span><br><span class="line"></span><br><span class="line"># b.Spark中应用fastutil的场景</span><br><span class="line">如果算子函数使用了外部变量。第一，你可以使用Broadcast广播变量优化。第二，可以使用Kryo序列化类库，提升序列化性能和效率。第三，如果外部变量是某种比较大的集合，那么可以考虑使用fastutil改写外部变量，首先从源头上就减少内存的占用，通过广播变量进一步减少内存占用，再通过Kryo序列化类库进一步减少内存占用。</span><br><span class="line">在你的算子函数里，也就是task要执行的计算逻辑里面，如果有逻辑中，出现，要创建比较大的Map、List等集合，可能会占用较大的内存空间，而且可能涉及到消耗性能的遍历、存取等集合操作，此时，可以考虑将这些集合类型使用fastutil类库重写，使用了fastutil集合类以后，就可以在一定程度上，减少task创建出来的集合类型的内存占用。避免executor内存频繁占满，频繁唤起GC，导致性能下降。</span><br><span class="line"></span><br><span class="line"># c.关于fastutil调优的说明</span><br><span class="line">fastutil其实没有你想象中的那么强大，也不会跟官网上说的效果那么一鸣惊人。广播变量、Kryo序列化类库、fastutil，都是之前所说的，对于性能来说，类似于一种调味品，烤鸡，本来就很好吃了，然后加了一点特质的孜然麻辣粉调料，就更加好吃了一点。分配资源、并行度、RDD架构与持久化，这三个就是烤鸡。broadcast、kryo、fastutil，类似于调料。</span><br><span class="line">比如说，你的spark作业，经过之前一些调优以后，大概30分钟运行完，现在加上broadcast、kryo、fastutil，也许就是优化到29分钟运行完、或者更好一点，也许就是28分钟、25分钟。</span><br><span class="line">shuffle调优，15分钟。groupByKey用reduceByKey改写，执行本地聚合，也许10分钟。跟公司申请更多的资源，比如资源更大的YARN队列，1分钟。</span><br></pre></td></tr></table></figure>
<h3 id="调节数据本地化等待时长"><a href="#调节数据本地化等待时长" class="headerlink" title="调节数据本地化等待时长"></a>调节数据本地化等待时长</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.task的locality有五种</span><br><span class="line">PROCESS_LOCAL：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中。计算数据的task由executor执行，数据在executor的BlockManager中，性能最好。</span><br><span class="line">NODE_LOCAL：节点本地化，代码和数据在同一个节点中。比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行，或者是，数据和task在一个节点上的不同executor中，数据需要在进程间进行传输。</span><br><span class="line">NO_PREF：对于task来说，数据从哪里获取都一样，没有好坏之分。</span><br><span class="line">RACK_LOCAL：机架本地化，数据和task在一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</span><br><span class="line">ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差。</span><br><span class="line"></span><br><span class="line"># b.Spark的任务调度</span><br><span class="line">Spark在Driver上，对Application的每一个stage的task进行分配之前都会计算出每个task要计算的是哪个分片数据。Spark的task分配算法优先会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据。</span><br><span class="line">但是，有时可能task没有机会分配到它的数据所在的节点。为什么呢，可能那个节点的计算资源和计算能力都满了。所以这种时候， Spark会等待一段时间，默认情况下是3s（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别。比如说，将task分配到靠它要计算的数据所在节点比较近的一个节点，然后进行计算。</span><br><span class="line">但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。</span><br><span class="line">对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO。如果要通过网络传输数据的话，性能肯定会下降的。大量网络传输，以及磁盘IO，都是性能的杀手。</span><br><span class="line"></span><br><span class="line"># c.什么时候要调节这个参数</span><br><span class="line">观察spark作业的运行日志。推荐大家在测试的时候，先用client模式在本地就直接可以看到比较全的日志。</span><br><span class="line">日志里面会显示：starting task…，PROCESS LOCAL、NODE LOCAL</span><br><span class="line">观察大部分task的数据本地化级别，如果大多都是PROCESS_LOCAL，那就不用调节了。</span><br><span class="line">如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长。</span><br><span class="line">要反复调节，每次调节完以后再运行并观察日志，看看大部分的task的本地化级别有没有提升，看看整个spark作业的运行时间有没有缩短。</span><br><span class="line">注意，不要本末倒置，不要本地化级别是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那还是不要调节了。</span><br><span class="line"></span><br><span class="line"># d.如何调节</span><br><span class="line">spark.locality.wait，默认是3s。6s，10s</span><br><span class="line">默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3s</span><br><span class="line">spark.locality.wait.process</span><br><span class="line">spark.locality.wait.node</span><br><span class="line">spark.locality.wait.rack</span><br><span class="line">new SparkConf().set(&quot;spark.locality.wait&quot;, &quot;10&quot;)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="JVM调优"><a href="#JVM调优" class="headerlink" title="JVM调优"></a>JVM调优</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.降低cache操作的内存占比</span><br><span class="line">spark中，堆内存又被划分成了两块，一块是专门用来给RDD的cache、persist操作进行RDD数据缓存用的。另外一块用来给spark算子函数的运行使用的，存放函数中自己创建的对象。</span><br><span class="line">默认情况下，给RDD cache操作的内存占比，是0.6，60%的内存都给了cache操作了。但是问题是，如果某些情况下cache不是那么的紧张，问题在于task算子函数中创建的对象过多，然后内存又不太大，导致了频繁的minor gc，甚至频繁full gc，导致spark频繁的停止工作。性能影响会很大。</span><br><span class="line">针对上述这种情况，可以在任务运行界面，去查看你的spark作业的运行统计，可以看到每个stage的运行情况，包括每个task的运行时间、gc时间等等。如果发现gc太频繁，时间太长。此时就可以适当调节这个比例。</span><br><span class="line">降低cache操作的内存占比，大不了用persist操作，选择将一部分缓存的RDD数据写入磁盘，或者序列化方式，配合Kryo序列化类，减少RDD缓存的内存占用。降低cache操作内存占比，对应的，算子函数的内存占比就提升了。这个时候，可能就可以减少minor gc的频率，同时减少full gc的频率。对性能的提升是有一定的帮助的。</span><br><span class="line">一句话，让task执行算子函数时，有更多的内存可以使用。</span><br><span class="line">spark.storage.memoryFraction，0.6 -&gt; 0.5 -&gt; 0.4 -&gt; 0.2</span><br><span class="line"></span><br><span class="line"># b.调节executor堆外内存与连接等待时长</span><br><span class="line">executor堆外内存: </span><br><span class="line">    有时候，如果你的spark作业处理的数据量特别大，几亿数据量。然后spark作业一运行，时不时的报错，shuffle file cannot find，executor、task lost，out of memory（内存溢出）。</span><br><span class="line">    可能是executor的堆外内存不太够用，导致executor在运行的过程中，可能会内存溢出，可能导致后续的stage的task在运行的时候，要从一些executor中去拉取shuffle map output文件，但是executor可能已经挂掉了，关联的block manager也没有了。所以会报shuffle output file not found，resubmitting task，executor lost。spark作业彻底崩溃。</span><br><span class="line">    上述情况下，就可以去考虑调节一下executor的堆外内存。也许就可以避免报错。此外，有时堆外内存调节的比较大的时候，对于性能来说，也会带来一定的提升。</span><br><span class="line">    可以调节堆外内存的上限：</span><br><span class="line">    --conf spark.yarn.executor.memoryOverhead&#x3D;2048</span><br><span class="line">    spark-submit脚本里面，去用--conf的方式，去添加配置。用new SparkConf().set()这种方式去设置是没有用的！一定要在spark-submit脚本中去设置。</span><br><span class="line">    spark.yarn.executor.memoryOverhead（看名字，顾名思义，针对的是基于yarn的提交模式）</span><br><span class="line">    默认情况下，这个堆外内存上限大概是300M。通常在项目中，真正处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行。此时就会去调节这个参数，到至少1G（1024M），甚至说2G、4G。</span><br><span class="line">    通常这个参数调节上去以后，就会避免掉某些JVM OOM的异常问题，同时呢，会让整体spark作业的性能，得到较大的提升。</span><br><span class="line">调节连接等待时长:</span><br><span class="line">    executor会优先从自己本地关联的BlockManager中获取某份数据。如果本地block manager没有的话，那么会通过TransferService，去远程连接其他节点上executor的block manager去获取。</span><br><span class="line">    而此时上面executor去远程连接的那个executor，因为task创建的对象特别大，特别多，</span><br><span class="line">    频繁的让JVM堆内存满溢，正在进行垃圾回收。而处于垃圾回收过程中，所有的工作线程全部停止，相当于只要一旦进行垃圾回收，spark &#x2F; executor停止工作，无法提供响应。</span><br><span class="line">    此时呢，就会没有响应，无法建立网络连接，会卡住。spark默认的网络连接的超时时长，是60s，如果卡住60s都无法建立连接的话，那么就宣告失败了。</span><br><span class="line">    报错几次，几次都拉取不到数据的话，可能会导致spark作业的崩溃。也可能会导致DAGScheduler，反复提交几次stage。TaskScheduler反复提交几次task。大大延长我们的spark作业的运行时间。</span><br><span class="line">    可以考虑调节连接的超时时长：</span><br><span class="line">    --conf spark.core.connection.ack.wait.timeout&#x3D;300</span><br><span class="line">    spark-submit脚本，切记，不是在new SparkConf().set()这种方式来设置的。</span><br><span class="line">    spark.core.connection.ack.wait.timeout（spark core，connection，连接，ack，wait timeout，建立不上连接的时候，超时等待时长）</span><br><span class="line">    调节这个值比较大以后，通常来说，可以避免部分的偶尔出现的某某文件拉取失败，某某文件lost掉了。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Shuffle调优"><a href="#Shuffle调优" class="headerlink" title="Shuffle调优"></a>Shuffle调优</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.原理概述</span><br><span class="line">什么样的情况下，会发生shuffle</span><br><span class="line">在spark中，主要是以下几个算子：groupByKey、reduceByKey、countByKey、join，等等。</span><br><span class="line">什么是shuffle</span><br><span class="line">groupByKey，要把分布在集群各个节点上的数据中的同一个key，对应的values，都要集中到一块儿，集中到集群中同一个节点上，更严密一点说，就是集中到一个节点的一个executor的一个task中。然后呢，集中一个key对应的values之后，才能交给我们来进行处理，&lt;key, Iterable&lt;value&gt;&gt;。</span><br><span class="line">reduceByKey，算子函数去对values集合进行reduce操作，最后变成一个value。</span><br><span class="line">countByKey需要在一个task中，获取到一个key对应的所有的value，然后进行计数，统计一共有多少个value。</span><br><span class="line">join，RDD&lt;key, value&gt;，RDD&lt;key, value&gt;，只要是两个RDD中，key相同对应的2个value，都能到一个节点的executor的task中，给我们进行处理。</span><br><span class="line">shuffle，一定是分为两个stage来完成的。因为这其实是个逆向的过程，不是stage决定shuffle，是shuffle决定stage。</span><br><span class="line">在某个action触发job的时候，DAGScheduler，会负责划分job为多个stage。划分的依据，就是，如果发现有会触发shuffle操作的算子，比如reduceByKey，就将这个操作的前半部分，以及之前所有的RDD和transformation操作，划分为一个stage。shuffle操作的后半部分，以及后面的，直到action为止的RDD和transformation操作，划分为另外一个stage。</span><br><span class="line"></span><br><span class="line"># b.合并map端输出文件</span><br><span class="line">不合并map端输出文件的话，会怎么样: </span><br><span class="line">    100个节点（每个节点一个executor）：100个executor</span><br><span class="line">    每个executor：2个cpu core</span><br><span class="line">    总共1000个task：每个executor平均10个task</span><br><span class="line">    每个节点，10个task，每个节点会输出多少份map端文件？10 * 1000&#x3D;1万个文件</span><br><span class="line">    总共有多少份map端输出文件？100 * 10000 &#x3D; 100万。</span><br><span class="line">    第一个stage，每个task，都会给第二个stage的每个task创建一份map端的输出文件</span><br><span class="line">    第二个stage，每个task，会到各个节点上面去，拉取第一个stage每个task输出的，属于自己的那一份文件。</span><br><span class="line">    shuffle中的写磁盘的操作，基本上就是shuffle中性能消耗最为严重的部分。</span><br><span class="line">    通过上面的分析，一个普通的生产环境的spark job的一个shuffle环节，会写入磁盘100万个文件。</span><br><span class="line">    磁盘IO对性能和spark作业执行速度的影响，是极其惊人和吓人的。</span><br><span class="line">    基本上，spark作业的性能，都消耗在shuffle中了，虽然不只是shuffle的map端输出文件这一个部分，但是这里也是非常大的一个性能消耗点。</span><br><span class="line">开启shuffle map端输出文件合并的机制:</span><br><span class="line">    new SparkConf().set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;)</span><br><span class="line">    默认情况下，是不开启的，就是会发生如上所述的大量map端输出文件的操作，严重影响性能。</span><br><span class="line">合并map端输出文件，对咱们的spark的性能有哪些方面的影响:</span><br><span class="line">    map task写入磁盘文件的IO，减少：100万文件 -&gt; 20万文件</span><br><span class="line">    第二个stage，原本要拉取第一个stage的task数量份文件，1000个task，第二个stage的每个task，都要拉取1000份文件，走网络传输。合并以后，100个节点，每个节点2个cpu core，第二个stage的每个task，主要拉取100 * 2 &#x3D; 200个文件即可。此时网络传输的性能消耗也大大减少。</span><br><span class="line">    分享一下，实际在生产环境中，使用了spark.shuffle.consolidateFiles机制以后，实际的性能调优的效果：对于上述的这种生产环境的配置，性能的提升，还是相当的可观的。spark作业，5个小时 -&gt; 2~3个小时。</span><br><span class="line">    大家不要小看这个map端输出文件合并机制。实际上，在数据量比较大，你自己本身做了前面的性能调优，executor上去-&gt;cpu core上去-&gt;并行度（task数量）上去，shuffle没调优，shuffle就很糟糕了。大量的map端输出文件的产生，对性能有比较恶劣的影响。</span><br><span class="line">    这个时候，去开启这个机制，可以很有效的提升性能</span><br><span class="line"></span><br><span class="line"># c.调节map端内存缓冲与reduce端内存占比</span><br><span class="line">默认情况下可能出现的问题:</span><br><span class="line">    默认情况下，shuffle的map task，输出到磁盘文件的时候，统一都会先写入每个task自己关联的一个内存缓冲区。</span><br><span class="line">    这个缓冲区大小，默认是32kb。</span><br><span class="line">    每一次，当内存缓冲区满溢之后，才会进行spill溢写操作，溢写到磁盘文件中去。</span><br><span class="line">    reduce端task，在拉取到数据之后，会用hashmap的数据格式，来对各个key对应的values进行汇聚。</span><br><span class="line">    针对每个key对应的values，执行我们自定义的聚合函数的代码，比如_ + _（把所有values累加起来）。</span><br><span class="line">    reduce task，在进行汇聚、聚合等操作的时候，实际上，使用的就是自己对应的executor的内存，executor（jvm进程，堆），默认executor内存中划分给reduce task进行聚合的比例是0.2。</span><br><span class="line">    问题来了，因为比例是0.2，所以，理论上，很有可能会出现，拉取过来的数据很多，那么在内存中，放不下。这个时候，默认的行为就是将在内存放不下的数据都spill（溢写）到磁盘文件中去。</span><br><span class="line">    在数据量比较大的情况下，可能频繁地发生reduce端的磁盘文件的读写。</span><br><span class="line">调优方式:</span><br><span class="line">    调节map task内存缓冲：spark.shuffle.file.buffer，默认32k（spark 1.3.x不是这个参数，后面还有一个后缀，kb。spark 1.5.x以后，变了，就是现在这个参数）</span><br><span class="line">    调节reduce端聚合内存占比：spark.shuffle.memoryFraction，0.2</span><br><span class="line">在实际生产环境中，我们在什么时候来调节两个参数:</span><br><span class="line">    看Spark UI，如果你的公司是决定采用standalone模式，那么狠简单，你的spark跑起来，会显示一个Spark UI的地址，4040的端口。进去观察每个stage的详情，有哪些executor，有哪些task，每个task的shuffle write和shuffle read的量，shuffle的磁盘和内存读写的数据量。如果是用的yarn模式来提交，从yarn的界面进去，点击对应的application，进入Spark UI，查看详情。</span><br><span class="line">    如果发现shuffle 磁盘的write和read，很大。这个时候，就意味着最好调节一些shuffle的参数。首先当然是考虑开启map端输出文件合并机制。其次调节上面说的那两个参数。调节的时候的原则：spark.shuffle.file.buffer每次扩大一倍，然后看看效果，64，128。spark.shuffle.memoryFraction，每次提高0.1，看看效果。</span><br><span class="line">    不能调节的太大，太大了以后过犹不及，因为内存资源是有限的，你这里调节的太大了，其他环节的内存使用就会有问题了。</span><br><span class="line">调节以后的效果:</span><br><span class="line">    map task内存缓冲变大了，减少spill到磁盘文件的次数。reduce端聚合内存变大了，减少spill到磁盘的次数，而且减少了后面聚合读取磁盘文件的数量。</span><br><span class="line"></span><br><span class="line"># d.HashShuffleManager与SortShuffleManager</span><br><span class="line">shuffle调优概述:</span><br><span class="line">    大多数Spark作业的性能主要就是消耗在了shuffle环 节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。</span><br><span class="line">    因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。</span><br><span class="line">    因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</span><br><span class="line">ShuffleManager发展概述:</span><br><span class="line">    在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。</span><br><span class="line">    在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</span><br><span class="line">    因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于 HashShuffleManager来说，有了一定的改进。</span><br><span class="line">    主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</span><br><span class="line">    在spark 1.5.x以后，对于shuffle manager又出来了一种新的manager，tungsten-sort（钨丝），钨丝sort shuffle manager。</span><br><span class="line">    官网上一般说，钨丝sort shuffle manager，效果跟sort shuffle manager是差不多的。</span><br><span class="line">    但是，唯一的不同之处在于，钨丝manager，是使用了自己实现的一套内存管理机制，性能上有很大的提升，而且可以避免shuffle过程中产生的大量的OOM，GC，等等内存相关的异常。</span><br><span class="line">hash、sort、tungsten-sort。如何来选择:</span><br><span class="line">    需不需要数据默认就让spark给你进行排序？就好像mapreduce，默认就是有按照key的排序。如果不需要的话，其实还是建议搭建就使用最基本的HashShuffleManager，因为最开始就是考虑的是不排序，换取高性能。</span><br><span class="line">    什么时候需要用sort shuffle manager？如果你需要你的那些数据按key排序了，那么就选择这种吧，而且要注意，reduce task的数量应该是超过200的，这样sort、merge（多个文件合并成一个）的机制，才能生效。但是这里要注意，你一定要自己考量一下，有没有必要在shuffle的过程中，就做这个事情，毕竟对性能是有影响的。</span><br><span class="line">    如果你不需要排序，而且你希望你的每个task输出的文件最终是会合并成一份的，你自己认为可以减少性能开销。可以去调节bypassMergeThreshold这个阈值，比如你的reduce task数量是500，默认阈值是200，所以默认还是会进行sort和直接merge的。可以将阈值调节成550，不会进行sort，按照hash的做法，每个reduce task创建一份输出文件，最后合并成一份文件。（一定要提醒大家，这个参数，其实我们通常不会在生产环境里去使用，也没有经过验证说，这样的方式，到底有多少性能的提升）</span><br><span class="line">    如果你想选用sort based shuffle manager，而且你们公司的spark版本比较高，是1.5.x版本的，那么可以考虑去尝试使用tungsten-sort shuffle manager。看看性能的提升与稳定性怎么样。</span><br><span class="line">    总结：</span><br><span class="line">        在生产环境中，不建议大家贸然使用第三点和第四点：</span><br><span class="line">        如果你不想要你的数据在shuffle时排序，那么就自己设置一下，用hash shuffle manager。</span><br><span class="line">        如果你的确是需要你的数据在shuffle时进行排序的，那么就默认不用动，默认就是sort shuffle manager。或者是什么？如果你压根儿不care是否排序这个事儿，那么就默认让他就是sort的。调节一些其他的参数（consolidation机制）。（80%，都是用这种）</span><br><span class="line">        spark.shuffle.manager：hash、sort、tungsten-sort</span><br><span class="line">        spark.shuffle.sort.bypassMergeThreshold：200。自己可以设定一个阈值，默认是200，当reduce task数量少于等于200，map task创建的输出文件小于等于200的，最后会将所有的输出文件合并为一份文件。这样做的好处，就是避免了sort排序，节省了性能开销，而且还能将多个reduce task的文件合并成一份文件，节省了reduce task拉取数据的时候的磁盘IO的开销。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="算子调优"><a href="#算子调优" class="headerlink" title="算子调优"></a>算子调优</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.MapPartitions提升Map类操作性能</span><br><span class="line">spark中，最基本的原则，就是每个task处理一个RDD的partition。</span><br><span class="line">MapPartitions操作的优点：</span><br><span class="line">    如果是普通的map，比如一个partition中有1万条数据。ok，那么你的function要执行和计算1万次。但是，使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。</span><br><span class="line">MapPartitions的缺点：</span><br><span class="line">    如果是普通的map操作，一次function的执行就处理一条数据。那么如果内存不够用的情况下，比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。</span><br><span class="line">    所以说普通的map操作通常不会导致内存的OOM异常。</span><br><span class="line">    但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。</span><br><span class="line">MapPartitions使用场景:</span><br><span class="line">    当分析的数据量不是特别大的时候，都可以用这种MapPartitions系列操作，性能还是非常不错的，是有提升的。比如原来是15分钟，（曾经有一次性能调优），12分钟。10分钟-&gt;9分钟。</span><br><span class="line">    但是也有过出问题的经验，MapPartitions只要一用，直接OOM，内存溢出，崩溃。</span><br><span class="line">    在项目中，自己先去估算一下RDD的数据量，以及每个partition的量，还有自己分配给每个executor的内存资源。看看一下子内存容纳所有的partition数据行不行。</span><br><span class="line">    如果行，可以试一下，能跑通就好。性能肯定是有提升的。但是试了以后，发现OOM了，那就放弃吧。</span><br><span class="line"></span><br><span class="line"># b.filter过后使用coalesce减少分区数量</span><br><span class="line">默认情况下，经过了filter之后，RDD中的每个partition的数据量，可能都不太一样了。（原本每个partition的数据量可能是差不多的）</span><br><span class="line">可能出现的问题：</span><br><span class="line">    每个partition数据量变少了，但是在后面进行处理的时候，还是要跟partition数量一样数量的task，来进行处理，有点浪费task计算资源。</span><br><span class="line">    每个partition的数据量不一样，会导致后面的每个task处理每个partition的时候，每个task要处理的数据量就不同，这样就会导致有些task运行的速度很快，有些task运行的速度很慢。这就是数据倾斜。</span><br><span class="line">针对上述的两个问题，我们希望应该能够怎么样:</span><br><span class="line">    针对第一个问题，我们希望可以进行partition的压缩，因为数据量变少了，那么partition其实也完全可以对应的变少。比如原来是4个partition，现在完全可以变成2个partition。那么就只要用后面的2个task来处理即可。就不会造成task计算资源的浪费。（不必要，针对只有一点点数据的partition，还去启动一个task来计算）</span><br><span class="line">    针对第二个问题，其实解决方案跟第一个问题是一样的，也是去压缩partition，尽量让每个partition的数据量差不多。那么后面的task分配到的partition的数据量也就差不多。不会造成有的task运行速度特别慢，有的task运行速度特别快。避免了数据倾斜的问题。</span><br><span class="line">解决问题方法:</span><br><span class="line">    调用coalesce算子,主要就是用于在filter操作之后，针对每个partition的数据量各不相同的情况，来压缩partition的数量，而且让每个partition的数据量都尽量均匀紧凑。</span><br><span class="line">    从而便于后面的task进行计算操作，在某种程度上，能够一定程度的提升性能。</span><br><span class="line"></span><br><span class="line"># c.使用foreachPartition优化写数据库性能</span><br><span class="line">默认的foreach的性能缺陷在哪里:</span><br><span class="line">    首先，对于每条数据，都要单独去调用一次function，task为每个数据，都要去执行一次function函数。如果100万条数据，（一个partition），调用100万次。性能比较差。</span><br><span class="line">    另外一个非常非常重要的一点，如果每个数据，你都去创建一个数据库连接的话，那么你就得创建100万次数据库连接。</span><br><span class="line">    但是要注意的是，数据库连接的创建和销毁，都是非常非常消耗性能的。虽然我们之前已经用了数据库连接池，只是创建了固定数量的数据库连接。</span><br><span class="line">    你还是得多次通过数据库连接，往数据库（MySQL）发送一条SQL语句，然后MySQL需要去执行这条SQL语句。如果有100万条数据，那么就是100万次发送SQL语句。</span><br><span class="line">    以上两点（数据库连接，多次发送SQL语句），都是非常消耗性能的。</span><br><span class="line">用了foreachPartition算子之后，好处在哪里：</span><br><span class="line">    对于我们写的function函数，就调用一次，一次传入一个partition所有的数据。</span><br><span class="line">    只要创建或者获取一个数据库连接就可以。</span><br><span class="line">    只要向数据库发送一次SQL语句和多组参数即可。</span><br><span class="line">注意，与mapPartitions操作一样，如果一个partition的数量真的特别特别大，比如是100万，那基本上就不太靠谱了。很有可能会发生OOM，内存溢出的问题。</span><br><span class="line"></span><br><span class="line"># d.使用repartition解决Spark SQL低并行度的性能问题</span><br><span class="line">你设置的这个并行度，在哪些情况下会生效？什么情况下不会生效?</span><br><span class="line">    如果你压根儿没有使用Spark SQL（DataFrame），那么你整个spark application默认所有stage的并行度都是你设置的那个参数。（除非你使用coalesce算子缩减过partition数量）。</span><br><span class="line">    问题来了，用Spark SQL的情况下，stage的并行度没法自己指定。Spark SQL自己会默认根据hive表对应的hdfs文件的block，自动设置Spark SQL查询所在的那个stage的并行度。你自己通过spark.default.parallelism参数指定的并行度，只会在没有Spark SQL的stage中生效。</span><br><span class="line">    比如你第一个stage，用了Spark SQL从hive表中查询出了一些数据，然后做了一些transformation操作，接着做了一个shuffle操作（groupByKey）。</span><br><span class="line">    下一个stage，在shuffle操作之后，做了一些transformation操作。</span><br><span class="line">    hive表，对应了一个hdfs文件，有20个block。你自己设置了spark.default.parallelism参数为100。</span><br><span class="line">    你的第一个stage的并行度，是不受你的控制的，就只有20个task。第二个stage，才会变成你自己设置的那个并行度，100。</span><br><span class="line">可能出现的问题:</span><br><span class="line">    Spark SQL默认情况下，它的那个并行度，咱们没法设置。可能导致的问题，也许没什么问题，也许很有问题。</span><br><span class="line">    Spark SQL所在的那个stage中，后面的那些transformation操作，可能会有非常复杂的业务逻辑，甚至说复杂的算法。</span><br><span class="line">    如果你的Spark SQL默认把task数量设置的很少，20个，然后每个task要处理为数不少的数据量，然后还要执行特别复杂的算法。</span><br><span class="line">    这个时候，就会导致第一个stage的速度，特别慢。第二个stage1000个task非常快。</span><br><span class="line">解决Spark SQL无法设置并行度和task数量的办法:</span><br><span class="line">    repartition算子，你用Spark SQL这一步的并行度和task数量，肯定是没有办法去改变了。</span><br><span class="line">    但是呢，可以将你用Spark SQL查询出来的RDD，使用repartition算子去重新进行分区，此时可以分成多个partition。</span><br><span class="line">    然后呢，从repartition以后的RDD，再往后，并行度和task数量，就会按照你预期的来了。就可以避免跟Spark SQL绑定在一个stage中的算子，只能使用少量的task去处理大量数据以及复杂的算法逻辑。</span><br><span class="line"></span><br><span class="line"># e.reduceByKey本地聚合</span><br><span class="line">reduceByKey，相较于普通的shuffle操作（比如groupByKey），它的一个特点，就是说，会进行map端的本地聚合。对map端给下个stage每个task创建的输出文件中，写数据之前，就会进行本地的combiner操作，也就是说对每一个key，对应的values，都会执行你的算子函数（_ + _）</span><br><span class="line">用reduceByKey对性能的提升:</span><br><span class="line">    在本地进行聚合以后，在map端的数据量就变少了，减少磁盘IO。而且可以减少磁盘空间的占用。</span><br><span class="line">    下一个stage，拉取数据的量，也就变少了。减少网络的数据传输的性能消耗。</span><br><span class="line">    在reduce端进行数据缓存的内存占用变少了。</span><br><span class="line">    reduce端，要进行聚合的数据量也变少了。</span><br><span class="line">reduceByKey在什么情况下使用:</span><br><span class="line">    非常普通的，比如说，就是要实现类似于wordcount程序一样的，对每个key对应的值，进行某种数据公式或者算法的计算（累加、类乘）。</span><br><span class="line">    对于一些类似于要对每个key进行一些字符串拼接的这种较为复杂的操作，可以自己衡量一下，其实有时，也是可以使用reduceByKey来实现的。</span><br><span class="line">    但是不太好实现。如果真能够实现出来，对性能绝对是有帮助的。（shuffle基本上就占了整个spark作业的90%以上的性能消耗，主要能对shuffle进行一定的调优，都是有价值的）</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="troubleshooting"><a href="#troubleshooting" class="headerlink" title="troubleshooting"></a>troubleshooting</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.控制shuffle reduce端缓冲大小以避免OOM</span><br><span class="line">问题描述:</span><br><span class="line">    map端的task是不断的输出数据的，数据量可能是很大的。</span><br><span class="line">    但是，其实reduce端的task，并不是等到map端task将属于自己的那份数据全部写入磁盘文件之后，再去拉取的。</span><br><span class="line">    map端写一点数据，reduce端task就会拉取一小部分数据，立即进行后面的聚合、算子函数的应用。</span><br><span class="line">    每次reduece能够拉取多少数据，就由buffer来决定。因为拉取过来的数据，都是先放在buffer中的。然后才用后面的executor分配的堆内存占比（0.2），hashmap，去进行后续的聚合、函数的执行。</span><br><span class="line">reduce端缓冲大小的另外一面，关于性能调优的一面:</span><br><span class="line">    假如Map端输出的数据量也不是特别大，然后你的整个application的资源也特别充足。200个executor、5个cpu core、10G内存。</span><br><span class="line">    其实可以尝试去增加这个reduce端缓冲大小的，比如从48M，变成96M。那么这样的话，每次reduce task能够拉取的数据量就很大。需要拉取的次数也就变少了。比如原先需要拉取100次，现在只要拉取50次就可以执行完了。</span><br><span class="line">    对网络传输性能开销的减少，以及reduce端聚合操作执行的次数的减少，都是有帮助的。</span><br><span class="line">    最终达到的效果，就应该是性能上的一定程度上的提升。</span><br><span class="line">    注意，一定要在资源充足的前提下做此操作。</span><br><span class="line">reduce端缓冲（buffer），可能会出现的问题及解决方式:</span><br><span class="line">    可能会出现，默认是48MB，也许大多数时候，reduce端task一边拉取一边计算，不一定一直都会拉满48M的数据。大多数时候，拉取个10M数据，就计算掉了。</span><br><span class="line">    大多数时候，也许不会出现什么问题。但是有的时候，map端的数据量特别大，然后写出的速度特别快。reduce端所有task，拉取的时候，全部达到自己的缓冲的最大极限值，缓冲区48M，全部填满。</span><br><span class="line">    这个时候，再加上你的reduce端执行的聚合函数的代码，可能会创建大量的对象。也许，一下子内存就撑不住了，就会OOM。reduce端的内存中，就会发生内存溢出的问题。</span><br><span class="line">针对上述的可能出现的问题，我们该怎么来解决:</span><br><span class="line">    这个时候，就应该减少reduce端task缓冲的大小。我宁愿多拉取几次，但是每次同时能够拉取到reduce端每个task的数量比较少，就不容易发生OOM内存溢出的问题。（比如，可以调节成12M）</span><br><span class="line">    在实际生产环境中，我们都是碰到过这种问题的。这是典型的以性能换执行的原理。reduce端缓冲小了，不容易OOM了，但是，性能一定是有所下降的，你要拉取的次数就多了。就走更多的网络传输开销。</span><br><span class="line">    这种时候，只能采取牺牲性能的方式了，spark作业，首先，第一要义，就是一定要让它可以跑起来。</span><br><span class="line">    new SparkConf().set(spark.reducer.maxSizeInFlight，”48”)</span><br><span class="line"></span><br><span class="line"># b.解决JVM GC导致的shuffle文件拉取失败</span><br><span class="line">问题描述:</span><br><span class="line">    有时会出现一种情况，在spark的作业中，log日志会提示shuffle file not found。（spark作业中，非常常见的）而且有的时候，它是偶尔才会出现的一种情况。有的时候，出现这种情况以后，重新去提交task。重新执行一遍，发现就好了。没有这种错误了。</span><br><span class="line">    log怎么看？用client模式去提交你的spark作业。比如standalone client或yarn client。一提交作业，直接可以在本地看到更新的log。</span><br><span class="line">问题原因：</span><br><span class="line">    比如，executor的JVM进程可能内存不够用了。那么此时就会执行GC。minor GC or full GC。此时就会导致executor内，所有的工作线程全部停止。比如BlockManager，基于netty的网络通信。</span><br><span class="line">    下一个stage的executor，可能还没有停止掉的task想要去上一个stage的task所在的exeuctor去拉取属于自己的数据，结果由于对方正在gc，就导致拉取了半天没有拉取到。</span><br><span class="line">    就很可能会报出shuffle file not found。但是，可能下一个stage又重新提交了task以后，再执行就没有问题了，因为可能第二次就没有碰到JVM在gc了。</span><br><span class="line">解决方案:</span><br><span class="line">    spark.shuffle.io.maxRetries 3</span><br><span class="line">    第一个参数，意思就是说，shuffle文件拉取的时候，如果没有拉取到（拉取失败），最多或重试几次（会重新拉取几次文件），默认是3次。</span><br><span class="line">    spark.shuffle.io.retryWait 5s</span><br><span class="line">    第二个参数，意思就是说，每一次重试拉取文件的时间间隔，默认是5s钟。</span><br><span class="line">    默认情况下，假如说第一个stage的executor正在进行漫长的full gc。第二个stage的executor尝试去拉取文件，结果没有拉取到，默认情况下，会反复重试拉取3次，每次间隔是五秒钟。最多只会等待3 * 5s &#x3D; 15s。如果15s内，没有拉取到shuffle file。就会报出shuffle file not found。</span><br><span class="line">    针对这种情况，我们完全可以进行预备性的参数调节。增大上述两个参数的值，达到比较大的一个值，尽量保证第二个stage的task，一定能够拉取到上一个stage的输出文件。避免报shuffle file not found。然后可能会重新提交stage和task去执行。那样反而对性能也不好。</span><br><span class="line">    spark.shuffle.io.maxRetries 60</span><br><span class="line">    spark.shuffle.io.retryWait 60s</span><br><span class="line">    最多可以忍受1个小时没有拉取到shuffle file。只是去设置一个最大的可能的值。full gc不可能1个小时都没结束吧。</span><br><span class="line">    这样呢，就可以尽量避免因为gc导致的shuffle file not found，无法拉取到的问题。</span><br><span class="line"></span><br><span class="line"># c.YARN队列资源不足导致的application直接失败</span><br><span class="line">问题描述:</span><br><span class="line">    如果说，你是基于yarn来提交spark。比如yarn-cluster或者yarn-client。你可以指定提交到某个hadoop队列上的。每个队列都是可以有自己的资源的。</span><br><span class="line">    跟大家说一个生产环境中的，给spark用的yarn资源队列的情况：500G内存，200个cpu core。</span><br><span class="line">    比如说，某个spark application，在spark-submit里面你自己配了，executor，80个。每个executor，4G内存。每个executor，2个cpu core。你的spark作业每次运行，大概要消耗掉320G内存，以及160个cpu core。</span><br><span class="line">    乍看起来，咱们的队列资源，是足够的，500G内存，280个cpu core。</span><br><span class="line">    首先，第一点，你的spark作业实际运行起来以后，耗费掉的资源量，可能是比你在spark-submit里面配置的，以及你预期的，是要大一些的。400G内存，190个cpu core。</span><br><span class="line">    那么这个时候，的确，咱们的队列资源还是有一些剩余的。但问题是如果你同时又提交了一个spark作业上去，一模一样的。那就可能会出问题。</span><br><span class="line">    第二个spark作业，又要申请320G内存+160个cpu core。结果，发现队列资源不足。</span><br><span class="line">    此时，可能会出现两种情况：（备注，具体出现哪种情况，跟你的YARN、Hadoop的版本，你们公司的一些运维参数，以及配置、硬件、资源肯能都有关系）</span><br><span class="line">        YARN，发现资源不足时，你的spark作业，并没有hang在那里，等待资源的分配，而是直接打印一行fail的log，直接就fail掉了。</span><br><span class="line">        YARN，发现资源不足，你的spark作业，就hang在那里。一直等待之前的spark作业执行完，等待有资源分配给自己来执行。</span><br><span class="line">解决方案:</span><br><span class="line">    在你的J2EE（我们这个项目里面，spark作业的运行， J2EE平台触发的，执行spark-submit脚本的平台）进行限制，同时只能提交一个spark作业到yarn上去执行，确保一个spark作业的资源肯定是有的。</span><br><span class="line">    你应该采用一些简单的调度区分的方式，比如说，有的spark作业可能是要长时间运行的，比如运行30分钟。有的spark作业，可能是短时间运行的，可能就运行2分钟。此时，都提交到一个队列上去，肯定不合适。很可能出现30分钟的作业卡住后面一大堆2分钟的作业。分队列，可以申请（跟你们的YARN、Hadoop运维的同事申请）。你自己给自己搞两个调度队列。每个队列的根据你要执行的作业的情况来设置。在你的J2EE程序里面，要判断，如果是长时间运行的作业，就干脆都提交到某一个固定的队列里面去把。如果是短时间运行的作业，就统一提交到另外一个队列里面去。这样，避免了长时间运行的作业，阻塞了短时间运行的作业。</span><br><span class="line">    你的队列里面，无论何时，只会有一个作业在里面运行。那么此时，就应该用我们之前讲过的性能调优的手段，去将每个队列能承载的最大的资源，分配给你的每一个spark作业，比如80个executor，6G的内存，3个cpu core。尽量让你的spark作业每一次运行，都达到最满的资源使用率，最快的速度，最好的性能。并行度，240个cpu core，720个task。</span><br><span class="line">    在J2EE中，通过线程池的方式（一个线程池对应一个资源队列），来实现上述我们说的方案。</span><br><span class="line">    </span><br><span class="line"># d.解决各种序列化导致的报错</span><br><span class="line">问题描述:</span><br><span class="line">    用client模式去提交spark作业，观察本地打印出来的log。如果出现了类似于Serializable、Serialize等等字眼报错的log，那么恭喜大家，就碰到了序列化问题导致的报错。</span><br><span class="line">序列化报错及解决方法:</span><br><span class="line">    你的算子函数里面，如果使用到了外部的自定义类型的变量，那么此时，就要求你的自定义类型，必须是可序列化的。</span><br><span class="line">        final Teacher teacher &#x3D; new Teacher(&quot;leo&quot;);</span><br><span class="line">        studentsRDD.foreach(new VoidFunction() &#123;</span><br><span class="line">            public void call(Row row) throws Exception &#123;</span><br><span class="line">               String teacherName &#x3D; teacher.getName();</span><br><span class="line">               .... </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        public class Teacher implements Serializable &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    如果要将自定义的类型，作为RDD的元素类型，那么自定义的类型也必须是可以序列化的。</span><br><span class="line">        JavaPairRDD&lt;Integer, Teacher&gt; teacherRDD</span><br><span class="line">        JavaPairRDD&lt;Integer, Student&gt; studentRDD</span><br><span class="line">        studentRDD.join(teacherRDD)</span><br><span class="line">        public class Teacher implements Serializable &#123;</span><br><span class="line">        &#125;</span><br><span class="line">        public class Student implements Serializable &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    不能在上述两种情况下，去使用一些第三方的，不支持序列化的类型。</span><br><span class="line">        Connection conn &#x3D;</span><br><span class="line">        studentsRDD.foreach(new VoidFunction() &#123;</span><br><span class="line">            public void call(Row row) throws Exception &#123;</span><br><span class="line">                            conn.....</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    Connection是不支持序列化的</span><br><span class="line"></span><br><span class="line"># e.解决算子函数返回NULL导致的问题</span><br><span class="line">在有些算子函数里面，是需要我们有一个返回值的。但是，有时候不需要返回值。我们如果直接返回NULL的话，是会报错的。</span><br><span class="line">    Scala.Math(NULL)，异常</span><br><span class="line">解决方案:</span><br><span class="line">    在返回的时候，返回一些特殊的值，不要返回null，比如&quot;-999&quot;</span><br><span class="line">    在通过算子获取到了一个RDD之后，可以对这个RDD执行filter操作，进行数据过滤。filter内，可以对数据进行判定，如果是-999，那么就返回false，给过滤掉就可以了。</span><br><span class="line">    大家不要忘了，之前咱们讲过的那个算子调优里面的coalesce算子，在filter之后，可以使用coalesce算子压缩一下RDD的partition的数量，让各个partition的数据比较紧凑一些。也能提升一些性能。</span><br><span class="line"></span><br><span class="line"># f.解决yarn-client模式导致的网卡流量激增问题</span><br><span class="line">Spark-On-Yarn任务执行流程:</span><br><span class="line">    Driver到底是什么?我们写的spark程序，打成jar包，用spark-submit来提交。jar包中的一个main类，通过jvm的命令启动起来。</span><br><span class="line">    JVM进程，其实就是Driver进程。Driver进程启动起来以后，执行我们自己写的main函数，从new SparkContext()开始。</span><br><span class="line">    driver接收到属于自己的executor进程的注册之后，就可以去进行我们写的spark作业代码的执行了。此时会一行一行的去执行咱们写的那些spark代码。</span><br><span class="line">    执行到某个action操作的时候，就会触发一个job，然后DAGScheduler会将job划分为一个一个的stage，为每个stage都创建指定数量的task。</span><br><span class="line">    TaskScheduler将每个stage的task分配到各个executor上面去执行。task就会执行咱们写的算子函数。</span><br><span class="line">    spark在yarn-client模式下，application的注册（executor的申请）和计算task的调度，是分离开来的。</span><br><span class="line">    standalone模式下，这两个操作都是driver负责的。ApplicationMaster(ExecutorLauncher)负责executor的申请，driver负责job和stage的划分，以及task的创建、分配和调度。</span><br><span class="line">    每种计算框架（mr、spark），如果想要在yarn上执行自己的计算应用，那么就必须自己实现和提供一个ApplicationMaster。相当于是实现了yarn提供的接口，spark自己开发的一个类。</span><br><span class="line">yarn-client模式下，会产生什么样的问题呢:</span><br><span class="line">    由于driver是启动在本地机器的，而且driver是全权负责所有的任务的调度的，也就是说要跟yarn集群上运行的多个executor进行频繁的通信（中间有task的启动消息、task的执行统计消息、task的运行状态、shuffle的输出结果）。</span><br><span class="line">    想象一下，比如你的executor有100个，stage有10个，task有1000个。每个stage运行的时候，都有1000个task提交到executor上面去运行，平均每个executor有10个task。接下来问题来了，driver要频繁地跟executor上运行的1000个task进行通信。通信消息特别多，通信的频率特别高。运行完一个stage，接着运行下一个stage，又是频繁的通信。</span><br><span class="line">    在整个spark运行的生命周期内，都会频繁的去进行通信和调度。所有这一切通信和调度都是从你的本地机器上发出去的，和接收到的。这是最要命的地方。你的本地机器，很可能在30分钟内（spark作业运行的周期内），进行频繁大量的网络通信。那么此时，你的本地机器的网络通信负载是非常非常高的。会导致你的本地机器的网卡流量会激增！</span><br><span class="line">    你的本地机器的网卡流量激增，当然不是一件好事了。因为在一些大的公司里面，对每台机器的使用情况，都是有监控的。不会允许单个机器出现耗费大量网络带宽等等这种资源的情况。</span><br><span class="line">解决方案:</span><br><span class="line">    实际上解决的方法很简单，就是心里要清楚，yarn-client模式是什么情况下，可以使用的？yarn-client模式，通常咱们就只会使用在测试环境中，你写好了某个spark作业，打了一个jar包，在某台测试机器上，用yarn-client模式去提交一下。</span><br><span class="line">    因为测试的行为是偶尔为之的，不会长时间连续提交大量的spark作业去测试。还有一点好处，yarn-client模式提交，可以在本地机器观察到详细全面的log。通过查看log，可以去解决线上报错的故障（troubleshooting）、对性能进行观察并进行性能调优。</span><br><span class="line">    实际上线了以后，在生产环境中，都得用yarn-cluster模式，去提交你的spark作业。</span><br><span class="line">    yarn-cluster模式，就跟你的本地机器引起的网卡流量激增的问题，就没有关系了。也就是说，就算有问题，也应该是yarn运维团队和基础运维团队之间的事情了。</span><br><span class="line">    使用了yarn-cluster模式以后，就不是你的本地机器运行Driver，进行task调度了。是yarn集群中，某个节点会运行driver进程，负责task调度。</span><br><span class="line"></span><br><span class="line"># g.解决yarn-cluster模式的JVM栈内存溢出问题</span><br><span class="line">问题描述:</span><br><span class="line">    有的时候，运行一些包含了spark sql的spark作业，可能会碰到yarn-client模式下，可以正常提交运行。yarn-cluster模式下，可能无法提交运行的，会报出JVM的PermGen（永久代）的内存溢出，OOM。</span><br><span class="line">    yarn-client模式下，driver是运行在本地机器上的，spark使用的JVM的PermGen的配置，是本地的spark-class文件（spark客户端是默认有配置的），JVM的永久代的大小是128M，这个是没有问题的。但是在yarn-cluster模式下，driver是运行在yarn集群的某个节点上的，使用的是没有经过配置的默认设置（PermGen永久代大小），82M。</span><br><span class="line">    spark-sql，它的内部是要进行很复杂的SQL的语义解析、语法树的转换等等，特别复杂。在这种复杂的情况下，如果说你的sql本身特别复杂的话，很可能会比较导致性能的消耗，内存的消耗。可能对PermGen永久代的占用会比较大。</span><br><span class="line">    所以，此时，如果对永久代的占用需求，超过了82M的话，但是呢又在128M以内，就会出现如上所述的问题，yarn-client模式下，默认是128M，这个还能运行，如果在yarn-cluster模式下，默认是82M，就有问题了。会报出PermGen Out of Memory error log。</span><br><span class="line">解决方案:</span><br><span class="line">    既然是JVM的PermGen永久代内存溢出，那么就是内存不够用。我们就给yarn-cluster模式下的driver的PermGen多设置一些。</span><br><span class="line">    spark-submit脚本中，加入以下配置即可：</span><br><span class="line">    --conf spark.driver.extraJavaOptions&#x3D;&quot;-XX:PermSize&#x3D;128M -XX:MaxPermSize&#x3D;256M&quot;</span><br><span class="line">    这个就设置了driver永久代的大小，默认是128M，最大是256M。这样的话，就可以基本保证你的spark作业不会出现上述的yarn-cluster模式导致的永久代内存溢出的问题。</span><br><span class="line">    spark sql中，写sql，要注意一个问题：</span><br><span class="line">    如果sql有大量的or语句。比如where keywords&#x3D;&#39;&#39; or keywords&#x3D;&#39;&#39; or keywords&#x3D;&#39;&#39;</span><br><span class="line">    当达到or语句，有成百上千的时候，此时可能就会出现一个driver端的jvm stack overflow，JVM栈内存溢出的问题。</span><br><span class="line">    JVM栈内存溢出，基本上就是由于调用的方法层级过多，因为产生了大量的，非常深的，超出了JVM栈深度限制的递归方法。我们的猜测，spark sql有大量or语句的时候，spark sql内部源码中，在解析sql，比如转换成语法树，或者进行执行计划的生成的时候，对or的处理是递归。or特别多的话，就会发生大量的递归。</span><br><span class="line">    JVM Stack Memory Overflow，栈内存溢出。这种时候，建议不要搞那么复杂的spark sql语句。采用替代方案：将一条sql语句，拆解成多条sql语句来执行。</span><br><span class="line">    每条sql语句，就只有100个or子句以内。一条一条SQL语句来执行。根据生产环境经验的测试，一条sql语句，100个or子句以内，是还可以的。通常情况下，不会报那个栈内存溢出。</span><br><span class="line"></span><br><span class="line"># h.错误的持久化方式以及checkpoint的使用</span><br><span class="line">错误的持久化使用方式：</span><br><span class="line">    usersRDD，想要对这个RDD做一个cache，希望能够在后面多次使用这个RDD的时候，不用反复重新计算RDD。可以直接使用通过各个节点上的executor的BlockManager管理的内存 &#x2F; 磁盘上的数据，避免重新反复计算RDD。</span><br><span class="line">    usersRDD.cache()</span><br><span class="line">    usersRDD.count()</span><br><span class="line">    usersRDD.take()</span><br><span class="line">    上面这种方式，不要说会不会生效了，实际上是会报错的。会报什么错误呢？会报一大堆file not found的错误。</span><br><span class="line">正确的持久化使用方式：</span><br><span class="line">    usersRDD</span><br><span class="line">    usersRDD &#x3D; usersRDD.cache() &#x2F;&#x2F; Java</span><br><span class="line">    val cachedUsersRDD &#x3D; usersRDD.cache() &#x2F;&#x2F; Scala</span><br><span class="line">    之后再去使用usersRDD，或者cachedUsersRDD就可以了。</span><br><span class="line">checkpoint的使用:</span><br><span class="line">    对于持久化，大多数时候都是会正常工作的。但有些时候会出现意外。比如说，缓存在内存中的数据，可能莫名其妙就丢失掉了。或者说，存储在磁盘文件中的数据，莫名其妙就没了，文件被误删了。</span><br><span class="line">    出现上述情况的时候，如果要对这个RDD执行某些操作，可能会发现RDD的某个partition找不到了。接下来的task就会对消失的partition重新计算，计算完以后再缓存和使用。</span><br><span class="line">    有些时候，计算某个RDD，可能是极其耗时的。可能RDD之前有大量的父RDD。那么如果你要重新计算一个partition，可能要重新计算之前所有的父RDD对应的partition。</span><br><span class="line">    这种情况下，就可以选择对这个RDD进行checkpoint，以防万一。进行checkpoint，就是说，会将RDD的数据，持久化一份到容错的文件系统上（比如hdfs）。</span><br><span class="line">    在对这个RDD进行计算的时候，如果发现它的缓存数据不见了。优先就是先找一下有没有checkpoint数据（到hdfs上面去找）。如果有的话，就使用checkpoint数据了。不至于去重新计算。</span><br><span class="line">    checkpoint，其实就是可以作为是cache的一个备胎。如果cache失效了，checkpoint就可以上来使用了。</span><br><span class="line">    checkpoint有利有弊，利在于，提高了spark作业的可靠性，一旦发生问题，还是很可靠的，不用重新计算大量的rdd。但是弊在于，进行checkpoint操作的时候，也就是将rdd数据写入hdfs中的时候，还是会消耗性能的。checkpoint，用性能换可靠性。</span><br><span class="line">checkpoint原理：</span><br><span class="line">    在代码中，用SparkContext，设置一个checkpoint目录，可以是一个容错文件系统的目录，比如hdfs。</span><br><span class="line">    在代码中，对需要进行checkpoint的rdd，执行RDD.checkpoint()。</span><br><span class="line">    RDDCheckpointData（spark内部的API），接管你的RDD，会标记为marked for checkpoint，准备进行checkpoint。</span><br><span class="line">    你的job运行完之后，会调用一个finalRDD.doCheckpoint()方法，会顺着rdd lineage，回溯扫描，发现有标记为待checkpoint的rdd，就会进行二次标记，inProgressCheckpoint，正在接受checkpoint操作。</span><br><span class="line">    job执行完之后，就会启动一个内部的新job，去将标记为inProgressCheckpoint的rdd的数据，都写入hdfs文件中。（备注，如果rdd之前cache过，会直接从缓存中获取数据，写入hdfs中。如果没有cache过，那么就会重新计算一遍这个rdd，再checkpoint）。</span><br><span class="line">    将checkpoint过的rdd之前的依赖rdd，改成一个CheckpointRDD*，强制改变你的rdd的lineage。后面如果rdd的cache数据获取失败，直接会通过它的上游CheckpointRDD，去容错的文件系统，比如hdfs中，获取checkpoint的数据。</span><br><span class="line">checkpoint的使用：</span><br><span class="line">    sc.checkpointFile(&quot;hdfs:&#x2F;&#x2F;&quot;)，设置checkpoint目录</span><br><span class="line">    对RDD执行checkpoint操作</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="数据倾斜解决方案"><a href="#数据倾斜解决方案" class="headerlink" title="数据倾斜解决方案"></a>数据倾斜解决方案</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">数据倾斜的解决，跟之前讲解的性能调优，有一点异曲同工之妙。</span><br><span class="line">性能调优中最有效最直接最简单的方式就是加资源加并行度，并注意RDD架构（复用同一个RDD，加上cache缓存）。相对于前面，shuffle、jvm等是次要的。</span><br><span class="line"># a.原理以及现象分析</span><br><span class="line">数据倾斜怎么出现的:</span><br><span class="line">    在执行shuffle操作的时候，是按照key，来进行values的数据的输出、拉取和聚合的。同一个key的values，一定是分配到一个reduce task进行处理的。</span><br><span class="line">    多个key对应的values，比如一共是90万。可能某个key对应了88万数据，被分配到一个task上去面去执行。</span><br><span class="line">    另外两个task，可能各分配到了1万数据，可能是数百个key，对应的1万条数据。这样就会出现数据倾斜问题。</span><br><span class="line">    想象一下，出现数据倾斜以后的运行的情况。很糟糕！其中两个task，各分配到了1万数据，可能同时在10分钟内都运行完了。另外一个task有88万条，88 * 10 &#x3D;  880分钟 &#x3D; 14.5个小时。</span><br><span class="line">    大家看，本来另外两个task很快就运行完毕了（10分钟），但是由于一个拖后腿的家伙，第三个task，要14.5个小时才能运行完，就导致整个spark作业，也得14.5个小时才能运行完。</span><br><span class="line">发生数据倾斜以后的现象:</span><br><span class="line">    你的大部分的task，都执行的特别特别快，（你要用client模式，standalone client，yarn client，本地机器一执行spark-submit脚本，就会开始打印log），task175 finished，剩下几个task，执行的特别特别慢，前面的task，一般1s可以执行完5个，最后发现1000个task，998，999 task，要执行1个小时，2个小时才能执行完一个task。</span><br><span class="line">    运行的时候，其他task都执行完了，也没什么特别的问题，但是有的task，就是会突然间报了一个OOM，JVM Out Of Memory，内存溢出了，task failed，task lost，resubmitting task。反复执行几次都到了某个task就是跑不通，最后就挂掉。</span><br><span class="line">    某个task就直接OOM，那么基本上也是因为数据倾斜了，task分配的数量实在是太大了！所以内存放不下，然后你的task每处理一条数据，还要创建大量的对象，内存爆掉了。</span><br><span class="line">定位数据倾斜出现的原因与出现问题的位置:</span><br><span class="line">    出现数据倾斜的原因，基本只可能是因为发生了shuffle操作，在shuffle的过程中，出现了数据倾斜的问题。因为某个或者某些key对应的数据，远远的高于其他的key。</span><br><span class="line">    你在自己的程序里面找找，哪些地方用了会产生shuffle的算子，groupByKey、countByKey、reduceByKey、join</span><br><span class="line">    log一般会报是在你的哪一行代码，导致了OOM异常。或者看log，看看是执行到了第几个stage。spark代码，是怎么划分成一个一个的stage的。哪一个stage生成的task特别慢，就能够自己用肉眼去对你的spark代码进行stage的划分，就能够通过stage定位到你的代码，到底哪里发生了数据倾斜。</span><br><span class="line">    </span><br><span class="line"># b.聚合源数据以及过滤导致倾斜的key</span><br><span class="line">聚合源数据:</span><br><span class="line">    一些聚合的操作，比如groupByKey、reduceByKey，groupByKey说白了就是拿到每个key对应的values。reduceByKey说白了就是对每个key对应的values执行一定的计算。</span><br><span class="line">    这些操作，比如groupByKey和reduceByKey，包括之前说的join。都是在spark作业中执行的。spark作业的数据来源，通常是哪里呢？90%的情况下，数据来源都是hive表（hdfs，大数据分布式存储系统）。</span><br><span class="line">    hdfs上存储的大数据。hive表中的数据通常是怎么出来的呢？有了spark以后，hive比较适合做什么事情？hive就是适合做离线的，晚上凌晨跑的，ETL（extract transform load，数据的采集、清洗、导入），hive sql，去做这些事情，从而去形成一个完整的hive中的数据仓库。说白了，数据仓库，就是一堆表。</span><br><span class="line">    spark作业的源表，hive表，通常情况下来说，也是通过某些hive etl生成的。hive etl可能是晚上凌晨在那儿跑。今天跑昨天的数据。</span><br><span class="line">    数据倾斜，某个key对应的80万数据，某些key对应几百条，某些key对应几十条。现在咱们直接在生成hive表的hive etl中对数据进行聚合。比如按key来分组，将key对应的所有的values全部用一种特殊的格式拼接到一个字符串里面去，比如“key&#x3D;sessionid, value: action_seq&#x3D;1|user_id&#x3D;1|search_keyword&#x3D;火锅|category_id&#x3D;001;action_seq&#x3D;2|user_id&#x3D;1|search_keyword&#x3D;涮肉|category_id&#x3D;001”。</span><br><span class="line">    对key进行group，在spark中，拿到key&#x3D;sessionid，values&lt;Iterable&gt;。hive etl中，直接对key进行了聚合。那么也就意味着，每个key就只对应一条数据。在spark中，就不需要再去执行groupByKey+map这种操作了。直接对每个key对应的values字符串进行map操作，进行你需要的操作即可。</span><br><span class="line">    spark中，可能对这个操作，就不需要执行shffule操作了，也就根本不可能导致数据倾斜。</span><br><span class="line">    或者是对每个key在hive etl中进行聚合，对所有values聚合一下，不一定是拼接起来，可能是直接进行计算。reduceByKey计算函数应用在hive etl中，从而得到每个key的values。</span><br><span class="line">    你可能没有办法对每个key聚合出来一条数据。那么也可以做一个妥协，对每个key对应的数据，10万条。有好几个粒度，比如10万条里面包含了几个城市、几天、几个地区的数据，现在放粗粒度。直接就按照城市粒度，做一下聚合，几个城市，几天、几个地区粒度的数据，都给聚合起来。</span><br><span class="line">    比如说city_id date area_id</span><br><span class="line">        select ... from ... group by city_id</span><br><span class="line">    尽量去聚合，减少每个key对应的数量，也许聚合到比较粗的粒度之后，原先有10万数据量的key，现在只有1万数据量。减轻数据倾斜的现象和问题。</span><br><span class="line">过滤导致倾斜的key:</span><br><span class="line">    如果你能够接受某些数据在spark作业中直接就摒弃掉不使用。比如说，总共有100万个key。只有2个key是数据量达到10万的。其他所有的key，对应的数量都是几十万。</span><br><span class="line">    这个时候，你自己可以去取舍，如果业务和需求可以理解和接受的话，在你从hive表查询源数据的时候，直接在sql中用where条件，过滤掉某几个key。</span><br><span class="line">    那么这几个原先有大量数据，会导致数据倾斜的key，被过滤掉之后，那么在你的spark作业中，自然就不会发生数据倾斜了。</span><br><span class="line"></span><br><span class="line"># c.提高shuffle操作reduce并行度</span><br><span class="line">问题描述:</span><br><span class="line">    将reduce task的数量变多，就可以让每个reduce task分配到更少的数据量。这样的话也许就可以缓解甚至是基本解决掉数据倾斜的问题。</span><br><span class="line">提升shuffle reduce端并行度的操作方法:</span><br><span class="line">    很简单，主要给我们所有的shuffle算子，比如groupByKey、countByKey、reduceByKey。在调用的时候，传入进去一个参数。那个数字，就代表了那个shuffle操作的reduce端的并行度。那么在进行shuffle操作的时候，就会对应着创建指定数量的reduce task。</span><br><span class="line">这样的话，就可以让每个reduce task分配到更少的数据。基本可以缓解数据倾斜的问题。</span><br><span class="line">    比如说，原本某个task分配数据特别多，直接OOM，内存溢出了，程序没法运行，直接挂掉。按照log，找到发生数据倾斜的shuffle操作，给它传入一个并行度数字，这样的话，原先那个task分配到的数据，肯定会变少。就至少可以避免OOM的情况，程序至少是可以跑的。</span><br><span class="line">提升shuffle reduce并行度的缺陷:</span><br><span class="line">    治标不治本的意思，因为它没有从根本上改变数据倾斜的本质和问题。不像第一个和第二个方案（直接避免了数据倾斜的发生）。原理没有改变，只是说，尽可能地去缓解和减轻shuffle reduce task的数据压力，以及数据倾斜的问题。</span><br><span class="line">实际生产环境中的经验：</span><br><span class="line">    如果最理想的情况下，提升并行度以后，减轻了数据倾斜的问题，或者甚至可以让数据倾斜的现象忽略不计，那么就最好。就不用做其他的数据倾斜解决方案了。</span><br><span class="line">    不太理想的情况下，比如之前某个task运行特别慢，要5个小时，现在稍微快了一点，变成了4个小时。或者是原先运行到某个task，直接OOM，现在至少不会OOM了，但是那个task运行特别慢，要5个小时才能跑完。</span><br><span class="line">    </span><br><span class="line"># d.使用随机key实现双重聚合</span><br><span class="line">使用场景:</span><br><span class="line">    groupByKey、reduceByKey比较适合使用这种方式。join咱们通常不会这样来做，后面会讲三种针对不同的join造成的数据倾斜的问题的解决方案。</span><br><span class="line">解决方案:</span><br><span class="line">    第一轮聚合的时候，对key进行打散，将原先一样的key，变成不一样的key，相当于是将每个key分为多组。</span><br><span class="line">先针对多个组，进行key的局部聚合。接着，再去除掉每个key的前缀，然后对所有的key进行全局的聚合。</span><br><span class="line">    对groupByKey、reduceByKey造成的数据倾斜，有比较好的效果。如果说，之前的方案，都没法解决数据倾斜的问题，那么就只能依靠这一种方式了。</span><br><span class="line"></span><br><span class="line"># e.将reduce join转换为map join</span><br><span class="line">使用方式:</span><br><span class="line">    普通的join，那么肯定是要走shuffle。既然是走shuffle，那么普通的join就肯定是走的是reduce join。那怎么将reduce join 转换为mapjoin呢？先将所有相同的key，对应的value汇聚到一个task中，然后再进行join。</span><br><span class="line">使用场景:</span><br><span class="line">    如果两个RDD要进行join，其中一个RDD是比较小的。比如一个RDD是100万数据，一个RDD是1万数据。（一个RDD是1亿数据，一个RDD是100万数据）。</span><br><span class="line">    其中一个RDD必须是比较小的，broadcast出去那个小RDD的数据以后，就会在每个executor的block manager中都保存一份。要确保你的内存足够存放那个小RDD中的数据。</span><br><span class="line">    这种方式下，根本不会发生shuffle操作，肯定也不会发生数据倾斜。从根本上杜绝了join操作可能导致的数据倾斜的问题。</span><br><span class="line">不适合的情况:</span><br><span class="line">    两个RDD都比较大，那么这个时候，你去将其中一个RDD做成broadcast，就很笨拙了。很可能导致内存不足。最终导致内存溢出，程序挂掉。</span><br><span class="line">    而且其中某些key（或者是某个key），还发生了数据倾斜。此时可以采用最后两种方式。</span><br><span class="line">    对于join这种操作，不光是考虑数据倾斜的问题。即使是没有数据倾斜问题，也完全可以优先考虑，用我们讲的这种高级的reduce join转map join的技术，不要用普通的join，去通过shuffle，进行数据的join。完全可以通过简单的map，使用map join的方式，牺牲一点内存资源。在可行的情况下，优先这么使用。</span><br><span class="line">    </span><br><span class="line"># f.sample采样倾斜key单独进行join</span><br><span class="line">实现思路:</span><br><span class="line">    将发生数据倾斜的key，单独拉出来，放到一个RDD中去。就用这个原本会倾斜的key RDD跟其他RDD单独去join一下，这个时候key对应的数据可能就会分散到多个task中去进行join操作。</span><br><span class="line">    就不至于说是，这个key跟之前其他的key混合在一个RDD中时，肯定是会导致一个key对应的所有数据都到一个task中去，就会导致数据倾斜。</span><br><span class="line">使用场景:</span><br><span class="line">    优先对于join，肯定是希望能够采用上一个方案，即reduce join转换map join。两个RDD数据都比较大，那么就不要那么搞了。</span><br><span class="line">    针对你的RDD的数据，你可以自己把它转换成一个中间表，或者是直接用countByKey()的方式，你可以看一下这个RDD各个key对应的数据量。此时如果你发现整个RDD就一个，或者少数几个key对应的数据量特别多。尽量建议，比如就是一个key对应的数据量特别多。</span><br><span class="line">    此时可以采用这种方案，单拉出来那个最多的key，单独进行join，尽可能地将key分散到各个task上去进行join操作。</span><br><span class="line">不适合的情况:</span><br><span class="line">    如果一个RDD中，导致数据倾斜的key特别多。那么此时，最好还是不要这样了。</span><br><span class="line">    </span><br><span class="line"># g.使用随机数以及扩容表进行join</span><br><span class="line">使用场景:</span><br><span class="line">    选择一个RDD，要用flatMap，进行扩容，将每条数据，映射为多条数据，每个映射出来的数据，都带了一个n以内的随机数，通常来说会选择10。</span><br><span class="line">    将另外一个RDD，做普通的map映射操作，每条数据都打上一个10以内的随机数。</span><br><span class="line">    最后将两个处理后的RDD进行join操作。</span><br><span class="line">局限性:</span><br><span class="line">    因为你的两个RDD都很大，所以你没有办法去将某一个RDD扩的特别大，一般咱们就是10倍。</span><br><span class="line">    如果就是10倍的话，那么数据倾斜问题的确是只能说是缓解和减轻，不能说彻底解决。</span><br><span class="line">    sample采样倾斜key并单独进行join,将key，从另外一个RDD中过滤出的数据，可能只有一条或者几条，此时，咱们可以任意进行扩容，扩成1000倍。将从第一个RDD中拆分出来的那个倾斜key RDD，打上1000以内的一个随机数。</span><br><span class="line">    这种情况下，还可以配合上，提升shuffle reduce并行度，join(rdd, 1000)。通常情况下，效果还是非常不错的。</span><br><span class="line">    打散成100份，甚至1000份，2000份，去进行join，那么就肯定没有数据倾斜的问题了吧。</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="实时计算程序性能调优"><a href="#实时计算程序性能调优" class="headerlink" title="实时计算程序性能调优"></a>实时计算程序性能调优</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># a.并行化数据接收：处理多个topic的数据时比较有效</span><br><span class="line">    int numStreams &#x3D; 5;</span><br><span class="line">    List&lt;JavaPairDStream&lt;String, String&gt;&gt; kafkaStreams &#x3D; new ArrayList&lt;JavaPairDStream&lt;String, String&gt;&gt;(numStreams);</span><br><span class="line">    for (int i &#x3D; 0; i &lt; numStreams; i++) &#123;</span><br><span class="line">        kafkaStreams.add(KafkaUtils.createStream(...));</span><br><span class="line">    &#125;</span><br><span class="line">    JavaPairDStream&lt;String, String&gt; unifiedStream &#x3D; streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));</span><br><span class="line">    unifiedStream.print();</span><br><span class="line"></span><br><span class="line"># b.spark.streaming.blockInterval</span><br><span class="line">    增加block数量，增加每个batch rdd的partition数量，增加处理并行度,receiver从数据源源源不断地获取到数据；首先是会按照block interval，将指定时间间隔的数据，收集为一个block；默认时间是200ms，官方推荐不要小于50ms；接着呢，会将指定batch interval时间间隔内的block，合并为一个batch；创建为一个rdd，然后启动一个job，去处理这个batch rdd中的数据</span><br><span class="line">    batch rdd，它的partition数量是多少呢？一个batch有多少个block，就有多少个partition；就意味着并行度是多少；就意味着每个batch rdd有多少个task会并行计算和处理。</span><br><span class="line">    当然是希望可以比默认的task数量和并行度再多一些了；可以手动调节block interval；减少block interval；每个batch可以包含更多的block；有更多的partition；也就有更多的task并行处理每个batch rdd。定死了，初始的rdd过来，直接就是固定的partition数量了</span><br><span class="line"></span><br><span class="line"># c.inputStream.repartition(&lt;number of partitions&gt;)</span><br><span class="line">    重分区，增加每个batch rdd的partition数量,有些时候，希望对某些dstream中的rdd进行定制化的分区</span><br><span class="line">    对dstream中的rdd进行重分区，去重分区成指定数量的分区，这样也可以提高指定dstream的rdd的计算并行度</span><br><span class="line">    </span><br><span class="line"># d.调节并行度</span><br><span class="line">    spark.default.parallelism</span><br><span class="line">    reduceByKey(numPartitions)</span><br><span class="line"></span><br><span class="line"># e.使用Kryo序列化机制</span><br><span class="line">    spark streaming，也是有不少序列化的场景的,提高序列化task发送到executor上执行的性能，如果task很多的时候，task序列化和反序列化的性能开销也比较可观</span><br><span class="line">    默认输入数据的存储级别是StorageLevel.MEMORY_AND_DISK_SER_2，receiver接收到数据，默认就会进行持久化操作；</span><br><span class="line">    首先序列化数据，存储到内存中；如果内存资源不够大，那么就写入磁盘；而且，还会写一份冗余副本到其他executor的block manager中，进行数据冗余。</span><br><span class="line"></span><br><span class="line"># f.batch interval</span><br><span class="line">    每个的处理时间必须小于batch interval,实际上你的spark streaming跑起来以后，其实都是可以在spark ui上观察它的运行情况的；可以看到batch的处理时间；</span><br><span class="line">    如果发现batch的处理时间大于batch interval，就必须调节batch interval</span><br><span class="line">    尽量不要让batch处理时间大于batch interval,比如你的batch每隔5秒生成一次；你的batch处理时间要达到6秒；就会出现，batch在你的内存中日积月累，一直囤积着，没法及时计算掉，释放内存空间；而且对内存空间的占用越来越大，那么此时会导致内存空间快速消耗</span><br><span class="line">    如果发现batch处理时间比batch interval要大，就尽量将batch interval调节大一些</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
</search>
